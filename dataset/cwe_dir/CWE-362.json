[
    {
        "cve_id": "CVE-2018-20836",
        "func_name": "torvalds/linux/smp_task_done",
        "description": "An issue was discovered in the Linux kernel before 4.20. There is a race condition in smp_task_timedout() and smp_task_done() in drivers/scsi/libsas/sas_expander.c, leading to a use-after-free.",
        "git_url": "https://github.com/torvalds/linux/commit/b90cd6f2b905905fb42671009dc0e27c310a16ae",
        "commit_title": "scsi: libsas: fix a race condition when smp task timeout",
        "commit_text": " When the lldd is processing the complete sas task in interrupt and set the task stat as SAS_TASK_STATE_DONE, the smp timeout timer is able to be triggered at the same time. And smp_task_timedout() will complete the task wheter the SAS_TASK_STATE_DONE is set or not. Then the sas task may freed before lldd end the interrupt process. Thus a use-after-free will happen.  Fix this by calling the complete() only when SAS_TASK_STATE_DONE is not set. And remove the check of the return value of the del_timer(). Once the LLDD sets DONE, it must call task->done(), which will call smp_task_done()->complete() and the task will be completed and freed correctly. ",
        "func_before": "static void smp_task_done(struct sas_task *task)\n{\n\tif (!del_timer(&task->slow_task->timer))\n\t\treturn;\n\tcomplete(&task->slow_task->completion);\n}",
        "func": "static void smp_task_done(struct sas_task *task)\n{\n\tdel_timer(&task->slow_task->timer);\n\tcomplete(&task->slow_task->completion);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,5 @@\n static void smp_task_done(struct sas_task *task)\n {\n-\tif (!del_timer(&task->slow_task->timer))\n-\t\treturn;\n+\tdel_timer(&task->slow_task->timer);\n \tcomplete(&task->slow_task->completion);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!del_timer(&task->slow_task->timer))",
                "\t\treturn;"
            ],
            "added_lines": [
                "\tdel_timer(&task->slow_task->timer);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-20836",
        "func_name": "torvalds/linux/smp_task_timedout",
        "description": "An issue was discovered in the Linux kernel before 4.20. There is a race condition in smp_task_timedout() and smp_task_done() in drivers/scsi/libsas/sas_expander.c, leading to a use-after-free.",
        "git_url": "https://github.com/torvalds/linux/commit/b90cd6f2b905905fb42671009dc0e27c310a16ae",
        "commit_title": "scsi: libsas: fix a race condition when smp task timeout",
        "commit_text": " When the lldd is processing the complete sas task in interrupt and set the task stat as SAS_TASK_STATE_DONE, the smp timeout timer is able to be triggered at the same time. And smp_task_timedout() will complete the task wheter the SAS_TASK_STATE_DONE is set or not. Then the sas task may freed before lldd end the interrupt process. Thus a use-after-free will happen.  Fix this by calling the complete() only when SAS_TASK_STATE_DONE is not set. And remove the check of the return value of the del_timer(). Once the LLDD sets DONE, it must call task->done(), which will call smp_task_done()->complete() and the task will be completed and freed correctly. ",
        "func_before": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n\n\tcomplete(&task->slow_task->completion);\n}",
        "func": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\t\tcomplete(&task->slow_task->completion);\n\t}\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,9 +5,9 @@\n \tunsigned long flags;\n \n \tspin_lock_irqsave(&task->task_state_lock, flags);\n-\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n+\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n \t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n+\t\tcomplete(&task->slow_task->completion);\n+\t}\n \tspin_unlock_irqrestore(&task->task_state_lock, flags);\n-\n-\tcomplete(&task->slow_task->completion);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))",
                "",
                "\tcomplete(&task->slow_task->completion);"
            ],
            "added_lines": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {",
                "\t\tcomplete(&task->slow_task->completion);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11815",
        "func_name": "torvalds/linux/rds_tcp_kill_sock",
        "description": "An issue was discovered in rds_tcp_kill_sock in net/rds/tcp.c in the Linux kernel before 5.0.8. There is a race condition leading to a use-after-free, related to net namespace cleanup.",
        "git_url": "https://github.com/torvalds/linux/commit/cb66ddd156203daefb8d71158036b27b0e2caf63",
        "commit_title": "net: rds: force to destroy connection if t_sock is NULL in rds_tcp_kill_sock().",
        "commit_text": " When it is to cleanup net namespace, rds_tcp_exit_net() will call rds_tcp_kill_sock(), if t_sock is NULL, it will not call rds_conn_destroy(), rds_conn_path_destroy() and rds_tcp_conn_free() to free connection, and the worker cp_conn_w is not stopped, afterwards the net is freed in net_drop_ns(); While cp_conn_w rds_connect_worker() will call rds_tcp_conn_path_connect() and reference 'net' which has already been freed.  In rds_tcp_conn_path_connect(), rds_tcp_set_callbacks() will set t_sock = sock before sock->ops->connect, but if connect() is failed, it will call rds_tcp_restore_callbacks() and set t_sock = NULL, if connect is always failed, rds_connect_worker() will try to reconnect all the time, so rds_tcp_kill_sock() will never to cancel worker cp_conn_w and free the connections.  Therefore, the condition !tc->t_sock is not needed if it is going to do cleanup_net->rds_tcp_exit_net->rds_tcp_kill_sock, because tc->t_sock is always NULL, and there is on other path to cancel cp_conn_w and free connection. So this patch is to fix this.  rds_tcp_kill_sock(): ... if (net != c_net || !tc->t_sock) ...  ================================================================== BUG: KASAN: use-after-free in inet_create+0xbcc/0xd28 net/ipv4/af_inet.c:340 Read of size 4 at addr ffff8003496a4684 by task kworker/u8:4/3721  CPU: 3 PID: 3721 Comm: kworker/u8:4 Not tainted 5.1.0 #11 Hardware name: linux,dummy-virt (DT) Workqueue: krdsd rds_connect_worker Call trace:  dump_backtrace+0x0/0x3c0 arch/arm64/kernel/time.c:53  show_stack+0x28/0x38 arch/arm64/kernel/traps.c:152  __dump_stack lib/dump_stack.c:77 [inline]  dump_stack+0x120/0x188 lib/dump_stack.c:113  print_address_description+0x68/0x278 mm/kasan/report.c:253  kasan_report_error mm/kasan/report.c:351 [inline]  kasan_report+0x21c/0x348 mm/kasan/report.c:409  __asan_report_load4_noabort+0x30/0x40 mm/kasan/report.c:429  inet_create+0xbcc/0xd28 net/ipv4/af_inet.c:340  __sock_create+0x4f8/0x770 net/socket.c:1276  sock_create_kern+0x50/0x68 net/socket.c:1322  rds_tcp_conn_path_connect+0x2b4/0x690 net/rds/tcp_connect.c:114  rds_connect_worker+0x108/0x1d0 net/rds/threads.c:175  process_one_work+0x6e8/0x1700 kernel/workqueue.c:2153  worker_thread+0x3b0/0xdd0 kernel/workqueue.c:2296  kthread+0x2f0/0x378 kernel/kthread.c:255  ret_from_fork+0x10/0x18 arch/arm64/kernel/entry.S:1117  Allocated by task 687:  save_stack mm/kasan/kasan.c:448 [inline]  set_track mm/kasan/kasan.c:460 [inline]  kasan_kmalloc+0xd4/0x180 mm/kasan/kasan.c:553  kasan_slab_alloc+0x14/0x20 mm/kasan/kasan.c:490  slab_post_alloc_hook mm/slab.h:444 [inline]  slab_alloc_node mm/slub.c:2705 [inline]  slab_alloc mm/slub.c:2713 [inline]  kmem_cache_alloc+0x14c/0x388 mm/slub.c:2718  kmem_cache_zalloc include/linux/slab.h:697 [inline]  net_alloc net/core/net_namespace.c:384 [inline]  copy_net_ns+0xc4/0x2d0 net/core/net_namespace.c:424  create_new_namespaces+0x300/0x658 kernel/nsproxy.c:107  unshare_nsproxy_namespaces+0xa0/0x198 kernel/nsproxy.c:206  ksys_unshare+0x340/0x628 kernel/fork.c:2577  __do_sys_unshare kernel/fork.c:2645 [inline]  __se_sys_unshare kernel/fork.c:2643 [inline]  __arm64_sys_unshare+0x38/0x58 kernel/fork.c:2643  __invoke_syscall arch/arm64/kernel/syscall.c:35 [inline]  invoke_syscall arch/arm64/kernel/syscall.c:47 [inline]  el0_svc_common+0x168/0x390 arch/arm64/kernel/syscall.c:83  el0_svc_handler+0x60/0xd0 arch/arm64/kernel/syscall.c:129  el0_svc+0x8/0xc arch/arm64/kernel/entry.S:960  Freed by task 264:  save_stack mm/kasan/kasan.c:448 [inline]  set_track mm/kasan/kasan.c:460 [inline]  __kasan_slab_free+0x114/0x220 mm/kasan/kasan.c:521  kasan_slab_free+0x10/0x18 mm/kasan/kasan.c:528  slab_free_hook mm/slub.c:1370 [inline]  slab_free_freelist_hook mm/slub.c:1397 [inline]  slab_free mm/slub.c:2952 [inline]  kmem_cache_free+0xb8/0x3a8 mm/slub.c:2968  net_free net/core/net_namespace.c:400 [inline]  net_drop_ns.part.6+0x78/0x90 net/core/net_namespace.c:407  net_drop_ns net/core/net_namespace.c:406 [inline]  cleanup_net+0x53c/0x6d8 net/core/net_namespace.c:569  process_one_work+0x6e8/0x1700 kernel/workqueue.c:2153  worker_thread+0x3b0/0xdd0 kernel/workqueue.c:2296  kthread+0x2f0/0x378 kernel/kthread.c:255  ret_from_fork+0x10/0x18 arch/arm64/kernel/entry.S:1117  The buggy address belongs to the object at ffff8003496a3f80  which belongs to the cache net_namespace of size 7872 The buggy address is located 1796 bytes inside of  7872-byte region [ffff8003496a3f80, ffff8003496a5e40) The buggy address belongs to the page: page:ffff7e000d25a800 count:1 mapcount:0 mapping:ffff80036ce4b000 index:0x0 compound_mapcount: 0 flags: 0xffffe0000008100(slab|head) raw: 0ffffe0000008100 dead000000000100 dead000000000200 ffff80036ce4b000 raw: 0000000000000000 0000000080040004 00000001ffffffff 0000000000000000 page dumped because: kasan: bad access detected  Memory state around the buggy address:  ffff8003496a4580: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb  ffff8003496a4600: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb >ffff8003496a4680: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb                    ^  ffff8003496a4700: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb  ffff8003496a4780: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb ================================================================== ",
        "func_before": "static void rds_tcp_kill_sock(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\trtn->rds_tcp_listen_sock = NULL;\n\trds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net || !tc->t_sock)\n\t\t\tcontinue;\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t\t} else {\n\t\t\tlist_del(&tc->t_tcp_node);\n\t\t\ttc->t_tcp_node_detached = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}",
        "func": "static void rds_tcp_kill_sock(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\trtn->rds_tcp_listen_sock = NULL;\n\trds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net)\n\t\t\tcontinue;\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t\t} else {\n\t\t\tlist_del(&tc->t_tcp_node);\n\t\t\ttc->t_tcp_node_detached = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n \tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n \t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n \n-\t\tif (net != c_net || !tc->t_sock)\n+\t\tif (net != c_net)\n \t\t\tcontinue;\n \t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n \t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (net != c_net || !tc->t_sock)"
            ],
            "added_lines": [
                "\t\tif (net != c_net)"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-12448",
        "func_name": "GNOME/gvfs/do_query_info",
        "description": "An issue was discovered in GNOME gvfs 1.29.4 through 1.41.2. daemon/gvfsbackendadmin.c has race conditions because the admin backend doesn't implement query_info_on_read/write.",
        "git_url": "https://github.com/GNOME/gvfs/commit/764e9af7522e3096c0f44613c330377d31c9bbb5",
        "commit_title": "admin: Add query_info_on_read/write functionality",
        "commit_text": " Admin backend doesn't implement query_info_on_read/write which might potentially lead to some race conditions which aren't really wanted especially in case of admin backend. For example, in file_copy_fallback(), g_file_query_info() is used if g_file_input_stream_query_info() is not supported, which in theory means that the info might be obtained from the different file then it is opened. Let's add this missing functionality to prevent this possibility.",
        "func_before": "static void\ndo_query_info (GVfsBackend *backend,\n               GVfsJobQueryInfo *query_info_job,\n               const char *filename,\n               GFileQueryInfoFlags flags,\n               GFileInfo *info,\n               GFileAttributeMatcher *matcher)\n{\n  GVfsBackendAdmin *self = G_VFS_BACKEND_ADMIN (backend);\n  GVfsJob *job = G_VFS_JOB (query_info_job);\n  GError *error = NULL;\n  GFile *file;\n  GFileInfo *real_info;\n\n  if (!check_permission (self, job))\n    return;\n\n  file = g_file_new_for_path (filename);\n  real_info = g_file_query_info (file, query_info_job->attributes,\n                                 flags, job->cancellable, &error);\n  g_object_unref (file);\n\n  if (error != NULL)\n    goto out;\n\n  /* Override read/write flags, since the above call will use access()\n   * to determine permissions, which does not honor our privileged\n   * capabilities.\n   */\n  g_file_info_set_attribute_boolean (real_info,\n                                     G_FILE_ATTRIBUTE_ACCESS_CAN_READ, TRUE);\n  g_file_info_set_attribute_boolean (real_info,\n                                     G_FILE_ATTRIBUTE_ACCESS_CAN_WRITE, TRUE);\n  g_file_info_set_attribute_boolean (real_info,\n                                     G_FILE_ATTRIBUTE_ACCESS_CAN_DELETE, TRUE);\n  g_file_info_set_attribute_boolean (real_info,\n                                     G_FILE_ATTRIBUTE_ACCESS_CAN_RENAME, TRUE);\n\n  g_file_info_copy_into (real_info, info);\n  g_object_unref (real_info);\n\n out:\n  complete_job (job, error);\n}",
        "func": "static void\ndo_query_info (GVfsBackend *backend,\n               GVfsJobQueryInfo *query_info_job,\n               const char *filename,\n               GFileQueryInfoFlags flags,\n               GFileInfo *info,\n               GFileAttributeMatcher *matcher)\n{\n  GVfsBackendAdmin *self = G_VFS_BACKEND_ADMIN (backend);\n  GVfsJob *job = G_VFS_JOB (query_info_job);\n  GError *error = NULL;\n  GFile *file;\n  GFileInfo *real_info;\n\n  if (!check_permission (self, job))\n    return;\n\n  file = g_file_new_for_path (filename);\n  real_info = g_file_query_info (file, query_info_job->attributes,\n                                 flags, job->cancellable, &error);\n  g_object_unref (file);\n\n  if (error != NULL)\n    goto out;\n\n  fix_file_info (real_info);\n  g_file_info_copy_into (real_info, info);\n  g_object_unref (real_info);\n\n out:\n  complete_job (job, error);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -23,19 +23,7 @@\n   if (error != NULL)\n     goto out;\n \n-  /* Override read/write flags, since the above call will use access()\n-   * to determine permissions, which does not honor our privileged\n-   * capabilities.\n-   */\n-  g_file_info_set_attribute_boolean (real_info,\n-                                     G_FILE_ATTRIBUTE_ACCESS_CAN_READ, TRUE);\n-  g_file_info_set_attribute_boolean (real_info,\n-                                     G_FILE_ATTRIBUTE_ACCESS_CAN_WRITE, TRUE);\n-  g_file_info_set_attribute_boolean (real_info,\n-                                     G_FILE_ATTRIBUTE_ACCESS_CAN_DELETE, TRUE);\n-  g_file_info_set_attribute_boolean (real_info,\n-                                     G_FILE_ATTRIBUTE_ACCESS_CAN_RENAME, TRUE);\n-\n+  fix_file_info (real_info);\n   g_file_info_copy_into (real_info, info);\n   g_object_unref (real_info);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "  /* Override read/write flags, since the above call will use access()",
                "   * to determine permissions, which does not honor our privileged",
                "   * capabilities.",
                "   */",
                "  g_file_info_set_attribute_boolean (real_info,",
                "                                     G_FILE_ATTRIBUTE_ACCESS_CAN_READ, TRUE);",
                "  g_file_info_set_attribute_boolean (real_info,",
                "                                     G_FILE_ATTRIBUTE_ACCESS_CAN_WRITE, TRUE);",
                "  g_file_info_set_attribute_boolean (real_info,",
                "                                     G_FILE_ATTRIBUTE_ACCESS_CAN_DELETE, TRUE);",
                "  g_file_info_set_attribute_boolean (real_info,",
                "                                     G_FILE_ATTRIBUTE_ACCESS_CAN_RENAME, TRUE);",
                ""
            ],
            "added_lines": [
                "  fix_file_info (real_info);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-12448",
        "func_name": "GNOME/gvfs/g_vfs_backend_admin_class_init",
        "description": "An issue was discovered in GNOME gvfs 1.29.4 through 1.41.2. daemon/gvfsbackendadmin.c has race conditions because the admin backend doesn't implement query_info_on_read/write.",
        "git_url": "https://github.com/GNOME/gvfs/commit/764e9af7522e3096c0f44613c330377d31c9bbb5",
        "commit_title": "admin: Add query_info_on_read/write functionality",
        "commit_text": " Admin backend doesn't implement query_info_on_read/write which might potentially lead to some race conditions which aren't really wanted especially in case of admin backend. For example, in file_copy_fallback(), g_file_query_info() is used if g_file_input_stream_query_info() is not supported, which in theory means that the info might be obtained from the different file then it is opened. Let's add this missing functionality to prevent this possibility.",
        "func_before": "static void\ng_vfs_backend_admin_class_init (GVfsBackendAdminClass * klass)\n{\n  GObjectClass *object_class = G_OBJECT_CLASS (klass);\n  GVfsBackendClass *backend_class = G_VFS_BACKEND_CLASS (klass);\n\n  object_class->finalize = do_finalize;\n\n  backend_class->mount = do_mount;\n  backend_class->open_for_read = do_open_for_read;\n  backend_class->query_info = do_query_info;\n  backend_class->read = do_read;\n  backend_class->create = do_create;\n  backend_class->append_to = do_append_to;\n  backend_class->replace = do_replace;\n  backend_class->write = do_write;\n  backend_class->close_read = do_close_read;\n  backend_class->close_write = do_close_write;\n  backend_class->seek_on_read = do_seek_on_read;\n  backend_class->seek_on_write = do_seek_on_write;\n  backend_class->enumerate = do_enumerate;\n  backend_class->truncate = do_truncate;\n  backend_class->make_directory = do_make_directory;\n  backend_class->make_symlink = do_make_symlink;\n  backend_class->query_fs_info = do_query_fs_info;\n  backend_class->create_dir_monitor = do_create_dir_monitor;\n  backend_class->create_file_monitor = do_create_file_monitor;\n  backend_class->set_display_name = do_set_display_name;\n  backend_class->set_attribute = do_set_attribute;\n  backend_class->delete = do_delete;\n  backend_class->move = do_move;\n  backend_class->query_settable_attributes = do_query_settable_attributes;\n  backend_class->query_writable_namespaces = do_query_writable_namespaces;\n}",
        "func": "static void\ng_vfs_backend_admin_class_init (GVfsBackendAdminClass * klass)\n{\n  GObjectClass *object_class = G_OBJECT_CLASS (klass);\n  GVfsBackendClass *backend_class = G_VFS_BACKEND_CLASS (klass);\n\n  object_class->finalize = do_finalize;\n\n  backend_class->mount = do_mount;\n  backend_class->open_for_read = do_open_for_read;\n  backend_class->query_info = do_query_info;\n  backend_class->query_info_on_read = do_query_info_on_read;\n  backend_class->query_info_on_write = do_query_info_on_write;\n  backend_class->read = do_read;\n  backend_class->create = do_create;\n  backend_class->append_to = do_append_to;\n  backend_class->replace = do_replace;\n  backend_class->write = do_write;\n  backend_class->close_read = do_close_read;\n  backend_class->close_write = do_close_write;\n  backend_class->seek_on_read = do_seek_on_read;\n  backend_class->seek_on_write = do_seek_on_write;\n  backend_class->enumerate = do_enumerate;\n  backend_class->truncate = do_truncate;\n  backend_class->make_directory = do_make_directory;\n  backend_class->make_symlink = do_make_symlink;\n  backend_class->query_fs_info = do_query_fs_info;\n  backend_class->create_dir_monitor = do_create_dir_monitor;\n  backend_class->create_file_monitor = do_create_file_monitor;\n  backend_class->set_display_name = do_set_display_name;\n  backend_class->set_attribute = do_set_attribute;\n  backend_class->delete = do_delete;\n  backend_class->move = do_move;\n  backend_class->query_settable_attributes = do_query_settable_attributes;\n  backend_class->query_writable_namespaces = do_query_writable_namespaces;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,8 @@\n   backend_class->mount = do_mount;\n   backend_class->open_for_read = do_open_for_read;\n   backend_class->query_info = do_query_info;\n+  backend_class->query_info_on_read = do_query_info_on_read;\n+  backend_class->query_info_on_write = do_query_info_on_write;\n   backend_class->read = do_read;\n   backend_class->create = do_create;\n   backend_class->append_to = do_append_to;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  backend_class->query_info_on_read = do_query_info_on_read;",
                "  backend_class->query_info_on_write = do_query_info_on_write;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-12450",
        "func_name": "GNOME/glib/file_copy_fallback",
        "description": "file_copy_fallback in gio/gfile.c in GNOME GLib 2.15.0 through 2.61.1 does not properly restrict file permissions while a copy operation is in progress. Instead, default permissions are used.",
        "git_url": "https://github.com/GNOME/glib/commit/d8f8f4d637ce43f8699ba94c9b7648beda0ca174",
        "commit_title": "gfile: Limit access to files when copying",
        "commit_text": " file_copy_fallback creates new files with default permissions and set the correct permissions after the operation is finished. This might cause that the files can be accessible by more users during the operation than expected. Use G_FILE_CREATE_PRIVATE for the new files to limit access to those files.",
        "func_before": "static gboolean\nfile_copy_fallback (GFile                  *source,\n                    GFile                  *destination,\n                    GFileCopyFlags          flags,\n                    GCancellable           *cancellable,\n                    GFileProgressCallback   progress_callback,\n                    gpointer                progress_callback_data,\n                    GError                **error)\n{\n  gboolean ret = FALSE;\n  GFileInputStream *file_in = NULL;\n  GInputStream *in = NULL;\n  GOutputStream *out = NULL;\n  GFileInfo *info = NULL;\n  const char *target;\n  char *attrs_to_read;\n  gboolean do_set_attributes = FALSE;\n\n  /* need to know the file type */\n  info = g_file_query_info (source,\n                            G_FILE_ATTRIBUTE_STANDARD_TYPE \",\" G_FILE_ATTRIBUTE_STANDARD_SYMLINK_TARGET,\n                            G_FILE_QUERY_INFO_NOFOLLOW_SYMLINKS,\n                            cancellable,\n                            error);\n  if (!info)\n    goto out;\n\n  /* Maybe copy the symlink? */\n  if ((flags & G_FILE_COPY_NOFOLLOW_SYMLINKS) &&\n      g_file_info_get_file_type (info) == G_FILE_TYPE_SYMBOLIC_LINK)\n    {\n      target = g_file_info_get_symlink_target (info);\n      if (target)\n        {\n          if (!copy_symlink (destination, flags, cancellable, target, error))\n            goto out;\n\n          ret = TRUE;\n          goto out;\n        }\n        /* ... else fall back on a regular file copy */\n    }\n  /* Handle \"special\" files (pipes, device nodes, ...)? */\n  else if (g_file_info_get_file_type (info) == G_FILE_TYPE_SPECIAL)\n    {\n      /* FIXME: could try to recreate device nodes and others? */\n      g_set_error_literal (error, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED,\n                           _(\"Can’t copy special file\"));\n      goto out;\n    }\n\n  /* Everything else should just fall back on a regular copy. */\n\n  file_in = open_source_for_copy (source, destination, flags, cancellable, error);\n  if (!file_in)\n    goto out;\n  in = G_INPUT_STREAM (file_in);\n\n  if (!build_attribute_list_for_copy (destination, flags, &attrs_to_read,\n                                      cancellable, error))\n    goto out;\n\n  if (attrs_to_read != NULL)\n    {\n      GError *tmp_error = NULL;\n\n      /* Ok, ditch the previous lightweight info (on Unix we just\n       * called lstat()); at this point we gather all the information\n       * we need about the source from the opened file descriptor.\n       */\n      g_object_unref (info);\n\n      info = g_file_input_stream_query_info (file_in, attrs_to_read,\n                                             cancellable, &tmp_error);\n      if (!info)\n        {\n          /* Not all gvfs backends implement query_info_on_read(), we\n           * can just fall back to the pathname again.\n           * https://bugzilla.gnome.org/706254\n           */\n          if (g_error_matches (tmp_error, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED))\n            {\n              g_clear_error (&tmp_error);\n              info = g_file_query_info (source, attrs_to_read, G_FILE_QUERY_INFO_NOFOLLOW_SYMLINKS,\n                                        cancellable, error);\n            }\n          else\n            {\n              g_free (attrs_to_read);\n              g_propagate_error (error, tmp_error);\n              goto out;\n            }\n        }\n      g_free (attrs_to_read);\n      if (!info)\n        goto out;\n\n      do_set_attributes = TRUE;\n    }\n\n  /* In the local file path, we pass down the source info which\n   * includes things like unix::mode, to ensure that the target file\n   * is not created with different permissions from the source file.\n   *\n   * If a future API like g_file_replace_with_info() is added, switch\n   * this code to use that.\n   */\n  if (G_IS_LOCAL_FILE (destination))\n    {\n      if (flags & G_FILE_COPY_OVERWRITE)\n        out = (GOutputStream*)_g_local_file_output_stream_replace (_g_local_file_get_filename (G_LOCAL_FILE (destination)),\n                                                                   FALSE, NULL,\n                                                                   flags & G_FILE_COPY_BACKUP,\n                                                                   G_FILE_CREATE_REPLACE_DESTINATION,\n                                                                   info,\n                                                                   cancellable, error);\n      else\n        out = (GOutputStream*)_g_local_file_output_stream_create (_g_local_file_get_filename (G_LOCAL_FILE (destination)),\n                                                                  FALSE, 0, info,\n                                                                  cancellable, error);\n    }\n  else if (flags & G_FILE_COPY_OVERWRITE)\n    {\n      out = (GOutputStream *)g_file_replace (destination,\n                                             NULL,\n                                             flags & G_FILE_COPY_BACKUP,\n                                             G_FILE_CREATE_REPLACE_DESTINATION,\n                                             cancellable, error);\n    }\n  else\n    {\n      out = (GOutputStream *)g_file_create (destination, 0, cancellable, error);\n    }\n\n  if (!out)\n    goto out;\n\n#ifdef __linux__\n  if (G_IS_FILE_DESCRIPTOR_BASED (in) && G_IS_FILE_DESCRIPTOR_BASED (out))\n    {\n      GError *reflink_err = NULL;\n\n      if (!btrfs_reflink_with_progress (in, out, info, cancellable,\n                                        progress_callback, progress_callback_data,\n                                        &reflink_err))\n        {\n          if (g_error_matches (reflink_err, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED))\n            {\n              g_clear_error (&reflink_err);\n            }\n          else\n            {\n              g_propagate_error (error, reflink_err);\n              goto out;\n            }\n        }\n      else\n        {\n          ret = TRUE;\n          goto out;\n        }\n    }\n#endif\n\n#ifdef HAVE_SPLICE\n  if (G_IS_FILE_DESCRIPTOR_BASED (in) && G_IS_FILE_DESCRIPTOR_BASED (out))\n    {\n      GError *splice_err = NULL;\n\n      if (!splice_stream_with_progress (in, out, cancellable,\n                                        progress_callback, progress_callback_data,\n                                        &splice_err))\n        {\n          if (g_error_matches (splice_err, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED))\n            {\n              g_clear_error (&splice_err);\n            }\n          else\n            {\n              g_propagate_error (error, splice_err);\n              goto out;\n            }\n        }\n      else\n        {\n          ret = TRUE;\n          goto out;\n        }\n    }\n\n#endif\n\n  /* A plain read/write loop */\n  if (!copy_stream_with_progress (in, out, source, cancellable,\n                                  progress_callback, progress_callback_data,\n                                  error))\n    goto out;\n\n  ret = TRUE;\n out:\n  if (in)\n    {\n      /* Don't care about errors in source here */\n      (void) g_input_stream_close (in, cancellable, NULL);\n      g_object_unref (in);\n    }\n\n  if (out)\n    {\n      /* But write errors on close are bad! */\n      if (!g_output_stream_close (out, cancellable, ret ? error : NULL))\n        ret = FALSE;\n      g_object_unref (out);\n    }\n\n  /* Ignore errors here. Failure to copy metadata is not a hard error */\n  /* TODO: set these attributes /before/ we do the rename() on Unix */\n  if (ret && do_set_attributes)\n    {\n      g_file_set_attributes_from_info (destination,\n                                       info,\n                                       G_FILE_QUERY_INFO_NOFOLLOW_SYMLINKS,\n                                       cancellable,\n                                       NULL);\n    }\n\n  g_clear_object (&info);\n\n  return ret;\n}",
        "func": "static gboolean\nfile_copy_fallback (GFile                  *source,\n                    GFile                  *destination,\n                    GFileCopyFlags          flags,\n                    GCancellable           *cancellable,\n                    GFileProgressCallback   progress_callback,\n                    gpointer                progress_callback_data,\n                    GError                **error)\n{\n  gboolean ret = FALSE;\n  GFileInputStream *file_in = NULL;\n  GInputStream *in = NULL;\n  GOutputStream *out = NULL;\n  GFileInfo *info = NULL;\n  const char *target;\n  char *attrs_to_read;\n  gboolean do_set_attributes = FALSE;\n\n  /* need to know the file type */\n  info = g_file_query_info (source,\n                            G_FILE_ATTRIBUTE_STANDARD_TYPE \",\" G_FILE_ATTRIBUTE_STANDARD_SYMLINK_TARGET,\n                            G_FILE_QUERY_INFO_NOFOLLOW_SYMLINKS,\n                            cancellable,\n                            error);\n  if (!info)\n    goto out;\n\n  /* Maybe copy the symlink? */\n  if ((flags & G_FILE_COPY_NOFOLLOW_SYMLINKS) &&\n      g_file_info_get_file_type (info) == G_FILE_TYPE_SYMBOLIC_LINK)\n    {\n      target = g_file_info_get_symlink_target (info);\n      if (target)\n        {\n          if (!copy_symlink (destination, flags, cancellable, target, error))\n            goto out;\n\n          ret = TRUE;\n          goto out;\n        }\n        /* ... else fall back on a regular file copy */\n    }\n  /* Handle \"special\" files (pipes, device nodes, ...)? */\n  else if (g_file_info_get_file_type (info) == G_FILE_TYPE_SPECIAL)\n    {\n      /* FIXME: could try to recreate device nodes and others? */\n      g_set_error_literal (error, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED,\n                           _(\"Can’t copy special file\"));\n      goto out;\n    }\n\n  /* Everything else should just fall back on a regular copy. */\n\n  file_in = open_source_for_copy (source, destination, flags, cancellable, error);\n  if (!file_in)\n    goto out;\n  in = G_INPUT_STREAM (file_in);\n\n  if (!build_attribute_list_for_copy (destination, flags, &attrs_to_read,\n                                      cancellable, error))\n    goto out;\n\n  if (attrs_to_read != NULL)\n    {\n      GError *tmp_error = NULL;\n\n      /* Ok, ditch the previous lightweight info (on Unix we just\n       * called lstat()); at this point we gather all the information\n       * we need about the source from the opened file descriptor.\n       */\n      g_object_unref (info);\n\n      info = g_file_input_stream_query_info (file_in, attrs_to_read,\n                                             cancellable, &tmp_error);\n      if (!info)\n        {\n          /* Not all gvfs backends implement query_info_on_read(), we\n           * can just fall back to the pathname again.\n           * https://bugzilla.gnome.org/706254\n           */\n          if (g_error_matches (tmp_error, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED))\n            {\n              g_clear_error (&tmp_error);\n              info = g_file_query_info (source, attrs_to_read, G_FILE_QUERY_INFO_NOFOLLOW_SYMLINKS,\n                                        cancellable, error);\n            }\n          else\n            {\n              g_free (attrs_to_read);\n              g_propagate_error (error, tmp_error);\n              goto out;\n            }\n        }\n      g_free (attrs_to_read);\n      if (!info)\n        goto out;\n\n      do_set_attributes = TRUE;\n    }\n\n  /* In the local file path, we pass down the source info which\n   * includes things like unix::mode, to ensure that the target file\n   * is not created with different permissions from the source file.\n   *\n   * If a future API like g_file_replace_with_info() is added, switch\n   * this code to use that.\n   */\n  if (G_IS_LOCAL_FILE (destination))\n    {\n      if (flags & G_FILE_COPY_OVERWRITE)\n        out = (GOutputStream*)_g_local_file_output_stream_replace (_g_local_file_get_filename (G_LOCAL_FILE (destination)),\n                                                                   FALSE, NULL,\n                                                                   flags & G_FILE_COPY_BACKUP,\n                                                                   G_FILE_CREATE_REPLACE_DESTINATION |\n                                                                   G_FILE_CREATE_PRIVATE, info,\n                                                                   cancellable, error);\n      else\n        out = (GOutputStream*)_g_local_file_output_stream_create (_g_local_file_get_filename (G_LOCAL_FILE (destination)),\n                                                                  FALSE, G_FILE_CREATE_PRIVATE, info,\n                                                                  cancellable, error);\n    }\n  else if (flags & G_FILE_COPY_OVERWRITE)\n    {\n      out = (GOutputStream *)g_file_replace (destination,\n                                             NULL,\n                                             flags & G_FILE_COPY_BACKUP,\n                                             G_FILE_CREATE_REPLACE_DESTINATION |\n                                             G_FILE_CREATE_PRIVATE,\n                                             cancellable, error);\n    }\n  else\n    {\n      out = (GOutputStream *)g_file_create (destination, G_FILE_CREATE_PRIVATE, cancellable, error);\n    }\n\n  if (!out)\n    goto out;\n\n#ifdef __linux__\n  if (G_IS_FILE_DESCRIPTOR_BASED (in) && G_IS_FILE_DESCRIPTOR_BASED (out))\n    {\n      GError *reflink_err = NULL;\n\n      if (!btrfs_reflink_with_progress (in, out, info, cancellable,\n                                        progress_callback, progress_callback_data,\n                                        &reflink_err))\n        {\n          if (g_error_matches (reflink_err, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED))\n            {\n              g_clear_error (&reflink_err);\n            }\n          else\n            {\n              g_propagate_error (error, reflink_err);\n              goto out;\n            }\n        }\n      else\n        {\n          ret = TRUE;\n          goto out;\n        }\n    }\n#endif\n\n#ifdef HAVE_SPLICE\n  if (G_IS_FILE_DESCRIPTOR_BASED (in) && G_IS_FILE_DESCRIPTOR_BASED (out))\n    {\n      GError *splice_err = NULL;\n\n      if (!splice_stream_with_progress (in, out, cancellable,\n                                        progress_callback, progress_callback_data,\n                                        &splice_err))\n        {\n          if (g_error_matches (splice_err, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED))\n            {\n              g_clear_error (&splice_err);\n            }\n          else\n            {\n              g_propagate_error (error, splice_err);\n              goto out;\n            }\n        }\n      else\n        {\n          ret = TRUE;\n          goto out;\n        }\n    }\n\n#endif\n\n  /* A plain read/write loop */\n  if (!copy_stream_with_progress (in, out, source, cancellable,\n                                  progress_callback, progress_callback_data,\n                                  error))\n    goto out;\n\n  ret = TRUE;\n out:\n  if (in)\n    {\n      /* Don't care about errors in source here */\n      (void) g_input_stream_close (in, cancellable, NULL);\n      g_object_unref (in);\n    }\n\n  if (out)\n    {\n      /* But write errors on close are bad! */\n      if (!g_output_stream_close (out, cancellable, ret ? error : NULL))\n        ret = FALSE;\n      g_object_unref (out);\n    }\n\n  /* Ignore errors here. Failure to copy metadata is not a hard error */\n  /* TODO: set these attributes /before/ we do the rename() on Unix */\n  if (ret && do_set_attributes)\n    {\n      g_file_set_attributes_from_info (destination,\n                                       info,\n                                       G_FILE_QUERY_INFO_NOFOLLOW_SYMLINKS,\n                                       cancellable,\n                                       NULL);\n    }\n\n  g_clear_object (&info);\n\n  return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -111,12 +111,12 @@\n         out = (GOutputStream*)_g_local_file_output_stream_replace (_g_local_file_get_filename (G_LOCAL_FILE (destination)),\n                                                                    FALSE, NULL,\n                                                                    flags & G_FILE_COPY_BACKUP,\n-                                                                   G_FILE_CREATE_REPLACE_DESTINATION,\n-                                                                   info,\n+                                                                   G_FILE_CREATE_REPLACE_DESTINATION |\n+                                                                   G_FILE_CREATE_PRIVATE, info,\n                                                                    cancellable, error);\n       else\n         out = (GOutputStream*)_g_local_file_output_stream_create (_g_local_file_get_filename (G_LOCAL_FILE (destination)),\n-                                                                  FALSE, 0, info,\n+                                                                  FALSE, G_FILE_CREATE_PRIVATE, info,\n                                                                   cancellable, error);\n     }\n   else if (flags & G_FILE_COPY_OVERWRITE)\n@@ -124,12 +124,13 @@\n       out = (GOutputStream *)g_file_replace (destination,\n                                              NULL,\n                                              flags & G_FILE_COPY_BACKUP,\n-                                             G_FILE_CREATE_REPLACE_DESTINATION,\n+                                             G_FILE_CREATE_REPLACE_DESTINATION |\n+                                             G_FILE_CREATE_PRIVATE,\n                                              cancellable, error);\n     }\n   else\n     {\n-      out = (GOutputStream *)g_file_create (destination, 0, cancellable, error);\n+      out = (GOutputStream *)g_file_create (destination, G_FILE_CREATE_PRIVATE, cancellable, error);\n     }\n \n   if (!out)",
        "diff_line_info": {
            "deleted_lines": [
                "                                                                   G_FILE_CREATE_REPLACE_DESTINATION,",
                "                                                                   info,",
                "                                                                  FALSE, 0, info,",
                "                                             G_FILE_CREATE_REPLACE_DESTINATION,",
                "      out = (GOutputStream *)g_file_create (destination, 0, cancellable, error);"
            ],
            "added_lines": [
                "                                                                   G_FILE_CREATE_REPLACE_DESTINATION |",
                "                                                                   G_FILE_CREATE_PRIVATE, info,",
                "                                                                  FALSE, G_FILE_CREATE_PRIVATE, info,",
                "                                             G_FILE_CREATE_REPLACE_DESTINATION |",
                "                                             G_FILE_CREATE_PRIVATE,",
                "      out = (GOutputStream *)g_file_create (destination, G_FILE_CREATE_PRIVATE, cancellable, error);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13226",
        "func_name": "linuxdeepin/deepin-clone/main",
        "description": "deepin-clone before 1.1.3 uses a predictable path /tmp/.deepin-clone/mount/<block-dev-basename> in the Helper::temporaryMountDevice() function to temporarily mount a file system as root. An unprivileged user can prepare a symlink at this location to have the file system mounted in an arbitrary location. By winning a race condition, the attacker can also enter the mount point, thereby preventing a subsequent unmount of the file system.",
        "git_url": "https://github.com/linuxdeepin/deepin-clone/commit/e079f3e2712b4f8c28e3e63e71ba1a1f90fce1ab",
        "commit_title": "fix: Do not use the \"/tmp\" directory",
        "commit_text": " https://github.com/linuxdeepin/deepin-clone/issues/16 https://bugzilla.opensuse.org/show_bug.cgi?id=1130388",
        "func_before": "int main(int argc, char *argv[])\n{\n    QCoreApplication *a;\n\n    if (isTUIMode(argc, argv)) {\n        Global::isTUIMode = true;\n\n        a = new QCoreApplication(argc, argv);\n    }\n#ifdef ENABLE_GUI\n    else {\n        ConsoleAppender *consoleAppender = new ConsoleAppender;\n        consoleAppender->setFormat(logFormat);\n\n        RollingFileAppender *rollingFileAppender = new RollingFileAppender(\"/tmp/.deepin-clone.log\");\n        rollingFileAppender->setFormat(logFormat);\n        rollingFileAppender->setLogFilesLimit(5);\n        rollingFileAppender->setDatePattern(RollingFileAppender::DailyRollover);\n\n        logger->registerAppender(consoleAppender);\n        logger->registerAppender(rollingFileAppender);\n\n        if (qEnvironmentVariableIsSet(\"PKEXEC_UID\")) {\n            const quint32 pkexec_uid = qgetenv(\"PKEXEC_UID\").toUInt();\n            const QDir user_home(getpwuid(pkexec_uid)->pw_dir);\n\n            QFile pam_file(user_home.absoluteFilePath(\".pam_environment\"));\n\n            if (pam_file.open(QIODevice::ReadOnly)) {\n                while (!pam_file.atEnd()) {\n                    const QByteArray &line = pam_file.readLine().simplified();\n\n                    if (line.startsWith(\"QT_SCALE_FACTOR\")) {\n                        const QByteArrayList &list = line.split('=');\n\n                        if (list.count() == 2) {\n                            qputenv(\"QT_SCALE_FACTOR\", list.last());\n                            break;\n                        }\n                    }\n                }\n\n                pam_file.close();\n            }\n        }\n\n        DApplication::loadDXcbPlugin();\n        DApplication *app = new DApplication(argc, argv);\n\n        app->setAttribute(Qt::AA_UseHighDpiPixmaps);\n\n        if (!qApp->setSingleInstance(\"_deepin_clone_\")) {\n            qCritical() << \"As well as the process is running\";\n\n            return -1;\n        }\n\n        if (!app->loadTranslator()) {\n            dError(\"Load translator failed\");\n        }\n\n        app->setApplicationDisplayName(QObject::tr(\"Deepin Clone\"));\n        app->setApplicationDescription(QObject::tr(\"Deepin Clone is a backup and restore tool in deepin. \"\n                                                   \"It supports disk or partition clone, backup and restore, and other functions.\"));\n        app->setApplicationAcknowledgementPage(\"https://www.deepin.org/acknowledgments/deepin-clone/\");\n        app->setTheme(\"light\");\n        a = app;\n    }\n#endif\n\n    a->setApplicationName(\"deepin-clone\");\n#ifdef ENABLE_GUI\n    a->setApplicationVersion(DApplication::buildVersion(\"1.0.0.1\"));\n#else\n    a->setApplicationVersion(\"1.0.0.1\");\n#endif\n    a->setOrganizationName(\"deepin\");\n\n    CommandLineParser parser;\n\n    QFile arguments_file(\"/lib/live/mount/medium/.tmp/deepin-clone.arguments\");\n    QStringList arguments;\n\n    bool load_arg_from_file = arguments_file.exists() && !Global::isTUIMode && !a->arguments().contains(\"--tui\");\n\n    if (load_arg_from_file) {\n        arguments.append(a->arguments().first());\n\n        if (!arguments_file.open(QIODevice::ReadOnly)) {\n            qCritical() << \"Open \\\"/lib/live/mount/medium/.tmp/deepin-clone.arguments\\\" failed, error:\" << arguments_file.errorString();\n        } else {\n            while (!arguments_file.atEnd()) {\n                const QString &arg = QString::fromUtf8(arguments_file.readLine().trimmed());\n\n                if (!arg.isEmpty())\n                    arguments.append(arg);\n            }\n\n            arguments_file.close();\n            arguments_file.remove();\n        }\n\n        qDebug() << arguments;\n    } else {\n        arguments = a->arguments();\n    }\n\n    parser.process(arguments);\n\n    ConsoleAppender *consoleAppender = new ConsoleAppender;\n    consoleAppender->setFormat(logFormat);\n\n    RollingFileAppender *rollingFileAppender = new RollingFileAppender(parser.logFile());\n    rollingFileAppender->setFormat(logFormat);\n    rollingFileAppender->setLogFilesLimit(5);\n    rollingFileAppender->setDatePattern(RollingFileAppender::DailyRollover);\n\n    logger->registerCategoryAppender(\"deepin.ghost\", consoleAppender);\n    logger->registerCategoryAppender(\"deepin.ghost\", rollingFileAppender);\n\n    parser.parse();\n\n    if (load_arg_from_file) {\n        dCDebug(\"Load arguments from \\\"%s\\\"\", qPrintable(arguments_file.fileName()));\n    }\n\n    dCInfo(\"Application command line: %s\", qPrintable(arguments.join(' ')));\n\n    if (Global::debugLevel == 0) {\n        QLoggingCategory::setFilterRules(\"deepin.ghost.debug=false\");\n    }\n\n    if (Global::isTUIMode) {\n        if (!parser.target().isEmpty()) {\n            CloneJob *job = new CloneJob;\n\n            QObject::connect(job, &QThread::finished, a, &QCoreApplication::quit);\n\n            job->start(parser.source(), parser.target());\n        }\n    }\n#ifdef ENABLE_GUI\n    else {\n        if (!parser.isSetOverride())\n            Global::isOverride = true;\n\n        if (!parser.isSetDebug())\n            Global::debugLevel = 2;\n\n        MainWindow *window = new MainWindow;\n\n        window->setFixedSize(860, 660);\n        window->setStyleSheet(DThemeManager::instance()->getQssForWidget(\"main\", window));\n        window->setWindowIcon(QIcon::fromTheme(\"deepin-clone\"));\n        window->setWindowFlags(Qt::WindowCloseButtonHint | Qt::WindowMinimizeButtonHint | Qt::WindowSystemMenuHint);\n        window->titlebar()->setIcon(window->windowIcon());\n        window->titlebar()->setTitle(QString());\n#if DTK_VERSION > DTK_VERSION_CHECK(2, 0, 6, 0)\n        window->titlebar()->setBackgroundTransparent(true);\n#endif\n        window->show();\n\n        qApp->setProductIcon(window->windowIcon());\n\n        if (!parser.source().isEmpty()) {\n            window->startWithFile(parser.source(), parser.target());\n        }\n\n        QObject::connect(a, &QCoreApplication::aboutToQuit, window, &MainWindow::deleteLater);\n        QDesktopServices::setUrlHandler(\"https\", window, \"openUrl\");\n    }\n#endif\n\n    int exitCode = Global::isTUIMode ? a->exec() : qApp->exec();\n    QString log_backup_file = parser.logBackupFile();\n\n    if (log_backup_file.startsWith(\"serial://\")) {\n        log_backup_file = Helper::parseSerialUrl(log_backup_file);\n    }\n\n    if (log_backup_file.isEmpty()) {\n        return exitCode;\n    }\n\n    if (!QFile::copy(parser.logFile(), log_backup_file)) {\n        dCWarning(\"failed to copy log file to \\\"%s\\\"\", qPrintable(log_backup_file));\n    }\n\n    return exitCode;\n}",
        "func": "int main(int argc, char *argv[])\n{\n    QCoreApplication *a;\n\n    if (isTUIMode(argc, argv)) {\n        Global::isTUIMode = true;\n\n        a = new QCoreApplication(argc, argv);\n    }\n#ifdef ENABLE_GUI\n    else {\n        ConsoleAppender *consoleAppender = new ConsoleAppender;\n        consoleAppender->setFormat(logFormat);\n\n        const QString log_file(\"/var/log/deepin-clone.log\");\n\n        RollingFileAppender *rollingFileAppender = new RollingFileAppender(log_file);\n        rollingFileAppender->setFormat(logFormat);\n        rollingFileAppender->setLogFilesLimit(5);\n        rollingFileAppender->setDatePattern(RollingFileAppender::DailyRollover);\n\n        logger->registerAppender(rollingFileAppender);\n        logger->registerAppender(consoleAppender);\n\n        if (qEnvironmentVariableIsSet(\"PKEXEC_UID\")) {\n            const quint32 pkexec_uid = qgetenv(\"PKEXEC_UID\").toUInt();\n\n            DApplication::customQtThemeConfigPathByUserHome(getpwuid(pkexec_uid)->pw_dir);\n        }\n\n        DApplication::loadDXcbPlugin();\n        DApplication *app = new DApplication(argc, argv);\n\n        app->setAttribute(Qt::AA_UseHighDpiPixmaps);\n\n        if (!qApp->setSingleInstance(\"_deepin_clone_\")) {\n            qCritical() << \"As well as the process is running\";\n\n            return -1;\n        }\n\n        if (!app->loadTranslator()) {\n            dError(\"Load translator failed\");\n        }\n\n        app->setApplicationDisplayName(QObject::tr(\"Deepin Clone\"));\n        app->setApplicationDescription(QObject::tr(\"Deepin Clone is a backup and restore tool in deepin. \"\n                                                   \"It supports disk or partition clone, backup and restore, and other functions.\"));\n        app->setApplicationAcknowledgementPage(\"https://www.deepin.org/acknowledgments/deepin-clone/\");\n        app->setTheme(\"light\");\n        a = app;\n    }\n#endif\n\n    a->setApplicationName(\"deepin-clone\");\n#ifdef ENABLE_GUI\n    a->setApplicationVersion(DApplication::buildVersion(\"1.0.0.1\"));\n#else\n    a->setApplicationVersion(\"1.0.0.1\");\n#endif\n    a->setOrganizationName(\"deepin\");\n\n    CommandLineParser parser;\n\n    QFile arguments_file(\"/lib/live/mount/medium/.tmp/deepin-clone.arguments\");\n    QStringList arguments;\n\n    bool load_arg_from_file = arguments_file.exists() && !Global::isTUIMode && !a->arguments().contains(\"--tui\");\n\n    if (load_arg_from_file) {\n        arguments.append(a->arguments().first());\n\n        if (!arguments_file.open(QIODevice::ReadOnly)) {\n            qCritical() << \"Open \\\"/lib/live/mount/medium/.tmp/deepin-clone.arguments\\\" failed, error:\" << arguments_file.errorString();\n        } else {\n            while (!arguments_file.atEnd()) {\n                const QString &arg = QString::fromUtf8(arguments_file.readLine().trimmed());\n\n                if (!arg.isEmpty())\n                    arguments.append(arg);\n            }\n\n            arguments_file.close();\n            arguments_file.remove();\n        }\n\n        qDebug() << arguments;\n    } else {\n        arguments = a->arguments();\n    }\n\n    parser.process(arguments);\n\n    ConsoleAppender *consoleAppender = new ConsoleAppender;\n    consoleAppender->setFormat(logFormat);\n\n    RollingFileAppender *rollingFileAppender = new RollingFileAppender(parser.logFile());\n    rollingFileAppender->setFormat(logFormat);\n    rollingFileAppender->setLogFilesLimit(5);\n    rollingFileAppender->setDatePattern(RollingFileAppender::DailyRollover);\n\n    logger->registerCategoryAppender(\"deepin.ghost\", consoleAppender);\n    logger->registerCategoryAppender(\"deepin.ghost\", rollingFileAppender);\n\n    parser.parse();\n\n    if (load_arg_from_file) {\n        dCDebug(\"Load arguments from \\\"%s\\\"\", qPrintable(arguments_file.fileName()));\n    }\n\n    dCInfo(\"Application command line: %s\", qPrintable(arguments.join(' ')));\n\n    if (Global::debugLevel == 0) {\n        QLoggingCategory::setFilterRules(\"deepin.ghost.debug=false\");\n    }\n\n    if (Global::isTUIMode) {\n        if (!parser.target().isEmpty()) {\n            CloneJob *job = new CloneJob;\n\n            QObject::connect(job, &QThread::finished, a, &QCoreApplication::quit);\n\n            job->start(parser.source(), parser.target());\n        }\n    }\n#ifdef ENABLE_GUI\n    else {\n        if (!parser.isSetOverride())\n            Global::isOverride = true;\n\n        if (!parser.isSetDebug())\n            Global::debugLevel = 2;\n\n        MainWindow *window = new MainWindow;\n\n        window->setFixedSize(860, 660);\n        window->setStyleSheet(DThemeManager::instance()->getQssForWidget(\"main\", window));\n        window->setWindowIcon(QIcon::fromTheme(\"deepin-clone\"));\n        window->setWindowFlags(Qt::WindowCloseButtonHint | Qt::WindowMinimizeButtonHint | Qt::WindowSystemMenuHint);\n        window->titlebar()->setIcon(window->windowIcon());\n        window->titlebar()->setTitle(QString());\n#if DTK_VERSION > DTK_VERSION_CHECK(2, 0, 6, 0)\n        window->titlebar()->setBackgroundTransparent(true);\n#endif\n        window->show();\n\n        qApp->setProductIcon(window->windowIcon());\n\n        if (!parser.source().isEmpty()) {\n            window->startWithFile(parser.source(), parser.target());\n        }\n\n        QObject::connect(a, &QCoreApplication::aboutToQuit, window, &MainWindow::deleteLater);\n        QDesktopServices::setUrlHandler(\"https\", window, \"openUrl\");\n    }\n#endif\n\n    int exitCode = Global::isTUIMode ? a->exec() : qApp->exec();\n    QString log_backup_file = parser.logBackupFile();\n\n    if (log_backup_file.startsWith(\"serial://\")) {\n        log_backup_file = Helper::parseSerialUrl(log_backup_file);\n    }\n\n    if (log_backup_file.isEmpty()) {\n        return exitCode;\n    }\n\n    if (!QFile::copy(parser.logFile(), log_backup_file)) {\n        dCWarning(\"failed to copy log file to \\\"%s\\\"\", qPrintable(log_backup_file));\n    }\n\n    return exitCode;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,36 +12,20 @@\n         ConsoleAppender *consoleAppender = new ConsoleAppender;\n         consoleAppender->setFormat(logFormat);\n \n-        RollingFileAppender *rollingFileAppender = new RollingFileAppender(\"/tmp/.deepin-clone.log\");\n+        const QString log_file(\"/var/log/deepin-clone.log\");\n+\n+        RollingFileAppender *rollingFileAppender = new RollingFileAppender(log_file);\n         rollingFileAppender->setFormat(logFormat);\n         rollingFileAppender->setLogFilesLimit(5);\n         rollingFileAppender->setDatePattern(RollingFileAppender::DailyRollover);\n \n+        logger->registerAppender(rollingFileAppender);\n         logger->registerAppender(consoleAppender);\n-        logger->registerAppender(rollingFileAppender);\n \n         if (qEnvironmentVariableIsSet(\"PKEXEC_UID\")) {\n             const quint32 pkexec_uid = qgetenv(\"PKEXEC_UID\").toUInt();\n-            const QDir user_home(getpwuid(pkexec_uid)->pw_dir);\n \n-            QFile pam_file(user_home.absoluteFilePath(\".pam_environment\"));\n-\n-            if (pam_file.open(QIODevice::ReadOnly)) {\n-                while (!pam_file.atEnd()) {\n-                    const QByteArray &line = pam_file.readLine().simplified();\n-\n-                    if (line.startsWith(\"QT_SCALE_FACTOR\")) {\n-                        const QByteArrayList &list = line.split('=');\n-\n-                        if (list.count() == 2) {\n-                            qputenv(\"QT_SCALE_FACTOR\", list.last());\n-                            break;\n-                        }\n-                    }\n-                }\n-\n-                pam_file.close();\n-            }\n+            DApplication::customQtThemeConfigPathByUserHome(getpwuid(pkexec_uid)->pw_dir);\n         }\n \n         DApplication::loadDXcbPlugin();",
        "diff_line_info": {
            "deleted_lines": [
                "        RollingFileAppender *rollingFileAppender = new RollingFileAppender(\"/tmp/.deepin-clone.log\");",
                "        logger->registerAppender(rollingFileAppender);",
                "            const QDir user_home(getpwuid(pkexec_uid)->pw_dir);",
                "            QFile pam_file(user_home.absoluteFilePath(\".pam_environment\"));",
                "",
                "            if (pam_file.open(QIODevice::ReadOnly)) {",
                "                while (!pam_file.atEnd()) {",
                "                    const QByteArray &line = pam_file.readLine().simplified();",
                "",
                "                    if (line.startsWith(\"QT_SCALE_FACTOR\")) {",
                "                        const QByteArrayList &list = line.split('=');",
                "",
                "                        if (list.count() == 2) {",
                "                            qputenv(\"QT_SCALE_FACTOR\", list.last());",
                "                            break;",
                "                        }",
                "                    }",
                "                }",
                "",
                "                pam_file.close();",
                "            }"
            ],
            "added_lines": [
                "        const QString log_file(\"/var/log/deepin-clone.log\");",
                "",
                "        RollingFileAppender *rollingFileAppender = new RollingFileAppender(log_file);",
                "        logger->registerAppender(rollingFileAppender);",
                "            DApplication::customQtThemeConfigPathByUserHome(getpwuid(pkexec_uid)->pw_dir);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13226",
        "func_name": "linuxdeepin/deepin-clone/DDeviceDiskInfoPrivate::openDataStream",
        "description": "deepin-clone before 1.1.3 uses a predictable path /tmp/.deepin-clone/mount/<block-dev-basename> in the Helper::temporaryMountDevice() function to temporarily mount a file system as root. An unprivileged user can prepare a symlink at this location to have the file system mounted in an arbitrary location. By winning a race condition, the attacker can also enter the mount point, thereby preventing a subsequent unmount of the file system.",
        "git_url": "https://github.com/linuxdeepin/deepin-clone/commit/e079f3e2712b4f8c28e3e63e71ba1a1f90fce1ab",
        "commit_title": "fix: Do not use the \"/tmp\" directory",
        "commit_text": " https://github.com/linuxdeepin/deepin-clone/issues/16 https://bugzilla.opensuse.org/show_bug.cgi?id=1130388",
        "func_before": "bool DDeviceDiskInfoPrivate::openDataStream(int index)\n{\n    if (process) {\n        process->deleteLater();\n    }\n\n    process = new QProcess();\n\n    QObject::connect(process, static_cast<void (QProcess::*)(int, QProcess::ExitStatus)>(&QProcess::finished),\n                     process, [this] (int code, QProcess::ExitStatus status) {\n        if (isClosing())\n            return;\n\n        if (status == QProcess::CrashExit) {\n            setErrorString(QObject::tr(\"process \\\"%1 %2\\\" crashed\").arg(process->program()).arg(process->arguments().join(\" \")));\n        } else if (code != 0) {\n            setErrorString(QObject::tr(\"Failed to perform process \\\"%1 %2\\\", error: %3\").arg(process->program()).arg(process->arguments().join(\" \")).arg(QString::fromUtf8(process->readAllStandardError())));\n        }\n    });\n\n    switch (currentScope) {\n    case DDiskInfo::Headgear: {\n        if (type != DDiskInfo::Disk) {\n            setErrorString(QObject::tr(\"\\\"%1\\\" is not a disk device\").arg(filePath()));\n\n            return false;\n        }\n\n        if (currentMode == DDiskInfo::Read) {\n            process->start(QStringLiteral(\"dd if=%1 bs=512 count=2048 status=none\").arg(filePath()), QIODevice::ReadOnly);\n        } else {\n            process->start(QStringLiteral(\"dd of=%1 bs=512 status=none conv=fsync\").arg(filePath()));\n        }\n\n        break;\n    }\n    case DDiskInfo::PartitionTable: {\n        if (type != DDiskInfo::Disk) {\n            setErrorString(QObject::tr(\"\\\"%1\\\" is not a disk device\").arg(filePath()));\n\n            return false;\n        }\n\n        if (currentMode == DDiskInfo::Read)\n            process->start(QStringLiteral(\"sfdisk -d %1\").arg(filePath()), QIODevice::ReadOnly);\n        else\n            process->start(QStringLiteral(\"sfdisk %1 --no-reread\").arg(filePath()));\n\n        break;\n    }\n    case DDiskInfo::Partition: {\n        const DPartInfo &part = (index == 0 && currentMode == DDiskInfo::Write) ? DDevicePartInfo(filePath()) : q->getPartByNumber(index);\n\n        if (!part) {\n            dCDebug(\"Part is null(index: %d)\", index);\n\n            return false;\n        }\n\n        dCDebug(\"Try open device: %s, mode: %s\", qPrintable(part.filePath()), currentMode == DDiskInfo::Read ? \"Read\" : \"Write\");\n\n        if (Helper::isMounted(part.filePath())) {\n            if (Helper::umountDevice(part.filePath())) {\n                part.d->mountPoint.clear();\n            } else {\n                setErrorString(QObject::tr(\"\\\"%1\\\" is busy\").arg(part.filePath()));\n\n                return false;\n            }\n        }\n\n        if (currentMode == DDiskInfo::Read) {\n            const QString &executer = Helper::getPartcloneExecuter(part);\n            process->start(QStringLiteral(\"%1 -s %2 -o - -c -z %3 -L /tmp/partclone.log\").arg(executer).arg(part.filePath()).arg(Global::bufferSize), QIODevice::ReadOnly);\n        } else {\n            process->start(QStringLiteral(\"partclone.restore -s - -o %2 -z %3 -L /tmp/partclone.log\").arg(part.filePath()).arg(Global::bufferSize));\n        }\n\n        break;\n    }\n    case DDiskInfo::JsonInfo: {\n        process->deleteLater();\n        process = 0;\n        buffer.setData(q->toJson());\n        break;\n    }\n    default:\n        return false;\n    }\n\n    if (process) {\n        if (!process->waitForStarted()) {\n            setErrorString(QObject::tr(\"Failed to start \\\"%1 %2\\\", error: %3\").arg(process->program()).arg(process->arguments().join(\" \")).arg(process->errorString()));\n\n            return false;\n        }\n\n        dCDebug(\"The \\\"%s %s\\\" command start finished\", qPrintable(process->program()), qPrintable(process->arguments().join(\" \")));\n    }\n\n    bool ok = process ? process->isOpen() : buffer.open(QIODevice::ReadOnly);\n\n    if (!ok) {\n        setErrorString(QObject::tr(\"Failed to open process, error: %1\").arg(process ? process->errorString(): buffer.errorString()));\n    }\n\n    return ok;\n}",
        "func": "bool DDeviceDiskInfoPrivate::openDataStream(int index)\n{\n    if (process) {\n        process->deleteLater();\n    }\n\n    process = new QProcess();\n\n    QObject::connect(process, static_cast<void (QProcess::*)(int, QProcess::ExitStatus)>(&QProcess::finished),\n                     process, [this] (int code, QProcess::ExitStatus status) {\n        if (isClosing())\n            return;\n\n        if (status == QProcess::CrashExit) {\n            setErrorString(QObject::tr(\"process \\\"%1 %2\\\" crashed\").arg(process->program()).arg(process->arguments().join(\" \")));\n        } else if (code != 0) {\n            setErrorString(QObject::tr(\"Failed to perform process \\\"%1 %2\\\", error: %3\").arg(process->program()).arg(process->arguments().join(\" \")).arg(QString::fromUtf8(process->readAllStandardError())));\n        }\n    });\n\n    switch (currentScope) {\n    case DDiskInfo::Headgear: {\n        if (type != DDiskInfo::Disk) {\n            setErrorString(QObject::tr(\"\\\"%1\\\" is not a disk device\").arg(filePath()));\n\n            return false;\n        }\n\n        if (currentMode == DDiskInfo::Read) {\n            process->start(QStringLiteral(\"dd if=%1 bs=512 count=2048 status=none\").arg(filePath()), QIODevice::ReadOnly);\n        } else {\n            process->start(QStringLiteral(\"dd of=%1 bs=512 status=none conv=fsync\").arg(filePath()));\n        }\n\n        break;\n    }\n    case DDiskInfo::PartitionTable: {\n        if (type != DDiskInfo::Disk) {\n            setErrorString(QObject::tr(\"\\\"%1\\\" is not a disk device\").arg(filePath()));\n\n            return false;\n        }\n\n        if (currentMode == DDiskInfo::Read)\n            process->start(QStringLiteral(\"sfdisk -d %1\").arg(filePath()), QIODevice::ReadOnly);\n        else\n            process->start(QStringLiteral(\"sfdisk %1 --no-reread\").arg(filePath()));\n\n        break;\n    }\n    case DDiskInfo::Partition: {\n        const DPartInfo &part = (index == 0 && currentMode == DDiskInfo::Write) ? DDevicePartInfo(filePath()) : q->getPartByNumber(index);\n\n        if (!part) {\n            dCDebug(\"Part is null(index: %d)\", index);\n\n            return false;\n        }\n\n        dCDebug(\"Try open device: %s, mode: %s\", qPrintable(part.filePath()), currentMode == DDiskInfo::Read ? \"Read\" : \"Write\");\n\n        if (Helper::isMounted(part.filePath())) {\n            if (Helper::umountDevice(part.filePath())) {\n                part.d->mountPoint.clear();\n            } else {\n                setErrorString(QObject::tr(\"\\\"%1\\\" is busy\").arg(part.filePath()));\n\n                return false;\n            }\n        }\n\n        if (currentMode == DDiskInfo::Read) {\n            const QString &executer = Helper::getPartcloneExecuter(part);\n            process->start(QStringLiteral(\"%1 -s %2 -o - -c -z %3 -L /var/log/partclone.log\").arg(executer).arg(part.filePath()).arg(Global::bufferSize), QIODevice::ReadOnly);\n        } else {\n            process->start(QStringLiteral(\"partclone.restore -s - -o %2 -z %3 -L /var/log/partclone.log\").arg(part.filePath()).arg(Global::bufferSize));\n        }\n\n        break;\n    }\n    case DDiskInfo::JsonInfo: {\n        process->deleteLater();\n        process = 0;\n        buffer.setData(q->toJson());\n        break;\n    }\n    default:\n        return false;\n    }\n\n    if (process) {\n        if (!process->waitForStarted()) {\n            setErrorString(QObject::tr(\"Failed to start \\\"%1 %2\\\", error: %3\").arg(process->program()).arg(process->arguments().join(\" \")).arg(process->errorString()));\n\n            return false;\n        }\n\n        dCDebug(\"The \\\"%s %s\\\" command start finished\", qPrintable(process->program()), qPrintable(process->arguments().join(\" \")));\n    }\n\n    bool ok = process ? process->isOpen() : buffer.open(QIODevice::ReadOnly);\n\n    if (!ok) {\n        setErrorString(QObject::tr(\"Failed to open process, error: %1\").arg(process ? process->errorString(): buffer.errorString()));\n    }\n\n    return ok;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -71,9 +71,9 @@\n \n         if (currentMode == DDiskInfo::Read) {\n             const QString &executer = Helper::getPartcloneExecuter(part);\n-            process->start(QStringLiteral(\"%1 -s %2 -o - -c -z %3 -L /tmp/partclone.log\").arg(executer).arg(part.filePath()).arg(Global::bufferSize), QIODevice::ReadOnly);\n+            process->start(QStringLiteral(\"%1 -s %2 -o - -c -z %3 -L /var/log/partclone.log\").arg(executer).arg(part.filePath()).arg(Global::bufferSize), QIODevice::ReadOnly);\n         } else {\n-            process->start(QStringLiteral(\"partclone.restore -s - -o %2 -z %3 -L /tmp/partclone.log\").arg(part.filePath()).arg(Global::bufferSize));\n+            process->start(QStringLiteral(\"partclone.restore -s - -o %2 -z %3 -L /var/log/partclone.log\").arg(part.filePath()).arg(Global::bufferSize));\n         }\n \n         break;",
        "diff_line_info": {
            "deleted_lines": [
                "            process->start(QStringLiteral(\"%1 -s %2 -o - -c -z %3 -L /tmp/partclone.log\").arg(executer).arg(part.filePath()).arg(Global::bufferSize), QIODevice::ReadOnly);",
                "            process->start(QStringLiteral(\"partclone.restore -s - -o %2 -z %3 -L /tmp/partclone.log\").arg(part.filePath()).arg(Global::bufferSize));"
            ],
            "added_lines": [
                "            process->start(QStringLiteral(\"%1 -s %2 -o - -c -z %3 -L /var/log/partclone.log\").arg(executer).arg(part.filePath()).arg(Global::bufferSize), QIODevice::ReadOnly);",
                "            process->start(QStringLiteral(\"partclone.restore -s - -o %2 -z %3 -L /var/log/partclone.log\").arg(part.filePath()).arg(Global::bufferSize));"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13226",
        "func_name": "linuxdeepin/deepin-clone/Helper::temporaryMountDevice",
        "description": "deepin-clone before 1.1.3 uses a predictable path /tmp/.deepin-clone/mount/<block-dev-basename> in the Helper::temporaryMountDevice() function to temporarily mount a file system as root. An unprivileged user can prepare a symlink at this location to have the file system mounted in an arbitrary location. By winning a race condition, the attacker can also enter the mount point, thereby preventing a subsequent unmount of the file system.",
        "git_url": "https://github.com/linuxdeepin/deepin-clone/commit/e079f3e2712b4f8c28e3e63e71ba1a1f90fce1ab",
        "commit_title": "fix: Do not use the \"/tmp\" directory",
        "commit_text": " https://github.com/linuxdeepin/deepin-clone/issues/16 https://bugzilla.opensuse.org/show_bug.cgi?id=1130388",
        "func_before": "QString Helper::temporaryMountDevice(const QString &device, const QString &name, bool readonly)\n{\n    QString mount_point = mountPoint(device);\n\n    if (!mount_point.isEmpty())\n        return mount_point;\n\n    mount_point = \"%1/.%2/mount/%3\";\n    const QStringList &tmp_paths = QStandardPaths::standardLocations(QStandardPaths::TempLocation);\n\n    mount_point = mount_point.arg(tmp_paths.isEmpty() ? \"/tmp\" : tmp_paths.first()).arg(qApp->applicationName()).arg(name);\n\n    if (!QDir::current().mkpath(mount_point)) {\n        dCError(\"mkpath \\\"%s\\\" failed\", qPrintable(mount_point));\n\n        return QString();\n    }\n\n    if (!mountDevice(device, mount_point, readonly)) {\n        dCError(\"Mount the device \\\"%s\\\" to \\\"%s\\\" failed\", qPrintable(device), qPrintable(mount_point));\n\n        return QString();\n    }\n\n    return mount_point;\n}",
        "func": "QString Helper::temporaryMountDevice(const QString &device, const QString &name, bool readonly)\n{\n    QString mount_point = mountPoint(device);\n\n    if (!mount_point.isEmpty())\n        return mount_point;\n\n    mount_point = \"%1/.%2/mount/%3\";\n    const QStringList &tmp_paths = QStandardPaths::standardLocations(QStandardPaths::RuntimeLocation);\n\n    mount_point = mount_point.arg(tmp_paths.isEmpty() ? \"/run/user/0\" : tmp_paths.first()).arg(qApp->applicationName()).arg(name);\n\n    if (!QDir::current().mkpath(mount_point)) {\n        dCError(\"mkpath \\\"%s\\\" failed\", qPrintable(mount_point));\n\n        return QString();\n    }\n\n    if (!mountDevice(device, mount_point, readonly)) {\n        dCError(\"Mount the device \\\"%s\\\" to \\\"%s\\\" failed\", qPrintable(device), qPrintable(mount_point));\n\n        return QString();\n    }\n\n    return mount_point;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,9 +6,9 @@\n         return mount_point;\n \n     mount_point = \"%1/.%2/mount/%3\";\n-    const QStringList &tmp_paths = QStandardPaths::standardLocations(QStandardPaths::TempLocation);\n+    const QStringList &tmp_paths = QStandardPaths::standardLocations(QStandardPaths::RuntimeLocation);\n \n-    mount_point = mount_point.arg(tmp_paths.isEmpty() ? \"/tmp\" : tmp_paths.first()).arg(qApp->applicationName()).arg(name);\n+    mount_point = mount_point.arg(tmp_paths.isEmpty() ? \"/run/user/0\" : tmp_paths.first()).arg(qApp->applicationName()).arg(name);\n \n     if (!QDir::current().mkpath(mount_point)) {\n         dCError(\"mkpath \\\"%s\\\" failed\", qPrintable(mount_point));",
        "diff_line_info": {
            "deleted_lines": [
                "    const QStringList &tmp_paths = QStandardPaths::standardLocations(QStandardPaths::TempLocation);",
                "    mount_point = mount_point.arg(tmp_paths.isEmpty() ? \"/tmp\" : tmp_paths.first()).arg(qApp->applicationName()).arg(name);"
            ],
            "added_lines": [
                "    const QStringList &tmp_paths = QStandardPaths::standardLocations(QStandardPaths::RuntimeLocation);",
                "    mount_point = mount_point.arg(tmp_paths.isEmpty() ? \"/run/user/0\" : tmp_paths.first()).arg(qApp->applicationName()).arg(name);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13226",
        "func_name": "linuxdeepin/deepin-clone/Helper::processExec",
        "description": "deepin-clone before 1.1.3 uses a predictable path /tmp/.deepin-clone/mount/<block-dev-basename> in the Helper::temporaryMountDevice() function to temporarily mount a file system as root. An unprivileged user can prepare a symlink at this location to have the file system mounted in an arbitrary location. By winning a race condition, the attacker can also enter the mount point, thereby preventing a subsequent unmount of the file system.",
        "git_url": "https://github.com/linuxdeepin/deepin-clone/commit/e079f3e2712b4f8c28e3e63e71ba1a1f90fce1ab",
        "commit_title": "fix: Do not use the \"/tmp\" directory",
        "commit_text": " https://github.com/linuxdeepin/deepin-clone/issues/16 https://bugzilla.opensuse.org/show_bug.cgi?id=1130388",
        "func_before": "int Helper::processExec(QProcess *process, const QString &command, int timeout, QIODevice::OpenMode mode)\n{\n    m_processStandardOutput.clear();\n    m_processStandardError.clear();\n\n    QEventLoop loop;\n    QTimer timer;\n\n    timer.setSingleShot(true);\n    timer.setInterval(timeout);\n\n    timer.connect(&timer, &QTimer::timeout, &loop, &QEventLoop::quit);\n    loop.connect(process, static_cast<void(QProcess::*)(int)>(&QProcess::finished), &loop, &QEventLoop::exit);\n\n    // 防止子进程输出信息将管道塞满导致进程阻塞\n    process->connect(process, &QProcess::readyReadStandardError, process, [process] {\n        m_processStandardError.append(process->readAllStandardError());\n    });\n    process->connect(process, &QProcess::readyReadStandardOutput, process, [process] {\n        m_processStandardOutput.append(process->readAllStandardOutput());\n    });\n\n    if (timeout > 0) {\n        timer.start();\n    } else {\n        QTimer::singleShot(10000, process, [process] {\n            dCWarning(\"\\\"%s %s\\\" running for more than 10 seconds, state=%d, pid_file_exist=%d\",\n                      qPrintable(process->program()), qPrintable(process->arguments().join(\" \")),\n                      (int)process->state(), (int)QFile::exists(QString(\"/proc/%1\").arg(process->pid())));\n        });\n    }\n\n    if (Global::debugLevel > 1)\n        dCDebug(\"Exec: \\\"%s\\\", timeout: %d\", qPrintable(command), timeout);\n\n    process->start(command, mode);\n    process->waitForStarted();\n\n    if (process->error() != QProcess::UnknownError) {\n        dCError(process->errorString());\n\n        return -1;\n    }\n\n    if (process->state() == QProcess::Running) {\n        loop.exec();\n    }\n\n    if (process->state() != QProcess::NotRunning) {\n        dCDebug(\"The \\\"%s\\\" timeout, timeout: %d\", qPrintable(command), timeout);\n\n        if (QFile::exists(QString(\"/proc/%1\").arg(process->pid()))) {\n            process->terminate();\n            process->waitForFinished();\n        } else {\n            dCDebug(\"The \\\"%s\\\" is quit, but the QProcess object state is not NotRunning\");\n        }\n    }\n\n    m_processStandardOutput.append(process->readAllStandardOutput());\n    m_processStandardError.append(process->readAllStandardError());\n\n    if (Global::debugLevel > 1) {\n        dCDebug(\"Done: \\\"%s\\\", exit code: %d\", qPrintable(command), process->exitCode());\n\n        if (process->exitCode() != 0) {\n            dCError(\"error: \\\"%s\\\"\\nstdout: \\\"%s\\\"\", qPrintable(m_processStandardError), qPrintable(m_processStandardOutput));\n        }\n    }\n\n    return process->exitCode();\n}",
        "func": "int Helper::processExec(QProcess *process, const QString &command, int timeout, QIODevice::OpenMode mode)\n{\n    m_processStandardOutput.clear();\n    m_processStandardError.clear();\n\n    QEventLoop loop;\n    QTimer timer;\n\n    timer.setSingleShot(true);\n    timer.setInterval(timeout);\n\n    timer.connect(&timer, &QTimer::timeout, &loop, &QEventLoop::quit);\n    loop.connect(process, static_cast<void(QProcess::*)(int)>(&QProcess::finished), &loop, &QEventLoop::exit);\n\n    // 防止子进程输出信息将管道塞满导致进程阻塞\n    process->connect(process, &QProcess::readyReadStandardError, process, [process] {\n        m_processStandardError.append(process->readAllStandardError());\n    });\n    process->connect(process, &QProcess::readyReadStandardOutput, process, [process] {\n        m_processStandardOutput.append(process->readAllStandardOutput());\n    });\n\n    if (timeout > 0) {\n        timer.start();\n    } else {\n        QTimer::singleShot(10000, process, [process] {\n            dCWarning(\"\\\"%s %s\\\" running for more than 10 seconds, state=%d, pid_file_exist=%d\",\n                      qPrintable(process->program()), qPrintable(process->arguments().join(\" \")),\n                      (int)process->state(), (int)QFile::exists(QString(\"/proc/%1\").arg(process->pid())));\n        });\n    }\n\n    if (Global::debugLevel > 1)\n        dCDebug(\"Exec: \\\"%s\\\", timeout: %d\", qPrintable(command), timeout);\n\n    process->start(command, mode);\n    process->waitForStarted();\n\n    if (process->error() != QProcess::UnknownError) {\n        dCError(process->errorString());\n\n        return -1;\n    }\n\n    if (process->state() == QProcess::Running) {\n        loop.exec();\n    }\n\n    if (process->state() != QProcess::NotRunning) {\n        dCDebug(\"The \\\"%s\\\" timeout, timeout: %d\", qPrintable(command), timeout);\n\n        // QT Bug，某种情况下(未知) QProcess::state 返回的状态有误，导致进程已退出却未能正确获取到其当前状态\n        // 因此,额外通过系统文件判断进程是否还存在\n        if (QFile::exists(QString(\"/proc/%1\").arg(process->pid()))) {\n            process->terminate();\n            process->waitForFinished();\n        } else {\n            dCDebug(\"The \\\"%s\\\" is quit, but the QProcess object state is not NotRunning\");\n        }\n    }\n\n    m_processStandardOutput.append(process->readAllStandardOutput());\n    m_processStandardError.append(process->readAllStandardError());\n\n    if (Global::debugLevel > 1) {\n        dCDebug(\"Done: \\\"%s\\\", exit code: %d\", qPrintable(command), process->exitCode());\n\n        if (process->exitCode() != 0) {\n            dCError(\"error: \\\"%s\\\"\\nstdout: \\\"%s\\\"\", qPrintable(m_processStandardError), qPrintable(m_processStandardOutput));\n        }\n    }\n\n    return process->exitCode();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -49,6 +49,8 @@\n     if (process->state() != QProcess::NotRunning) {\n         dCDebug(\"The \\\"%s\\\" timeout, timeout: %d\", qPrintable(command), timeout);\n \n+        // QT Bug，某种情况下(未知) QProcess::state 返回的状态有误，导致进程已退出却未能正确获取到其当前状态\n+        // 因此,额外通过系统文件判断进程是否还存在\n         if (QFile::exists(QString(\"/proc/%1\").arg(process->pid()))) {\n             process->terminate();\n             process->waitForFinished();",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        // QT Bug，某种情况下(未知) QProcess::state 返回的状态有误，导致进程已退出却未能正确获取到其当前状态",
                "        // 因此,额外通过系统文件判断进程是否还存在"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13226",
        "func_name": "linuxdeepin/deepin-clone/Helper::getPartitionSizeInfo",
        "description": "deepin-clone before 1.1.3 uses a predictable path /tmp/.deepin-clone/mount/<block-dev-basename> in the Helper::temporaryMountDevice() function to temporarily mount a file system as root. An unprivileged user can prepare a symlink at this location to have the file system mounted in an arbitrary location. By winning a race condition, the attacker can also enter the mount point, thereby preventing a subsequent unmount of the file system.",
        "git_url": "https://github.com/linuxdeepin/deepin-clone/commit/e079f3e2712b4f8c28e3e63e71ba1a1f90fce1ab",
        "commit_title": "fix: Do not use the \"/tmp\" directory",
        "commit_text": " https://github.com/linuxdeepin/deepin-clone/issues/16 https://bugzilla.opensuse.org/show_bug.cgi?id=1130388",
        "func_before": "bool Helper::getPartitionSizeInfo(const QString &partDevice, qint64 *used, qint64 *free, int *blockSize)\n{\n    QProcess process;\n    QStringList env_list = QProcess::systemEnvironment();\n\n    env_list.append(\"LANG=C\");\n    process.setEnvironment(env_list);\n\n    if (Helper::isMounted(partDevice)) {\n        process.start(QString(\"df -B1 -P %1\").arg(partDevice));\n        process.waitForFinished();\n\n        if (process.exitCode() != 0) {\n            dCError(\"Call df failed: %s\", qPrintable(process.readAllStandardError()));\n\n            return false;\n        }\n\n        QByteArray output = process.readAll();\n        const QByteArrayList &lines = output.trimmed().split('\\n');\n\n        if (lines.count() != 2)\n            return false;\n\n        output = lines.last().simplified();\n\n        const QByteArrayList &values = output.split(' ');\n\n        if (values.count() != 6)\n            return false;\n\n        bool ok = false;\n\n        if (used)\n            *used = values.at(2).toLongLong(&ok);\n\n        if (!ok)\n            return false;\n\n        if (free)\n            *free = values.at(3).toLongLong(&ok);\n\n        if (!ok)\n            return false;\n\n        return true;\n    } else {\n        process.start(QString(\"%1 -s %2 -c -q -C -L /tmp/partclone.log\").arg(getPartcloneExecuter(DDevicePartInfo(partDevice))).arg(partDevice));\n        process.setStandardOutputFile(\"/dev/null\");\n        process.setReadChannel(QProcess::StandardError);\n        process.waitForStarted();\n\n        qint64 used_block = -1;\n        qint64 free_block = -1;\n\n        while (process.waitForReadyRead(5000)) {\n            const QByteArray &data = process.readAll();\n\n            for (QByteArray line : data.split('\\n')) {\n                line = line.simplified();\n\n                if (QString::fromLatin1(line).contains(QRegularExpression(\"\\\\berror\\\\b\"))) {\n                    dCError(\"Call \\\"%s %s\\\" failed: \\\"%s\\\"\", qPrintable(process.program()), qPrintable(process.arguments().join(' ')), line.constData());\n                }\n\n                if (line.startsWith(\"Space in use:\")) {\n                    bool ok = false;\n                    const QByteArray &value = line.split(' ').value(6, \"-1\");\n\n                    used_block = value.toLongLong(&ok);\n\n                    if (!ok) {\n                        dCError(\"String to LongLong failed, String: %s\", value.constData());\n\n                        return false;\n                    }\n                } else if (line.startsWith(\"Free Space:\")) {\n                    bool ok = false;\n                    const QByteArray &value = line.split(' ').value(5, \"-1\");\n\n                    free_block = value.toLongLong(&ok);\n\n                    if (!ok) {\n                        dCError(\"String to LongLong failed, String: %s\", value.constData());\n\n                        return false;\n                    }\n                } else if (line.startsWith(\"Block size:\")) {\n                    bool ok = false;\n                    const QByteArray &value = line.split(' ').value(2, \"-1\");\n\n                    int block_size = value.toInt(&ok);\n\n                    if (!ok) {\n                        dCError(\"String to Int failed, String: %s\", value.constData());\n\n                        return false;\n                    }\n\n                    if (used_block < 0 || free_block < 0 || block_size < 0)\n                        return false;\n\n                    if (used)\n                        *used = used_block * block_size;\n\n                    if (free)\n                        *free = free_block * block_size;\n\n                    if (blockSize)\n                        *blockSize = block_size;\n\n                    process.terminate();\n                    process.waitForFinished();\n\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}",
        "func": "bool Helper::getPartitionSizeInfo(const QString &partDevice, qint64 *used, qint64 *free, int *blockSize)\n{\n    QProcess process;\n    QStringList env_list = QProcess::systemEnvironment();\n\n    env_list.append(\"LANG=C\");\n    process.setEnvironment(env_list);\n\n    if (Helper::isMounted(partDevice)) {\n        process.start(QString(\"df -B1 -P %1\").arg(partDevice));\n        process.waitForFinished();\n\n        if (process.exitCode() != 0) {\n            dCError(\"Call df failed: %s\", qPrintable(process.readAllStandardError()));\n\n            return false;\n        }\n\n        QByteArray output = process.readAll();\n        const QByteArrayList &lines = output.trimmed().split('\\n');\n\n        if (lines.count() != 2)\n            return false;\n\n        output = lines.last().simplified();\n\n        const QByteArrayList &values = output.split(' ');\n\n        if (values.count() != 6)\n            return false;\n\n        bool ok = false;\n\n        if (used)\n            *used = values.at(2).toLongLong(&ok);\n\n        if (!ok)\n            return false;\n\n        if (free)\n            *free = values.at(3).toLongLong(&ok);\n\n        if (!ok)\n            return false;\n\n        return true;\n    } else {\n        process.start(QString(\"%1 -s %2 -c -q -C -L /var/log/partclone.log\").arg(getPartcloneExecuter(DDevicePartInfo(partDevice))).arg(partDevice));\n        process.setStandardOutputFile(\"/dev/null\");\n        process.setReadChannel(QProcess::StandardError);\n        process.waitForStarted();\n\n        qint64 used_block = -1;\n        qint64 free_block = -1;\n\n        while (process.waitForReadyRead(5000)) {\n            const QByteArray &data = process.readAll();\n\n            for (QByteArray line : data.split('\\n')) {\n                line = line.simplified();\n\n                if (QString::fromLatin1(line).contains(QRegularExpression(\"\\\\berror\\\\b\"))) {\n                    dCError(\"Call \\\"%s %s\\\" failed: \\\"%s\\\"\", qPrintable(process.program()), qPrintable(process.arguments().join(' ')), line.constData());\n                }\n\n                if (line.startsWith(\"Space in use:\")) {\n                    bool ok = false;\n                    const QByteArray &value = line.split(' ').value(6, \"-1\");\n\n                    used_block = value.toLongLong(&ok);\n\n                    if (!ok) {\n                        dCError(\"String to LongLong failed, String: %s\", value.constData());\n\n                        return false;\n                    }\n                } else if (line.startsWith(\"Free Space:\")) {\n                    bool ok = false;\n                    const QByteArray &value = line.split(' ').value(5, \"-1\");\n\n                    free_block = value.toLongLong(&ok);\n\n                    if (!ok) {\n                        dCError(\"String to LongLong failed, String: %s\", value.constData());\n\n                        return false;\n                    }\n                } else if (line.startsWith(\"Block size:\")) {\n                    bool ok = false;\n                    const QByteArray &value = line.split(' ').value(2, \"-1\");\n\n                    int block_size = value.toInt(&ok);\n\n                    if (!ok) {\n                        dCError(\"String to Int failed, String: %s\", value.constData());\n\n                        return false;\n                    }\n\n                    if (used_block < 0 || free_block < 0 || block_size < 0)\n                        return false;\n\n                    if (used)\n                        *used = used_block * block_size;\n\n                    if (free)\n                        *free = free_block * block_size;\n\n                    if (blockSize)\n                        *blockSize = block_size;\n\n                    process.terminate();\n                    process.waitForFinished();\n\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -45,7 +45,7 @@\n \n         return true;\n     } else {\n-        process.start(QString(\"%1 -s %2 -c -q -C -L /tmp/partclone.log\").arg(getPartcloneExecuter(DDevicePartInfo(partDevice))).arg(partDevice));\n+        process.start(QString(\"%1 -s %2 -c -q -C -L /var/log/partclone.log\").arg(getPartcloneExecuter(DDevicePartInfo(partDevice))).arg(partDevice));\n         process.setStandardOutputFile(\"/dev/null\");\n         process.setReadChannel(QProcess::StandardError);\n         process.waitForStarted();",
        "diff_line_info": {
            "deleted_lines": [
                "        process.start(QString(\"%1 -s %2 -c -q -C -L /tmp/partclone.log\").arg(getPartcloneExecuter(DDevicePartInfo(partDevice))).arg(partDevice));"
            ],
            "added_lines": [
                "        process.start(QString(\"%1 -s %2 -c -q -C -L /var/log/partclone.log\").arg(getPartcloneExecuter(DDevicePartInfo(partDevice))).arg(partDevice));"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13226",
        "func_name": "linuxdeepin/deepin-clone/BootDoctor::fix",
        "description": "deepin-clone before 1.1.3 uses a predictable path /tmp/.deepin-clone/mount/<block-dev-basename> in the Helper::temporaryMountDevice() function to temporarily mount a file system as root. An unprivileged user can prepare a symlink at this location to have the file system mounted in an arbitrary location. By winning a race condition, the attacker can also enter the mount point, thereby preventing a subsequent unmount of the file system.",
        "git_url": "https://github.com/linuxdeepin/deepin-clone/commit/e079f3e2712b4f8c28e3e63e71ba1a1f90fce1ab",
        "commit_title": "fix: Do not use the \"/tmp\" directory",
        "commit_text": " https://github.com/linuxdeepin/deepin-clone/issues/16 https://bugzilla.opensuse.org/show_bug.cgi?id=1130388",
        "func_before": "bool BootDoctor::fix(const QString &partDevice)\n{\n    m_lastErrorString.clear();\n\n    DDevicePartInfo part_info(partDevice);\n    const QString part_old_uuid = part_info.uuid();\n\n    if (Helper::processExec(\"lsblk -s -d -n -o UUID\") == 0) {\n        if (Helper::lastProcessStandardOutput().contains(part_old_uuid.toLatin1())) {\n            // reset uuid\n            if (Helper::resetPartUUID(part_info)) {\n                QThread::sleep(1);\n                part_info.refresh();\n\n                qDebug() << part_old_uuid << part_info.uuid();\n            } else {\n                dCWarning(\"Failed to reset uuid\");\n            }\n        }\n    }\n\n    bool device_is_mounted = Helper::isMounted(partDevice);\n    const QString &mount_root = Helper::temporaryMountDevice(partDevice, QFileInfo(partDevice).fileName());\n\n    if (mount_root.isEmpty()) {\n        m_lastErrorString = QObject::tr(\"Failed to mount partition \\\"%1\\\"\").arg(partDevice);\n        goto failed;\n    }\n\n    {\n        const QStringList &tmp_paths = QStandardPaths::standardLocations(QStandardPaths::TempLocation);\n        const QString tmp_dir = (tmp_paths.isEmpty() ? \"/tmp\" : tmp_paths.first()) + \"/.deepin-clone\";\n\n        if (!QDir::current().mkpath(tmp_dir)) {\n            dCError(\"mkpath \\\"%s\\\" failed\", qPrintable(tmp_dir));\n            goto failed;\n        }\n\n        const QString &repo_path = tmp_dir + \"/repo.iso\";\n\n        if (!QFile::exists(repo_path)\n                && !QFile::copy(QString(\":/repo_%1.iso\").arg(HOST_ARCH), repo_path)) {\n            dCError(\"copy file failed, new name: %s\", qPrintable(repo_path));\n            goto failed;\n        }\n\n        bool ok = false;\n\n        const QString &repo_mount_point = mount_root + \"/deepin-clone\";\n        QFile file_boot_fix(mount_root + \"/boot_fix.sh\");\n\n        do {\n            if (!QDir(mount_root).exists(\"deepin-clone\") && !QDir(mount_root).mkdir(\"deepin-clone\")) {\n                dCError(\"Create \\\"deepin-clone\\\" dir failed(\\\"%s\\\")\", qPrintable(mount_root));\n                break;\n            }\n\n            if (!Helper::mountDevice(repo_path, repo_mount_point, true)) {\n                m_lastErrorString = QObject::tr(\"Failed to mount partition \\\"%1\\\"\").arg(repo_path);\n                break;\n            }\n\n            if (file_boot_fix.exists()) {\n                file_boot_fix.remove();\n            }\n\n            if (!QFile::copy(QString(\":/scripts/boot_fix_%1.sh\").arg(\n                     #if defined(HOST_ARCH_x86_64) || defined(HOST_ARCH_i386) || defined(HOST_ARCH_i686)\n                                 \"x86\"\n                     #elif defined(HOST_ARCH_mips64) || defined(HOST_ARCH_mips32)\n                                 \"mips\"\n                     #elif defined(HOST_ARCH_sw_64)\n                                 \"sw_64\"\n                     #elif defined(HOST_ARCH_aarch64)\n                                 \"aarch64\"\n                     #else\n                     #pragma message \"Machine: \" HOST_ARCH\n                                \"unknow\"\n                     #endif\n                                 ), file_boot_fix.fileName())) {\n                dCError(\"copy file failed, new name: %s\", qPrintable(file_boot_fix.fileName()));\n                break;\n            }\n\n            if (!file_boot_fix.setPermissions(file_boot_fix.permissions() | QFile::ExeUser)) {\n                dCError(\"Set \\\"%s\\\" permissions failed\", qPrintable(file_boot_fix.fileName()));\n                break;\n            }\n\n            if (Helper::processExec(QString(\"mount --bind -v --bind /dev %1/dev\").arg(mount_root)) != 0) {\n                dCError(\"Failed to bind /dev\");\n                break;\n            }\n\n            if (Helper::processExec(QString(\"mount --bind -v --bind /dev/pts %1/dev/pts\").arg(mount_root)) != 0) {\n                dCError(\"Failed to bind /dev/pts\");\n                break;\n            }\n\n            if (Helper::processExec(QString(\"mount --bind -v --bind /proc %1/proc\").arg(mount_root)) != 0) {\n                dCError(\"Failed to bind /proc\");\n                break;\n            }\n\n            if (Helper::processExec(QString(\"mount --bind -v --bind /sys %1/sys\").arg(mount_root)) != 0) {\n                dCError(\"Failed to bind /sys\");\n                break;\n            }\n\n            ok = true;\n        } while (0);\n\n        QProcess process;\n\n        if (ok) {\n            const QString &parent_device = Helper::parentDevice(partDevice);\n\n            bool is_efi = false;\n\n            if (!parent_device.isEmpty()) {\n                DDeviceDiskInfo info(parent_device);\n\n                dCDebug(\"Disk partition table type: %d\", info.ptType());\n\n                if (info.ptType() == DDeviceDiskInfo::GPT) {\n                    for (const DPartInfo &part : info.childrenPartList()) {\n                        if (part.guidType() == DPartInfo::EFI_SP_None) {\n                            const QString &efi_path = mount_root + \"/boot/efi\";\n\n                            QDir::current().mkpath(efi_path);\n\n                            if (Helper::processExec(QString(\"mount %1 %2\").arg(part.filePath()).arg(efi_path)) != 0) {\n                                dCError(\"Failed to mount EFI partition\");\n                                m_lastErrorString = QObject::tr(\"Failed to mount partition \\\"%1\\\"\").arg(part.filePath());\n                                ok = false;\n                                break;\n                            }\n\n                            is_efi = true;\n\n                            break;\n                        }\n                    }\n\n                    if (!is_efi && m_lastErrorString.isEmpty()) {\n                        m_lastErrorString = QObject::tr(\"EFI partition not found\");\n                        ok = false;\n                    }\n                } else if (info.ptType() == DDeviceDiskInfo::Unknow) {\n                    m_lastErrorString = QObject::tr(\"Unknown partition style\");\n                    ok = false;\n                }\n            }\n\n            if (ok) {\n                process.setProcessChannelMode(QProcess::MergedChannels);\n                process.start(QString(\"chroot %1 ./boot_fix.sh %2 %3 /deepin-clone\")\n                              .arg(mount_root)\n                              .arg(parent_device)\n                              .arg(is_efi ? \"true\" : \"false\"));\n\n                while (process.waitForReadyRead()) {\n                    const QByteArray &data = process.readAll().simplified().constData();\n\n                    dCDebug(data.constData());\n                }\n\n                process.waitForFinished(-1);\n\n                switch (process.exitCode()) {\n                case 1:\n                    m_lastErrorString = QObject::tr(\"Boot for install system failed\");\n                    break;\n                case 2:\n                    m_lastErrorString = QObject::tr(\"Boot for update system failed\");\n                    break;\n                default:\n                    break;\n                }\n            }\n        }\n\n        // clear\n        Helper::processExec(\"umount \" + repo_mount_point);\n        QDir(mount_root).rmdir(\"deepin-clone\");\n        file_boot_fix.remove();\n        Helper::processExec(\"umount \" + mount_root + \"/dev/pts\");\n        Helper::processExec(\"umount \" + mount_root + \"/dev\");\n        Helper::processExec(\"umount \" + mount_root + \"/proc\");\n        Helper::processExec(\"umount \" + mount_root + \"/sys\");\n        Helper::processExec(\"umount \" + mount_root + \"/boot/efi\");\n\n        if (ok && process.exitCode() == 0) {\n            if (part_old_uuid != part_info.uuid()) {\n                dCDebug(\"Reset the uuid from \\\"%s\\\" to \\\"%s\\\"\", qPrintable(part_old_uuid), qPrintable(part_info.uuid()));\n\n                // update /etc/fstab\n                QFile file(mount_root + \"/etc/fstab\");\n\n                if (file.exists() && file.open(QIODevice::ReadWrite)) {\n                    QByteArray data = file.readAll();\n\n                    if (file.seek(0)) {\n                        file.write(data.replace(part_old_uuid.toLatin1(), part_info.uuid().toLatin1()));\n                    }\n\n                    file.close();\n                } else {\n                    dCWarning(\"Failed to update /etc/fstab, error: %s\", qPrintable(file.errorString()));\n                }\n\n                file.setFileName(mount_root + \"/etc/crypttab\");\n\n                if (file.exists() && file.open(QIODevice::ReadWrite)) {\n                    QByteArray data = file.readAll();\n\n                    if (file.seek(0)) {\n                        file.write(data.replace(part_old_uuid.toLatin1(), part_info.uuid().toLatin1()));\n                    }\n\n                    file.close();\n                } else {\n                    dCWarning(\"Failed to update /etc/crypttab, error: %s\", qPrintable(file.errorString()));\n                }\n            }\n\n            if (!device_is_mounted)\n                Helper::umountDevice(partDevice);\n\n            return true;\n        }\n    }\n\nfailed:\n    if (!device_is_mounted)\n        Helper::umountDevice(partDevice);\n\n    if (m_lastErrorString.isEmpty())\n        m_lastErrorString = QObject::tr(\"Boot for repair system failed\");\n\n    dCDebug(\"Restore partition uuid\");\n\n    if (!Helper::resetPartUUID(part_info, part_old_uuid.toLatin1())) {\n        dCWarning(\"Failed to restore partition uuid, part: %s, uuid: %s\", qPrintable(partDevice), qPrintable(part_old_uuid));\n    }\n\n    return false;\n}",
        "func": "bool BootDoctor::fix(const QString &partDevice)\n{\n    m_lastErrorString.clear();\n\n    DDevicePartInfo part_info(partDevice);\n    const QString part_old_uuid = part_info.uuid();\n\n    if (Helper::processExec(\"lsblk -s -d -n -o UUID\") == 0) {\n        if (Helper::lastProcessStandardOutput().contains(part_old_uuid.toLatin1())) {\n            // reset uuid\n            if (Helper::resetPartUUID(part_info)) {\n                QThread::sleep(1);\n                part_info.refresh();\n\n                qDebug() << part_old_uuid << part_info.uuid();\n            } else {\n                dCWarning(\"Failed to reset uuid\");\n            }\n        }\n    }\n\n    bool device_is_mounted = Helper::isMounted(partDevice);\n    const QString &mount_root = Helper::temporaryMountDevice(partDevice, QFileInfo(partDevice).fileName());\n\n    if (mount_root.isEmpty()) {\n        m_lastErrorString = QObject::tr(\"Failed to mount partition \\\"%1\\\"\").arg(partDevice);\n        goto failed;\n    }\n\n    {\n        const QString tmp_dir = \"/var/cache/deepin-clone\";\n\n        if (!QDir::current().mkpath(tmp_dir)) {\n            dCError(\"mkpath \\\"%s\\\" failed\", qPrintable(tmp_dir));\n            goto failed;\n        }\n\n        const QString &repo_path = tmp_dir + \"/repo.iso\";\n\n        if (!QFile::exists(repo_path)\n                && !QFile::copy(QString(\":/repo_%1.iso\").arg(HOST_ARCH), repo_path)) {\n            dCError(\"copy file failed, new name: %s\", qPrintable(repo_path));\n            goto failed;\n        }\n\n        bool ok = false;\n\n        const QString &repo_mount_point = mount_root + \"/deepin-clone\";\n        QFile file_boot_fix(mount_root + \"/boot_fix.sh\");\n\n        do {\n            if (!QDir(mount_root).exists(\"deepin-clone\") && !QDir(mount_root).mkdir(\"deepin-clone\")) {\n                dCError(\"Create \\\"deepin-clone\\\" dir failed(\\\"%s\\\")\", qPrintable(mount_root));\n                break;\n            }\n\n            if (!Helper::mountDevice(repo_path, repo_mount_point, true)) {\n                m_lastErrorString = QObject::tr(\"Failed to mount partition \\\"%1\\\"\").arg(repo_path);\n                break;\n            }\n\n            if (file_boot_fix.exists()) {\n                file_boot_fix.remove();\n            }\n\n            if (!QFile::copy(QString(\":/scripts/boot_fix_%1.sh\").arg(\n                     #if defined(HOST_ARCH_x86_64) || defined(HOST_ARCH_i386) || defined(HOST_ARCH_i686)\n                                 \"x86\"\n                     #elif defined(HOST_ARCH_mips64) || defined(HOST_ARCH_mips32)\n                                 \"mips\"\n                     #elif defined(HOST_ARCH_sw_64)\n                                 \"sw_64\"\n                     #elif defined(HOST_ARCH_aarch64)\n                                 \"aarch64\"\n                     #else\n                     #pragma message \"Machine: \" HOST_ARCH\n                                \"unknow\"\n                     #endif\n                                 ), file_boot_fix.fileName())) {\n                dCError(\"copy file failed, new name: %s\", qPrintable(file_boot_fix.fileName()));\n                break;\n            }\n\n            if (!file_boot_fix.setPermissions(file_boot_fix.permissions() | QFile::ExeUser)) {\n                dCError(\"Set \\\"%s\\\" permissions failed\", qPrintable(file_boot_fix.fileName()));\n                break;\n            }\n\n            if (Helper::processExec(QString(\"mount --bind -v --bind /dev %1/dev\").arg(mount_root)) != 0) {\n                dCError(\"Failed to bind /dev\");\n                break;\n            }\n\n            if (Helper::processExec(QString(\"mount --bind -v --bind /dev/pts %1/dev/pts\").arg(mount_root)) != 0) {\n                dCError(\"Failed to bind /dev/pts\");\n                break;\n            }\n\n            if (Helper::processExec(QString(\"mount --bind -v --bind /proc %1/proc\").arg(mount_root)) != 0) {\n                dCError(\"Failed to bind /proc\");\n                break;\n            }\n\n            if (Helper::processExec(QString(\"mount --bind -v --bind /sys %1/sys\").arg(mount_root)) != 0) {\n                dCError(\"Failed to bind /sys\");\n                break;\n            }\n\n            ok = true;\n        } while (0);\n\n        QProcess process;\n\n        if (ok) {\n            const QString &parent_device = Helper::parentDevice(partDevice);\n\n            bool is_efi = false;\n\n            if (!parent_device.isEmpty()) {\n                DDeviceDiskInfo info(parent_device);\n\n                dCDebug(\"Disk partition table type: %d\", info.ptType());\n\n                if (info.ptType() == DDeviceDiskInfo::GPT) {\n                    for (const DPartInfo &part : info.childrenPartList()) {\n                        if (part.guidType() == DPartInfo::EFI_SP_None) {\n                            const QString &efi_path = mount_root + \"/boot/efi\";\n\n                            QDir::current().mkpath(efi_path);\n\n                            if (Helper::processExec(QString(\"mount %1 %2\").arg(part.filePath()).arg(efi_path)) != 0) {\n                                dCError(\"Failed to mount EFI partition\");\n                                m_lastErrorString = QObject::tr(\"Failed to mount partition \\\"%1\\\"\").arg(part.filePath());\n                                ok = false;\n                                break;\n                            }\n\n                            is_efi = true;\n\n                            break;\n                        }\n                    }\n\n                    if (!is_efi && m_lastErrorString.isEmpty()) {\n                        m_lastErrorString = QObject::tr(\"EFI partition not found\");\n                        ok = false;\n                    }\n                } else if (info.ptType() == DDeviceDiskInfo::Unknow) {\n                    m_lastErrorString = QObject::tr(\"Unknown partition style\");\n                    ok = false;\n                }\n            }\n\n            if (ok) {\n                process.setProcessChannelMode(QProcess::MergedChannels);\n                process.start(QString(\"chroot %1 ./boot_fix.sh %2 %3 /deepin-clone\")\n                              .arg(mount_root)\n                              .arg(parent_device)\n                              .arg(is_efi ? \"true\" : \"false\"));\n\n                while (process.waitForReadyRead()) {\n                    const QByteArray &data = process.readAll().simplified().constData();\n\n                    dCDebug(data.constData());\n                }\n\n                process.waitForFinished(-1);\n\n                switch (process.exitCode()) {\n                case 1:\n                    m_lastErrorString = QObject::tr(\"Boot for install system failed\");\n                    break;\n                case 2:\n                    m_lastErrorString = QObject::tr(\"Boot for update system failed\");\n                    break;\n                default:\n                    break;\n                }\n            }\n        }\n\n        // clear\n        Helper::processExec(\"umount \" + repo_mount_point);\n        QDir(mount_root).rmdir(\"deepin-clone\");\n        file_boot_fix.remove();\n        Helper::processExec(\"umount \" + mount_root + \"/dev/pts\");\n        Helper::processExec(\"umount \" + mount_root + \"/dev\");\n        Helper::processExec(\"umount \" + mount_root + \"/proc\");\n        Helper::processExec(\"umount \" + mount_root + \"/sys\");\n        Helper::processExec(\"umount \" + mount_root + \"/boot/efi\");\n\n        if (ok && process.exitCode() == 0) {\n            if (part_old_uuid != part_info.uuid()) {\n                dCDebug(\"Reset the uuid from \\\"%s\\\" to \\\"%s\\\"\", qPrintable(part_old_uuid), qPrintable(part_info.uuid()));\n\n                // update /etc/fstab\n                QFile file(mount_root + \"/etc/fstab\");\n\n                if (file.exists() && file.open(QIODevice::ReadWrite)) {\n                    QByteArray data = file.readAll();\n\n                    if (file.seek(0)) {\n                        file.write(data.replace(part_old_uuid.toLatin1(), part_info.uuid().toLatin1()));\n                    }\n\n                    file.close();\n                } else {\n                    dCWarning(\"Failed to update /etc/fstab, error: %s\", qPrintable(file.errorString()));\n                }\n\n                file.setFileName(mount_root + \"/etc/crypttab\");\n\n                if (file.exists() && file.open(QIODevice::ReadWrite)) {\n                    QByteArray data = file.readAll();\n\n                    if (file.seek(0)) {\n                        file.write(data.replace(part_old_uuid.toLatin1(), part_info.uuid().toLatin1()));\n                    }\n\n                    file.close();\n                } else {\n                    dCWarning(\"Failed to update /etc/crypttab, error: %s\", qPrintable(file.errorString()));\n                }\n            }\n\n            if (!device_is_mounted)\n                Helper::umountDevice(partDevice);\n\n            return true;\n        }\n    }\n\nfailed:\n    if (!device_is_mounted)\n        Helper::umountDevice(partDevice);\n\n    if (m_lastErrorString.isEmpty())\n        m_lastErrorString = QObject::tr(\"Boot for repair system failed\");\n\n    dCDebug(\"Restore partition uuid\");\n\n    if (!Helper::resetPartUUID(part_info, part_old_uuid.toLatin1())) {\n        dCWarning(\"Failed to restore partition uuid, part: %s, uuid: %s\", qPrintable(partDevice), qPrintable(part_old_uuid));\n    }\n\n    return false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,8 +28,7 @@\n     }\n \n     {\n-        const QStringList &tmp_paths = QStandardPaths::standardLocations(QStandardPaths::TempLocation);\n-        const QString tmp_dir = (tmp_paths.isEmpty() ? \"/tmp\" : tmp_paths.first()) + \"/.deepin-clone\";\n+        const QString tmp_dir = \"/var/cache/deepin-clone\";\n \n         if (!QDir::current().mkpath(tmp_dir)) {\n             dCError(\"mkpath \\\"%s\\\" failed\", qPrintable(tmp_dir));",
        "diff_line_info": {
            "deleted_lines": [
                "        const QStringList &tmp_paths = QStandardPaths::standardLocations(QStandardPaths::TempLocation);",
                "        const QString tmp_dir = (tmp_paths.isEmpty() ? \"/tmp\" : tmp_paths.first()) + \"/.deepin-clone\";"
            ],
            "added_lines": [
                "        const QString tmp_dir = \"/var/cache/deepin-clone\";"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13233",
        "func_name": "torvalds/linux/insn_get_seg_base",
        "description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation.",
        "git_url": "https://github.com/torvalds/linux/commit/de9f869616dd95e95c00bdd6b0fcd3421e8a4323",
        "commit_title": "x86/insn-eval: Fix use-after-free access to LDT entry",
        "commit_text": " get_desc() computes a pointer into the LDT while holding a lock that protects the LDT from being freed, but then drops the lock and returns the (now potentially dangling) pointer to its caller.  Fix it by giving the caller a copy of the LDT entry instead.  Cc: stable@vger.kernel.org",
        "func_before": "unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn -1L;\n\n\tif (v8086_mode(regs))\n\t\t/*\n\t\t * Base is simply the segment selector shifted 4\n\t\t * bits to the right.\n\t\t */\n\t\treturn (unsigned long)(sel << 4);\n\n\tif (user_64bit_mode(regs)) {\n\t\t/*\n\t\t * Only FS or GS will have a base address, the rest of\n\t\t * the segments' bases are forced to 0.\n\t\t */\n\t\tunsigned long base;\n\n\t\tif (seg_reg_idx == INAT_SEG_REG_FS)\n\t\t\trdmsrl(MSR_FS_BASE, base);\n\t\telse if (seg_reg_idx == INAT_SEG_REG_GS)\n\t\t\t/*\n\t\t\t * swapgs was called at the kernel entry point. Thus,\n\t\t\t * MSR_KERNEL_GS_BASE will have the user-space GS base.\n\t\t\t */\n\t\t\trdmsrl(MSR_KERNEL_GS_BASE, base);\n\t\telse\n\t\t\tbase = 0;\n\t\treturn base;\n\t}\n\n\t/* In protected mode the segment selector cannot be null. */\n\tif (!sel)\n\t\treturn -1L;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -1L;\n\n\treturn get_desc_base(desc);\n}",
        "func": "unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn -1L;\n\n\tif (v8086_mode(regs))\n\t\t/*\n\t\t * Base is simply the segment selector shifted 4\n\t\t * bits to the right.\n\t\t */\n\t\treturn (unsigned long)(sel << 4);\n\n\tif (user_64bit_mode(regs)) {\n\t\t/*\n\t\t * Only FS or GS will have a base address, the rest of\n\t\t * the segments' bases are forced to 0.\n\t\t */\n\t\tunsigned long base;\n\n\t\tif (seg_reg_idx == INAT_SEG_REG_FS)\n\t\t\trdmsrl(MSR_FS_BASE, base);\n\t\telse if (seg_reg_idx == INAT_SEG_REG_GS)\n\t\t\t/*\n\t\t\t * swapgs was called at the kernel entry point. Thus,\n\t\t\t * MSR_KERNEL_GS_BASE will have the user-space GS base.\n\t\t\t */\n\t\t\trdmsrl(MSR_KERNEL_GS_BASE, base);\n\t\telse\n\t\t\tbase = 0;\n\t\treturn base;\n\t}\n\n\t/* In protected mode the segment selector cannot be null. */\n\tif (!sel)\n\t\treturn -1L;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn -1L;\n\n\treturn get_desc_base(&desc);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n {\n-\tstruct desc_struct *desc;\n+\tstruct desc_struct desc;\n \tshort sel;\n \n \tsel = get_segment_selector(regs, seg_reg_idx);\n@@ -38,9 +38,8 @@\n \tif (!sel)\n \t\treturn -1L;\n \n-\tdesc = get_desc(sel);\n-\tif (!desc)\n+\tif (!get_desc(&desc, sel))\n \t\treturn -1L;\n \n-\treturn get_desc_base(desc);\n+\treturn get_desc_base(&desc);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct desc_struct *desc;",
                "\tdesc = get_desc(sel);",
                "\tif (!desc)",
                "\treturn get_desc_base(desc);"
            ],
            "added_lines": [
                "\tstruct desc_struct desc;",
                "\tif (!get_desc(&desc, sel))",
                "\treturn get_desc_base(&desc);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13233",
        "func_name": "torvalds/linux/get_desc",
        "description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation.",
        "git_url": "https://github.com/torvalds/linux/commit/de9f869616dd95e95c00bdd6b0fcd3421e8a4323",
        "commit_title": "x86/insn-eval: Fix use-after-free access to LDT entry",
        "commit_text": " get_desc() computes a pointer into the LDT while holding a lock that protects the LDT from being freed, but then drops the lock and returns the (now potentially dangling) pointer to its caller.  Fix it by giving the caller a copy of the LDT entry instead.  Cc: stable@vger.kernel.org",
        "func_before": "static struct desc_struct *get_desc(unsigned short sel)\n{\n\tstruct desc_ptr gdt_desc = {0, 0};\n\tunsigned long desc_base;\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\tif ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT) {\n\t\tstruct desc_struct *desc = NULL;\n\t\tstruct ldt_struct *ldt;\n\n\t\t/* Bits [15:3] contain the index of the desired entry. */\n\t\tsel >>= 3;\n\n\t\tmutex_lock(&current->active_mm->context.lock);\n\t\tldt = current->active_mm->context.ldt;\n\t\tif (ldt && sel < ldt->nr_entries)\n\t\t\tdesc = &ldt->entries[sel];\n\n\t\tmutex_unlock(&current->active_mm->context.lock);\n\n\t\treturn desc;\n\t}\n#endif\n\tnative_store_gdt(&gdt_desc);\n\n\t/*\n\t * Segment descriptors have a size of 8 bytes. Thus, the index is\n\t * multiplied by 8 to obtain the memory offset of the desired descriptor\n\t * from the base of the GDT. As bits [15:3] of the segment selector\n\t * contain the index, it can be regarded as multiplied by 8 already.\n\t * All that remains is to clear bits [2:0].\n\t */\n\tdesc_base = sel & ~(SEGMENT_RPL_MASK | SEGMENT_TI_MASK);\n\n\tif (desc_base > gdt_desc.size)\n\t\treturn NULL;\n\n\treturn (struct desc_struct *)(gdt_desc.address + desc_base);\n}",
        "func": "static bool get_desc(struct desc_struct *out, unsigned short sel)\n{\n\tstruct desc_ptr gdt_desc = {0, 0};\n\tunsigned long desc_base;\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\tif ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT) {\n\t\tbool success = false;\n\t\tstruct ldt_struct *ldt;\n\n\t\t/* Bits [15:3] contain the index of the desired entry. */\n\t\tsel >>= 3;\n\n\t\tmutex_lock(&current->active_mm->context.lock);\n\t\tldt = current->active_mm->context.ldt;\n\t\tif (ldt && sel < ldt->nr_entries) {\n\t\t\t*out = ldt->entries[sel];\n\t\t\tsuccess = true;\n\t\t}\n\n\t\tmutex_unlock(&current->active_mm->context.lock);\n\n\t\treturn success;\n\t}\n#endif\n\tnative_store_gdt(&gdt_desc);\n\n\t/*\n\t * Segment descriptors have a size of 8 bytes. Thus, the index is\n\t * multiplied by 8 to obtain the memory offset of the desired descriptor\n\t * from the base of the GDT. As bits [15:3] of the segment selector\n\t * contain the index, it can be regarded as multiplied by 8 already.\n\t * All that remains is to clear bits [2:0].\n\t */\n\tdesc_base = sel & ~(SEGMENT_RPL_MASK | SEGMENT_TI_MASK);\n\n\tif (desc_base > gdt_desc.size)\n\t\treturn false;\n\n\t*out = *(struct desc_struct *)(gdt_desc.address + desc_base);\n\treturn true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,11 @@\n-static struct desc_struct *get_desc(unsigned short sel)\n+static bool get_desc(struct desc_struct *out, unsigned short sel)\n {\n \tstruct desc_ptr gdt_desc = {0, 0};\n \tunsigned long desc_base;\n \n #ifdef CONFIG_MODIFY_LDT_SYSCALL\n \tif ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT) {\n-\t\tstruct desc_struct *desc = NULL;\n+\t\tbool success = false;\n \t\tstruct ldt_struct *ldt;\n \n \t\t/* Bits [15:3] contain the index of the desired entry. */\n@@ -13,12 +13,14 @@\n \n \t\tmutex_lock(&current->active_mm->context.lock);\n \t\tldt = current->active_mm->context.ldt;\n-\t\tif (ldt && sel < ldt->nr_entries)\n-\t\t\tdesc = &ldt->entries[sel];\n+\t\tif (ldt && sel < ldt->nr_entries) {\n+\t\t\t*out = ldt->entries[sel];\n+\t\t\tsuccess = true;\n+\t\t}\n \n \t\tmutex_unlock(&current->active_mm->context.lock);\n \n-\t\treturn desc;\n+\t\treturn success;\n \t}\n #endif\n \tnative_store_gdt(&gdt_desc);\n@@ -33,7 +35,8 @@\n \tdesc_base = sel & ~(SEGMENT_RPL_MASK | SEGMENT_TI_MASK);\n \n \tif (desc_base > gdt_desc.size)\n-\t\treturn NULL;\n+\t\treturn false;\n \n-\treturn (struct desc_struct *)(gdt_desc.address + desc_base);\n+\t*out = *(struct desc_struct *)(gdt_desc.address + desc_base);\n+\treturn true;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static struct desc_struct *get_desc(unsigned short sel)",
                "\t\tstruct desc_struct *desc = NULL;",
                "\t\tif (ldt && sel < ldt->nr_entries)",
                "\t\t\tdesc = &ldt->entries[sel];",
                "\t\treturn desc;",
                "\t\treturn NULL;",
                "\treturn (struct desc_struct *)(gdt_desc.address + desc_base);"
            ],
            "added_lines": [
                "static bool get_desc(struct desc_struct *out, unsigned short sel)",
                "\t\tbool success = false;",
                "\t\tif (ldt && sel < ldt->nr_entries) {",
                "\t\t\t*out = ldt->entries[sel];",
                "\t\t\tsuccess = true;",
                "\t\t}",
                "\t\treturn success;",
                "\t\treturn false;",
                "\t*out = *(struct desc_struct *)(gdt_desc.address + desc_base);",
                "\treturn true;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13233",
        "func_name": "torvalds/linux/insn_get_code_seg_params",
        "description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation.",
        "git_url": "https://github.com/torvalds/linux/commit/de9f869616dd95e95c00bdd6b0fcd3421e8a4323",
        "commit_title": "x86/insn-eval: Fix use-after-free access to LDT entry",
        "commit_text": " get_desc() computes a pointer into the LDT while holding a lock that protects the LDT from being freed, but then drops the lock and returns the (now potentially dangling) pointer to its caller.  Fix it by giving the caller a copy of the LDT entry instead.  Cc: stable@vger.kernel.org",
        "func_before": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc->type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc->l << 1) | desc->d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "func": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc.type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc.l << 1) | desc.d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n int insn_get_code_seg_params(struct pt_regs *regs)\n {\n-\tstruct desc_struct *desc;\n+\tstruct desc_struct desc;\n \tshort sel;\n \n \tif (v8086_mode(regs))\n@@ -11,8 +11,7 @@\n \tif (sel < 0)\n \t\treturn sel;\n \n-\tdesc = get_desc(sel);\n-\tif (!desc)\n+\tif (!get_desc(&desc, sel))\n \t\treturn -EINVAL;\n \n \t/*\n@@ -20,10 +19,10 @@\n \t * determines whether a segment contains data or code. If this is a data\n \t * segment, return error.\n \t */\n-\tif (!(desc->type & BIT(3)))\n+\tif (!(desc.type & BIT(3)))\n \t\treturn -EINVAL;\n \n-\tswitch ((desc->l << 1) | desc->d) {\n+\tswitch ((desc.l << 1) | desc.d) {\n \tcase 0: /*\n \t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n \t\t * both 16-bit.",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct desc_struct *desc;",
                "\tdesc = get_desc(sel);",
                "\tif (!desc)",
                "\tif (!(desc->type & BIT(3)))",
                "\tswitch ((desc->l << 1) | desc->d) {"
            ],
            "added_lines": [
                "\tstruct desc_struct desc;",
                "\tif (!get_desc(&desc, sel))",
                "\tif (!(desc.type & BIT(3)))",
                "\tswitch ((desc.l << 1) | desc.d) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13233",
        "func_name": "torvalds/linux/get_seg_limit",
        "description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation.",
        "git_url": "https://github.com/torvalds/linux/commit/de9f869616dd95e95c00bdd6b0fcd3421e8a4323",
        "commit_title": "x86/insn-eval: Fix use-after-free access to LDT entry",
        "commit_text": " get_desc() computes a pointer into the LDT while holding a lock that protects the LDT from being freed, but then drops the lock and returns the (now potentially dangling) pointer to its caller.  Fix it by giving the caller a copy of the LDT entry instead.  Cc: stable@vger.kernel.org",
        "func_before": "static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct *desc;\n\tunsigned long limit;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn 0;\n\n\tif (user_64bit_mode(regs) || v8086_mode(regs))\n\t\treturn -1L;\n\n\tif (!sel)\n\t\treturn 0;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn 0;\n\n\t/*\n\t * If the granularity bit is set, the limit is given in multiples\n\t * of 4096. This also means that the 12 least significant bits are\n\t * not tested when checking the segment limits. In practice,\n\t * this means that the segment ends in (limit << 12) + 0xfff.\n\t */\n\tlimit = get_desc_limit(desc);\n\tif (desc->g)\n\t\tlimit = (limit << 12) + 0xfff;\n\n\treturn limit;\n}",
        "func": "static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct desc;\n\tunsigned long limit;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn 0;\n\n\tif (user_64bit_mode(regs) || v8086_mode(regs))\n\t\treturn -1L;\n\n\tif (!sel)\n\t\treturn 0;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn 0;\n\n\t/*\n\t * If the granularity bit is set, the limit is given in multiples\n\t * of 4096. This also means that the 12 least significant bits are\n\t * not tested when checking the segment limits. In practice,\n\t * this means that the segment ends in (limit << 12) + 0xfff.\n\t */\n\tlimit = get_desc_limit(&desc);\n\tif (desc.g)\n\t\tlimit = (limit << 12) + 0xfff;\n\n\treturn limit;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n {\n-\tstruct desc_struct *desc;\n+\tstruct desc_struct desc;\n \tunsigned long limit;\n \tshort sel;\n \n@@ -14,8 +14,7 @@\n \tif (!sel)\n \t\treturn 0;\n \n-\tdesc = get_desc(sel);\n-\tif (!desc)\n+\tif (!get_desc(&desc, sel))\n \t\treturn 0;\n \n \t/*\n@@ -24,8 +23,8 @@\n \t * not tested when checking the segment limits. In practice,\n \t * this means that the segment ends in (limit << 12) + 0xfff.\n \t */\n-\tlimit = get_desc_limit(desc);\n-\tif (desc->g)\n+\tlimit = get_desc_limit(&desc);\n+\tif (desc.g)\n \t\tlimit = (limit << 12) + 0xfff;\n \n \treturn limit;",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct desc_struct *desc;",
                "\tdesc = get_desc(sel);",
                "\tif (!desc)",
                "\tlimit = get_desc_limit(desc);",
                "\tif (desc->g)"
            ],
            "added_lines": [
                "\tstruct desc_struct desc;",
                "\tif (!get_desc(&desc, sel))",
                "\tlimit = get_desc_limit(&desc);",
                "\tif (desc.g)"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11922",
        "func_name": "facebook/zstd/ZSTD_encodeSequences",
        "description": "A race condition in the one-pass compression functions of Zstandard prior to version 1.3.8 could allow an attacker to write bytes out of bounds if an output buffer smaller than the recommended size was used.",
        "git_url": "https://github.com/facebook/zstd/commit/3e5cdf1b6a85843e991d7d10f6a2567c15580da0",
        "commit_title": "fixed T36302429",
        "commit_text": "",
        "func_before": "static size_t ZSTD_encodeSequences(\n            void* dst, size_t dstCapacity,\n            FSE_CTable const* CTable_MatchLength, BYTE const* mlCodeTable,\n            FSE_CTable const* CTable_OffsetBits, BYTE const* ofCodeTable,\n            FSE_CTable const* CTable_LitLength, BYTE const* llCodeTable,\n            seqDef const* sequences, size_t nbSeq, int longOffsets, int bmi2)\n{\n#if DYNAMIC_BMI2\n    if (bmi2) {\n        return ZSTD_encodeSequences_bmi2(dst, dstCapacity,\n                                         CTable_MatchLength, mlCodeTable,\n                                         CTable_OffsetBits, ofCodeTable,\n                                         CTable_LitLength, llCodeTable,\n                                         sequences, nbSeq, longOffsets);\n    }\n#endif\n    (void)bmi2;\n    return ZSTD_encodeSequences_default(dst, dstCapacity,\n                                        CTable_MatchLength, mlCodeTable,\n                                        CTable_OffsetBits, ofCodeTable,\n                                        CTable_LitLength, llCodeTable,\n                                        sequences, nbSeq, longOffsets);\n}",
        "func": "static size_t ZSTD_encodeSequences(\n            void* dst, size_t dstCapacity,\n            FSE_CTable const* CTable_MatchLength, BYTE const* mlCodeTable,\n            FSE_CTable const* CTable_OffsetBits, BYTE const* ofCodeTable,\n            FSE_CTable const* CTable_LitLength, BYTE const* llCodeTable,\n            seqDef const* sequences, size_t nbSeq, int longOffsets, int bmi2)\n{\n    DEBUGLOG(5, \"ZSTD_encodeSequences: dstCapacity = %u\", (unsigned)dstCapacity);\n#if DYNAMIC_BMI2\n    if (bmi2) {\n        return ZSTD_encodeSequences_bmi2(dst, dstCapacity,\n                                         CTable_MatchLength, mlCodeTable,\n                                         CTable_OffsetBits, ofCodeTable,\n                                         CTable_LitLength, llCodeTable,\n                                         sequences, nbSeq, longOffsets);\n    }\n#endif\n    (void)bmi2;\n    return ZSTD_encodeSequences_default(dst, dstCapacity,\n                                        CTable_MatchLength, mlCodeTable,\n                                        CTable_OffsetBits, ofCodeTable,\n                                        CTable_LitLength, llCodeTable,\n                                        sequences, nbSeq, longOffsets);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n             FSE_CTable const* CTable_LitLength, BYTE const* llCodeTable,\n             seqDef const* sequences, size_t nbSeq, int longOffsets, int bmi2)\n {\n+    DEBUGLOG(5, \"ZSTD_encodeSequences: dstCapacity = %u\", (unsigned)dstCapacity);\n #if DYNAMIC_BMI2\n     if (bmi2) {\n         return ZSTD_encodeSequences_bmi2(dst, dstCapacity,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    DEBUGLOG(5, \"ZSTD_encodeSequences: dstCapacity = %u\", (unsigned)dstCapacity);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11922",
        "func_name": "facebook/zstd/ZSTD_compressSequences_internal",
        "description": "A race condition in the one-pass compression functions of Zstandard prior to version 1.3.8 could allow an attacker to write bytes out of bounds if an output buffer smaller than the recommended size was used.",
        "git_url": "https://github.com/facebook/zstd/commit/3e5cdf1b6a85843e991d7d10f6a2567c15580da0",
        "commit_title": "fixed T36302429",
        "commit_text": "",
        "func_before": "MEM_STATIC size_t\nZSTD_compressSequences_internal(seqStore_t* seqStorePtr,\n                              ZSTD_entropyCTables_t const* prevEntropy,\n                              ZSTD_entropyCTables_t* nextEntropy,\n                              ZSTD_CCtx_params const* cctxParams,\n                              void* dst, size_t dstCapacity,\n                              void* workspace, size_t wkspSize,\n                              const int bmi2)\n{\n    const int longOffsets = cctxParams->cParams.windowLog > STREAM_ACCUMULATOR_MIN;\n    ZSTD_strategy const strategy = cctxParams->cParams.strategy;\n    U32 count[MaxSeq+1];\n    FSE_CTable* CTable_LitLength = nextEntropy->fse.litlengthCTable;\n    FSE_CTable* CTable_OffsetBits = nextEntropy->fse.offcodeCTable;\n    FSE_CTable* CTable_MatchLength = nextEntropy->fse.matchlengthCTable;\n    U32 LLtype, Offtype, MLtype;   /* compressed, raw or rle */\n    const seqDef* const sequences = seqStorePtr->sequencesStart;\n    const BYTE* const ofCodeTable = seqStorePtr->ofCode;\n    const BYTE* const llCodeTable = seqStorePtr->llCode;\n    const BYTE* const mlCodeTable = seqStorePtr->mlCode;\n    BYTE* const ostart = (BYTE*)dst;\n    BYTE* const oend = ostart + dstCapacity;\n    BYTE* op = ostart;\n    size_t const nbSeq = seqStorePtr->sequences - seqStorePtr->sequencesStart;\n    BYTE* seqHead;\n    BYTE* lastNCount = NULL;\n\n    ZSTD_STATIC_ASSERT(HUF_WORKSPACE_SIZE >= (1<<MAX(MLFSELog,LLFSELog)));\n\n    /* Compress literals */\n    {   const BYTE* const literals = seqStorePtr->litStart;\n        size_t const litSize = seqStorePtr->lit - literals;\n        int const disableLiteralCompression = (cctxParams->cParams.strategy == ZSTD_fast) && (cctxParams->cParams.targetLength > 0);\n        size_t const cSize = ZSTD_compressLiterals(\n                                    &prevEntropy->huf, &nextEntropy->huf,\n                                    cctxParams->cParams.strategy, disableLiteralCompression,\n                                    op, dstCapacity,\n                                    literals, litSize,\n                                    workspace, wkspSize,\n                                    bmi2);\n        if (ZSTD_isError(cSize))\n          return cSize;\n        assert(cSize <= dstCapacity);\n        op += cSize;\n    }\n\n    /* Sequences Header */\n    if ((oend-op) < 3 /*max nbSeq Size*/ + 1 /*seqHead*/) return ERROR(dstSize_tooSmall);\n    if (nbSeq < 0x7F)\n        *op++ = (BYTE)nbSeq;\n    else if (nbSeq < LONGNBSEQ)\n        op[0] = (BYTE)((nbSeq>>8) + 0x80), op[1] = (BYTE)nbSeq, op+=2;\n    else\n        op[0]=0xFF, MEM_writeLE16(op+1, (U16)(nbSeq - LONGNBSEQ)), op+=3;\n    if (nbSeq==0) {\n        /* Copy the old tables over as if we repeated them */\n        memcpy(&nextEntropy->fse, &prevEntropy->fse, sizeof(prevEntropy->fse));\n        return op - ostart;\n    }\n\n    /* seqHead : flags for FSE encoding type */\n    seqHead = op++;\n\n    /* convert length/distances into codes */\n    ZSTD_seqToCodes(seqStorePtr);\n    /* build CTable for Literal Lengths */\n    {   U32 max = MaxLL;\n        size_t const mostFrequent = HIST_countFast_wksp(count, &max, llCodeTable, nbSeq, workspace, wkspSize);   /* can't fail */\n        DEBUGLOG(5, \"Building LL table\");\n        nextEntropy->fse.litlength_repeatMode = prevEntropy->fse.litlength_repeatMode;\n        LLtype = ZSTD_selectEncodingType(&nextEntropy->fse.litlength_repeatMode, count, max, mostFrequent, nbSeq, LLFSELog, prevEntropy->fse.litlengthCTable, LL_defaultNorm, LL_defaultNormLog, ZSTD_defaultAllowed, strategy);\n        assert(set_basic < set_compressed && set_rle < set_compressed);\n        assert(!(LLtype < set_compressed && nextEntropy->fse.litlength_repeatMode != FSE_repeat_none)); /* We don't copy tables */\n        {   size_t const countSize = ZSTD_buildCTable(op, oend - op, CTable_LitLength, LLFSELog, (symbolEncodingType_e)LLtype,\n                                                    count, max, llCodeTable, nbSeq, LL_defaultNorm, LL_defaultNormLog, MaxLL,\n                                                    prevEntropy->fse.litlengthCTable, sizeof(prevEntropy->fse.litlengthCTable),\n                                                    workspace, wkspSize);\n            if (ZSTD_isError(countSize)) return countSize;\n            if (LLtype == set_compressed)\n                lastNCount = op;\n            op += countSize;\n    }   }\n    /* build CTable for Offsets */\n    {   U32 max = MaxOff;\n        size_t const mostFrequent = HIST_countFast_wksp(count, &max, ofCodeTable, nbSeq, workspace, wkspSize);  /* can't fail */\n        /* We can only use the basic table if max <= DefaultMaxOff, otherwise the offsets are too large */\n        ZSTD_defaultPolicy_e const defaultPolicy = (max <= DefaultMaxOff) ? ZSTD_defaultAllowed : ZSTD_defaultDisallowed;\n        DEBUGLOG(5, \"Building OF table\");\n        nextEntropy->fse.offcode_repeatMode = prevEntropy->fse.offcode_repeatMode;\n        Offtype = ZSTD_selectEncodingType(&nextEntropy->fse.offcode_repeatMode, count, max, mostFrequent, nbSeq, OffFSELog, prevEntropy->fse.offcodeCTable, OF_defaultNorm, OF_defaultNormLog, defaultPolicy, strategy);\n        assert(!(Offtype < set_compressed && nextEntropy->fse.offcode_repeatMode != FSE_repeat_none)); /* We don't copy tables */\n        {   size_t const countSize = ZSTD_buildCTable(op, oend - op, CTable_OffsetBits, OffFSELog, (symbolEncodingType_e)Offtype,\n                                                    count, max, ofCodeTable, nbSeq, OF_defaultNorm, OF_defaultNormLog, DefaultMaxOff,\n                                                    prevEntropy->fse.offcodeCTable, sizeof(prevEntropy->fse.offcodeCTable),\n                                                    workspace, wkspSize);\n            if (ZSTD_isError(countSize)) return countSize;\n            if (Offtype == set_compressed)\n                lastNCount = op;\n            op += countSize;\n    }   }\n    /* build CTable for MatchLengths */\n    {   U32 max = MaxML;\n        size_t const mostFrequent = HIST_countFast_wksp(count, &max, mlCodeTable, nbSeq, workspace, wkspSize);   /* can't fail */\n        DEBUGLOG(5, \"Building ML table\");\n        nextEntropy->fse.matchlength_repeatMode = prevEntropy->fse.matchlength_repeatMode;\n        MLtype = ZSTD_selectEncodingType(&nextEntropy->fse.matchlength_repeatMode, count, max, mostFrequent, nbSeq, MLFSELog, prevEntropy->fse.matchlengthCTable, ML_defaultNorm, ML_defaultNormLog, ZSTD_defaultAllowed, strategy);\n        assert(!(MLtype < set_compressed && nextEntropy->fse.matchlength_repeatMode != FSE_repeat_none)); /* We don't copy tables */\n        {   size_t const countSize = ZSTD_buildCTable(op, oend - op, CTable_MatchLength, MLFSELog, (symbolEncodingType_e)MLtype,\n                                                    count, max, mlCodeTable, nbSeq, ML_defaultNorm, ML_defaultNormLog, MaxML,\n                                                    prevEntropy->fse.matchlengthCTable, sizeof(prevEntropy->fse.matchlengthCTable),\n                                                    workspace, wkspSize);\n            if (ZSTD_isError(countSize)) return countSize;\n            if (MLtype == set_compressed)\n                lastNCount = op;\n            op += countSize;\n    }   }\n\n    *seqHead = (BYTE)((LLtype<<6) + (Offtype<<4) + (MLtype<<2));\n\n    {   size_t const bitstreamSize = ZSTD_encodeSequences(\n                                        op, oend - op,\n                                        CTable_MatchLength, mlCodeTable,\n                                        CTable_OffsetBits, ofCodeTable,\n                                        CTable_LitLength, llCodeTable,\n                                        sequences, nbSeq,\n                                        longOffsets, bmi2);\n        if (ZSTD_isError(bitstreamSize)) return bitstreamSize;\n        op += bitstreamSize;\n        /* zstd versions <= 1.3.4 mistakenly report corruption when\n         * FSE_readNCount() recieves a buffer < 4 bytes.\n         * Fixed by https://github.com/facebook/zstd/pull/1146.\n         * This can happen when the last set_compressed table present is 2\n         * bytes and the bitstream is only one byte.\n         * In this exceedingly rare case, we will simply emit an uncompressed\n         * block, since it isn't worth optimizing.\n         */\n        if (lastNCount && (op - lastNCount) < 4) {\n            /* NCountSize >= 2 && bitstreamSize > 0 ==> lastCountSize == 3 */\n            assert(op - lastNCount == 3);\n            DEBUGLOG(5, \"Avoiding bug in zstd decoder in versions <= 1.3.4 by \"\n                        \"emitting an uncompressed block.\");\n            return 0;\n        }\n    }\n\n    return op - ostart;\n}",
        "func": "MEM_STATIC size_t\nZSTD_compressSequences_internal(seqStore_t* seqStorePtr,\n                              ZSTD_entropyCTables_t const* prevEntropy,\n                              ZSTD_entropyCTables_t* nextEntropy,\n                              ZSTD_CCtx_params const* cctxParams,\n                              void* dst, size_t dstCapacity,\n                              void* workspace, size_t wkspSize,\n                              const int bmi2)\n{\n    const int longOffsets = cctxParams->cParams.windowLog > STREAM_ACCUMULATOR_MIN;\n    ZSTD_strategy const strategy = cctxParams->cParams.strategy;\n    U32 count[MaxSeq+1];\n    FSE_CTable* CTable_LitLength = nextEntropy->fse.litlengthCTable;\n    FSE_CTable* CTable_OffsetBits = nextEntropy->fse.offcodeCTable;\n    FSE_CTable* CTable_MatchLength = nextEntropy->fse.matchlengthCTable;\n    U32 LLtype, Offtype, MLtype;   /* compressed, raw or rle */\n    const seqDef* const sequences = seqStorePtr->sequencesStart;\n    const BYTE* const ofCodeTable = seqStorePtr->ofCode;\n    const BYTE* const llCodeTable = seqStorePtr->llCode;\n    const BYTE* const mlCodeTable = seqStorePtr->mlCode;\n    BYTE* const ostart = (BYTE*)dst;\n    BYTE* const oend = ostart + dstCapacity;\n    BYTE* op = ostart;\n    size_t const nbSeq = seqStorePtr->sequences - seqStorePtr->sequencesStart;\n    BYTE* seqHead;\n    BYTE* lastNCount = NULL;\n\n    ZSTD_STATIC_ASSERT(HUF_WORKSPACE_SIZE >= (1<<MAX(MLFSELog,LLFSELog)));\n\n    /* Compress literals */\n    {   const BYTE* const literals = seqStorePtr->litStart;\n        size_t const litSize = seqStorePtr->lit - literals;\n        int const disableLiteralCompression = (cctxParams->cParams.strategy == ZSTD_fast) && (cctxParams->cParams.targetLength > 0);\n        size_t const cSize = ZSTD_compressLiterals(\n                                    &prevEntropy->huf, &nextEntropy->huf,\n                                    cctxParams->cParams.strategy, disableLiteralCompression,\n                                    op, dstCapacity,\n                                    literals, litSize,\n                                    workspace, wkspSize,\n                                    bmi2);\n        if (ZSTD_isError(cSize))\n          return cSize;\n        assert(cSize <= dstCapacity);\n        op += cSize;\n    }\n\n    /* Sequences Header */\n    if ((oend-op) < 3 /*max nbSeq Size*/ + 1 /*seqHead*/) return ERROR(dstSize_tooSmall);\n    if (nbSeq < 0x7F)\n        *op++ = (BYTE)nbSeq;\n    else if (nbSeq < LONGNBSEQ)\n        op[0] = (BYTE)((nbSeq>>8) + 0x80), op[1] = (BYTE)nbSeq, op+=2;\n    else\n        op[0]=0xFF, MEM_writeLE16(op+1, (U16)(nbSeq - LONGNBSEQ)), op+=3;\n    if (nbSeq==0) {\n        /* Copy the old tables over as if we repeated them */\n        memcpy(&nextEntropy->fse, &prevEntropy->fse, sizeof(prevEntropy->fse));\n        return op - ostart;\n    }\n\n    /* seqHead : flags for FSE encoding type */\n    seqHead = op++;\n\n    /* convert length/distances into codes */\n    ZSTD_seqToCodes(seqStorePtr);\n    /* build CTable for Literal Lengths */\n    {   U32 max = MaxLL;\n        size_t const mostFrequent = HIST_countFast_wksp(count, &max, llCodeTable, nbSeq, workspace, wkspSize);   /* can't fail */\n        DEBUGLOG(5, \"Building LL table\");\n        nextEntropy->fse.litlength_repeatMode = prevEntropy->fse.litlength_repeatMode;\n        LLtype = ZSTD_selectEncodingType(&nextEntropy->fse.litlength_repeatMode, count, max, mostFrequent, nbSeq, LLFSELog, prevEntropy->fse.litlengthCTable, LL_defaultNorm, LL_defaultNormLog, ZSTD_defaultAllowed, strategy);\n        assert(set_basic < set_compressed && set_rle < set_compressed);\n        assert(!(LLtype < set_compressed && nextEntropy->fse.litlength_repeatMode != FSE_repeat_none)); /* We don't copy tables */\n        {   size_t const countSize = ZSTD_buildCTable(op, oend - op, CTable_LitLength, LLFSELog, (symbolEncodingType_e)LLtype,\n                                                    count, max, llCodeTable, nbSeq, LL_defaultNorm, LL_defaultNormLog, MaxLL,\n                                                    prevEntropy->fse.litlengthCTable, sizeof(prevEntropy->fse.litlengthCTable),\n                                                    workspace, wkspSize);\n            if (ZSTD_isError(countSize)) return countSize;\n            if (LLtype == set_compressed)\n                lastNCount = op;\n            op += countSize;\n    }   }\n    /* build CTable for Offsets */\n    {   U32 max = MaxOff;\n        size_t const mostFrequent = HIST_countFast_wksp(count, &max, ofCodeTable, nbSeq, workspace, wkspSize);  /* can't fail */\n        /* We can only use the basic table if max <= DefaultMaxOff, otherwise the offsets are too large */\n        ZSTD_defaultPolicy_e const defaultPolicy = (max <= DefaultMaxOff) ? ZSTD_defaultAllowed : ZSTD_defaultDisallowed;\n        DEBUGLOG(5, \"Building OF table\");\n        nextEntropy->fse.offcode_repeatMode = prevEntropy->fse.offcode_repeatMode;\n        Offtype = ZSTD_selectEncodingType(&nextEntropy->fse.offcode_repeatMode, count, max, mostFrequent, nbSeq, OffFSELog, prevEntropy->fse.offcodeCTable, OF_defaultNorm, OF_defaultNormLog, defaultPolicy, strategy);\n        assert(!(Offtype < set_compressed && nextEntropy->fse.offcode_repeatMode != FSE_repeat_none)); /* We don't copy tables */\n        {   size_t const countSize = ZSTD_buildCTable(op, oend - op, CTable_OffsetBits, OffFSELog, (symbolEncodingType_e)Offtype,\n                                                    count, max, ofCodeTable, nbSeq, OF_defaultNorm, OF_defaultNormLog, DefaultMaxOff,\n                                                    prevEntropy->fse.offcodeCTable, sizeof(prevEntropy->fse.offcodeCTable),\n                                                    workspace, wkspSize);\n            if (ZSTD_isError(countSize)) return countSize;\n            if (Offtype == set_compressed)\n                lastNCount = op;\n            op += countSize;\n    }   }\n    /* build CTable for MatchLengths */\n    {   U32 max = MaxML;\n        size_t const mostFrequent = HIST_countFast_wksp(count, &max, mlCodeTable, nbSeq, workspace, wkspSize);   /* can't fail */\n        DEBUGLOG(5, \"Building ML table (remaining space : %i)\", (int)(oend-op));\n        nextEntropy->fse.matchlength_repeatMode = prevEntropy->fse.matchlength_repeatMode;\n        MLtype = ZSTD_selectEncodingType(&nextEntropy->fse.matchlength_repeatMode, count, max, mostFrequent, nbSeq, MLFSELog, prevEntropy->fse.matchlengthCTable, ML_defaultNorm, ML_defaultNormLog, ZSTD_defaultAllowed, strategy);\n        assert(!(MLtype < set_compressed && nextEntropy->fse.matchlength_repeatMode != FSE_repeat_none)); /* We don't copy tables */\n        {   size_t const countSize = ZSTD_buildCTable(op, oend - op, CTable_MatchLength, MLFSELog, (symbolEncodingType_e)MLtype,\n                                                    count, max, mlCodeTable, nbSeq, ML_defaultNorm, ML_defaultNormLog, MaxML,\n                                                    prevEntropy->fse.matchlengthCTable, sizeof(prevEntropy->fse.matchlengthCTable),\n                                                    workspace, wkspSize);\n            if (ZSTD_isError(countSize)) return countSize;\n            if (MLtype == set_compressed)\n                lastNCount = op;\n            op += countSize;\n    }   }\n\n    *seqHead = (BYTE)((LLtype<<6) + (Offtype<<4) + (MLtype<<2));\n\n    {   size_t const bitstreamSize = ZSTD_encodeSequences(\n                                        op, oend - op,\n                                        CTable_MatchLength, mlCodeTable,\n                                        CTable_OffsetBits, ofCodeTable,\n                                        CTable_LitLength, llCodeTable,\n                                        sequences, nbSeq,\n                                        longOffsets, bmi2);\n        if (ZSTD_isError(bitstreamSize)) return bitstreamSize;\n        op += bitstreamSize;\n        /* zstd versions <= 1.3.4 mistakenly report corruption when\n         * FSE_readNCount() recieves a buffer < 4 bytes.\n         * Fixed by https://github.com/facebook/zstd/pull/1146.\n         * This can happen when the last set_compressed table present is 2\n         * bytes and the bitstream is only one byte.\n         * In this exceedingly rare case, we will simply emit an uncompressed\n         * block, since it isn't worth optimizing.\n         */\n        if (lastNCount && (op - lastNCount) < 4) {\n            /* NCountSize >= 2 && bitstreamSize > 0 ==> lastCountSize == 3 */\n            assert(op - lastNCount == 3);\n            DEBUGLOG(5, \"Avoiding bug in zstd decoder in versions <= 1.3.4 by \"\n                        \"emitting an uncompressed block.\");\n            return 0;\n        }\n    }\n\n    return op - ostart;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -101,7 +101,7 @@\n     /* build CTable for MatchLengths */\n     {   U32 max = MaxML;\n         size_t const mostFrequent = HIST_countFast_wksp(count, &max, mlCodeTable, nbSeq, workspace, wkspSize);   /* can't fail */\n-        DEBUGLOG(5, \"Building ML table\");\n+        DEBUGLOG(5, \"Building ML table (remaining space : %i)\", (int)(oend-op));\n         nextEntropy->fse.matchlength_repeatMode = prevEntropy->fse.matchlength_repeatMode;\n         MLtype = ZSTD_selectEncodingType(&nextEntropy->fse.matchlength_repeatMode, count, max, mostFrequent, nbSeq, MLFSELog, prevEntropy->fse.matchlengthCTable, ML_defaultNorm, ML_defaultNormLog, ZSTD_defaultAllowed, strategy);\n         assert(!(MLtype < set_compressed && nextEntropy->fse.matchlength_repeatMode != FSE_repeat_none)); /* We don't copy tables */",
        "diff_line_info": {
            "deleted_lines": [
                "        DEBUGLOG(5, \"Building ML table\");"
            ],
            "added_lines": [
                "        DEBUGLOG(5, \"Building ML table (remaining space : %i)\", (int)(oend-op));"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11922",
        "func_name": "facebook/zstd/ZSTD_encodeSequences_body",
        "description": "A race condition in the one-pass compression functions of Zstandard prior to version 1.3.8 could allow an attacker to write bytes out of bounds if an output buffer smaller than the recommended size was used.",
        "git_url": "https://github.com/facebook/zstd/commit/3e5cdf1b6a85843e991d7d10f6a2567c15580da0",
        "commit_title": "fixed T36302429",
        "commit_text": "",
        "func_before": "FORCE_INLINE_TEMPLATE size_t\nZSTD_encodeSequences_body(\n            void* dst, size_t dstCapacity,\n            FSE_CTable const* CTable_MatchLength, BYTE const* mlCodeTable,\n            FSE_CTable const* CTable_OffsetBits, BYTE const* ofCodeTable,\n            FSE_CTable const* CTable_LitLength, BYTE const* llCodeTable,\n            seqDef const* sequences, size_t nbSeq, int longOffsets)\n{\n    BIT_CStream_t blockStream;\n    FSE_CState_t  stateMatchLength;\n    FSE_CState_t  stateOffsetBits;\n    FSE_CState_t  stateLitLength;\n\n    CHECK_E(BIT_initCStream(&blockStream, dst, dstCapacity), dstSize_tooSmall); /* not enough space remaining */\n\n    /* first symbols */\n    FSE_initCState2(&stateMatchLength, CTable_MatchLength, mlCodeTable[nbSeq-1]);\n    FSE_initCState2(&stateOffsetBits,  CTable_OffsetBits,  ofCodeTable[nbSeq-1]);\n    FSE_initCState2(&stateLitLength,   CTable_LitLength,   llCodeTable[nbSeq-1]);\n    BIT_addBits(&blockStream, sequences[nbSeq-1].litLength, LL_bits[llCodeTable[nbSeq-1]]);\n    if (MEM_32bits()) BIT_flushBits(&blockStream);\n    BIT_addBits(&blockStream, sequences[nbSeq-1].matchLength, ML_bits[mlCodeTable[nbSeq-1]]);\n    if (MEM_32bits()) BIT_flushBits(&blockStream);\n    if (longOffsets) {\n        U32 const ofBits = ofCodeTable[nbSeq-1];\n        int const extraBits = ofBits - MIN(ofBits, STREAM_ACCUMULATOR_MIN-1);\n        if (extraBits) {\n            BIT_addBits(&blockStream, sequences[nbSeq-1].offset, extraBits);\n            BIT_flushBits(&blockStream);\n        }\n        BIT_addBits(&blockStream, sequences[nbSeq-1].offset >> extraBits,\n                    ofBits - extraBits);\n    } else {\n        BIT_addBits(&blockStream, sequences[nbSeq-1].offset, ofCodeTable[nbSeq-1]);\n    }\n    BIT_flushBits(&blockStream);\n\n    {   size_t n;\n        for (n=nbSeq-2 ; n<nbSeq ; n--) {      /* intentional underflow */\n            BYTE const llCode = llCodeTable[n];\n            BYTE const ofCode = ofCodeTable[n];\n            BYTE const mlCode = mlCodeTable[n];\n            U32  const llBits = LL_bits[llCode];\n            U32  const ofBits = ofCode;\n            U32  const mlBits = ML_bits[mlCode];\n            DEBUGLOG(6, \"encoding: litlen:%2u - matchlen:%2u - offCode:%7u\",\n                        sequences[n].litLength,\n                        sequences[n].matchLength + MINMATCH,\n                        sequences[n].offset);\n                                                                            /* 32b*/  /* 64b*/\n                                                                            /* (7)*/  /* (7)*/\n            FSE_encodeSymbol(&blockStream, &stateOffsetBits, ofCode);       /* 15 */  /* 15 */\n            FSE_encodeSymbol(&blockStream, &stateMatchLength, mlCode);      /* 24 */  /* 24 */\n            if (MEM_32bits()) BIT_flushBits(&blockStream);                  /* (7)*/\n            FSE_encodeSymbol(&blockStream, &stateLitLength, llCode);        /* 16 */  /* 33 */\n            if (MEM_32bits() || (ofBits+mlBits+llBits >= 64-7-(LLFSELog+MLFSELog+OffFSELog)))\n                BIT_flushBits(&blockStream);                                /* (7)*/\n            BIT_addBits(&blockStream, sequences[n].litLength, llBits);\n            if (MEM_32bits() && ((llBits+mlBits)>24)) BIT_flushBits(&blockStream);\n            BIT_addBits(&blockStream, sequences[n].matchLength, mlBits);\n            if (MEM_32bits() || (ofBits+mlBits+llBits > 56)) BIT_flushBits(&blockStream);\n            if (longOffsets) {\n                int const extraBits = ofBits - MIN(ofBits, STREAM_ACCUMULATOR_MIN-1);\n                if (extraBits) {\n                    BIT_addBits(&blockStream, sequences[n].offset, extraBits);\n                    BIT_flushBits(&blockStream);                            /* (7)*/\n                }\n                BIT_addBits(&blockStream, sequences[n].offset >> extraBits,\n                            ofBits - extraBits);                            /* 31 */\n            } else {\n                BIT_addBits(&blockStream, sequences[n].offset, ofBits);     /* 31 */\n            }\n            BIT_flushBits(&blockStream);                                    /* (7)*/\n    }   }\n\n    DEBUGLOG(6, \"ZSTD_encodeSequences: flushing ML state with %u bits\", stateMatchLength.stateLog);\n    FSE_flushCState(&blockStream, &stateMatchLength);\n    DEBUGLOG(6, \"ZSTD_encodeSequences: flushing Off state with %u bits\", stateOffsetBits.stateLog);\n    FSE_flushCState(&blockStream, &stateOffsetBits);\n    DEBUGLOG(6, \"ZSTD_encodeSequences: flushing LL state with %u bits\", stateLitLength.stateLog);\n    FSE_flushCState(&blockStream, &stateLitLength);\n\n    {   size_t const streamSize = BIT_closeCStream(&blockStream);\n        if (streamSize==0) return ERROR(dstSize_tooSmall);   /* not enough space */\n        return streamSize;\n    }\n}",
        "func": "FORCE_INLINE_TEMPLATE size_t\nZSTD_encodeSequences_body(\n            void* dst, size_t dstCapacity,\n            FSE_CTable const* CTable_MatchLength, BYTE const* mlCodeTable,\n            FSE_CTable const* CTable_OffsetBits, BYTE const* ofCodeTable,\n            FSE_CTable const* CTable_LitLength, BYTE const* llCodeTable,\n            seqDef const* sequences, size_t nbSeq, int longOffsets)\n{\n    BIT_CStream_t blockStream;\n    FSE_CState_t  stateMatchLength;\n    FSE_CState_t  stateOffsetBits;\n    FSE_CState_t  stateLitLength;\n\n    CHECK_E(BIT_initCStream(&blockStream, dst, dstCapacity), dstSize_tooSmall); /* not enough space remaining */\n    DEBUGLOG(6, \"available space for bitstream : %i  (dstCapacity=%u)\",\n                (int)(blockStream.endPtr - blockStream.startPtr),\n                (unsigned)dstCapacity);\n\n    /* first symbols */\n    FSE_initCState2(&stateMatchLength, CTable_MatchLength, mlCodeTable[nbSeq-1]);\n    FSE_initCState2(&stateOffsetBits,  CTable_OffsetBits,  ofCodeTable[nbSeq-1]);\n    FSE_initCState2(&stateLitLength,   CTable_LitLength,   llCodeTable[nbSeq-1]);\n    BIT_addBits(&blockStream, sequences[nbSeq-1].litLength, LL_bits[llCodeTable[nbSeq-1]]);\n    if (MEM_32bits()) BIT_flushBits(&blockStream);\n    BIT_addBits(&blockStream, sequences[nbSeq-1].matchLength, ML_bits[mlCodeTable[nbSeq-1]]);\n    if (MEM_32bits()) BIT_flushBits(&blockStream);\n    if (longOffsets) {\n        U32 const ofBits = ofCodeTable[nbSeq-1];\n        int const extraBits = ofBits - MIN(ofBits, STREAM_ACCUMULATOR_MIN-1);\n        if (extraBits) {\n            BIT_addBits(&blockStream, sequences[nbSeq-1].offset, extraBits);\n            BIT_flushBits(&blockStream);\n        }\n        BIT_addBits(&blockStream, sequences[nbSeq-1].offset >> extraBits,\n                    ofBits - extraBits);\n    } else {\n        BIT_addBits(&blockStream, sequences[nbSeq-1].offset, ofCodeTable[nbSeq-1]);\n    }\n    BIT_flushBits(&blockStream);\n\n    {   size_t n;\n        for (n=nbSeq-2 ; n<nbSeq ; n--) {      /* intentional underflow */\n            BYTE const llCode = llCodeTable[n];\n            BYTE const ofCode = ofCodeTable[n];\n            BYTE const mlCode = mlCodeTable[n];\n            U32  const llBits = LL_bits[llCode];\n            U32  const ofBits = ofCode;\n            U32  const mlBits = ML_bits[mlCode];\n            DEBUGLOG(6, \"encoding: litlen:%2u - matchlen:%2u - offCode:%7u\",\n                        sequences[n].litLength,\n                        sequences[n].matchLength + MINMATCH,\n                        sequences[n].offset);\n                                                                            /* 32b*/  /* 64b*/\n                                                                            /* (7)*/  /* (7)*/\n            FSE_encodeSymbol(&blockStream, &stateOffsetBits, ofCode);       /* 15 */  /* 15 */\n            FSE_encodeSymbol(&blockStream, &stateMatchLength, mlCode);      /* 24 */  /* 24 */\n            if (MEM_32bits()) BIT_flushBits(&blockStream);                  /* (7)*/\n            FSE_encodeSymbol(&blockStream, &stateLitLength, llCode);        /* 16 */  /* 33 */\n            if (MEM_32bits() || (ofBits+mlBits+llBits >= 64-7-(LLFSELog+MLFSELog+OffFSELog)))\n                BIT_flushBits(&blockStream);                                /* (7)*/\n            BIT_addBits(&blockStream, sequences[n].litLength, llBits);\n            if (MEM_32bits() && ((llBits+mlBits)>24)) BIT_flushBits(&blockStream);\n            BIT_addBits(&blockStream, sequences[n].matchLength, mlBits);\n            if (MEM_32bits() || (ofBits+mlBits+llBits > 56)) BIT_flushBits(&blockStream);\n            if (longOffsets) {\n                int const extraBits = ofBits - MIN(ofBits, STREAM_ACCUMULATOR_MIN-1);\n                if (extraBits) {\n                    BIT_addBits(&blockStream, sequences[n].offset, extraBits);\n                    BIT_flushBits(&blockStream);                            /* (7)*/\n                }\n                BIT_addBits(&blockStream, sequences[n].offset >> extraBits,\n                            ofBits - extraBits);                            /* 31 */\n            } else {\n                BIT_addBits(&blockStream, sequences[n].offset, ofBits);     /* 31 */\n            }\n            BIT_flushBits(&blockStream);                                    /* (7)*/\n            DEBUGLOG(7, \"remaining space : %i\", (int)(blockStream.endPtr - blockStream.ptr));\n    }   }\n\n    DEBUGLOG(6, \"ZSTD_encodeSequences: flushing ML state with %u bits\", stateMatchLength.stateLog);\n    FSE_flushCState(&blockStream, &stateMatchLength);\n    DEBUGLOG(6, \"ZSTD_encodeSequences: flushing Off state with %u bits\", stateOffsetBits.stateLog);\n    FSE_flushCState(&blockStream, &stateOffsetBits);\n    DEBUGLOG(6, \"ZSTD_encodeSequences: flushing LL state with %u bits\", stateLitLength.stateLog);\n    FSE_flushCState(&blockStream, &stateLitLength);\n\n    {   size_t const streamSize = BIT_closeCStream(&blockStream);\n        if (streamSize==0) return ERROR(dstSize_tooSmall);   /* not enough space */\n        return streamSize;\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,9 @@\n     FSE_CState_t  stateLitLength;\n \n     CHECK_E(BIT_initCStream(&blockStream, dst, dstCapacity), dstSize_tooSmall); /* not enough space remaining */\n+    DEBUGLOG(6, \"available space for bitstream : %i  (dstCapacity=%u)\",\n+                (int)(blockStream.endPtr - blockStream.startPtr),\n+                (unsigned)dstCapacity);\n \n     /* first symbols */\n     FSE_initCState2(&stateMatchLength, CTable_MatchLength, mlCodeTable[nbSeq-1]);\n@@ -71,6 +74,7 @@\n                 BIT_addBits(&blockStream, sequences[n].offset, ofBits);     /* 31 */\n             }\n             BIT_flushBits(&blockStream);                                    /* (7)*/\n+            DEBUGLOG(7, \"remaining space : %i\", (int)(blockStream.endPtr - blockStream.ptr));\n     }   }\n \n     DEBUGLOG(6, \"ZSTD_encodeSequences: flushing ML state with %u bits\", stateMatchLength.stateLog);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    DEBUGLOG(6, \"available space for bitstream : %i  (dstCapacity=%u)\",",
                "                (int)(blockStream.endPtr - blockStream.startPtr),",
                "                (unsigned)dstCapacity);",
                "            DEBUGLOG(7, \"remaining space : %i\", (int)(blockStream.endPtr - blockStream.ptr));"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11922",
        "func_name": "facebook/zstd/ZSTD_buildCTable",
        "description": "A race condition in the one-pass compression functions of Zstandard prior to version 1.3.8 could allow an attacker to write bytes out of bounds if an output buffer smaller than the recommended size was used.",
        "git_url": "https://github.com/facebook/zstd/commit/3e5cdf1b6a85843e991d7d10f6a2567c15580da0",
        "commit_title": "fixed T36302429",
        "commit_text": "",
        "func_before": "MEM_STATIC size_t\nZSTD_buildCTable(void* dst, size_t dstCapacity,\n                FSE_CTable* nextCTable, U32 FSELog, symbolEncodingType_e type,\n                U32* count, U32 max,\n                const BYTE* codeTable, size_t nbSeq,\n                const S16* defaultNorm, U32 defaultNormLog, U32 defaultMax,\n                const FSE_CTable* prevCTable, size_t prevCTableSize,\n                void* workspace, size_t workspaceSize)\n{\n    BYTE* op = (BYTE*)dst;\n    const BYTE* const oend = op + dstCapacity;\n\n    switch (type) {\n    case set_rle:\n        *op = codeTable[0];\n        CHECK_F(FSE_buildCTable_rle(nextCTable, (BYTE)max));\n        return 1;\n    case set_repeat:\n        memcpy(nextCTable, prevCTable, prevCTableSize);\n        return 0;\n    case set_basic:\n        CHECK_F(FSE_buildCTable_wksp(nextCTable, defaultNorm, defaultMax, defaultNormLog, workspace, workspaceSize));  /* note : could be pre-calculated */\n        return 0;\n    case set_compressed: {\n        S16 norm[MaxSeq + 1];\n        size_t nbSeq_1 = nbSeq;\n        const U32 tableLog = FSE_optimalTableLog(FSELog, nbSeq, max);\n        if (count[codeTable[nbSeq-1]] > 1) {\n            count[codeTable[nbSeq-1]]--;\n            nbSeq_1--;\n        }\n        assert(nbSeq_1 > 1);\n        CHECK_F(FSE_normalizeCount(norm, tableLog, count, nbSeq_1, max));\n        {   size_t const NCountSize = FSE_writeNCount(op, oend - op, norm, max, tableLog);   /* overflow protected */\n            if (FSE_isError(NCountSize)) return NCountSize;\n            CHECK_F(FSE_buildCTable_wksp(nextCTable, norm, max, tableLog, workspace, workspaceSize));\n            return NCountSize;\n        }\n    }\n    default: return assert(0), ERROR(GENERIC);\n    }\n}",
        "func": "MEM_STATIC size_t\nZSTD_buildCTable(void* dst, size_t dstCapacity,\n                FSE_CTable* nextCTable, U32 FSELog, symbolEncodingType_e type,\n                U32* count, U32 max,\n                const BYTE* codeTable, size_t nbSeq,\n                const S16* defaultNorm, U32 defaultNormLog, U32 defaultMax,\n                const FSE_CTable* prevCTable, size_t prevCTableSize,\n                void* workspace, size_t workspaceSize)\n{\n    BYTE* op = (BYTE*)dst;\n    const BYTE* const oend = op + dstCapacity;\n    DEBUGLOG(6, \"ZSTD_buildCTable (dstCapacity=%u)\", (unsigned)dstCapacity);\n\n    switch (type) {\n    case set_rle:\n        CHECK_F(FSE_buildCTable_rle(nextCTable, (BYTE)max));\n        if (dstCapacity==0) return ERROR(dstSize_tooSmall);\n        *op = codeTable[0];\n        return 1;\n    case set_repeat:\n        memcpy(nextCTable, prevCTable, prevCTableSize);\n        return 0;\n    case set_basic:\n        CHECK_F(FSE_buildCTable_wksp(nextCTable, defaultNorm, defaultMax, defaultNormLog, workspace, workspaceSize));  /* note : could be pre-calculated */\n        return 0;\n    case set_compressed: {\n        S16 norm[MaxSeq + 1];\n        size_t nbSeq_1 = nbSeq;\n        const U32 tableLog = FSE_optimalTableLog(FSELog, nbSeq, max);\n        if (count[codeTable[nbSeq-1]] > 1) {\n            count[codeTable[nbSeq-1]]--;\n            nbSeq_1--;\n        }\n        assert(nbSeq_1 > 1);\n        CHECK_F(FSE_normalizeCount(norm, tableLog, count, nbSeq_1, max));\n        {   size_t const NCountSize = FSE_writeNCount(op, oend - op, norm, max, tableLog);   /* overflow protected */\n            if (FSE_isError(NCountSize)) return NCountSize;\n            CHECK_F(FSE_buildCTable_wksp(nextCTable, norm, max, tableLog, workspace, workspaceSize));\n            return NCountSize;\n        }\n    }\n    default: return assert(0), ERROR(GENERIC);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,11 +9,13 @@\n {\n     BYTE* op = (BYTE*)dst;\n     const BYTE* const oend = op + dstCapacity;\n+    DEBUGLOG(6, \"ZSTD_buildCTable (dstCapacity=%u)\", (unsigned)dstCapacity);\n \n     switch (type) {\n     case set_rle:\n+        CHECK_F(FSE_buildCTable_rle(nextCTable, (BYTE)max));\n+        if (dstCapacity==0) return ERROR(dstSize_tooSmall);\n         *op = codeTable[0];\n-        CHECK_F(FSE_buildCTable_rle(nextCTable, (BYTE)max));\n         return 1;\n     case set_repeat:\n         memcpy(nextCTable, prevCTable, prevCTableSize);",
        "diff_line_info": {
            "deleted_lines": [
                "        CHECK_F(FSE_buildCTable_rle(nextCTable, (BYTE)max));"
            ],
            "added_lines": [
                "    DEBUGLOG(6, \"ZSTD_buildCTable (dstCapacity=%u)\", (unsigned)dstCapacity);",
                "        CHECK_F(FSE_buildCTable_rle(nextCTable, (BYTE)max));",
                "        if (dstCapacity==0) return ERROR(dstSize_tooSmall);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-10906",
        "func_name": "torvalds/linux/arc_emac_tx",
        "description": "An issue was discovered in drivers/net/ethernet/arc/emac_main.c in the Linux kernel before 4.5. A use-after-free is caused by a race condition between the functions arc_emac_tx and arc_emac_tx_clean.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c278c253f3d992c6994d08aa0efb2b6806ca396f",
        "commit_title": "There is a race between arc_emac_tx() and arc_emac_tx_clean().",
        "commit_text": "sk_buff got freed by arc_emac_tx_clean() while arc_emac_tx() submitting sk_buff.  In order to free sk_buff arc_emac_tx_clean() checks:     if ((info & FOR_EMAC) || !txbd->data)         break;     ...     dev_kfree_skb_irq(skb);  If condition false, arc_emac_tx_clean() free sk_buff.  In order to submit txbd, arc_emac_tx() do:     priv->tx_buff[*txbd_curr].skb = skb;     ...     priv->txbd[*txbd_curr].data = cpu_to_le32(addr);     ...     ...  <== arc_emac_tx_clean() check condition here     ...  <== (info & FOR_EMAC) is false     ...  <== !txbd->data is false     ...     *info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);  In order to reproduce the situation, run device:     # iperf -s run on host:     # iperf -t 600 -c <device-ip-addr>  [   28.396284] ------------[ cut here ]------------ [   28.400912] kernel BUG at .../net/core/skbuff.c:1355! [   28.414019] Internal error: Oops - BUG: 0 [#1] SMP ARM [   28.419150] Modules linked in: [   28.422219] CPU: 0 PID: 0 Comm: swapper/0 Tainted: G    B           4.4.0+ #120 [   28.429516] Hardware name: Rockchip (Device Tree) [   28.434216] task: c0665070 ti: c0660000 task.ti: c0660000 [   28.439622] PC is at skb_put+0x10/0x54 [   28.443381] LR is at arc_emac_poll+0x260/0x474 [   28.447821] pc : [<c03af580>]    lr : [<c028fec4>]    psr: a0070113 [   28.447821] sp : c0661e58  ip : eea68502  fp : ef377000 [   28.459280] r10: 0000012c  r9 : f08b2000  r8 : eeb57100 [   28.464498] r7 : 00000000  r6 : ef376594  r5 : 00000077  r4 : ef376000 [   28.471015] r3 : 0030488b  r2 : ef13e880  r1 : 000005ee  r0 : eeb57100 [   28.477534] Flags: NzCv  IRQs on  FIQs on  Mode SVC_32  ISA ARM  Segment none [   28.484658] Control: 10c5387d  Table: 8eaf004a  DAC: 00000051 [   28.490396] Process swapper/0 (pid: 0, stack limit = 0xc0660210) [   28.496393] Stack: (0xc0661e58 to 0xc0662000) [   28.500745] 1e40:                                                       00000002 00000000 [   28.508913] 1e60: 00000000 ef376520 00000028 f08b23b8 00000000 ef376520 ef7b6900 c028fc64 [   28.517082] 1e80: 2f158000 c0661ea8 c0661eb0 0000012c c065e900 c03bdeac ffff95e9 c0662100 [   28.525250] 1ea0: c0663924 00000028 c0661ea8 c0661ea8 c0661eb0 c0661eb0 0000001e c0660000 [   28.533417] 1ec0: 40000003 00000008 c0695a00 0000000a c066208c 00000100 c0661ee0 c0027410 [   28.541584] 1ee0: ef0fb700 2f158000 00200000 ffff95e8 00000004 c0662100 c0662080 00000003 [   28.549751] 1f00: 00000000 00000000 00000000 c065b45c 0000001e ef005000 c0647a30 00000000 [   28.557919] 1f20: 00000000 c0027798 00000000 c005cf40 f0802100 c0662ffc c0661f60 f0803100 [   28.566088] 1f40: c0661fb8 c00093bc c000ffb4 60070013 ffffffff c0661f94 c0661fb8 c00137d4 [   28.574267] 1f60: 00000001 00000000 00000000 c001ffa0 00000000 c0660000 00000000 c065a364 [   28.582441] 1f80: c0661fb8 c0647a30 00000000 00000000 00000000 c0661fb0 c000ffb0 c000ffb4 [   28.590608] 1fa0: 60070013 ffffffff 00000051 00000000 00000000 c005496c c0662400 c061bc40 [   28.598776] 1fc0: ffffffff ffffffff 00000000 c061b680 00000000 c0647a30 00000000 c0695294 [   28.606943] 1fe0: c0662488 c0647a2c c066619c 6000406a 413fc090 6000807c 00000000 00000000 [   28.615127] [<c03af580>] (skb_put) from [<ef376520>] (0xef376520) [   28.621218] Code: e5902054 e590c090 e3520000 0a000000 (e7f001f2) [   28.627307] ---[ end trace 4824734e2243fdb6 ]---  [   34.377068] Internal error: Oops: 17 [#1] SMP ARM [   34.382854] Modules linked in: [   34.385947] CPU: 0 PID: 3 Comm: ksoftirqd/0 Not tainted 4.4.0+ #120 [   34.392219] Hardware name: Rockchip (Device Tree) [   34.396937] task: ef02d040 ti: ef05c000 task.ti: ef05c000 [   34.402376] PC is at __dev_kfree_skb_irq+0x4/0x80 [   34.407121] LR is at arc_emac_poll+0x130/0x474 [   34.411583] pc : [<c03bb640>]    lr : [<c028fd94>]    psr: 60030013 [   34.411583] sp : ef05de68  ip : 0008e83c  fp : ef377000 [   34.423062] r10: c001bec4  r9 : 00000000  r8 : f08b24c8 [   34.428296] r7 : f08b2400  r6 : 00000075  r5 : 00000019  r4 : ef376000 [   34.434827] r3 : 00060000  r2 : 00000042  r1 : 00000001  r0 : 00000000 [   34.441365] Flags: nZCv  IRQs on  FIQs on  Mode SVC_32  ISA ARM  Segment none [   34.448507] Control: 10c5387d  Table: 8f25c04a  DAC: 00000051 [   34.454262] Process ksoftirqd/0 (pid: 3, stack limit = 0xef05c210) [   34.460449] Stack: (0xef05de68 to 0xef05e000) [   34.464827] de60:                   ef376000 c028fd94 00000000 c0669480 c0669480 ef376520 [   34.473022] de80: 00000028 00000001 00002ae4 ef376520 ef7b6900 c028fc64 2f158000 ef05dec0 [   34.481215] dea0: ef05dec8 0000012c c065e900 c03bdeac ffff983f c0662100 c0663924 00000028 [   34.489409] dec0: ef05dec0 ef05dec0 ef05dec8 ef05dec8 ef7b6000 ef05c000 40000003 00000008 [   34.497600] dee0: c0695a00 0000000a c066208c 00000100 ef05def8 c0027410 ef7b6000 40000000 [   34.505795] df00: 04208040 ffff983e 00000004 c0662100 c0662080 00000003 ef05c000 ef027340 [   34.513985] df20: ef05c000 c0666c2c 00000000 00000001 00000002 00000000 00000000 c0027568 [   34.522176] df40: ef027340 c003ef48 ef027300 00000000 ef027340 c003edd4 00000000 00000000 [   34.530367] df60: 00000000 c003c37c ffffff7f 00000001 00000000 ef027340 00000000 00030003 [   34.538559] df80: ef05df80 ef05df80 00000000 00000000 ef05df90 ef05df90 ef05dfac ef027300 [   34.546750] dfa0: c003c2a4 00000000 00000000 c000f578 00000000 00000000 00000000 00000000 [   34.554939] dfc0: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 [   34.563129] dfe0: 00000000 00000000 00000000 00000000 00000013 00000000 ffffffff dfff7fff [   34.571360] [<c03bb640>] (__dev_kfree_skb_irq) from [<c028fd94>] (arc_emac_poll+0x130/0x474) [   34.579840] [<c028fd94>] (arc_emac_poll) from [<c03bdeac>] (net_rx_action+0xdc/0x28c) [   34.587712] [<c03bdeac>] (net_rx_action) from [<c0027410>] (__do_softirq+0xcc/0x1f8) [   34.595482] [<c0027410>] (__do_softirq) from [<c0027568>] (run_ksoftirqd+0x2c/0x50) [   34.603168] [<c0027568>] (run_ksoftirqd) from [<c003ef48>] (smpboot_thread_fn+0x174/0x18c) [   34.611466] [<c003ef48>] (smpboot_thread_fn) from [<c003c37c>] (kthread+0xd8/0xec) [   34.619075] [<c003c37c>] (kthread) from [<c000f578>] (ret_from_fork+0x14/0x3c) [   34.626317] Code: e8bd8010 e3a00000 e12fff1e e92d4010 (e59030a4) [   34.632572] ---[ end trace cca5a3d86a82249a ]---  ",
        "func_before": "static int arc_emac_tx(struct sk_buff *skb, struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tunsigned int len, *txbd_curr = &priv->txbd_curr;\n\tstruct net_device_stats *stats = &ndev->stats;\n\t__le32 *info = &priv->txbd[*txbd_curr].info;\n\tdma_addr_t addr;\n\n\tif (skb_padto(skb, ETH_ZLEN))\n\t\treturn NETDEV_TX_OK;\n\n\tlen = max_t(unsigned int, ETH_ZLEN, skb->len);\n\n\tif (unlikely(!arc_emac_tx_avail(priv))) {\n\t\tnetif_stop_queue(ndev);\n\t\tnetdev_err(ndev, \"BUG! Tx Ring full when queue awake!\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\taddr = dma_map_single(&ndev->dev, (void *)skb->data, len,\n\t\t\t      DMA_TO_DEVICE);\n\n\tif (unlikely(dma_mapping_error(&ndev->dev, addr))) {\n\t\tstats->tx_dropped++;\n\t\tstats->tx_errors++;\n\t\tdev_kfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tdma_unmap_addr_set(&priv->tx_buff[*txbd_curr], addr, addr);\n\tdma_unmap_len_set(&priv->tx_buff[*txbd_curr], len, len);\n\n\tpriv->tx_buff[*txbd_curr].skb = skb;\n\tpriv->txbd[*txbd_curr].data = cpu_to_le32(addr);\n\n\t/* Make sure pointer to data buffer is set */\n\twmb();\n\n\tskb_tx_timestamp(skb);\n\n\t*info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);\n\n\t/* Increment index to point to the next BD */\n\t*txbd_curr = (*txbd_curr + 1) % TX_BD_NUM;\n\n\t/* Ensure that tx_clean() sees the new txbd_curr before\n\t * checking the queue status. This prevents an unneeded wake\n\t * of the queue in tx_clean().\n\t */\n\tsmp_mb();\n\n\tif (!arc_emac_tx_avail(priv)) {\n\t\tnetif_stop_queue(ndev);\n\t\t/* Refresh tx_dirty */\n\t\tsmp_mb();\n\t\tif (arc_emac_tx_avail(priv))\n\t\t\tnetif_start_queue(ndev);\n\t}\n\n\tarc_reg_set(priv, R_STATUS, TXPL_MASK);\n\n\treturn NETDEV_TX_OK;\n}",
        "func": "static int arc_emac_tx(struct sk_buff *skb, struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tunsigned int len, *txbd_curr = &priv->txbd_curr;\n\tstruct net_device_stats *stats = &ndev->stats;\n\t__le32 *info = &priv->txbd[*txbd_curr].info;\n\tdma_addr_t addr;\n\n\tif (skb_padto(skb, ETH_ZLEN))\n\t\treturn NETDEV_TX_OK;\n\n\tlen = max_t(unsigned int, ETH_ZLEN, skb->len);\n\n\tif (unlikely(!arc_emac_tx_avail(priv))) {\n\t\tnetif_stop_queue(ndev);\n\t\tnetdev_err(ndev, \"BUG! Tx Ring full when queue awake!\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\taddr = dma_map_single(&ndev->dev, (void *)skb->data, len,\n\t\t\t      DMA_TO_DEVICE);\n\n\tif (unlikely(dma_mapping_error(&ndev->dev, addr))) {\n\t\tstats->tx_dropped++;\n\t\tstats->tx_errors++;\n\t\tdev_kfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tdma_unmap_addr_set(&priv->tx_buff[*txbd_curr], addr, addr);\n\tdma_unmap_len_set(&priv->tx_buff[*txbd_curr], len, len);\n\n\tpriv->txbd[*txbd_curr].data = cpu_to_le32(addr);\n\n\t/* Make sure pointer to data buffer is set */\n\twmb();\n\n\tskb_tx_timestamp(skb);\n\n\t*info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);\n\n\t/* Make sure info word is set */\n\twmb();\n\n\tpriv->tx_buff[*txbd_curr].skb = skb;\n\n\t/* Increment index to point to the next BD */\n\t*txbd_curr = (*txbd_curr + 1) % TX_BD_NUM;\n\n\t/* Ensure that tx_clean() sees the new txbd_curr before\n\t * checking the queue status. This prevents an unneeded wake\n\t * of the queue in tx_clean().\n\t */\n\tsmp_mb();\n\n\tif (!arc_emac_tx_avail(priv)) {\n\t\tnetif_stop_queue(ndev);\n\t\t/* Refresh tx_dirty */\n\t\tsmp_mb();\n\t\tif (arc_emac_tx_avail(priv))\n\t\t\tnetif_start_queue(ndev);\n\t}\n\n\tarc_reg_set(priv, R_STATUS, TXPL_MASK);\n\n\treturn NETDEV_TX_OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -29,7 +29,6 @@\n \tdma_unmap_addr_set(&priv->tx_buff[*txbd_curr], addr, addr);\n \tdma_unmap_len_set(&priv->tx_buff[*txbd_curr], len, len);\n \n-\tpriv->tx_buff[*txbd_curr].skb = skb;\n \tpriv->txbd[*txbd_curr].data = cpu_to_le32(addr);\n \n \t/* Make sure pointer to data buffer is set */\n@@ -38,6 +37,11 @@\n \tskb_tx_timestamp(skb);\n \n \t*info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);\n+\n+\t/* Make sure info word is set */\n+\twmb();\n+\n+\tpriv->tx_buff[*txbd_curr].skb = skb;\n \n \t/* Increment index to point to the next BD */\n \t*txbd_curr = (*txbd_curr + 1) % TX_BD_NUM;",
        "diff_line_info": {
            "deleted_lines": [
                "\tpriv->tx_buff[*txbd_curr].skb = skb;"
            ],
            "added_lines": [
                "",
                "\t/* Make sure info word is set */",
                "\twmb();",
                "",
                "\tpriv->tx_buff[*txbd_curr].skb = skb;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-10906",
        "func_name": "torvalds/linux/arc_emac_tx_clean",
        "description": "An issue was discovered in drivers/net/ethernet/arc/emac_main.c in the Linux kernel before 4.5. A use-after-free is caused by a race condition between the functions arc_emac_tx and arc_emac_tx_clean.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c278c253f3d992c6994d08aa0efb2b6806ca396f",
        "commit_title": "There is a race between arc_emac_tx() and arc_emac_tx_clean().",
        "commit_text": "sk_buff got freed by arc_emac_tx_clean() while arc_emac_tx() submitting sk_buff.  In order to free sk_buff arc_emac_tx_clean() checks:     if ((info & FOR_EMAC) || !txbd->data)         break;     ...     dev_kfree_skb_irq(skb);  If condition false, arc_emac_tx_clean() free sk_buff.  In order to submit txbd, arc_emac_tx() do:     priv->tx_buff[*txbd_curr].skb = skb;     ...     priv->txbd[*txbd_curr].data = cpu_to_le32(addr);     ...     ...  <== arc_emac_tx_clean() check condition here     ...  <== (info & FOR_EMAC) is false     ...  <== !txbd->data is false     ...     *info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);  In order to reproduce the situation, run device:     # iperf -s run on host:     # iperf -t 600 -c <device-ip-addr>  [   28.396284] ------------[ cut here ]------------ [   28.400912] kernel BUG at .../net/core/skbuff.c:1355! [   28.414019] Internal error: Oops - BUG: 0 [#1] SMP ARM [   28.419150] Modules linked in: [   28.422219] CPU: 0 PID: 0 Comm: swapper/0 Tainted: G    B           4.4.0+ #120 [   28.429516] Hardware name: Rockchip (Device Tree) [   28.434216] task: c0665070 ti: c0660000 task.ti: c0660000 [   28.439622] PC is at skb_put+0x10/0x54 [   28.443381] LR is at arc_emac_poll+0x260/0x474 [   28.447821] pc : [<c03af580>]    lr : [<c028fec4>]    psr: a0070113 [   28.447821] sp : c0661e58  ip : eea68502  fp : ef377000 [   28.459280] r10: 0000012c  r9 : f08b2000  r8 : eeb57100 [   28.464498] r7 : 00000000  r6 : ef376594  r5 : 00000077  r4 : ef376000 [   28.471015] r3 : 0030488b  r2 : ef13e880  r1 : 000005ee  r0 : eeb57100 [   28.477534] Flags: NzCv  IRQs on  FIQs on  Mode SVC_32  ISA ARM  Segment none [   28.484658] Control: 10c5387d  Table: 8eaf004a  DAC: 00000051 [   28.490396] Process swapper/0 (pid: 0, stack limit = 0xc0660210) [   28.496393] Stack: (0xc0661e58 to 0xc0662000) [   28.500745] 1e40:                                                       00000002 00000000 [   28.508913] 1e60: 00000000 ef376520 00000028 f08b23b8 00000000 ef376520 ef7b6900 c028fc64 [   28.517082] 1e80: 2f158000 c0661ea8 c0661eb0 0000012c c065e900 c03bdeac ffff95e9 c0662100 [   28.525250] 1ea0: c0663924 00000028 c0661ea8 c0661ea8 c0661eb0 c0661eb0 0000001e c0660000 [   28.533417] 1ec0: 40000003 00000008 c0695a00 0000000a c066208c 00000100 c0661ee0 c0027410 [   28.541584] 1ee0: ef0fb700 2f158000 00200000 ffff95e8 00000004 c0662100 c0662080 00000003 [   28.549751] 1f00: 00000000 00000000 00000000 c065b45c 0000001e ef005000 c0647a30 00000000 [   28.557919] 1f20: 00000000 c0027798 00000000 c005cf40 f0802100 c0662ffc c0661f60 f0803100 [   28.566088] 1f40: c0661fb8 c00093bc c000ffb4 60070013 ffffffff c0661f94 c0661fb8 c00137d4 [   28.574267] 1f60: 00000001 00000000 00000000 c001ffa0 00000000 c0660000 00000000 c065a364 [   28.582441] 1f80: c0661fb8 c0647a30 00000000 00000000 00000000 c0661fb0 c000ffb0 c000ffb4 [   28.590608] 1fa0: 60070013 ffffffff 00000051 00000000 00000000 c005496c c0662400 c061bc40 [   28.598776] 1fc0: ffffffff ffffffff 00000000 c061b680 00000000 c0647a30 00000000 c0695294 [   28.606943] 1fe0: c0662488 c0647a2c c066619c 6000406a 413fc090 6000807c 00000000 00000000 [   28.615127] [<c03af580>] (skb_put) from [<ef376520>] (0xef376520) [   28.621218] Code: e5902054 e590c090 e3520000 0a000000 (e7f001f2) [   28.627307] ---[ end trace 4824734e2243fdb6 ]---  [   34.377068] Internal error: Oops: 17 [#1] SMP ARM [   34.382854] Modules linked in: [   34.385947] CPU: 0 PID: 3 Comm: ksoftirqd/0 Not tainted 4.4.0+ #120 [   34.392219] Hardware name: Rockchip (Device Tree) [   34.396937] task: ef02d040 ti: ef05c000 task.ti: ef05c000 [   34.402376] PC is at __dev_kfree_skb_irq+0x4/0x80 [   34.407121] LR is at arc_emac_poll+0x130/0x474 [   34.411583] pc : [<c03bb640>]    lr : [<c028fd94>]    psr: 60030013 [   34.411583] sp : ef05de68  ip : 0008e83c  fp : ef377000 [   34.423062] r10: c001bec4  r9 : 00000000  r8 : f08b24c8 [   34.428296] r7 : f08b2400  r6 : 00000075  r5 : 00000019  r4 : ef376000 [   34.434827] r3 : 00060000  r2 : 00000042  r1 : 00000001  r0 : 00000000 [   34.441365] Flags: nZCv  IRQs on  FIQs on  Mode SVC_32  ISA ARM  Segment none [   34.448507] Control: 10c5387d  Table: 8f25c04a  DAC: 00000051 [   34.454262] Process ksoftirqd/0 (pid: 3, stack limit = 0xef05c210) [   34.460449] Stack: (0xef05de68 to 0xef05e000) [   34.464827] de60:                   ef376000 c028fd94 00000000 c0669480 c0669480 ef376520 [   34.473022] de80: 00000028 00000001 00002ae4 ef376520 ef7b6900 c028fc64 2f158000 ef05dec0 [   34.481215] dea0: ef05dec8 0000012c c065e900 c03bdeac ffff983f c0662100 c0663924 00000028 [   34.489409] dec0: ef05dec0 ef05dec0 ef05dec8 ef05dec8 ef7b6000 ef05c000 40000003 00000008 [   34.497600] dee0: c0695a00 0000000a c066208c 00000100 ef05def8 c0027410 ef7b6000 40000000 [   34.505795] df00: 04208040 ffff983e 00000004 c0662100 c0662080 00000003 ef05c000 ef027340 [   34.513985] df20: ef05c000 c0666c2c 00000000 00000001 00000002 00000000 00000000 c0027568 [   34.522176] df40: ef027340 c003ef48 ef027300 00000000 ef027340 c003edd4 00000000 00000000 [   34.530367] df60: 00000000 c003c37c ffffff7f 00000001 00000000 ef027340 00000000 00030003 [   34.538559] df80: ef05df80 ef05df80 00000000 00000000 ef05df90 ef05df90 ef05dfac ef027300 [   34.546750] dfa0: c003c2a4 00000000 00000000 c000f578 00000000 00000000 00000000 00000000 [   34.554939] dfc0: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 [   34.563129] dfe0: 00000000 00000000 00000000 00000000 00000013 00000000 ffffffff dfff7fff [   34.571360] [<c03bb640>] (__dev_kfree_skb_irq) from [<c028fd94>] (arc_emac_poll+0x130/0x474) [   34.579840] [<c028fd94>] (arc_emac_poll) from [<c03bdeac>] (net_rx_action+0xdc/0x28c) [   34.587712] [<c03bdeac>] (net_rx_action) from [<c0027410>] (__do_softirq+0xcc/0x1f8) [   34.595482] [<c0027410>] (__do_softirq) from [<c0027568>] (run_ksoftirqd+0x2c/0x50) [   34.603168] [<c0027568>] (run_ksoftirqd) from [<c003ef48>] (smpboot_thread_fn+0x174/0x18c) [   34.611466] [<c003ef48>] (smpboot_thread_fn) from [<c003c37c>] (kthread+0xd8/0xec) [   34.619075] [<c003c37c>] (kthread) from [<c000f578>] (ret_from_fork+0x14/0x3c) [   34.626317] Code: e8bd8010 e3a00000 e12fff1e e92d4010 (e59030a4) [   34.632572] ---[ end trace cca5a3d86a82249a ]---  ",
        "func_before": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
        "func": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\t\ttx_buff->skb = NULL;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n \t\tstruct sk_buff *skb = tx_buff->skb;\n \t\tunsigned int info = le32_to_cpu(txbd->info);\n \n-\t\tif ((info & FOR_EMAC) || !txbd->data)\n+\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n \t\t\tbreak;\n \n \t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n@@ -39,6 +39,7 @@\n \n \t\ttxbd->data = 0;\n \t\ttxbd->info = 0;\n+\t\ttx_buff->skb = NULL;\n \n \t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif ((info & FOR_EMAC) || !txbd->data)"
            ],
            "added_lines": [
                "\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)",
                "\t\ttx_buff->skb = NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-17341",
        "func_name": "xen-project/xen/arch_iommu_populate_page_table",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service or gain privileges by leveraging a page-writability race condition during addition of a passed-through PCI device.",
        "git_url": "https://github.com/xen-project/xen/commit/1f0b0bb7773d537bcf169e021495d0986d9809fc",
        "commit_title": "IOMMU/x86: fix type ref-counting race upon IOMMU page table construction",
        "commit_text": " When arch_iommu_populate_page_table() gets invoked for an already running guest, simply looking at page types once isn't enough, as they may change at any time. Add logic to re-check the type after having mapped the page, unmapping it again if needed.  This is XSA-285.  Tentatively-Acked-by: Andrew Cooper <andrew.cooper3@citrix.com>",
        "func_before": "int arch_iommu_populate_page_table(struct domain *d)\n{\n    struct page_info *page;\n    int rc = 0, n = 0;\n\n    spin_lock(&d->page_alloc_lock);\n\n    if ( unlikely(d->is_dying) )\n        rc = -ESRCH;\n\n    while ( !rc && (page = page_list_remove_head(&d->page_list)) )\n    {\n        if ( is_hvm_domain(d) ||\n            (page->u.inuse.type_info & PGT_type_mask) == PGT_writable_page )\n        {\n            unsigned long mfn = mfn_x(page_to_mfn(page));\n            unsigned long gfn = mfn_to_gmfn(d, mfn);\n            unsigned int flush_flags = 0;\n\n            if ( gfn != gfn_x(INVALID_GFN) )\n            {\n                ASSERT(!(gfn >> DEFAULT_DOMAIN_ADDRESS_WIDTH));\n                BUG_ON(SHARED_M2P(gfn));\n                rc = iommu_map(d, _dfn(gfn), _mfn(mfn), PAGE_ORDER_4K,\n                               IOMMUF_readable | IOMMUF_writable,\n                               &flush_flags);\n            }\n            if ( rc )\n            {\n                page_list_add(page, &d->page_list);\n                break;\n            }\n        }\n        page_list_add_tail(page, &d->arch.relmem_list);\n        if ( !(++n & 0xff) && !page_list_empty(&d->page_list) &&\n             hypercall_preempt_check() )\n            rc = -ERESTART;\n    }\n\n    if ( !rc )\n    {\n        /*\n         * The expectation here is that generally there are many normal pages\n         * on relmem_list (the ones we put there) and only few being in an\n         * offline/broken state. The latter ones are always at the head of the\n         * list. Hence we first move the whole list, and then move back the\n         * first few entries.\n         */\n        page_list_move(&d->page_list, &d->arch.relmem_list);\n        while ( !page_list_empty(&d->page_list) &&\n                (page = page_list_first(&d->page_list),\n                 (page->count_info & (PGC_state|PGC_broken))) )\n        {\n            page_list_del(page, &d->page_list);\n            page_list_add_tail(page, &d->arch.relmem_list);\n        }\n    }\n\n    spin_unlock(&d->page_alloc_lock);\n\n    if ( !rc )\n        /*\n         * flush_flags are not tracked across hypercall pre-emption so\n         * assume a full flush is necessary.\n         */\n        rc = iommu_iotlb_flush_all(\n            d, IOMMU_FLUSHF_added | IOMMU_FLUSHF_modified);\n\n    if ( rc && rc != -ERESTART )\n        iommu_teardown(d);\n\n    return rc;\n}",
        "func": "int arch_iommu_populate_page_table(struct domain *d)\n{\n    struct page_info *page;\n    int rc = 0, n = 0;\n\n    spin_lock(&d->page_alloc_lock);\n\n    if ( unlikely(d->is_dying) )\n        rc = -ESRCH;\n\n    while ( !rc && (page = page_list_remove_head(&d->page_list)) )\n    {\n        if ( is_hvm_domain(d) ||\n            (page->u.inuse.type_info & PGT_type_mask) == PGT_writable_page )\n        {\n            unsigned long mfn = mfn_x(page_to_mfn(page));\n            unsigned long gfn = mfn_to_gmfn(d, mfn);\n            unsigned int flush_flags = 0;\n\n            if ( gfn != gfn_x(INVALID_GFN) )\n            {\n                ASSERT(!(gfn >> DEFAULT_DOMAIN_ADDRESS_WIDTH));\n                BUG_ON(SHARED_M2P(gfn));\n                rc = iommu_map(d, _dfn(gfn), _mfn(mfn), PAGE_ORDER_4K,\n                               IOMMUF_readable | IOMMUF_writable,\n                               &flush_flags);\n\n                /*\n                 * We may be working behind the back of a running guest, which\n                 * may change the type of a page at any time.  We can't prevent\n                 * this (for instance, by bumping the type count while mapping\n                 * the page) without causing legitimate guest type-change\n                 * operations to fail.  So after adding the page to the IOMMU,\n                 * check again to make sure this is still valid.  NB that the\n                 * writable entry in the iommu is harmless until later, when\n                 * the actual device gets assigned.\n                 */\n                if ( !rc && !is_hvm_domain(d) &&\n                     ((page->u.inuse.type_info & PGT_type_mask) !=\n                      PGT_writable_page) )\n                {\n                    rc = iommu_unmap(d, _dfn(gfn), PAGE_ORDER_4K, &flush_flags);\n                    /* If the type changed yet again, simply force a retry. */\n                    if ( !rc && ((page->u.inuse.type_info & PGT_type_mask) ==\n                                 PGT_writable_page) )\n                        rc = -ERESTART;\n                }\n            }\n            if ( rc )\n            {\n                page_list_add(page, &d->page_list);\n                break;\n            }\n        }\n        page_list_add_tail(page, &d->arch.relmem_list);\n        if ( !(++n & 0xff) && !page_list_empty(&d->page_list) &&\n             hypercall_preempt_check() )\n            rc = -ERESTART;\n    }\n\n    if ( !rc )\n    {\n        /*\n         * The expectation here is that generally there are many normal pages\n         * on relmem_list (the ones we put there) and only few being in an\n         * offline/broken state. The latter ones are always at the head of the\n         * list. Hence we first move the whole list, and then move back the\n         * first few entries.\n         */\n        page_list_move(&d->page_list, &d->arch.relmem_list);\n        while ( !page_list_empty(&d->page_list) &&\n                (page = page_list_first(&d->page_list),\n                 (page->count_info & (PGC_state|PGC_broken))) )\n        {\n            page_list_del(page, &d->page_list);\n            page_list_add_tail(page, &d->arch.relmem_list);\n        }\n    }\n\n    spin_unlock(&d->page_alloc_lock);\n\n    if ( !rc )\n        /*\n         * flush_flags are not tracked across hypercall pre-emption so\n         * assume a full flush is necessary.\n         */\n        rc = iommu_iotlb_flush_all(\n            d, IOMMU_FLUSHF_added | IOMMU_FLUSHF_modified);\n\n    if ( rc && rc != -ERESTART )\n        iommu_teardown(d);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,6 +24,27 @@\n                 rc = iommu_map(d, _dfn(gfn), _mfn(mfn), PAGE_ORDER_4K,\n                                IOMMUF_readable | IOMMUF_writable,\n                                &flush_flags);\n+\n+                /*\n+                 * We may be working behind the back of a running guest, which\n+                 * may change the type of a page at any time.  We can't prevent\n+                 * this (for instance, by bumping the type count while mapping\n+                 * the page) without causing legitimate guest type-change\n+                 * operations to fail.  So after adding the page to the IOMMU,\n+                 * check again to make sure this is still valid.  NB that the\n+                 * writable entry in the iommu is harmless until later, when\n+                 * the actual device gets assigned.\n+                 */\n+                if ( !rc && !is_hvm_domain(d) &&\n+                     ((page->u.inuse.type_info & PGT_type_mask) !=\n+                      PGT_writable_page) )\n+                {\n+                    rc = iommu_unmap(d, _dfn(gfn), PAGE_ORDER_4K, &flush_flags);\n+                    /* If the type changed yet again, simply force a retry. */\n+                    if ( !rc && ((page->u.inuse.type_info & PGT_type_mask) ==\n+                                 PGT_writable_page) )\n+                        rc = -ERESTART;\n+                }\n             }\n             if ( rc )\n             {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "                /*",
                "                 * We may be working behind the back of a running guest, which",
                "                 * may change the type of a page at any time.  We can't prevent",
                "                 * this (for instance, by bumping the type count while mapping",
                "                 * the page) without causing legitimate guest type-change",
                "                 * operations to fail.  So after adding the page to the IOMMU,",
                "                 * check again to make sure this is still valid.  NB that the",
                "                 * writable entry in the iommu is harmless until later, when",
                "                 * the actual device gets assigned.",
                "                 */",
                "                if ( !rc && !is_hvm_domain(d) &&",
                "                     ((page->u.inuse.type_info & PGT_type_mask) !=",
                "                      PGT_writable_page) )",
                "                {",
                "                    rc = iommu_unmap(d, _dfn(gfn), PAGE_ORDER_4K, &flush_flags);",
                "                    /* If the type changed yet again, simply force a retry. */",
                "                    if ( !rc && ((page->u.inuse.type_info & PGT_type_mask) ==",
                "                                 PGT_writable_page) )",
                "                        rc = -ERESTART;",
                "                }"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-6346",
        "func_name": "torvalds/linux/fanout_add",
        "description": "Race condition in net/packet/af_packet.c in the Linux kernel before 4.9.13 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a multithreaded application that makes PACKET_FANOUT setsockopt system calls.",
        "git_url": "https://github.com/torvalds/linux/commit/d199fab63c11998a602205f7ee7ff7c05c97164b",
        "commit_title": "packet: fix races in fanout_add()",
        "commit_text": " Multiple threads can call fanout_add() at the same time.  We need to grab fanout_mutex earlier to avoid races that could lead to one thread freeing po->rollover that was set by another thread.  Do the same in fanout_release(), for peace of mind, and to help us finding lockdep issues earlier.  Cc: Willem de Bruijn <willemb@google.com>",
        "func_before": "static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (!po->running)\n\t\treturn -EINVAL;\n\n\tif (po->fanout)\n\t\treturn -EALREADY;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\tpo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);\n\t\tif (!po->rollover)\n\t\t\treturn -ENOMEM;\n\t\tatomic_long_set(&po->rollover->num, 0);\n\t\tatomic_long_set(&po->rollover->num_huge, 0);\n\t\tatomic_long_set(&po->rollover->num_failed, 0);\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\tatomic_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tdev_add_pack(&match->prot_hook);\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\tif (match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (atomic_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\tatomic_inc(&match->sk_ref);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&fanout_mutex);\n\tif (err) {\n\t\tkfree(po->rollover);\n\t\tpo->rollover = NULL;\n\t}\n\treturn err;\n}",
        "func": "static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_rollover *rollover = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\n\terr = -EINVAL;\n\tif (!po->running)\n\t\tgoto out;\n\n\terr = -EALREADY;\n\tif (po->fanout)\n\t\tgoto out;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\terr = -ENOMEM;\n\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n\t\tif (!rollover)\n\t\t\tgoto out;\n\t\tatomic_long_set(&rollover->num, 0);\n\t\tatomic_long_set(&rollover->num_huge, 0);\n\t\tatomic_long_set(&rollover->num_failed, 0);\n\t\tpo->rollover = rollover;\n\t}\n\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\tatomic_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tdev_add_pack(&match->prot_hook);\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\tif (match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (atomic_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\tatomic_inc(&match->sk_ref);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\nout:\n\tif (err && rollover) {\n\t\tkfree(rollover);\n\t\tpo->rollover = NULL;\n\t}\n\tmutex_unlock(&fanout_mutex);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n {\n+\tstruct packet_rollover *rollover = NULL;\n \tstruct packet_sock *po = pkt_sk(sk);\n \tstruct packet_fanout *f, *match;\n \tu8 type = type_flags & 0xff;\n@@ -22,23 +23,28 @@\n \t\treturn -EINVAL;\n \t}\n \n+\tmutex_lock(&fanout_mutex);\n+\n+\terr = -EINVAL;\n \tif (!po->running)\n-\t\treturn -EINVAL;\n+\t\tgoto out;\n \n+\terr = -EALREADY;\n \tif (po->fanout)\n-\t\treturn -EALREADY;\n+\t\tgoto out;\n \n \tif (type == PACKET_FANOUT_ROLLOVER ||\n \t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n-\t\tpo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);\n-\t\tif (!po->rollover)\n-\t\t\treturn -ENOMEM;\n-\t\tatomic_long_set(&po->rollover->num, 0);\n-\t\tatomic_long_set(&po->rollover->num_huge, 0);\n-\t\tatomic_long_set(&po->rollover->num_failed, 0);\n+\t\terr = -ENOMEM;\n+\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n+\t\tif (!rollover)\n+\t\t\tgoto out;\n+\t\tatomic_long_set(&rollover->num, 0);\n+\t\tatomic_long_set(&rollover->num_huge, 0);\n+\t\tatomic_long_set(&rollover->num_failed, 0);\n+\t\tpo->rollover = rollover;\n \t}\n \n-\tmutex_lock(&fanout_mutex);\n \tmatch = NULL;\n \tlist_for_each_entry(f, &fanout_list, list) {\n \t\tif (f->id == id &&\n@@ -85,10 +91,10 @@\n \t\t}\n \t}\n out:\n-\tmutex_unlock(&fanout_mutex);\n-\tif (err) {\n-\t\tkfree(po->rollover);\n+\tif (err && rollover) {\n+\t\tkfree(rollover);\n \t\tpo->rollover = NULL;\n \t}\n+\tmutex_unlock(&fanout_mutex);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn -EINVAL;",
                "\t\treturn -EALREADY;",
                "\t\tpo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);",
                "\t\tif (!po->rollover)",
                "\t\t\treturn -ENOMEM;",
                "\t\tatomic_long_set(&po->rollover->num, 0);",
                "\t\tatomic_long_set(&po->rollover->num_huge, 0);",
                "\t\tatomic_long_set(&po->rollover->num_failed, 0);",
                "\tmutex_lock(&fanout_mutex);",
                "\tmutex_unlock(&fanout_mutex);",
                "\tif (err) {",
                "\t\tkfree(po->rollover);"
            ],
            "added_lines": [
                "\tstruct packet_rollover *rollover = NULL;",
                "\tmutex_lock(&fanout_mutex);",
                "",
                "\terr = -EINVAL;",
                "\t\tgoto out;",
                "\terr = -EALREADY;",
                "\t\tgoto out;",
                "\t\terr = -ENOMEM;",
                "\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);",
                "\t\tif (!rollover)",
                "\t\t\tgoto out;",
                "\t\tatomic_long_set(&rollover->num, 0);",
                "\t\tatomic_long_set(&rollover->num_huge, 0);",
                "\t\tatomic_long_set(&rollover->num_failed, 0);",
                "\t\tpo->rollover = rollover;",
                "\tif (err && rollover) {",
                "\t\tkfree(rollover);",
                "\tmutex_unlock(&fanout_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-6346",
        "func_name": "torvalds/linux/fanout_release",
        "description": "Race condition in net/packet/af_packet.c in the Linux kernel before 4.9.13 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a multithreaded application that makes PACKET_FANOUT setsockopt system calls.",
        "git_url": "https://github.com/torvalds/linux/commit/d199fab63c11998a602205f7ee7ff7c05c97164b",
        "commit_title": "packet: fix races in fanout_add()",
        "commit_text": " Multiple threads can call fanout_add() at the same time.  We need to grab fanout_mutex earlier to avoid races that could lead to one thread freeing po->rollover that was set by another thread.  Do the same in fanout_release(), for peace of mind, and to help us finding lockdep issues earlier.  Cc: Willem de Bruijn <willemb@google.com>",
        "func_before": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tf = po->fanout;\n\tif (!f)\n\t\treturn;\n\n\tmutex_lock(&fanout_mutex);\n\tpo->fanout = NULL;\n\n\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\tlist_del(&f->list);\n\t\tdev_remove_pack(&f->prot_hook);\n\t\tfanout_release_data(f);\n\t\tkfree(f);\n\t}\n\tmutex_unlock(&fanout_mutex);\n\n\tif (po->rollover)\n\t\tkfree_rcu(po->rollover, rcu);\n}",
        "func": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tmutex_lock(&fanout_mutex);\n\tf = po->fanout;\n\tif (f) {\n\t\tpo->fanout = NULL;\n\n\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\t\tlist_del(&f->list);\n\t\t\tdev_remove_pack(&f->prot_hook);\n\t\t\tfanout_release_data(f);\n\t\t\tkfree(f);\n\t\t}\n\n\t\tif (po->rollover)\n\t\t\tkfree_rcu(po->rollover, rcu);\n\t}\n\tmutex_unlock(&fanout_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,21 +3,20 @@\n \tstruct packet_sock *po = pkt_sk(sk);\n \tstruct packet_fanout *f;\n \n+\tmutex_lock(&fanout_mutex);\n \tf = po->fanout;\n-\tif (!f)\n-\t\treturn;\n+\tif (f) {\n+\t\tpo->fanout = NULL;\n \n-\tmutex_lock(&fanout_mutex);\n-\tpo->fanout = NULL;\n+\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n+\t\t\tlist_del(&f->list);\n+\t\t\tdev_remove_pack(&f->prot_hook);\n+\t\t\tfanout_release_data(f);\n+\t\t\tkfree(f);\n+\t\t}\n \n-\tif (atomic_dec_and_test(&f->sk_ref)) {\n-\t\tlist_del(&f->list);\n-\t\tdev_remove_pack(&f->prot_hook);\n-\t\tfanout_release_data(f);\n-\t\tkfree(f);\n+\t\tif (po->rollover)\n+\t\t\tkfree_rcu(po->rollover, rcu);\n \t}\n \tmutex_unlock(&fanout_mutex);\n-\n-\tif (po->rollover)\n-\t\tkfree_rcu(po->rollover, rcu);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!f)",
                "\t\treturn;",
                "\tmutex_lock(&fanout_mutex);",
                "\tpo->fanout = NULL;",
                "\tif (atomic_dec_and_test(&f->sk_ref)) {",
                "\t\tlist_del(&f->list);",
                "\t\tdev_remove_pack(&f->prot_hook);",
                "\t\tfanout_release_data(f);",
                "\t\tkfree(f);",
                "",
                "\tif (po->rollover)",
                "\t\tkfree_rcu(po->rollover, rcu);"
            ],
            "added_lines": [
                "\tmutex_lock(&fanout_mutex);",
                "\tif (f) {",
                "\t\tpo->fanout = NULL;",
                "\t\tif (atomic_dec_and_test(&f->sk_ref)) {",
                "\t\t\tlist_del(&f->list);",
                "\t\t\tdev_remove_pack(&f->prot_hook);",
                "\t\t\tfanout_release_data(f);",
                "\t\t\tkfree(f);",
                "\t\t}",
                "\t\tif (po->rollover)",
                "\t\t\tkfree_rcu(po->rollover, rcu);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-24949",
        "func_name": "MisterTea/EternalTerminal/parseRangesToRequests",
        "description": "A privilege escalation to root exists in Eternal Terminal prior to version 6.2.0. This is due to the combination of a race condition, buffer overflow, and logic bug all in PipeSocketHandler::listen().",
        "git_url": "https://github.com/MisterTea/EternalTerminal/commit/900348bb8bc96e1c7ba4888ac8480f643c43d3c3",
        "commit_title": "red fixes (#468)",
        "commit_text": " * red fixes\r \r * remove magic number",
        "func_before": "vector<PortForwardSourceRequest> parseRangesToRequests(const string& input) {\n  vector<PortForwardSourceRequest> pfsrs;\n  auto j = split(input, ',');\n  for (auto& pair : j) {\n    vector<string> sourceDestination = split(pair, ':');\n    try {\n      if (sourceDestination[0].find_first_not_of(\"0123456789-\") !=\n              string::npos &&\n          sourceDestination[1].find_first_not_of(\"0123456789-\") !=\n              string::npos) {\n        PortForwardSourceRequest pfsr;\n        pfsr.mutable_source()->set_name(sourceDestination[0]);\n        pfsr.mutable_destination()->set_name(sourceDestination[1]);\n        pfsrs.push_back(pfsr);\n      } else if (sourceDestination[0].find('-') != string::npos &&\n                 sourceDestination[1].find('-') != string::npos) {\n        vector<string> sourcePortRange = split(sourceDestination[0], '-');\n        int sourcePortStart = stoi(sourcePortRange[0]);\n        int sourcePortEnd = stoi(sourcePortRange[1]);\n\n        vector<string> destinationPortRange = split(sourceDestination[1], '-');\n        int destinationPortStart = stoi(destinationPortRange[0]);\n        int destinationPortEnd = stoi(destinationPortRange[1]);\n\n        if (sourcePortEnd - sourcePortStart !=\n            destinationPortEnd - destinationPortStart) {\n          STFATAL << \"source/destination port range mismatch\";\n          exit(1);\n        } else {\n          int portRangeLength = sourcePortEnd - sourcePortStart + 1;\n          for (int i = 0; i < portRangeLength; ++i) {\n            PortForwardSourceRequest pfsr;\n            pfsr.mutable_source()->set_port(sourcePortStart + i);\n            pfsr.mutable_destination()->set_port(destinationPortStart + i);\n            pfsrs.push_back(pfsr);\n          }\n        }\n      } else if (sourceDestination[0].find('-') != string::npos ||\n                 sourceDestination[1].find('-') != string::npos) {\n        STFATAL << \"Invalid port range syntax: if source is range, \"\n                   \"destination must be range\";\n      } else {\n        PortForwardSourceRequest pfsr;\n        pfsr.mutable_source()->set_port(stoi(sourceDestination[0]));\n        pfsr.mutable_destination()->set_port(stoi(sourceDestination[1]));\n        pfsrs.push_back(pfsr);\n      }\n    } catch (const std::logic_error& lr) {\n      STFATAL << \"Logic error: \" << lr.what();\n      exit(1);\n    }\n  }\n  return pfsrs;\n}",
        "func": "vector<PortForwardSourceRequest> parseRangesToRequests(const string& input) {\n  vector<PortForwardSourceRequest> pfsrs;\n  auto j = split(input, ',');\n  for (auto& pair : j) {\n    vector<string> sourceDestination = split(pair, ':');\n    try {\n      if (sourceDestination[0].find_first_not_of(\"0123456789-\") !=\n              string::npos &&\n          sourceDestination[1].find_first_not_of(\"0123456789-\") !=\n              string::npos) {\n        PortForwardSourceRequest pfsr;\n        pfsr.set_environmentvariable(sourceDestination[0]);\n        pfsr.mutable_destination()->set_name(sourceDestination[1]);\n        pfsrs.push_back(pfsr);\n      } else if (sourceDestination[0].find('-') != string::npos &&\n                 sourceDestination[1].find('-') != string::npos) {\n        vector<string> sourcePortRange = split(sourceDestination[0], '-');\n        int sourcePortStart = stoi(sourcePortRange[0]);\n        int sourcePortEnd = stoi(sourcePortRange[1]);\n\n        vector<string> destinationPortRange = split(sourceDestination[1], '-');\n        int destinationPortStart = stoi(destinationPortRange[0]);\n        int destinationPortEnd = stoi(destinationPortRange[1]);\n\n        if (sourcePortEnd - sourcePortStart !=\n            destinationPortEnd - destinationPortStart) {\n          STFATAL << \"source/destination port range mismatch\";\n          exit(1);\n        } else {\n          int portRangeLength = sourcePortEnd - sourcePortStart + 1;\n          for (int i = 0; i < portRangeLength; ++i) {\n            PortForwardSourceRequest pfsr;\n            pfsr.mutable_source()->set_port(sourcePortStart + i);\n            pfsr.mutable_destination()->set_port(destinationPortStart + i);\n            pfsrs.push_back(pfsr);\n          }\n        }\n      } else if (sourceDestination[0].find('-') != string::npos ||\n                 sourceDestination[1].find('-') != string::npos) {\n        STFATAL << \"Invalid port range syntax: if source is range, \"\n                   \"destination must be range\";\n      } else {\n        PortForwardSourceRequest pfsr;\n        pfsr.mutable_source()->set_port(stoi(sourceDestination[0]));\n        pfsr.mutable_destination()->set_port(stoi(sourceDestination[1]));\n        pfsrs.push_back(pfsr);\n      }\n    } catch (const std::logic_error& lr) {\n      STFATAL << \"Logic error: \" << lr.what();\n      exit(1);\n    }\n  }\n  return pfsrs;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n           sourceDestination[1].find_first_not_of(\"0123456789-\") !=\n               string::npos) {\n         PortForwardSourceRequest pfsr;\n-        pfsr.mutable_source()->set_name(sourceDestination[0]);\n+        pfsr.set_environmentvariable(sourceDestination[0]);\n         pfsr.mutable_destination()->set_name(sourceDestination[1]);\n         pfsrs.push_back(pfsr);\n       } else if (sourceDestination[0].find('-') != string::npos &&",
        "diff_line_info": {
            "deleted_lines": [
                "        pfsr.mutable_source()->set_name(sourceDestination[0]);"
            ],
            "added_lines": [
                "        pfsr.set_environmentvariable(sourceDestination[0]);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-24949",
        "func_name": "MisterTea/EternalTerminal/PipeSocketHandler::connect",
        "description": "A privilege escalation to root exists in Eternal Terminal prior to version 6.2.0. This is due to the combination of a race condition, buffer overflow, and logic bug all in PipeSocketHandler::listen().",
        "git_url": "https://github.com/MisterTea/EternalTerminal/commit/900348bb8bc96e1c7ba4888ac8480f643c43d3c3",
        "commit_title": "red fixes (#468)",
        "commit_text": " * red fixes\r \r * remove magic number",
        "func_before": "int PipeSocketHandler::connect(const SocketEndpoint& endpoint) {\n  lock_guard<std::recursive_mutex> mutexGuard(globalMutex);\n\n  string pipePath = endpoint.name();\n  sockaddr_un remote;\n\n  int sockFd = ::socket(AF_UNIX, SOCK_STREAM, 0);\n  FATAL_FAIL(sockFd);\n  initSocket(sockFd);\n  remote.sun_family = AF_UNIX;\n  strcpy(remote.sun_path, pipePath.c_str());\n\n  VLOG(3) << \"Connecting to \" << endpoint << \" with fd \" << sockFd;\n  int result =\n      ::connect(sockFd, (struct sockaddr*)&remote, sizeof(sockaddr_un));\n  auto localErrno = GetErrno();\n  if (result < 0 && localErrno != EINPROGRESS) {\n    VLOG(3) << \"Connection result: \" << result << \" (\" << strerror(localErrno)\n            << \")\";\n#ifdef WIN32\n    ::shutdown(sockFd, SD_BOTH);\n#else\n    ::shutdown(sockFd, SHUT_RDWR);\n#endif\n#ifdef _MSC_VER\n    FATAL_FAIL(::closesocket(sockFd));\n#else\n    FATAL_FAIL(::close(sockFd));\n#endif\n    sockFd = -1;\n    SetErrno(localErrno);\n    return sockFd;\n  }\n\n  fd_set fdset;\n  FD_ZERO(&fdset);\n  FD_SET(sockFd, &fdset);\n  timeval tv;\n  tv.tv_sec = 3; /* 3 second timeout */\n  tv.tv_usec = 0;\n  VLOG(4) << \"Before selecting sockFd\";\n  select(sockFd + 1, NULL, &fdset, NULL, &tv);\n\n  if (FD_ISSET(sockFd, &fdset)) {\n    VLOG(4) << \"sockFd \" << sockFd << \" is selected\";\n    int so_error;\n    socklen_t len = sizeof so_error;\n\n    FATAL_FAIL(\n        ::getsockopt(sockFd, SOL_SOCKET, SO_ERROR, (char*)&so_error, &len));\n\n    if (so_error == 0) {\n      LOG(INFO) << \"Connected to endpoint \" << endpoint;\n      // Initialize the socket again once it's blocking to make sure timeouts\n      // are set\n      initSocket(sockFd);\n\n      // if we get here, we must have connected successfully\n    } else {\n      LOG(INFO) << \"Error connecting to \" << endpoint << \": \" << so_error << \" \"\n                << strerror(so_error);\n#ifdef _MSC_VER\n      FATAL_FAIL(::closesocket(sockFd));\n#else\n      FATAL_FAIL(::close(sockFd));\n#endif\n      sockFd = -1;\n    }\n  } else {\n    auto localErrno = GetErrno();\n    LOG(INFO) << \"Error connecting to \" << endpoint << \": \" << localErrno << \" \"\n              << strerror(localErrno);\n#ifdef _MSC_VER\n    FATAL_FAIL(::closesocket(sockFd));\n#else\n    FATAL_FAIL(::close(sockFd));\n#endif\n    sockFd = -1;\n  }\n\n  LOG(INFO) << sockFd << \" is a good socket\";\n  if (sockFd >= 0) {\n    addToActiveSockets(sockFd);\n  }\n  return sockFd;\n}",
        "func": "int PipeSocketHandler::connect(const SocketEndpoint& endpoint) {\n  lock_guard<std::recursive_mutex> mutexGuard(globalMutex);\n\n  string pipePath = endpoint.name();\n  sockaddr_un remote;\n\n  int sockFd = ::socket(AF_UNIX, SOCK_STREAM, 0);\n  FATAL_FAIL(sockFd);\n  initSocket(sockFd);\n  remote.sun_family = AF_UNIX;\n  strncpy(remote.sun_path, pipePath.c_str(), sizeof(remote.sun_path));\n\n  VLOG(3) << \"Connecting to \" << endpoint << \" with fd \" << sockFd;\n  int result =\n      ::connect(sockFd, (struct sockaddr*)&remote, sizeof(sockaddr_un));\n  auto localErrno = GetErrno();\n  if (result < 0 && localErrno != EINPROGRESS) {\n    VLOG(3) << \"Connection result: \" << result << \" (\" << strerror(localErrno)\n            << \")\";\n#ifdef WIN32\n    ::shutdown(sockFd, SD_BOTH);\n#else\n    ::shutdown(sockFd, SHUT_RDWR);\n#endif\n#ifdef _MSC_VER\n    FATAL_FAIL(::closesocket(sockFd));\n#else\n    FATAL_FAIL(::close(sockFd));\n#endif\n    sockFd = -1;\n    SetErrno(localErrno);\n    return sockFd;\n  }\n\n  fd_set fdset;\n  FD_ZERO(&fdset);\n  FD_SET(sockFd, &fdset);\n  timeval tv;\n  tv.tv_sec = 3; /* 3 second timeout */\n  tv.tv_usec = 0;\n  VLOG(4) << \"Before selecting sockFd\";\n  select(sockFd + 1, NULL, &fdset, NULL, &tv);\n\n  if (FD_ISSET(sockFd, &fdset)) {\n    VLOG(4) << \"sockFd \" << sockFd << \" is selected\";\n    int so_error;\n    socklen_t len = sizeof so_error;\n\n    FATAL_FAIL(\n        ::getsockopt(sockFd, SOL_SOCKET, SO_ERROR, (char*)&so_error, &len));\n\n    if (so_error == 0) {\n      LOG(INFO) << \"Connected to endpoint \" << endpoint;\n      // Initialize the socket again once it's blocking to make sure timeouts\n      // are set\n      initSocket(sockFd);\n\n      // if we get here, we must have connected successfully\n    } else {\n      LOG(INFO) << \"Error connecting to \" << endpoint << \": \" << so_error << \" \"\n                << strerror(so_error);\n#ifdef _MSC_VER\n      FATAL_FAIL(::closesocket(sockFd));\n#else\n      FATAL_FAIL(::close(sockFd));\n#endif\n      sockFd = -1;\n    }\n  } else {\n    auto localErrno = GetErrno();\n    LOG(INFO) << \"Error connecting to \" << endpoint << \": \" << localErrno << \" \"\n              << strerror(localErrno);\n#ifdef _MSC_VER\n    FATAL_FAIL(::closesocket(sockFd));\n#else\n    FATAL_FAIL(::close(sockFd));\n#endif\n    sockFd = -1;\n  }\n\n  LOG(INFO) << sockFd << \" is a good socket\";\n  if (sockFd >= 0) {\n    addToActiveSockets(sockFd);\n  }\n  return sockFd;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,7 @@\n   FATAL_FAIL(sockFd);\n   initSocket(sockFd);\n   remote.sun_family = AF_UNIX;\n-  strcpy(remote.sun_path, pipePath.c_str());\n+  strncpy(remote.sun_path, pipePath.c_str(), sizeof(remote.sun_path));\n \n   VLOG(3) << \"Connecting to \" << endpoint << \" with fd \" << sockFd;\n   int result =",
        "diff_line_info": {
            "deleted_lines": [
                "  strcpy(remote.sun_path, pipePath.c_str());"
            ],
            "added_lines": [
                "  strncpy(remote.sun_path, pipePath.c_str(), sizeof(remote.sun_path));"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-24949",
        "func_name": "MisterTea/EternalTerminal/PipeSocketHandler::listen",
        "description": "A privilege escalation to root exists in Eternal Terminal prior to version 6.2.0. This is due to the combination of a race condition, buffer overflow, and logic bug all in PipeSocketHandler::listen().",
        "git_url": "https://github.com/MisterTea/EternalTerminal/commit/900348bb8bc96e1c7ba4888ac8480f643c43d3c3",
        "commit_title": "red fixes (#468)",
        "commit_text": " * red fixes\r \r * remove magic number",
        "func_before": "set<int> PipeSocketHandler::listen(const SocketEndpoint& endpoint) {\n  lock_guard<std::recursive_mutex> guard(globalMutex);\n\n  string pipePath = endpoint.name();\n  if (pipeServerSockets.find(pipePath) != pipeServerSockets.end()) {\n    throw runtime_error(\"Tried to listen twice on the same path\");\n  }\n\n  sockaddr_un local;\n\n  int fd = socket(AF_UNIX, SOCK_STREAM, 0);\n  FATAL_FAIL(fd);\n  initServerSocket(fd);\n  local.sun_family = AF_UNIX; /* local is declared before socket() ^ */\n  strcpy(local.sun_path, pipePath.c_str());\n  unlink(local.sun_path);\n\n  FATAL_FAIL(::bind(fd, (struct sockaddr*)&local, sizeof(sockaddr_un)));\n  ::listen(fd, 5);\n#ifndef WIN32\n  FATAL_FAIL(::chmod(local.sun_path, S_IRUSR | S_IWUSR | S_IXUSR));\n#endif\n\n  pipeServerSockets[pipePath] = set<int>({fd});\n  return pipeServerSockets[pipePath];\n}",
        "func": "set<int> PipeSocketHandler::listen(const SocketEndpoint& endpoint) {\n  lock_guard<std::recursive_mutex> guard(globalMutex);\n\n  string pipePath = endpoint.name();\n  if (pipeServerSockets.find(pipePath) != pipeServerSockets.end()) {\n    throw runtime_error(\"Tried to listen twice on the same path\");\n  }\n\n  sockaddr_un local;\n\n  int fd = socket(AF_UNIX, SOCK_STREAM, 0);\n  FATAL_FAIL(fd);\n  initServerSocket(fd);\n  local.sun_family = AF_UNIX; /* local is declared before socket() ^ */\n  strncpy(local.sun_path, pipePath.c_str(), sizeof(local.sun_path));\n  unlink(local.sun_path);\n\n  FATAL_FAIL(::bind(fd, (struct sockaddr*)&local, sizeof(sockaddr_un)));\n  ::listen(fd, 5);\n#ifndef WIN32\n  FATAL_FAIL(::chmod(local.sun_path, S_IRUSR | S_IWUSR | S_IXUSR));\n#endif\n\n  pipeServerSockets[pipePath] = set<int>({fd});\n  return pipeServerSockets[pipePath];\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,7 @@\n   FATAL_FAIL(fd);\n   initServerSocket(fd);\n   local.sun_family = AF_UNIX; /* local is declared before socket() ^ */\n-  strcpy(local.sun_path, pipePath.c_str());\n+  strncpy(local.sun_path, pipePath.c_str(), sizeof(local.sun_path));\n   unlink(local.sun_path);\n \n   FATAL_FAIL(::bind(fd, (struct sockaddr*)&local, sizeof(sockaddr_un)));",
        "diff_line_info": {
            "deleted_lines": [
                "  strcpy(local.sun_path, pipePath.c_str());"
            ],
            "added_lines": [
                "  strncpy(local.sun_path, pipePath.c_str(), sizeof(local.sun_path));"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-24949",
        "func_name": "MisterTea/EternalTerminal/UserTerminalRouter::acceptNewConnection",
        "description": "A privilege escalation to root exists in Eternal Terminal prior to version 6.2.0. This is due to the combination of a race condition, buffer overflow, and logic bug all in PipeSocketHandler::listen().",
        "git_url": "https://github.com/MisterTea/EternalTerminal/commit/900348bb8bc96e1c7ba4888ac8480f643c43d3c3",
        "commit_title": "red fixes (#468)",
        "commit_text": " * red fixes\r \r * remove magic number",
        "func_before": "IdKeyPair UserTerminalRouter::acceptNewConnection() {\n  LOG(INFO) << \"Listening to id/key FIFO\";\n  int terminalFd = socketHandler->accept(serverFd);\n  if (terminalFd < 0) {\n    if (GetErrno() != EAGAIN && GetErrno() != EWOULDBLOCK) {\n      FATAL_FAIL(-1);  // STFATAL with the error\n    } else {\n      return IdKeyPair({\"\", \"\"});  // Nothing to accept this time\n    }\n  }\n\n  LOG(INFO) << \"Connected\";\n\n  try {\n    Packet packet;\n    if (!socketHandler->readPacket(terminalFd, &packet)) {\n      STFATAL << \"Missing user info packet\";\n    }\n    if (packet.getHeader() != TerminalPacketType::TERMINAL_USER_INFO) {\n      STFATAL << \"Got an invalid packet header: \" << int(packet.getHeader());\n    }\n    TerminalUserInfo tui = stringToProto<TerminalUserInfo>(packet.getPayload());\n    tui.set_fd(terminalFd);\n    idInfoMap[tui.id()] = tui;\n    return IdKeyPair({tui.id(), tui.passkey()});\n  } catch (const std::runtime_error &re) {\n    STFATAL << \"Router can't talk to terminal: \" << re.what();\n  }\n\n  STFATAL << \"Should never get here\";\n  return IdKeyPair({\"\", \"\"});\n}",
        "func": "IdKeyPair UserTerminalRouter::acceptNewConnection() {\n  lock_guard<recursive_mutex> guard(routerMutex);\n  LOG(INFO) << \"Listening to id/key FIFO\";\n  int terminalFd = socketHandler->accept(serverFd);\n  if (terminalFd < 0) {\n    if (GetErrno() != EAGAIN && GetErrno() != EWOULDBLOCK) {\n      FATAL_FAIL(-1);  // STFATAL with the error\n    } else {\n      return IdKeyPair({\"\", \"\"});  // Nothing to accept this time\n    }\n  }\n\n  LOG(INFO) << \"Connected\";\n\n  try {\n    Packet packet;\n    if (!socketHandler->readPacket(terminalFd, &packet)) {\n      STFATAL << \"Missing user info packet\";\n    }\n    if (packet.getHeader() != TerminalPacketType::TERMINAL_USER_INFO) {\n      STFATAL << \"Got an invalid packet header: \" << int(packet.getHeader());\n    }\n    TerminalUserInfo tui = stringToProto<TerminalUserInfo>(packet.getPayload());\n    tui.set_fd(terminalFd);\n    idInfoMap[tui.id()] = tui;\n    return IdKeyPair({tui.id(), tui.passkey()});\n  } catch (const std::runtime_error &re) {\n    STFATAL << \"Router can't talk to terminal: \" << re.what();\n  }\n\n  STFATAL << \"Should never get here\";\n  return IdKeyPair({\"\", \"\"});\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n IdKeyPair UserTerminalRouter::acceptNewConnection() {\n+  lock_guard<recursive_mutex> guard(routerMutex);\n   LOG(INFO) << \"Listening to id/key FIFO\";\n   int terminalFd = socketHandler->accept(serverFd);\n   if (terminalFd < 0) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  lock_guard<recursive_mutex> guard(routerMutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-24949",
        "func_name": "MisterTea/EternalTerminal/UserTerminalRouter::getInfoForId",
        "description": "A privilege escalation to root exists in Eternal Terminal prior to version 6.2.0. This is due to the combination of a race condition, buffer overflow, and logic bug all in PipeSocketHandler::listen().",
        "git_url": "https://github.com/MisterTea/EternalTerminal/commit/900348bb8bc96e1c7ba4888ac8480f643c43d3c3",
        "commit_title": "red fixes (#468)",
        "commit_text": " * red fixes\r \r * remove magic number",
        "func_before": "TerminalUserInfo UserTerminalRouter::getInfoForId(const string &id) {\n  auto it = idInfoMap.find(id);\n  if (it == idInfoMap.end()) {\n    STFATAL << \" Tried to read from an id that no longer exists\";\n  }\n  return it->second;\n}",
        "func": "TerminalUserInfo UserTerminalRouter::getInfoForId(const string &id) {\n  lock_guard<recursive_mutex> guard(routerMutex);\n  auto it = idInfoMap.find(id);\n  if (it == idInfoMap.end()) {\n    STFATAL << \" Tried to read from an id that no longer exists\";\n  }\n  return it->second;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n TerminalUserInfo UserTerminalRouter::getInfoForId(const string &id) {\n+  lock_guard<recursive_mutex> guard(routerMutex);\n   auto it = idInfoMap.find(id);\n   if (it == idInfoMap.end()) {\n     STFATAL << \" Tried to read from an id that no longer exists\";",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  lock_guard<recursive_mutex> guard(routerMutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-2959",
        "func_name": "torvalds/linux/pipe_resize_ring",
        "description": "A race condition was found in the Linux kernel's watch queue due to a missing lock in pipe_resize_ring(). The specific flaw exists within the handling of pipe buffers. The issue results from the lack of proper locking when performing operations on an object. This flaw allows a local user to crash the system or escalate their privileges on the system.",
        "git_url": "https://github.com/torvalds/linux/commit/189b0ddc245139af81198d1a3637cac74f96e13a",
        "commit_title": "pipe: Fix missing lock in pipe_resize_ring()",
        "commit_text": " pipe_resize_ring() needs to take the pipe->rd_wait.lock spinlock to prevent post_one_notification() from trying to insert into the ring whilst the ring is being replaced.  The occupancy check must be done after the lock is taken, and the lock must be taken after the new ring is allocated.  The bug can lead to an oops looking something like:   BUG: KASAN: use-after-free in post_one_notification.isra.0+0x62e/0x840  Read of size 4 at addr ffff88801cc72a70 by task poc/27196  ...  Call Trace:   post_one_notification.isra.0+0x62e/0x840   __post_watch_notification+0x3b7/0x650   key_create_or_update+0xb8b/0xd20   __do_sys_add_key+0x175/0x340   __x64_sys_add_key+0xbe/0x140   do_syscall_64+0x5c/0xc0   entry_SYSCALL_64_after_hwframe+0x44/0xae  Reported by Selim Enes Karaduman @Enesdex working with Trend Micro Zero Day Initiative. ",
        "func_before": "int pipe_resize_ring(struct pipe_inode_info *pipe, unsigned int nr_slots)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int head, tail, mask, n;\n\n\t/*\n\t * We can shrink the pipe, if arg is greater than the ring occupancy.\n\t * Since we don't expect a lot of shrink+grow operations, just free and\n\t * allocate again like we would do for growing.  If the pipe currently\n\t * contains more buffers than arg, then return busy.\n\t */\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tn = pipe_occupancy(pipe->head, pipe->tail);\n\tif (nr_slots < n)\n\t\treturn -EBUSY;\n\n\tbufs = kcalloc(nr_slots, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indices.\n\t */\n\tif (n > 0) {\n\t\tunsigned int h = head & mask;\n\t\tunsigned int t = tail & mask;\n\t\tif (h > t) {\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       n * sizeof(struct pipe_buffer));\n\t\t} else {\n\t\t\tunsigned int tsize = pipe->ring_size - t;\n\t\t\tif (h > 0)\n\t\t\t\tmemcpy(bufs + tsize, pipe->bufs,\n\t\t\t\t       h * sizeof(struct pipe_buffer));\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       tsize * sizeof(struct pipe_buffer));\n\t\t}\n\t}\n\n\thead = n;\n\ttail = 0;\n\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->ring_size = nr_slots;\n\tif (pipe->max_usage > nr_slots)\n\t\tpipe->max_usage = nr_slots;\n\tpipe->tail = tail;\n\tpipe->head = head;\n\n\t/* This might have made more room for writers */\n\twake_up_interruptible(&pipe->wr_wait);\n\treturn 0;\n}",
        "func": "int pipe_resize_ring(struct pipe_inode_info *pipe, unsigned int nr_slots)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int head, tail, mask, n;\n\n\tbufs = kcalloc(nr_slots, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs))\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\n\tn = pipe_occupancy(head, tail);\n\tif (nr_slots < n) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tkfree(bufs);\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indices.\n\t */\n\tif (n > 0) {\n\t\tunsigned int h = head & mask;\n\t\tunsigned int t = tail & mask;\n\t\tif (h > t) {\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       n * sizeof(struct pipe_buffer));\n\t\t} else {\n\t\t\tunsigned int tsize = pipe->ring_size - t;\n\t\t\tif (h > 0)\n\t\t\t\tmemcpy(bufs + tsize, pipe->bufs,\n\t\t\t\t       h * sizeof(struct pipe_buffer));\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       tsize * sizeof(struct pipe_buffer));\n\t\t}\n\t}\n\n\thead = n;\n\ttail = 0;\n\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->ring_size = nr_slots;\n\tif (pipe->max_usage > nr_slots)\n\t\tpipe->max_usage = nr_slots;\n\tpipe->tail = tail;\n\tpipe->head = head;\n\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\n\t/* This might have made more room for writers */\n\twake_up_interruptible(&pipe->wr_wait);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,23 +3,22 @@\n \tstruct pipe_buffer *bufs;\n \tunsigned int head, tail, mask, n;\n \n-\t/*\n-\t * We can shrink the pipe, if arg is greater than the ring occupancy.\n-\t * Since we don't expect a lot of shrink+grow operations, just free and\n-\t * allocate again like we would do for growing.  If the pipe currently\n-\t * contains more buffers than arg, then return busy.\n-\t */\n-\tmask = pipe->ring_size - 1;\n-\thead = pipe->head;\n-\ttail = pipe->tail;\n-\tn = pipe_occupancy(pipe->head, pipe->tail);\n-\tif (nr_slots < n)\n-\t\treturn -EBUSY;\n-\n \tbufs = kcalloc(nr_slots, sizeof(*bufs),\n \t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n \tif (unlikely(!bufs))\n \t\treturn -ENOMEM;\n+\n+\tspin_lock_irq(&pipe->rd_wait.lock);\n+\tmask = pipe->ring_size - 1;\n+\thead = pipe->head;\n+\ttail = pipe->tail;\n+\n+\tn = pipe_occupancy(head, tail);\n+\tif (nr_slots < n) {\n+\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n+\t\tkfree(bufs);\n+\t\treturn -EBUSY;\n+\t}\n \n \t/*\n \t * The pipe array wraps around, so just start the new one at zero\n@@ -52,6 +51,8 @@\n \tpipe->tail = tail;\n \tpipe->head = head;\n \n+\tspin_unlock_irq(&pipe->rd_wait.lock);\n+\n \t/* This might have made more room for writers */\n \twake_up_interruptible(&pipe->wr_wait);\n \treturn 0;",
        "diff_line_info": {
            "deleted_lines": [
                "\t/*",
                "\t * We can shrink the pipe, if arg is greater than the ring occupancy.",
                "\t * Since we don't expect a lot of shrink+grow operations, just free and",
                "\t * allocate again like we would do for growing.  If the pipe currently",
                "\t * contains more buffers than arg, then return busy.",
                "\t */",
                "\tmask = pipe->ring_size - 1;",
                "\thead = pipe->head;",
                "\ttail = pipe->tail;",
                "\tn = pipe_occupancy(pipe->head, pipe->tail);",
                "\tif (nr_slots < n)",
                "\t\treturn -EBUSY;",
                ""
            ],
            "added_lines": [
                "",
                "\tspin_lock_irq(&pipe->rd_wait.lock);",
                "\tmask = pipe->ring_size - 1;",
                "\thead = pipe->head;",
                "\ttail = pipe->tail;",
                "",
                "\tn = pipe_occupancy(head, tail);",
                "\tif (nr_slots < n) {",
                "\t\tspin_unlock_irq(&pipe->rd_wait.lock);",
                "\t\tkfree(bufs);",
                "\t\treturn -EBUSY;",
                "\t}",
                "\tspin_unlock_irq(&pipe->rd_wait.lock);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3028",
        "func_name": "torvalds/linux/pfkey_register",
        "description": "A race condition was found in the Linux kernel's IP framework for transforming packets (XFRM subsystem) when multiple calls to xfrm_probe_algs occurred simultaneously. This flaw could allow a local attacker to potentially trigger an out-of-bounds write or leak kernel heap memory by performing an out-of-bounds read and copying it into a socket.",
        "git_url": "https://github.com/torvalds/linux/commit/ba953a9d89a00c078b85f4b190bc1dde66fe16b5",
        "commit_title": "af_key: Do not call xfrm_probe_algs in parallel",
        "commit_text": " When namespace support was added to xfrm/afkey, it caused the previously single-threaded call to xfrm_probe_algs to become multi-threaded.  This is buggy and needs to be fixed with a mutex. ",
        "func_before": "static int pfkey_register(struct sock *sk, struct sk_buff *skb, const struct sadb_msg *hdr, void * const *ext_hdrs)\n{\n\tstruct pfkey_sock *pfk = pfkey_sk(sk);\n\tstruct sk_buff *supp_skb;\n\n\tif (hdr->sadb_msg_satype > SADB_SATYPE_MAX)\n\t\treturn -EINVAL;\n\n\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC) {\n\t\tif (pfk->registered&(1<<hdr->sadb_msg_satype))\n\t\t\treturn -EEXIST;\n\t\tpfk->registered |= (1<<hdr->sadb_msg_satype);\n\t}\n\n\txfrm_probe_algs();\n\n\tsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\n\tif (!supp_skb) {\n\t\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\n\t\t\tpfk->registered &= ~(1<<hdr->sadb_msg_satype);\n\n\t\treturn -ENOBUFS;\n\t}\n\n\tpfkey_broadcast(supp_skb, GFP_KERNEL, BROADCAST_REGISTERED, sk,\n\t\t\tsock_net(sk));\n\treturn 0;\n}",
        "func": "static int pfkey_register(struct sock *sk, struct sk_buff *skb, const struct sadb_msg *hdr, void * const *ext_hdrs)\n{\n\tstruct pfkey_sock *pfk = pfkey_sk(sk);\n\tstruct sk_buff *supp_skb;\n\n\tif (hdr->sadb_msg_satype > SADB_SATYPE_MAX)\n\t\treturn -EINVAL;\n\n\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC) {\n\t\tif (pfk->registered&(1<<hdr->sadb_msg_satype))\n\t\t\treturn -EEXIST;\n\t\tpfk->registered |= (1<<hdr->sadb_msg_satype);\n\t}\n\n\tmutex_lock(&pfkey_mutex);\n\txfrm_probe_algs();\n\n\tsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\n\tmutex_unlock(&pfkey_mutex);\n\n\tif (!supp_skb) {\n\t\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\n\t\t\tpfk->registered &= ~(1<<hdr->sadb_msg_satype);\n\n\t\treturn -ENOBUFS;\n\t}\n\n\tpfkey_broadcast(supp_skb, GFP_KERNEL, BROADCAST_REGISTERED, sk,\n\t\t\tsock_net(sk));\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,9 +12,12 @@\n \t\tpfk->registered |= (1<<hdr->sadb_msg_satype);\n \t}\n \n+\tmutex_lock(&pfkey_mutex);\n \txfrm_probe_algs();\n \n \tsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\n+\tmutex_unlock(&pfkey_mutex);\n+\n \tif (!supp_skb) {\n \t\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\n \t\t\tpfk->registered &= ~(1<<hdr->sadb_msg_satype);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_lock(&pfkey_mutex);",
                "\tmutex_unlock(&pfkey_mutex);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-39188",
        "func_name": "torvalds/linux/tlb_end_vma",
        "description": "An issue was discovered in include/asm-generic/tlb.h in the Linux kernel before 5.19. Because of a race condition (unmap_mapping_range versus munmap), a device driver can free a page while it still has stale TLB entries. This only occurs in situations with VM_PFNMAP VMAs.",
        "git_url": "https://github.com/torvalds/linux/commit/b67fbebd4cf980aecbcc750e1462128bffe8ae15",
        "commit_title": "mmu_gather: Force tlb-flush VM_PFNMAP vmas",
        "commit_text": " Jann reported a race between munmap() and unmap_mapping_range(), where unmap_mapping_range() will no-op once unmap_vmas() has unlinked the VMA; however munmap() will not yet have invalidated the TLBs.  Therefore unmap_mapping_range() will complete while there are still (stale) TLB entries for the specified range.  Mitigate this by force flushing TLBs for VM_PFNMAP ranges. ",
        "func_before": "static inline void tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\tif (tlb->fullmm || IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS))\n\t\treturn;\n\n\t/*\n\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n\t * the ranges growing with the unused space between consecutive VMAs,\n\t * but also the mmu_gather::vma_* flags from tlb_start_vma() rely on\n\t * this.\n\t */\n\ttlb_flush_mmu_tlbonly(tlb);\n}",
        "func": "static inline void tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\tif (tlb->fullmm)\n\t\treturn;\n\n\t/*\n\t * VM_PFNMAP is more fragile because the core mm will not track the\n\t * page mapcount -- there might not be page-frames for these PFNs after\n\t * all. Force flush TLBs for such ranges to avoid munmap() vs\n\t * unmap_mapping_range() races.\n\t */\n\tif (tlb->vma_pfn || !IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS)) {\n\t\t/*\n\t\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n\t\t * the ranges growing with the unused space between consecutive VMAs.\n\t\t */\n\t\ttlb_flush_mmu_tlbonly(tlb);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,13 +1,19 @@\n static inline void tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n {\n-\tif (tlb->fullmm || IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS))\n+\tif (tlb->fullmm)\n \t\treturn;\n \n \t/*\n-\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n-\t * the ranges growing with the unused space between consecutive VMAs,\n-\t * but also the mmu_gather::vma_* flags from tlb_start_vma() rely on\n-\t * this.\n+\t * VM_PFNMAP is more fragile because the core mm will not track the\n+\t * page mapcount -- there might not be page-frames for these PFNs after\n+\t * all. Force flush TLBs for such ranges to avoid munmap() vs\n+\t * unmap_mapping_range() races.\n \t */\n-\ttlb_flush_mmu_tlbonly(tlb);\n+\tif (tlb->vma_pfn || !IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS)) {\n+\t\t/*\n+\t\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n+\t\t * the ranges growing with the unused space between consecutive VMAs.\n+\t\t */\n+\t\ttlb_flush_mmu_tlbonly(tlb);\n+\t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (tlb->fullmm || IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS))",
                "\t * Do a TLB flush and reset the range at VMA boundaries; this avoids",
                "\t * the ranges growing with the unused space between consecutive VMAs,",
                "\t * but also the mmu_gather::vma_* flags from tlb_start_vma() rely on",
                "\t * this.",
                "\ttlb_flush_mmu_tlbonly(tlb);"
            ],
            "added_lines": [
                "\tif (tlb->fullmm)",
                "\t * VM_PFNMAP is more fragile because the core mm will not track the",
                "\t * page mapcount -- there might not be page-frames for these PFNs after",
                "\t * all. Force flush TLBs for such ranges to avoid munmap() vs",
                "\t * unmap_mapping_range() races.",
                "\tif (tlb->vma_pfn || !IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS)) {",
                "\t\t/*",
                "\t\t * Do a TLB flush and reset the range at VMA boundaries; this avoids",
                "\t\t * the ranges growing with the unused space between consecutive VMAs.",
                "\t\t */",
                "\t\ttlb_flush_mmu_tlbonly(tlb);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-39188",
        "func_name": "torvalds/linux/tlb_update_vma_flags",
        "description": "An issue was discovered in include/asm-generic/tlb.h in the Linux kernel before 5.19. Because of a race condition (unmap_mapping_range versus munmap), a device driver can free a page while it still has stale TLB entries. This only occurs in situations with VM_PFNMAP VMAs.",
        "git_url": "https://github.com/torvalds/linux/commit/b67fbebd4cf980aecbcc750e1462128bffe8ae15",
        "commit_title": "mmu_gather: Force tlb-flush VM_PFNMAP vmas",
        "commit_text": " Jann reported a race between munmap() and unmap_mapping_range(), where unmap_mapping_range() will no-op once unmap_vmas() has unlinked the VMA; however munmap() will not yet have invalidated the TLBs.  Therefore unmap_mapping_range() will complete while there are still (stale) TLB entries for the specified range.  Mitigate this by force flushing TLBs for VM_PFNMAP ranges. ",
        "func_before": "static inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\t/*\n\t * flush_tlb_range() implementations that look at VM_HUGETLB (tile,\n\t * mips-4k) flush only large pages.\n\t *\n\t * flush_tlb_range() implementations that flush I-TLB also flush D-TLB\n\t * (tile, xtensa, arm), so it's ok to just add VM_EXEC to an existing\n\t * range.\n\t *\n\t * We rely on tlb_end_vma() to issue a flush, such that when we reset\n\t * these values the batch is empty.\n\t */\n\ttlb->vma_huge = is_vm_hugetlb_page(vma);\n\ttlb->vma_exec = !!(vma->vm_flags & VM_EXEC);\n}",
        "func": "static inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\t/*\n\t * flush_tlb_range() implementations that look at VM_HUGETLB (tile,\n\t * mips-4k) flush only large pages.\n\t *\n\t * flush_tlb_range() implementations that flush I-TLB also flush D-TLB\n\t * (tile, xtensa, arm), so it's ok to just add VM_EXEC to an existing\n\t * range.\n\t *\n\t * We rely on tlb_end_vma() to issue a flush, such that when we reset\n\t * these values the batch is empty.\n\t */\n\ttlb->vma_huge = is_vm_hugetlb_page(vma);\n\ttlb->vma_exec = !!(vma->vm_flags & VM_EXEC);\n\ttlb->vma_pfn  = !!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,4 +14,5 @@\n \t */\n \ttlb->vma_huge = is_vm_hugetlb_page(vma);\n \ttlb->vma_exec = !!(vma->vm_flags & VM_EXEC);\n+\ttlb->vma_pfn  = !!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP));\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\ttlb->vma_pfn  = !!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP));"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40307",
        "func_name": "torvalds/linux/efi_capsule_release",
        "description": "An issue was discovered in the Linux kernel through 5.19.8. drivers/firmware/efi/capsule-loader.c has a race condition with a resultant use-after-free.",
        "git_url": "https://github.com/torvalds/linux/commit/9cb636b5f6a8cc6d1b50809ec8f8d33ae0c84c95",
        "commit_title": "efi: capsule-loader: Fix use-after-free in efi_capsule_write",
        "commit_text": " A race condition may occur if the user calls close() on another thread during a write() operation on the device node of the efi capsule.  This is a race condition that occurs between the efi_capsule_write() and efi_capsule_flush() functions of efi_capsule_fops, which ultimately results in UAF.  So, the page freeing process is modified to be done in efi_capsule_release() instead of efi_capsule_flush().  Cc: <stable@vger.kernel.org> # v4.9+ Link: https://lore.kernel.org/all/20220907102920.GA88602@ubuntu/",
        "func_before": "static int efi_capsule_release(struct inode *inode, struct file *file)\n{\n\tstruct capsule_info *cap_info = file->private_data;\n\n\tkfree(cap_info->pages);\n\tkfree(cap_info->phys);\n\tkfree(file->private_data);\n\tfile->private_data = NULL;\n\treturn 0;\n}",
        "func": "static int efi_capsule_release(struct inode *inode, struct file *file)\n{\n\tstruct capsule_info *cap_info = file->private_data;\n\n\tif (cap_info->index > 0 &&\n\t    (cap_info->header.headersize == 0 ||\n\t     cap_info->count < cap_info->total_size)) {\n\t\tpr_err(\"capsule upload not complete\\n\");\n\t\tefi_free_all_buff_pages(cap_info);\n\t}\n\n\tkfree(cap_info->pages);\n\tkfree(cap_info->phys);\n\tkfree(file->private_data);\n\tfile->private_data = NULL;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,13 @@\n static int efi_capsule_release(struct inode *inode, struct file *file)\n {\n \tstruct capsule_info *cap_info = file->private_data;\n+\n+\tif (cap_info->index > 0 &&\n+\t    (cap_info->header.headersize == 0 ||\n+\t     cap_info->count < cap_info->total_size)) {\n+\t\tpr_err(\"capsule upload not complete\\n\");\n+\t\tefi_free_all_buff_pages(cap_info);\n+\t}\n \n \tkfree(cap_info->pages);\n \tkfree(cap_info->phys);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (cap_info->index > 0 &&",
                "\t    (cap_info->header.headersize == 0 ||",
                "\t     cap_info->count < cap_info->total_size)) {",
                "\t\tpr_err(\"capsule upload not complete\\n\");",
                "\t\tefi_free_all_buff_pages(cap_info);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-41849",
        "func_name": "torvalds/linux/ufx_ops_open",
        "description": "drivers/video/fbdev/smscufx.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free if a physically proximate attacker removes a USB device while calling open(), aka a race condition between ufx_ops_open and ufx_usb_disconnect.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=5610bcfe8693c02e2e4c8b31427f1bdbdecc839c",
        "commit_title": "A race condition may occur if the user physically removes the",
        "commit_text": "USB device while calling open() for this device node.  This is a race condition between the ufx_ops_open() function and the ufx_usb_disconnect() function, which may eventually result in UAF.  So, add a mutex to the ufx_ops_open() and ufx_usb_disconnect() functions to avoid race contidion of krefs.  Cc: stable@vger.kernel.org ",
        "func_before": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized)\n\t\treturn -ENODEV;\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\treturn 0;\n}",
        "func": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\tmutex_lock(&disconnect_mutex);\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized) {\n\t\tmutex_unlock(&disconnect_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\tmutex_unlock(&disconnect_mutex);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,9 +8,13 @@\n \tif (user == 0 && !console)\n \t\treturn -EBUSY;\n \n+\tmutex_lock(&disconnect_mutex);\n+\n \t/* If the USB device is gone, we don't accept new opens */\n-\tif (dev->virtualized)\n+\tif (dev->virtualized) {\n+\t\tmutex_unlock(&disconnect_mutex);\n \t\treturn -ENODEV;\n+\t}\n \n \tdev->fb_count++;\n \n@@ -34,5 +38,7 @@\n \tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n \t\tinfo->node, user, info, dev->fb_count);\n \n+\tmutex_unlock(&disconnect_mutex);\n+\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (dev->virtualized)"
            ],
            "added_lines": [
                "\tmutex_lock(&disconnect_mutex);",
                "",
                "\tif (dev->virtualized) {",
                "\t\tmutex_unlock(&disconnect_mutex);",
                "\t}",
                "\tmutex_unlock(&disconnect_mutex);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-41849",
        "func_name": "torvalds/linux/ufx_usb_disconnect",
        "description": "drivers/video/fbdev/smscufx.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free if a physically proximate attacker removes a USB device while calling open(), aka a race condition between ufx_ops_open and ufx_usb_disconnect.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=5610bcfe8693c02e2e4c8b31427f1bdbdecc839c",
        "commit_title": "A race condition may occur if the user physically removes the",
        "commit_text": "USB device while calling open() for this device node.  This is a race condition between the ufx_ops_open() function and the ufx_usb_disconnect() function, which may eventually result in UAF.  So, add a mutex to the ufx_ops_open() and ufx_usb_disconnect() functions to avoid race contidion of krefs.  Cc: stable@vger.kernel.org ",
        "func_before": "static void ufx_usb_disconnect(struct usb_interface *interface)\n{\n\tstruct ufx_data *dev;\n\n\tdev = usb_get_intfdata(interface);\n\n\tpr_debug(\"USB disconnect starting\\n\");\n\n\t/* we virtualize until all fb clients release. Then we free */\n\tdev->virtualized = true;\n\n\t/* When non-active we'll update virtual framebuffer, but no new urbs */\n\tatomic_set(&dev->usb_active, 0);\n\n\tusb_set_intfdata(interface, NULL);\n\n\t/* if clients still have us open, will be freed on last close */\n\tif (dev->fb_count == 0)\n\t\tschedule_delayed_work(&dev->free_framebuffer_work, 0);\n\n\t/* release reference taken by kref_init in probe() */\n\tkref_put(&dev->kref, ufx_free);\n\n\t/* consider ufx_data freed */\n}",
        "func": "static void ufx_usb_disconnect(struct usb_interface *interface)\n{\n\tstruct ufx_data *dev;\n\n\tmutex_lock(&disconnect_mutex);\n\n\tdev = usb_get_intfdata(interface);\n\n\tpr_debug(\"USB disconnect starting\\n\");\n\n\t/* we virtualize until all fb clients release. Then we free */\n\tdev->virtualized = true;\n\n\t/* When non-active we'll update virtual framebuffer, but no new urbs */\n\tatomic_set(&dev->usb_active, 0);\n\n\tusb_set_intfdata(interface, NULL);\n\n\t/* if clients still have us open, will be freed on last close */\n\tif (dev->fb_count == 0)\n\t\tschedule_delayed_work(&dev->free_framebuffer_work, 0);\n\n\t/* release reference taken by kref_init in probe() */\n\tkref_put(&dev->kref, ufx_free);\n\n\t/* consider ufx_data freed */\n\n\tmutex_unlock(&disconnect_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,8 @@\n static void ufx_usb_disconnect(struct usb_interface *interface)\n {\n \tstruct ufx_data *dev;\n+\n+\tmutex_lock(&disconnect_mutex);\n \n \tdev = usb_get_intfdata(interface);\n \n@@ -22,4 +24,6 @@\n \tkref_put(&dev->kref, ufx_free);\n \n \t/* consider ufx_data freed */\n+\n+\tmutex_unlock(&disconnect_mutex);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tmutex_lock(&disconnect_mutex);",
                "",
                "\tmutex_unlock(&disconnect_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-41850",
        "func_name": "torvalds/linux/roccat_report_event",
        "description": "roccat_report_event in drivers/hid/hid-roccat.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free in certain situations where a report is received while copying a report->value is in progress.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=cacdb14b1c8d3804a3a7d31773bc7569837b71a4",
        "commit_title": "roccat_report_event() is responsible for registering",
        "commit_text": "roccat-related reports in struct roccat_device.  int roccat_report_event(int minor, u8 const *data) { \tstruct roccat_device *device; \tstruct roccat_reader *reader; \tstruct roccat_report *report; \tuint8_t *new_value;  \tdevice = devices[minor];  \tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC); \tif (!new_value) \t\treturn -ENOMEM;  \treport = &device->cbuf[device->cbuf_end];  \t/* passing NULL is safe */ \tkfree(report->value); \t...  The registered report is stored in the struct roccat_device member \"struct roccat_report cbuf[ROCCAT_CBUF_SIZE];\". If more reports are received than the \"ROCCAT_CBUF_SIZE\" value, kfree() the saved report from cbuf[0] and allocates a new reprot. Since there is no lock when this kfree() is performed, kfree() can be performed even while reading the saved report.  static ssize_t roccat_read(struct file *file, char __user *buffer, \t\tsize_t count, loff_t *ppos) { \tstruct roccat_reader *reader = file->private_data; \tstruct roccat_device *device = reader->device; \tstruct roccat_report *report; \tssize_t retval = 0, len; \tDECLARE_WAITQUEUE(wait, current);  \tmutex_lock(&device->cbuf_lock);  \t...  \treport = &device->cbuf[reader->cbuf_start]; \t/* \t * If report is larger than requested amount of data, rest of report \t * is lost! \t */ \tlen = device->report_size > count ? count : device->report_size;  \tif (copy_to_user(buffer, report->value, len)) { \t\tretval = -EFAULT; \t\tgoto exit_unlock; \t} \t...  The roccat_read() function receives the device->cbuf report and delivers it to the user through copy_to_user(). If the N+ROCCAT_CBUF_SIZE th report is received while copying of the Nth report->value is in progress, the pointer that copy_to_user() is working on is kfree()ed and UAF read may occur. (race condition)  Since the device node of this driver does not set separate permissions, this is not a security vulnerability, but because it is used for requesting screen display of profile or dpi settings, a user using the roccat device can apply udev to this device node or There is a possibility to use it by giving.  ",
        "func_before": "int roccat_report_event(int minor, u8 const *data)\n{\n\tstruct roccat_device *device;\n\tstruct roccat_reader *reader;\n\tstruct roccat_report *report;\n\tuint8_t *new_value;\n\n\tdevice = devices[minor];\n\n\tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC);\n\tif (!new_value)\n\t\treturn -ENOMEM;\n\n\treport = &device->cbuf[device->cbuf_end];\n\n\t/* passing NULL is safe */\n\tkfree(report->value);\n\n\treport->value = new_value;\n\tdevice->cbuf_end = (device->cbuf_end + 1) % ROCCAT_CBUF_SIZE;\n\n\tlist_for_each_entry(reader, &device->readers, node) {\n\t\t/*\n\t\t * As we already inserted one element, the buffer can't be\n\t\t * empty. If start and end are equal, buffer is full and we\n\t\t * increase start, so that slow reader misses one event, but\n\t\t * gets the newer ones in the right order.\n\t\t */\n\t\tif (reader->cbuf_start == device->cbuf_end)\n\t\t\treader->cbuf_start = (reader->cbuf_start + 1) % ROCCAT_CBUF_SIZE;\n\t}\n\n\twake_up_interruptible(&device->wait);\n\treturn 0;\n}",
        "func": "int roccat_report_event(int minor, u8 const *data)\n{\n\tstruct roccat_device *device;\n\tstruct roccat_reader *reader;\n\tstruct roccat_report *report;\n\tuint8_t *new_value;\n\n\tdevice = devices[minor];\n\n\tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC);\n\tif (!new_value)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&device->cbuf_lock);\n\n\treport = &device->cbuf[device->cbuf_end];\n\n\t/* passing NULL is safe */\n\tkfree(report->value);\n\n\treport->value = new_value;\n\tdevice->cbuf_end = (device->cbuf_end + 1) % ROCCAT_CBUF_SIZE;\n\n\tlist_for_each_entry(reader, &device->readers, node) {\n\t\t/*\n\t\t * As we already inserted one element, the buffer can't be\n\t\t * empty. If start and end are equal, buffer is full and we\n\t\t * increase start, so that slow reader misses one event, but\n\t\t * gets the newer ones in the right order.\n\t\t */\n\t\tif (reader->cbuf_start == device->cbuf_end)\n\t\t\treader->cbuf_start = (reader->cbuf_start + 1) % ROCCAT_CBUF_SIZE;\n\t}\n\n\tmutex_unlock(&device->cbuf_lock);\n\n\twake_up_interruptible(&device->wait);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,8 @@\n \tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC);\n \tif (!new_value)\n \t\treturn -ENOMEM;\n+\n+\tmutex_lock(&device->cbuf_lock);\n \n \treport = &device->cbuf[device->cbuf_end];\n \n@@ -30,6 +32,8 @@\n \t\t\treader->cbuf_start = (reader->cbuf_start + 1) % ROCCAT_CBUF_SIZE;\n \t}\n \n+\tmutex_unlock(&device->cbuf_lock);\n+\n \twake_up_interruptible(&device->wait);\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tmutex_lock(&device->cbuf_lock);",
                "\tmutex_unlock(&device->cbuf_lock);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3521",
        "func_name": "torvalds/linux/kcm_release",
        "description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function kcm_tx_work of the file net/kcm/kcmsock.c of the component kcm. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211018 is the identifier assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=ec7eede369fe5b0d085ac51fdbb95184f87bfc6c",
        "commit_title": "syzbot found that kcm_tx_work() could crash [1] in:",
        "commit_text": " \t/* Primarily for SOCK_SEQPACKET sockets */ \tif (likely(sk->sk_socket) && \t    test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) { <<*>>\tclear_bit(SOCK_NOSPACE, &sk->sk_socket->flags); \t\tsk->sk_write_space(sk); \t}  I think the reason is that another thread might concurrently run in kcm_release() and call sock_orphan(sk) while sk is not locked. kcm_tx_work() find sk->sk_socket being NULL.  [1] BUG: KASAN: null-ptr-deref in instrument_atomic_write include/linux/instrumented.h:86 [inline] BUG: KASAN: null-ptr-deref in clear_bit include/asm-generic/bitops/instrumented-atomic.h:41 [inline] BUG: KASAN: null-ptr-deref in kcm_tx_work+0xff/0x160 net/kcm/kcmsock.c:742 Write of size 8 at addr 0000000000000008 by task kworker/u4:3/53  CPU: 0 PID: 53 Comm: kworker/u4:3 Not tainted 5.19.0-rc3-next-20220621-syzkaller #0 Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011 Workqueue: kkcmd kcm_tx_work Call Trace: <TASK> __dump_stack lib/dump_stack.c:88 [inline] dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106 kasan_report+0xbe/0x1f0 mm/kasan/report.c:495 check_region_inline mm/kasan/generic.c:183 [inline] kasan_check_range+0x13d/0x180 mm/kasan/generic.c:189 instrument_atomic_write include/linux/instrumented.h:86 [inline] clear_bit include/asm-generic/bitops/instrumented-atomic.h:41 [inline] kcm_tx_work+0xff/0x160 net/kcm/kcmsock.c:742 process_one_work+0x996/0x1610 kernel/workqueue.c:2289 worker_thread+0x665/0x1080 kernel/workqueue.c:2436 kthread+0x2e9/0x3a0 kernel/kthread.c:376 ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:302 </TASK>  Cc: Tom Herbert <tom@herbertland.com> Link: https://lore.kernel.org/r/20221012133412.519394-1-edumazet@google.com ",
        "func_before": "static int kcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct kcm_sock *kcm;\n\tstruct kcm_mux *mux;\n\tstruct kcm_psock *psock;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tkcm = kcm_sk(sk);\n\tmux = kcm->mux;\n\n\tsock_orphan(sk);\n\tkfree_skb(kcm->seq_skb);\n\n\tlock_sock(sk);\n\t/* Purge queue under lock to avoid race condition with tx_work trying\n\t * to act when queue is nonempty. If tx_work runs after this point\n\t * it will just return.\n\t */\n\t__skb_queue_purge(&sk->sk_write_queue);\n\n\t/* Set tx_stopped. This is checked when psock is bound to a kcm and we\n\t * get a writespace callback. This prevents further work being queued\n\t * from the callback (unbinding the psock occurs after canceling work.\n\t */\n\tkcm->tx_stopped = 1;\n\n\trelease_sock(sk);\n\n\tspin_lock_bh(&mux->lock);\n\tif (kcm->tx_wait) {\n\t\t/* Take of tx_wait list, after this point there should be no way\n\t\t * that a psock will be assigned to this kcm.\n\t\t */\n\t\tlist_del(&kcm->wait_psock_list);\n\t\tkcm->tx_wait = false;\n\t}\n\tspin_unlock_bh(&mux->lock);\n\n\t/* Cancel work. After this point there should be no outside references\n\t * to the kcm socket.\n\t */\n\tcancel_work_sync(&kcm->tx_work);\n\n\tlock_sock(sk);\n\tpsock = kcm->tx_psock;\n\tif (psock) {\n\t\t/* A psock was reserved, so we need to kill it since it\n\t\t * may already have some bytes queued from a message. We\n\t\t * need to do this after removing kcm from tx_wait list.\n\t\t */\n\t\tkcm_abort_tx_psock(psock, EPIPE, false);\n\t\tunreserve_psock(kcm);\n\t}\n\trelease_sock(sk);\n\n\tWARN_ON(kcm->tx_wait);\n\tWARN_ON(kcm->tx_psock);\n\n\tsock->sk = NULL;\n\n\tkcm_done(kcm);\n\n\treturn 0;\n}",
        "func": "static int kcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct kcm_sock *kcm;\n\tstruct kcm_mux *mux;\n\tstruct kcm_psock *psock;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tkcm = kcm_sk(sk);\n\tmux = kcm->mux;\n\n\tlock_sock(sk);\n\tsock_orphan(sk);\n\tkfree_skb(kcm->seq_skb);\n\n\t/* Purge queue under lock to avoid race condition with tx_work trying\n\t * to act when queue is nonempty. If tx_work runs after this point\n\t * it will just return.\n\t */\n\t__skb_queue_purge(&sk->sk_write_queue);\n\n\t/* Set tx_stopped. This is checked when psock is bound to a kcm and we\n\t * get a writespace callback. This prevents further work being queued\n\t * from the callback (unbinding the psock occurs after canceling work.\n\t */\n\tkcm->tx_stopped = 1;\n\n\trelease_sock(sk);\n\n\tspin_lock_bh(&mux->lock);\n\tif (kcm->tx_wait) {\n\t\t/* Take of tx_wait list, after this point there should be no way\n\t\t * that a psock will be assigned to this kcm.\n\t\t */\n\t\tlist_del(&kcm->wait_psock_list);\n\t\tkcm->tx_wait = false;\n\t}\n\tspin_unlock_bh(&mux->lock);\n\n\t/* Cancel work. After this point there should be no outside references\n\t * to the kcm socket.\n\t */\n\tcancel_work_sync(&kcm->tx_work);\n\n\tlock_sock(sk);\n\tpsock = kcm->tx_psock;\n\tif (psock) {\n\t\t/* A psock was reserved, so we need to kill it since it\n\t\t * may already have some bytes queued from a message. We\n\t\t * need to do this after removing kcm from tx_wait list.\n\t\t */\n\t\tkcm_abort_tx_psock(psock, EPIPE, false);\n\t\tunreserve_psock(kcm);\n\t}\n\trelease_sock(sk);\n\n\tWARN_ON(kcm->tx_wait);\n\tWARN_ON(kcm->tx_psock);\n\n\tsock->sk = NULL;\n\n\tkcm_done(kcm);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,10 +11,10 @@\n \tkcm = kcm_sk(sk);\n \tmux = kcm->mux;\n \n+\tlock_sock(sk);\n \tsock_orphan(sk);\n \tkfree_skb(kcm->seq_skb);\n \n-\tlock_sock(sk);\n \t/* Purge queue under lock to avoid race condition with tx_work trying\n \t * to act when queue is nonempty. If tx_work runs after this point\n \t * it will just return.",
        "diff_line_info": {
            "deleted_lines": [
                "\tlock_sock(sk);"
            ],
            "added_lines": [
                "\tlock_sock(sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3564",
        "func_name": "kernel/git/bluetooth/bluetooth-next/l2cap_stream_rx",
        "description": "A vulnerability classified as critical was found in Linux Kernel. Affected by this vulnerability is the function l2cap_reassemble_sdu of the file net/bluetooth/l2cap_core.c of the component Bluetooth. The manipulation leads to use after free. It is recommended to apply a patch to fix this issue. The associated identifier of this vulnerability is VDB-211087.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bluetooth/bluetooth-next.git/commit/?h=89f9f3cb86b1c63badaf392a83dd661d56cc50b1",
        "commit_title": "Fix the race condition between the following two flows that run in",
        "commit_text": "parallel:  1. l2cap_reassemble_sdu -> chan->ops->recv (l2cap_sock_recv_cb) ->    __sock_queue_rcv_skb.  2. bt_sock_recvmsg -> skb_recv_datagram, skb_free_datagram.  An SKB can be queued by the first flow and immediately dequeued and freed by the second flow, therefore the callers of l2cap_reassemble_sdu can't use the SKB after that function returns. However, some places continue accessing struct l2cap_ctrl that resides in the SKB's CB for a short time after l2cap_reassemble_sdu returns, leading to a use-after-free condition (the stack trace is below, line numbers for kernel 5.19.8).  Fix it by keeping a local copy of struct l2cap_ctrl.  BUG: KASAN: use-after-free in l2cap_rx_state_recv (net/bluetooth/l2cap_core.c:6906) bluetooth Read of size 1 at addr ffff88812025f2f0 by task kworker/u17:3/43169  Workqueue: hci0 hci_rx_work [bluetooth] Call Trace:  <TASK>  dump_stack_lvl (lib/dump_stack.c:107 (discriminator 4))  print_report.cold (mm/kasan/report.c:314 mm/kasan/report.c:429)  ? l2cap_rx_state_recv (net/bluetooth/l2cap_core.c:6906) bluetooth  kasan_report (mm/kasan/report.c:162 mm/kasan/report.c:493)  ? l2cap_rx_state_recv (net/bluetooth/l2cap_core.c:6906) bluetooth  l2cap_rx_state_recv (net/bluetooth/l2cap_core.c:6906) bluetooth  l2cap_rx (net/bluetooth/l2cap_core.c:7236 net/bluetooth/l2cap_core.c:7271) bluetooth  ret_from_fork (arch/x86/entry/entry_64.S:306)  </TASK>  Allocated by task 43169:  kasan_save_stack (mm/kasan/common.c:39)  __kasan_slab_alloc (mm/kasan/common.c:45 mm/kasan/common.c:436 mm/kasan/common.c:469)  kmem_cache_alloc_node (mm/slab.h:750 mm/slub.c:3243 mm/slub.c:3293)  __alloc_skb (net/core/skbuff.c:414)  l2cap_recv_frag (./include/net/bluetooth/bluetooth.h:425 net/bluetooth/l2cap_core.c:8329) bluetooth  l2cap_recv_acldata (net/bluetooth/l2cap_core.c:8442) bluetooth  hci_rx_work (net/bluetooth/hci_core.c:3642 net/bluetooth/hci_core.c:3832) bluetooth  process_one_work (kernel/workqueue.c:2289)  worker_thread (./include/linux/list.h:292 kernel/workqueue.c:2437)  kthread (kernel/kthread.c:376)  ret_from_fork (arch/x86/entry/entry_64.S:306)  Freed by task 27920:  kasan_save_stack (mm/kasan/common.c:39)  kasan_set_track (mm/kasan/common.c:45)  kasan_set_free_info (mm/kasan/generic.c:372)  ____kasan_slab_free (mm/kasan/common.c:368 mm/kasan/common.c:328)  slab_free_freelist_hook (mm/slub.c:1780)  kmem_cache_free (mm/slub.c:3536 mm/slub.c:3553)  skb_free_datagram (./include/net/sock.h:1578 ./include/net/sock.h:1639 net/core/datagram.c:323)  bt_sock_recvmsg (net/bluetooth/af_bluetooth.c:295) bluetooth  l2cap_sock_recvmsg (net/bluetooth/l2cap_sock.c:1212) bluetooth  sock_read_iter (net/socket.c:1087)  new_sync_read (./include/linux/fs.h:2052 fs/read_write.c:401)  vfs_read (fs/read_write.c:482)  ksys_read (fs/read_write.c:620)  do_syscall_64 (arch/x86/entry/common.c:50 arch/x86/entry/common.c:80)  entry_SYSCALL_64_after_hwframe (arch/x86/entry/entry_64.S:120)  Link: https://lore.kernel.org/linux-bluetooth/CAKErNvoqga1WcmoR3-0875esY6TVWFQDandbVZncSiuGPBQXLA@mail.gmail.com/T/#u ",
        "func_before": "static int l2cap_stream_rx(struct l2cap_chan *chan, struct l2cap_ctrl *control,\n\t\t\t   struct sk_buff *skb)\n{\n\tBT_DBG(\"chan %p, control %p, skb %p, state %d\", chan, control, skb,\n\t       chan->rx_state);\n\n\tif (l2cap_classify_txseq(chan, control->txseq) ==\n\t    L2CAP_TXSEQ_EXPECTED) {\n\t\tl2cap_pass_to_tx(chan, control);\n\n\t\tBT_DBG(\"buffer_seq %u->%u\", chan->buffer_seq,\n\t\t       __next_seq(chan, chan->buffer_seq));\n\n\t\tchan->buffer_seq = __next_seq(chan, chan->buffer_seq);\n\n\t\tl2cap_reassemble_sdu(chan, skb, control);\n\t} else {\n\t\tif (chan->sdu) {\n\t\t\tkfree_skb(chan->sdu);\n\t\t\tchan->sdu = NULL;\n\t\t}\n\t\tchan->sdu_last_frag = NULL;\n\t\tchan->sdu_len = 0;\n\n\t\tif (skb) {\n\t\t\tBT_DBG(\"Freeing %p\", skb);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tchan->last_acked_seq = control->txseq;\n\tchan->expected_tx_seq = __next_seq(chan, control->txseq);\n\n\treturn 0;\n}",
        "func": "static int l2cap_stream_rx(struct l2cap_chan *chan, struct l2cap_ctrl *control,\n\t\t\t   struct sk_buff *skb)\n{\n\t/* l2cap_reassemble_sdu may free skb, hence invalidate control, so store\n\t * the txseq field in advance to use it after l2cap_reassemble_sdu\n\t * returns and to avoid the race condition, for example:\n\t *\n\t * The current thread calls:\n\t *   l2cap_reassemble_sdu\n\t *     chan->ops->recv == l2cap_sock_recv_cb\n\t *       __sock_queue_rcv_skb\n\t * Another thread calls:\n\t *   bt_sock_recvmsg\n\t *     skb_recv_datagram\n\t *     skb_free_datagram\n\t * Then the current thread tries to access control, but it was freed by\n\t * skb_free_datagram.\n\t */\n\tu16 txseq = control->txseq;\n\n\tBT_DBG(\"chan %p, control %p, skb %p, state %d\", chan, control, skb,\n\t       chan->rx_state);\n\n\tif (l2cap_classify_txseq(chan, txseq) == L2CAP_TXSEQ_EXPECTED) {\n\t\tl2cap_pass_to_tx(chan, control);\n\n\t\tBT_DBG(\"buffer_seq %u->%u\", chan->buffer_seq,\n\t\t       __next_seq(chan, chan->buffer_seq));\n\n\t\tchan->buffer_seq = __next_seq(chan, chan->buffer_seq);\n\n\t\tl2cap_reassemble_sdu(chan, skb, control);\n\t} else {\n\t\tif (chan->sdu) {\n\t\t\tkfree_skb(chan->sdu);\n\t\t\tchan->sdu = NULL;\n\t\t}\n\t\tchan->sdu_last_frag = NULL;\n\t\tchan->sdu_len = 0;\n\n\t\tif (skb) {\n\t\t\tBT_DBG(\"Freeing %p\", skb);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tchan->last_acked_seq = txseq;\n\tchan->expected_tx_seq = __next_seq(chan, txseq);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,27 @@\n static int l2cap_stream_rx(struct l2cap_chan *chan, struct l2cap_ctrl *control,\n \t\t\t   struct sk_buff *skb)\n {\n+\t/* l2cap_reassemble_sdu may free skb, hence invalidate control, so store\n+\t * the txseq field in advance to use it after l2cap_reassemble_sdu\n+\t * returns and to avoid the race condition, for example:\n+\t *\n+\t * The current thread calls:\n+\t *   l2cap_reassemble_sdu\n+\t *     chan->ops->recv == l2cap_sock_recv_cb\n+\t *       __sock_queue_rcv_skb\n+\t * Another thread calls:\n+\t *   bt_sock_recvmsg\n+\t *     skb_recv_datagram\n+\t *     skb_free_datagram\n+\t * Then the current thread tries to access control, but it was freed by\n+\t * skb_free_datagram.\n+\t */\n+\tu16 txseq = control->txseq;\n+\n \tBT_DBG(\"chan %p, control %p, skb %p, state %d\", chan, control, skb,\n \t       chan->rx_state);\n \n-\tif (l2cap_classify_txseq(chan, control->txseq) ==\n-\t    L2CAP_TXSEQ_EXPECTED) {\n+\tif (l2cap_classify_txseq(chan, txseq) == L2CAP_TXSEQ_EXPECTED) {\n \t\tl2cap_pass_to_tx(chan, control);\n \n \t\tBT_DBG(\"buffer_seq %u->%u\", chan->buffer_seq,\n@@ -28,8 +44,8 @@\n \t\t}\n \t}\n \n-\tchan->last_acked_seq = control->txseq;\n-\tchan->expected_tx_seq = __next_seq(chan, control->txseq);\n+\tchan->last_acked_seq = txseq;\n+\tchan->expected_tx_seq = __next_seq(chan, txseq);\n \n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (l2cap_classify_txseq(chan, control->txseq) ==",
                "\t    L2CAP_TXSEQ_EXPECTED) {",
                "\tchan->last_acked_seq = control->txseq;",
                "\tchan->expected_tx_seq = __next_seq(chan, control->txseq);"
            ],
            "added_lines": [
                "\t/* l2cap_reassemble_sdu may free skb, hence invalidate control, so store",
                "\t * the txseq field in advance to use it after l2cap_reassemble_sdu",
                "\t * returns and to avoid the race condition, for example:",
                "\t *",
                "\t * The current thread calls:",
                "\t *   l2cap_reassemble_sdu",
                "\t *     chan->ops->recv == l2cap_sock_recv_cb",
                "\t *       __sock_queue_rcv_skb",
                "\t * Another thread calls:",
                "\t *   bt_sock_recvmsg",
                "\t *     skb_recv_datagram",
                "\t *     skb_free_datagram",
                "\t * Then the current thread tries to access control, but it was freed by",
                "\t * skb_free_datagram.",
                "\t */",
                "\tu16 txseq = control->txseq;",
                "",
                "\tif (l2cap_classify_txseq(chan, txseq) == L2CAP_TXSEQ_EXPECTED) {",
                "\tchan->last_acked_seq = txseq;",
                "\tchan->expected_tx_seq = __next_seq(chan, txseq);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3564",
        "func_name": "kernel/git/bluetooth/bluetooth-next/l2cap_rx_state_recv",
        "description": "A vulnerability classified as critical was found in Linux Kernel. Affected by this vulnerability is the function l2cap_reassemble_sdu of the file net/bluetooth/l2cap_core.c of the component Bluetooth. The manipulation leads to use after free. It is recommended to apply a patch to fix this issue. The associated identifier of this vulnerability is VDB-211087.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bluetooth/bluetooth-next.git/commit/?h=89f9f3cb86b1c63badaf392a83dd661d56cc50b1",
        "commit_title": "Fix the race condition between the following two flows that run in",
        "commit_text": "parallel:  1. l2cap_reassemble_sdu -> chan->ops->recv (l2cap_sock_recv_cb) ->    __sock_queue_rcv_skb.  2. bt_sock_recvmsg -> skb_recv_datagram, skb_free_datagram.  An SKB can be queued by the first flow and immediately dequeued and freed by the second flow, therefore the callers of l2cap_reassemble_sdu can't use the SKB after that function returns. However, some places continue accessing struct l2cap_ctrl that resides in the SKB's CB for a short time after l2cap_reassemble_sdu returns, leading to a use-after-free condition (the stack trace is below, line numbers for kernel 5.19.8).  Fix it by keeping a local copy of struct l2cap_ctrl.  BUG: KASAN: use-after-free in l2cap_rx_state_recv (net/bluetooth/l2cap_core.c:6906) bluetooth Read of size 1 at addr ffff88812025f2f0 by task kworker/u17:3/43169  Workqueue: hci0 hci_rx_work [bluetooth] Call Trace:  <TASK>  dump_stack_lvl (lib/dump_stack.c:107 (discriminator 4))  print_report.cold (mm/kasan/report.c:314 mm/kasan/report.c:429)  ? l2cap_rx_state_recv (net/bluetooth/l2cap_core.c:6906) bluetooth  kasan_report (mm/kasan/report.c:162 mm/kasan/report.c:493)  ? l2cap_rx_state_recv (net/bluetooth/l2cap_core.c:6906) bluetooth  l2cap_rx_state_recv (net/bluetooth/l2cap_core.c:6906) bluetooth  l2cap_rx (net/bluetooth/l2cap_core.c:7236 net/bluetooth/l2cap_core.c:7271) bluetooth  ret_from_fork (arch/x86/entry/entry_64.S:306)  </TASK>  Allocated by task 43169:  kasan_save_stack (mm/kasan/common.c:39)  __kasan_slab_alloc (mm/kasan/common.c:45 mm/kasan/common.c:436 mm/kasan/common.c:469)  kmem_cache_alloc_node (mm/slab.h:750 mm/slub.c:3243 mm/slub.c:3293)  __alloc_skb (net/core/skbuff.c:414)  l2cap_recv_frag (./include/net/bluetooth/bluetooth.h:425 net/bluetooth/l2cap_core.c:8329) bluetooth  l2cap_recv_acldata (net/bluetooth/l2cap_core.c:8442) bluetooth  hci_rx_work (net/bluetooth/hci_core.c:3642 net/bluetooth/hci_core.c:3832) bluetooth  process_one_work (kernel/workqueue.c:2289)  worker_thread (./include/linux/list.h:292 kernel/workqueue.c:2437)  kthread (kernel/kthread.c:376)  ret_from_fork (arch/x86/entry/entry_64.S:306)  Freed by task 27920:  kasan_save_stack (mm/kasan/common.c:39)  kasan_set_track (mm/kasan/common.c:45)  kasan_set_free_info (mm/kasan/generic.c:372)  ____kasan_slab_free (mm/kasan/common.c:368 mm/kasan/common.c:328)  slab_free_freelist_hook (mm/slub.c:1780)  kmem_cache_free (mm/slub.c:3536 mm/slub.c:3553)  skb_free_datagram (./include/net/sock.h:1578 ./include/net/sock.h:1639 net/core/datagram.c:323)  bt_sock_recvmsg (net/bluetooth/af_bluetooth.c:295) bluetooth  l2cap_sock_recvmsg (net/bluetooth/l2cap_sock.c:1212) bluetooth  sock_read_iter (net/socket.c:1087)  new_sync_read (./include/linux/fs.h:2052 fs/read_write.c:401)  vfs_read (fs/read_write.c:482)  ksys_read (fs/read_write.c:620)  do_syscall_64 (arch/x86/entry/common.c:50 arch/x86/entry/common.c:80)  entry_SYSCALL_64_after_hwframe (arch/x86/entry/entry_64.S:120)  Link: https://lore.kernel.org/linux-bluetooth/CAKErNvoqga1WcmoR3-0875esY6TVWFQDandbVZncSiuGPBQXLA@mail.gmail.com/T/#u ",
        "func_before": "static int l2cap_rx_state_recv(struct l2cap_chan *chan,\n\t\t\t       struct l2cap_ctrl *control,\n\t\t\t       struct sk_buff *skb, u8 event)\n{\n\tint err = 0;\n\tbool skb_in_use = false;\n\n\tBT_DBG(\"chan %p, control %p, skb %p, event %d\", chan, control, skb,\n\t       event);\n\n\tswitch (event) {\n\tcase L2CAP_EV_RECV_IFRAME:\n\t\tswitch (l2cap_classify_txseq(chan, control->txseq)) {\n\t\tcase L2CAP_TXSEQ_EXPECTED:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\n\t\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\t\tBT_DBG(\"Busy, discarding expected seq %d\",\n\t\t\t\t       control->txseq);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tchan->expected_tx_seq = __next_seq(chan,\n\t\t\t\t\t\t\t   control->txseq);\n\n\t\t\tchan->buffer_seq = chan->expected_tx_seq;\n\t\t\tskb_in_use = true;\n\n\t\t\terr = l2cap_reassemble_sdu(chan, skb, control);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\n\t\t\tif (control->final) {\n\t\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT,\n\t\t\t\t\t\t\t&chan->conn_state)) {\n\t\t\t\t\tcontrol->final = 0;\n\t\t\t\t\tl2cap_retransmit_all(chan, control);\n\t\t\t\t\tl2cap_ertm_send(chan);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!test_bit(CONN_LOCAL_BUSY, &chan->conn_state))\n\t\t\t\tl2cap_send_ack(chan);\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_UNEXPECTED:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\n\t\t\t/* Can't issue SREJ frames in the local busy state.\n\t\t\t * Drop this frame, it will be seen as missing\n\t\t\t * when local busy is exited.\n\t\t\t */\n\t\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\t\tBT_DBG(\"Busy, discarding unexpected seq %d\",\n\t\t\t\t       control->txseq);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* There was a gap in the sequence, so an SREJ\n\t\t\t * must be sent for each missing frame.  The\n\t\t\t * current frame is stored for later use.\n\t\t\t */\n\t\t\tskb_queue_tail(&chan->srej_q, skb);\n\t\t\tskb_in_use = true;\n\t\t\tBT_DBG(\"Queued %p (queue len %d)\", skb,\n\t\t\t       skb_queue_len(&chan->srej_q));\n\n\t\t\tclear_bit(CONN_SREJ_ACT, &chan->conn_state);\n\t\t\tl2cap_seq_list_clear(&chan->srej_list);\n\t\t\tl2cap_send_srej(chan, control->txseq);\n\n\t\t\tchan->rx_state = L2CAP_RX_STATE_SREJ_SENT;\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_DUPLICATE:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_INVALID_IGNORE:\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_INVALID:\n\t\tdefault:\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase L2CAP_EV_RECV_RR:\n\t\tl2cap_pass_to_tx(chan, control);\n\t\tif (control->final) {\n\t\t\tclear_bit(CONN_REMOTE_BUSY, &chan->conn_state);\n\n\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT, &chan->conn_state) &&\n\t\t\t    !__chan_is_moving(chan)) {\n\t\t\t\tcontrol->final = 0;\n\t\t\t\tl2cap_retransmit_all(chan, control);\n\t\t\t}\n\n\t\t\tl2cap_ertm_send(chan);\n\t\t} else if (control->poll) {\n\t\t\tl2cap_send_i_or_rr_or_rnr(chan);\n\t\t} else {\n\t\t\tif (test_and_clear_bit(CONN_REMOTE_BUSY,\n\t\t\t\t\t       &chan->conn_state) &&\n\t\t\t    chan->unacked_frames)\n\t\t\t\t__set_retrans_timer(chan);\n\n\t\t\tl2cap_ertm_send(chan);\n\t\t}\n\t\tbreak;\n\tcase L2CAP_EV_RECV_RNR:\n\t\tset_bit(CONN_REMOTE_BUSY, &chan->conn_state);\n\t\tl2cap_pass_to_tx(chan, control);\n\t\tif (control && control->poll) {\n\t\t\tset_bit(CONN_SEND_FBIT, &chan->conn_state);\n\t\t\tl2cap_send_rr_or_rnr(chan, 0);\n\t\t}\n\t\t__clear_retrans_timer(chan);\n\t\tl2cap_seq_list_clear(&chan->retrans_list);\n\t\tbreak;\n\tcase L2CAP_EV_RECV_REJ:\n\t\tl2cap_handle_rej(chan, control);\n\t\tbreak;\n\tcase L2CAP_EV_RECV_SREJ:\n\t\tl2cap_handle_srej(chan, control);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (skb && !skb_in_use) {\n\t\tBT_DBG(\"Freeing %p\", skb);\n\t\tkfree_skb(skb);\n\t}\n\n\treturn err;\n}",
        "func": "static int l2cap_rx_state_recv(struct l2cap_chan *chan,\n\t\t\t       struct l2cap_ctrl *control,\n\t\t\t       struct sk_buff *skb, u8 event)\n{\n\tstruct l2cap_ctrl local_control;\n\tint err = 0;\n\tbool skb_in_use = false;\n\n\tBT_DBG(\"chan %p, control %p, skb %p, event %d\", chan, control, skb,\n\t       event);\n\n\tswitch (event) {\n\tcase L2CAP_EV_RECV_IFRAME:\n\t\tswitch (l2cap_classify_txseq(chan, control->txseq)) {\n\t\tcase L2CAP_TXSEQ_EXPECTED:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\n\t\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\t\tBT_DBG(\"Busy, discarding expected seq %d\",\n\t\t\t\t       control->txseq);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tchan->expected_tx_seq = __next_seq(chan,\n\t\t\t\t\t\t\t   control->txseq);\n\n\t\t\tchan->buffer_seq = chan->expected_tx_seq;\n\t\t\tskb_in_use = true;\n\n\t\t\t/* l2cap_reassemble_sdu may free skb, hence invalidate\n\t\t\t * control, so make a copy in advance to use it after\n\t\t\t * l2cap_reassemble_sdu returns and to avoid the race\n\t\t\t * condition, for example:\n\t\t\t *\n\t\t\t * The current thread calls:\n\t\t\t *   l2cap_reassemble_sdu\n\t\t\t *     chan->ops->recv == l2cap_sock_recv_cb\n\t\t\t *       __sock_queue_rcv_skb\n\t\t\t * Another thread calls:\n\t\t\t *   bt_sock_recvmsg\n\t\t\t *     skb_recv_datagram\n\t\t\t *     skb_free_datagram\n\t\t\t * Then the current thread tries to access control, but\n\t\t\t * it was freed by skb_free_datagram.\n\t\t\t */\n\t\t\tlocal_control = *control;\n\t\t\terr = l2cap_reassemble_sdu(chan, skb, control);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\n\t\t\tif (local_control.final) {\n\t\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT,\n\t\t\t\t\t\t\t&chan->conn_state)) {\n\t\t\t\t\tlocal_control.final = 0;\n\t\t\t\t\tl2cap_retransmit_all(chan, &local_control);\n\t\t\t\t\tl2cap_ertm_send(chan);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!test_bit(CONN_LOCAL_BUSY, &chan->conn_state))\n\t\t\t\tl2cap_send_ack(chan);\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_UNEXPECTED:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\n\t\t\t/* Can't issue SREJ frames in the local busy state.\n\t\t\t * Drop this frame, it will be seen as missing\n\t\t\t * when local busy is exited.\n\t\t\t */\n\t\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\t\tBT_DBG(\"Busy, discarding unexpected seq %d\",\n\t\t\t\t       control->txseq);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* There was a gap in the sequence, so an SREJ\n\t\t\t * must be sent for each missing frame.  The\n\t\t\t * current frame is stored for later use.\n\t\t\t */\n\t\t\tskb_queue_tail(&chan->srej_q, skb);\n\t\t\tskb_in_use = true;\n\t\t\tBT_DBG(\"Queued %p (queue len %d)\", skb,\n\t\t\t       skb_queue_len(&chan->srej_q));\n\n\t\t\tclear_bit(CONN_SREJ_ACT, &chan->conn_state);\n\t\t\tl2cap_seq_list_clear(&chan->srej_list);\n\t\t\tl2cap_send_srej(chan, control->txseq);\n\n\t\t\tchan->rx_state = L2CAP_RX_STATE_SREJ_SENT;\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_DUPLICATE:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_INVALID_IGNORE:\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_INVALID:\n\t\tdefault:\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase L2CAP_EV_RECV_RR:\n\t\tl2cap_pass_to_tx(chan, control);\n\t\tif (control->final) {\n\t\t\tclear_bit(CONN_REMOTE_BUSY, &chan->conn_state);\n\n\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT, &chan->conn_state) &&\n\t\t\t    !__chan_is_moving(chan)) {\n\t\t\t\tcontrol->final = 0;\n\t\t\t\tl2cap_retransmit_all(chan, control);\n\t\t\t}\n\n\t\t\tl2cap_ertm_send(chan);\n\t\t} else if (control->poll) {\n\t\t\tl2cap_send_i_or_rr_or_rnr(chan);\n\t\t} else {\n\t\t\tif (test_and_clear_bit(CONN_REMOTE_BUSY,\n\t\t\t\t\t       &chan->conn_state) &&\n\t\t\t    chan->unacked_frames)\n\t\t\t\t__set_retrans_timer(chan);\n\n\t\t\tl2cap_ertm_send(chan);\n\t\t}\n\t\tbreak;\n\tcase L2CAP_EV_RECV_RNR:\n\t\tset_bit(CONN_REMOTE_BUSY, &chan->conn_state);\n\t\tl2cap_pass_to_tx(chan, control);\n\t\tif (control && control->poll) {\n\t\t\tset_bit(CONN_SEND_FBIT, &chan->conn_state);\n\t\t\tl2cap_send_rr_or_rnr(chan, 0);\n\t\t}\n\t\t__clear_retrans_timer(chan);\n\t\tl2cap_seq_list_clear(&chan->retrans_list);\n\t\tbreak;\n\tcase L2CAP_EV_RECV_REJ:\n\t\tl2cap_handle_rej(chan, control);\n\t\tbreak;\n\tcase L2CAP_EV_RECV_SREJ:\n\t\tl2cap_handle_srej(chan, control);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (skb && !skb_in_use) {\n\t\tBT_DBG(\"Freeing %p\", skb);\n\t\tkfree_skb(skb);\n\t}\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n \t\t\t       struct l2cap_ctrl *control,\n \t\t\t       struct sk_buff *skb, u8 event)\n {\n+\tstruct l2cap_ctrl local_control;\n \tint err = 0;\n \tbool skb_in_use = false;\n \n@@ -26,15 +27,32 @@\n \t\t\tchan->buffer_seq = chan->expected_tx_seq;\n \t\t\tskb_in_use = true;\n \n+\t\t\t/* l2cap_reassemble_sdu may free skb, hence invalidate\n+\t\t\t * control, so make a copy in advance to use it after\n+\t\t\t * l2cap_reassemble_sdu returns and to avoid the race\n+\t\t\t * condition, for example:\n+\t\t\t *\n+\t\t\t * The current thread calls:\n+\t\t\t *   l2cap_reassemble_sdu\n+\t\t\t *     chan->ops->recv == l2cap_sock_recv_cb\n+\t\t\t *       __sock_queue_rcv_skb\n+\t\t\t * Another thread calls:\n+\t\t\t *   bt_sock_recvmsg\n+\t\t\t *     skb_recv_datagram\n+\t\t\t *     skb_free_datagram\n+\t\t\t * Then the current thread tries to access control, but\n+\t\t\t * it was freed by skb_free_datagram.\n+\t\t\t */\n+\t\t\tlocal_control = *control;\n \t\t\terr = l2cap_reassemble_sdu(chan, skb, control);\n \t\t\tif (err)\n \t\t\t\tbreak;\n \n-\t\t\tif (control->final) {\n+\t\t\tif (local_control.final) {\n \t\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT,\n \t\t\t\t\t\t\t&chan->conn_state)) {\n-\t\t\t\t\tcontrol->final = 0;\n-\t\t\t\t\tl2cap_retransmit_all(chan, control);\n+\t\t\t\t\tlocal_control.final = 0;\n+\t\t\t\t\tl2cap_retransmit_all(chan, &local_control);\n \t\t\t\t\tl2cap_ertm_send(chan);\n \t\t\t\t}\n \t\t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tif (control->final) {",
                "\t\t\t\t\tcontrol->final = 0;",
                "\t\t\t\t\tl2cap_retransmit_all(chan, control);"
            ],
            "added_lines": [
                "\tstruct l2cap_ctrl local_control;",
                "\t\t\t/* l2cap_reassemble_sdu may free skb, hence invalidate",
                "\t\t\t * control, so make a copy in advance to use it after",
                "\t\t\t * l2cap_reassemble_sdu returns and to avoid the race",
                "\t\t\t * condition, for example:",
                "\t\t\t *",
                "\t\t\t * The current thread calls:",
                "\t\t\t *   l2cap_reassemble_sdu",
                "\t\t\t *     chan->ops->recv == l2cap_sock_recv_cb",
                "\t\t\t *       __sock_queue_rcv_skb",
                "\t\t\t * Another thread calls:",
                "\t\t\t *   bt_sock_recvmsg",
                "\t\t\t *     skb_recv_datagram",
                "\t\t\t *     skb_free_datagram",
                "\t\t\t * Then the current thread tries to access control, but",
                "\t\t\t * it was freed by skb_free_datagram.",
                "\t\t\t */",
                "\t\t\tlocal_control = *control;",
                "\t\t\tif (local_control.final) {",
                "\t\t\t\t\tlocal_control.final = 0;",
                "\t\t\t\t\tl2cap_retransmit_all(chan, &local_control);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3566",
        "func_name": "torvalds/linux/tcp_setsockopt",
        "description": "A vulnerability, which was classified as problematic, was found in Linux Kernel. This affects the function tcp_getsockopt/tcp_setsockopt of the component TCP Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. The identifier VDB-211089 was assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=f49cd2f4d6170d27a2c61f1fecb03d8a70c91f57",
        "commit_title": "setsockopt(IPV6_ADDRFORM) and tcp_v6_connect() change icsk->icsk_af_ops",
        "commit_text": "under lock_sock(), but tcp_(get|set)sockopt() read it locklessly.  To avoid load/store tearing, we need to add READ_ONCE() and WRITE_ONCE() for the reads and writes.  Thanks to Eric Dumazet for providing the syzbot report:  BUG: KCSAN: data-race in tcp_setsockopt / tcp_v6_connect  write to 0xffff88813c624518 of 8 bytes by task 23936 on cpu 0: tcp_v6_connect+0x5b3/0xce0 net/ipv6/tcp_ipv6.c:240 __inet_stream_connect+0x159/0x6d0 net/ipv4/af_inet.c:660 inet_stream_connect+0x44/0x70 net/ipv4/af_inet.c:724 __sys_connect_file net/socket.c:1976 [inline] __sys_connect+0x197/0x1b0 net/socket.c:1993 __do_sys_connect net/socket.c:2003 [inline] __se_sys_connect net/socket.c:2000 [inline] __x64_sys_connect+0x3d/0x50 net/socket.c:2000 do_syscall_x64 arch/x86/entry/common.c:50 [inline] do_syscall_64+0x2b/0x70 arch/x86/entry/common.c:80 entry_SYSCALL_64_after_hwframe+0x63/0xcd  read to 0xffff88813c624518 of 8 bytes by task 23937 on cpu 1: tcp_setsockopt+0x147/0x1c80 net/ipv4/tcp.c:3789 sock_common_setsockopt+0x5d/0x70 net/core/sock.c:3585 __sys_setsockopt+0x212/0x2b0 net/socket.c:2252 __do_sys_setsockopt net/socket.c:2263 [inline] __se_sys_setsockopt net/socket.c:2260 [inline] __x64_sys_setsockopt+0x62/0x70 net/socket.c:2260 do_syscall_x64 arch/x86/entry/common.c:50 [inline] do_syscall_64+0x2b/0x70 arch/x86/entry/common.c:80 entry_SYSCALL_64_after_hwframe+0x63/0xcd  value changed: 0xffffffff8539af68 -> 0xffffffff8539aff8  Reported by Kernel Concurrency Sanitizer on: CPU: 1 PID: 23937 Comm: syz-executor.5 Not tainted 6.0.0-rc4-syzkaller-00331-g4ed9c1e971b1-dirty #0  Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 08/26/2022  ",
        "func_before": "int tcp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t   unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}",
        "func": "int tcp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t   unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */\n\t\treturn READ_ONCE(icsk->icsk_af_ops)->setsockopt(sk, level, optname,\n\t\t\t\t\t\t\t\toptval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,8 @@\n \tconst struct inet_connection_sock *icsk = inet_csk(sk);\n \n \tif (level != SOL_TCP)\n-\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n-\t\t\t\t\t\t     optval, optlen);\n+\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */\n+\t\treturn READ_ONCE(icsk->icsk_af_ops)->setsockopt(sk, level, optname,\n+\t\t\t\t\t\t\t\toptval, optlen);\n \treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,",
                "\t\t\t\t\t\t     optval, optlen);"
            ],
            "added_lines": [
                "\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */",
                "\t\treturn READ_ONCE(icsk->icsk_af_ops)->setsockopt(sk, level, optname,",
                "\t\t\t\t\t\t\t\toptval, optlen);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3566",
        "func_name": "torvalds/linux/tcp_getsockopt",
        "description": "A vulnerability, which was classified as problematic, was found in Linux Kernel. This affects the function tcp_getsockopt/tcp_setsockopt of the component TCP Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. The identifier VDB-211089 was assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=f49cd2f4d6170d27a2c61f1fecb03d8a70c91f57",
        "commit_title": "setsockopt(IPV6_ADDRFORM) and tcp_v6_connect() change icsk->icsk_af_ops",
        "commit_text": "under lock_sock(), but tcp_(get|set)sockopt() read it locklessly.  To avoid load/store tearing, we need to add READ_ONCE() and WRITE_ONCE() for the reads and writes.  Thanks to Eric Dumazet for providing the syzbot report:  BUG: KCSAN: data-race in tcp_setsockopt / tcp_v6_connect  write to 0xffff88813c624518 of 8 bytes by task 23936 on cpu 0: tcp_v6_connect+0x5b3/0xce0 net/ipv6/tcp_ipv6.c:240 __inet_stream_connect+0x159/0x6d0 net/ipv4/af_inet.c:660 inet_stream_connect+0x44/0x70 net/ipv4/af_inet.c:724 __sys_connect_file net/socket.c:1976 [inline] __sys_connect+0x197/0x1b0 net/socket.c:1993 __do_sys_connect net/socket.c:2003 [inline] __se_sys_connect net/socket.c:2000 [inline] __x64_sys_connect+0x3d/0x50 net/socket.c:2000 do_syscall_x64 arch/x86/entry/common.c:50 [inline] do_syscall_64+0x2b/0x70 arch/x86/entry/common.c:80 entry_SYSCALL_64_after_hwframe+0x63/0xcd  read to 0xffff88813c624518 of 8 bytes by task 23937 on cpu 1: tcp_setsockopt+0x147/0x1c80 net/ipv4/tcp.c:3789 sock_common_setsockopt+0x5d/0x70 net/core/sock.c:3585 __sys_setsockopt+0x212/0x2b0 net/socket.c:2252 __do_sys_setsockopt net/socket.c:2263 [inline] __se_sys_setsockopt net/socket.c:2260 [inline] __x64_sys_setsockopt+0x62/0x70 net/socket.c:2260 do_syscall_x64 arch/x86/entry/common.c:50 [inline] do_syscall_64+0x2b/0x70 arch/x86/entry/common.c:80 entry_SYSCALL_64_after_hwframe+0x63/0xcd  value changed: 0xffffffff8539af68 -> 0xffffffff8539aff8  Reported by Kernel Concurrency Sanitizer on: CPU: 1 PID: 23937 Comm: syz-executor.5 Not tainted 6.0.0-rc4-syzkaller-00331-g4ed9c1e971b1-dirty #0  Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 08/26/2022  ",
        "func_before": "int tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, USER_SOCKPTR(optval),\n\t\t\t\t USER_SOCKPTR(optlen));\n}",
        "func": "int tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */\n\t\treturn READ_ONCE(icsk->icsk_af_ops)->getsockopt(sk, level, optname,\n\t\t\t\t\t\t\t\toptval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, USER_SOCKPTR(optval),\n\t\t\t\t USER_SOCKPTR(optlen));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,8 +4,9 @@\n \tstruct inet_connection_sock *icsk = inet_csk(sk);\n \n \tif (level != SOL_TCP)\n-\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n-\t\t\t\t\t\t     optval, optlen);\n+\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */\n+\t\treturn READ_ONCE(icsk->icsk_af_ops)->getsockopt(sk, level, optname,\n+\t\t\t\t\t\t\t\toptval, optlen);\n \treturn do_tcp_getsockopt(sk, level, optname, USER_SOCKPTR(optval),\n \t\t\t\t USER_SOCKPTR(optlen));\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,",
                "\t\t\t\t\t\t     optval, optlen);"
            ],
            "added_lines": [
                "\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */",
                "\t\treturn READ_ONCE(icsk->icsk_af_ops)->getsockopt(sk, level, optname,",
                "\t\t\t\t\t\t\t\toptval, optlen);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3566",
        "func_name": "torvalds/linux/do_ipv6_setsockopt",
        "description": "A vulnerability, which was classified as problematic, was found in Linux Kernel. This affects the function tcp_getsockopt/tcp_setsockopt of the component TCP Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. The identifier VDB-211089 was assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=f49cd2f4d6170d27a2c61f1fecb03d8a70c91f57",
        "commit_title": "setsockopt(IPV6_ADDRFORM) and tcp_v6_connect() change icsk->icsk_af_ops",
        "commit_text": "under lock_sock(), but tcp_(get|set)sockopt() read it locklessly.  To avoid load/store tearing, we need to add READ_ONCE() and WRITE_ONCE() for the reads and writes.  Thanks to Eric Dumazet for providing the syzbot report:  BUG: KCSAN: data-race in tcp_setsockopt / tcp_v6_connect  write to 0xffff88813c624518 of 8 bytes by task 23936 on cpu 0: tcp_v6_connect+0x5b3/0xce0 net/ipv6/tcp_ipv6.c:240 __inet_stream_connect+0x159/0x6d0 net/ipv4/af_inet.c:660 inet_stream_connect+0x44/0x70 net/ipv4/af_inet.c:724 __sys_connect_file net/socket.c:1976 [inline] __sys_connect+0x197/0x1b0 net/socket.c:1993 __do_sys_connect net/socket.c:2003 [inline] __se_sys_connect net/socket.c:2000 [inline] __x64_sys_connect+0x3d/0x50 net/socket.c:2000 do_syscall_x64 arch/x86/entry/common.c:50 [inline] do_syscall_64+0x2b/0x70 arch/x86/entry/common.c:80 entry_SYSCALL_64_after_hwframe+0x63/0xcd  read to 0xffff88813c624518 of 8 bytes by task 23937 on cpu 1: tcp_setsockopt+0x147/0x1c80 net/ipv4/tcp.c:3789 sock_common_setsockopt+0x5d/0x70 net/core/sock.c:3585 __sys_setsockopt+0x212/0x2b0 net/socket.c:2252 __do_sys_setsockopt net/socket.c:2263 [inline] __se_sys_setsockopt net/socket.c:2260 [inline] __x64_sys_setsockopt+0x62/0x70 net/socket.c:2260 do_syscall_x64 arch/x86/entry/common.c:50 [inline] do_syscall_64+0x2b/0x70 arch/x86/entry/common.c:80 entry_SYSCALL_64_after_hwframe+0x63/0xcd  value changed: 0xffffffff8539af68 -> 0xffffffff8539aff8  Reported by Kernel Concurrency Sanitizer on: CPU: 1 PID: 23937 Comm: syz-executor.5 Not tainted 6.0.0-rc4-syzkaller-00331-g4ed9c1e971b1-dirty #0  Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 08/26/2022  ",
        "func_before": "int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (sockptr_is_null(optval))\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tsockopt_lock_sock(sk);\n\n\t/* Another thread has converted the socket into IPv4 with\n\t * IPV6_ADDRFORM concurrently.\n\t */\n\tif (unlikely(sk->sk_family != AF_INET6))\n\t\tgoto unlock;\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tif (sk->sk_prot != &tcpv6_prot) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t__ipv6_sock_mc_close(sk);\n\t\t\t__ipv6_sock_ac_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_dgram_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, prot);\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\n\t\t\t/* Disable all options not to allocate memory anymore,\n\t\t\t * but there is still a race.  See the lockless path\n\t\t\t * in udpv6_sendmsg() and ipv6_local_rxpmtu().\n\t\t\t */\n\t\t\tnp->rxopt.all = 0;\n\n\t\t\tinet6_cleanup_sock(sk);\n\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~INET_ECN_MASK;\n\t\t\tval |= np->tclass & INET_ECN_MASK;\n\t\t}\n\t\tif (np->tclass != val) {\n\t\t\tnp->tclass = val;\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !sockopt_ns_capable(net->user_ns, CAP_NET_RAW) &&\n\t\t    !sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FREEBIND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we also don't have a separate freebind bit for IPV6 */\n\t\tinet_sk(sk)->freebind = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t\tretv = ipv6_set_opt_hdr(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) ||\n\t\t\t sockptr_is_null(optval))\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_sockptr(&pkt, optval, sizeof(pkt))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!sk_dev_equal_l3scope(sk, pkt.ipi6_ifindex))\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tstruct ipcm6_cookie ipc6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\trefcount_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(opt + 1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\t\tipc6.opt = opt;\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, &ipc6);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\t\t\tint midx;\n\n\t\t\trcu_read_lock();\n\n\t\t\tdev = dev_get_by_index_rcu(net, val);\n\t\t\tif (!dev) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmidx = l3mdev_master_ifindex_rcu(dev);\n\n\t\t\trcu_read_unlock();\n\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != val &&\n\t\t\t    (!midx || midx != sk->sk_bound_dev_if))\n\t\t\t\tgoto e_inval;\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_MULTICAST_ALL:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->mc_all = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t\t    optlen);\n\t\telse\n\t\t\tretv = ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t     optlen);\n\t\tbreak;\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t\tretv = do_ipv6_mcast_group_source(sk, optname, optval, optlen);\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_set_mcast_msfilter(sk, optval,\n\t\t\t\t\t\t\t      optlen);\n\t\telse\n\t\t\tretv = ipv6_set_mcast_msfilter(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT_ISOLATE:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rtalert_isolate = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = __ip6_sock_set_addr_preferences(sk, val);\n\t\tbreak;\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\n\t\tif (val)\n\t\t\tstatic_branch_enable(&ip6_min_hopcount);\n\n\t\t/* tcp_v6_err() and tcp_v6_rcv() might read min_hopcount\n\t\t * while we are changing it.\n\t\t */\n\t\tWRITE_ONCE(np->min_hopcount, val);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tnp->autoflowlabel_set = 1;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVFRAGSIZE:\n\t\tnp->rxopt.bits.recvfragsize = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR_RFC4884:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 1)\n\t\t\tgoto e_inval;\n\t\tnp->recverr_rfc4884 = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\nunlock:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "func": "int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (sockptr_is_null(optval))\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tsockopt_lock_sock(sk);\n\n\t/* Another thread has converted the socket into IPv4 with\n\t * IPV6_ADDRFORM concurrently.\n\t */\n\tif (unlikely(sk->sk_family != AF_INET6))\n\t\tgoto unlock;\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tif (sk->sk_prot != &tcpv6_prot) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t__ipv6_sock_mc_close(sk);\n\t\t\t__ipv6_sock_ac_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n\t\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n\t\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv4_specific);\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_dgram_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, prot);\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\n\t\t\t/* Disable all options not to allocate memory anymore,\n\t\t\t * but there is still a race.  See the lockless path\n\t\t\t * in udpv6_sendmsg() and ipv6_local_rxpmtu().\n\t\t\t */\n\t\t\tnp->rxopt.all = 0;\n\n\t\t\tinet6_cleanup_sock(sk);\n\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~INET_ECN_MASK;\n\t\t\tval |= np->tclass & INET_ECN_MASK;\n\t\t}\n\t\tif (np->tclass != val) {\n\t\t\tnp->tclass = val;\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !sockopt_ns_capable(net->user_ns, CAP_NET_RAW) &&\n\t\t    !sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FREEBIND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we also don't have a separate freebind bit for IPV6 */\n\t\tinet_sk(sk)->freebind = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t\tretv = ipv6_set_opt_hdr(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) ||\n\t\t\t sockptr_is_null(optval))\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_sockptr(&pkt, optval, sizeof(pkt))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!sk_dev_equal_l3scope(sk, pkt.ipi6_ifindex))\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tstruct ipcm6_cookie ipc6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\trefcount_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(opt + 1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\t\tipc6.opt = opt;\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, &ipc6);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\t\t\tint midx;\n\n\t\t\trcu_read_lock();\n\n\t\t\tdev = dev_get_by_index_rcu(net, val);\n\t\t\tif (!dev) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmidx = l3mdev_master_ifindex_rcu(dev);\n\n\t\t\trcu_read_unlock();\n\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != val &&\n\t\t\t    (!midx || midx != sk->sk_bound_dev_if))\n\t\t\t\tgoto e_inval;\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_MULTICAST_ALL:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->mc_all = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t\t    optlen);\n\t\telse\n\t\t\tretv = ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t     optlen);\n\t\tbreak;\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t\tretv = do_ipv6_mcast_group_source(sk, optname, optval, optlen);\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_set_mcast_msfilter(sk, optval,\n\t\t\t\t\t\t\t      optlen);\n\t\telse\n\t\t\tretv = ipv6_set_mcast_msfilter(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT_ISOLATE:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rtalert_isolate = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = __ip6_sock_set_addr_preferences(sk, val);\n\t\tbreak;\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\n\t\tif (val)\n\t\t\tstatic_branch_enable(&ip6_min_hopcount);\n\n\t\t/* tcp_v6_err() and tcp_v6_rcv() might read min_hopcount\n\t\t * while we are changing it.\n\t\t */\n\t\tWRITE_ONCE(np->min_hopcount, val);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tnp->autoflowlabel_set = 1;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVFRAGSIZE:\n\t\tnp->rxopt.bits.recvfragsize = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR_RFC4884:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 1)\n\t\t\tgoto e_inval;\n\t\tnp->recverr_rfc4884 = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\nunlock:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -86,7 +86,8 @@\n \n \t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */\n \t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n-\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n+\t\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n+\t\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv4_specific);\n \t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n \t\t\t\tsk->sk_family = PF_INET;\n \t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;"
            ],
            "added_lines": [
                "\t\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */",
                "\t\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv4_specific);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3566",
        "func_name": "torvalds/linux/tcp_v6_connect",
        "description": "A vulnerability, which was classified as problematic, was found in Linux Kernel. This affects the function tcp_getsockopt/tcp_setsockopt of the component TCP Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. The identifier VDB-211089 was assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=f49cd2f4d6170d27a2c61f1fecb03d8a70c91f57",
        "commit_title": "setsockopt(IPV6_ADDRFORM) and tcp_v6_connect() change icsk->icsk_af_ops",
        "commit_text": "under lock_sock(), but tcp_(get|set)sockopt() read it locklessly.  To avoid load/store tearing, we need to add READ_ONCE() and WRITE_ONCE() for the reads and writes.  Thanks to Eric Dumazet for providing the syzbot report:  BUG: KCSAN: data-race in tcp_setsockopt / tcp_v6_connect  write to 0xffff88813c624518 of 8 bytes by task 23936 on cpu 0: tcp_v6_connect+0x5b3/0xce0 net/ipv6/tcp_ipv6.c:240 __inet_stream_connect+0x159/0x6d0 net/ipv4/af_inet.c:660 inet_stream_connect+0x44/0x70 net/ipv4/af_inet.c:724 __sys_connect_file net/socket.c:1976 [inline] __sys_connect+0x197/0x1b0 net/socket.c:1993 __do_sys_connect net/socket.c:2003 [inline] __se_sys_connect net/socket.c:2000 [inline] __x64_sys_connect+0x3d/0x50 net/socket.c:2000 do_syscall_x64 arch/x86/entry/common.c:50 [inline] do_syscall_64+0x2b/0x70 arch/x86/entry/common.c:80 entry_SYSCALL_64_after_hwframe+0x63/0xcd  read to 0xffff88813c624518 of 8 bytes by task 23937 on cpu 1: tcp_setsockopt+0x147/0x1c80 net/ipv4/tcp.c:3789 sock_common_setsockopt+0x5d/0x70 net/core/sock.c:3585 __sys_setsockopt+0x212/0x2b0 net/socket.c:2252 __do_sys_setsockopt net/socket.c:2263 [inline] __se_sys_setsockopt net/socket.c:2260 [inline] __x64_sys_setsockopt+0x62/0x70 net/socket.c:2260 do_syscall_x64 arch/x86/entry/common.c:50 [inline] do_syscall_64+0x2b/0x70 arch/x86/entry/common.c:80 entry_SYSCALL_64_after_hwframe+0x63/0xcd  value changed: 0xffffffff8539af68 -> 0xffffffff8539aff8  Reported by Kernel Concurrency Sanitizer on: CPU: 1 PID: 23937 Comm: syz-executor.5 Not tainted 6.0.0-rc4-syzkaller-00331-g4ed9c1e971b1-dirty #0  Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 08/26/2022  ",
        "func_before": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct inet_timewait_death_row *tcp_death_row;\n\tstruct ipv6_pinfo *np = tcp_inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct dst_entry *dst;\n\tstruct flowi6 fl6;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\tWRITE_ONCE(tp->write_seq, 0);\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tif (ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tif (sk_is_mptcp(sk))\n\t\t\tmptcpv6_handle_mapped(sk, true);\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tif (sk_is_mptcp(sk))\n\t\t\t\tmptcpv6_handle_mapped(sk, false);\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tfl6.flowi6_uid = sk->sk_uid;\n\n\topt = rcu_dereference_protected(np->opt, lockdep_sock_is_held(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\tdst = ip6_dst_lookup_flow(net, sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\ttcp_death_row = &sock_net(sk)->ipv4.tcp_death_row;\n\n\tif (!saddr) {\n\t\tstruct inet_bind_hashbucket *prev_addr_hashbucket = NULL;\n\t\tstruct in6_addr prev_v6_rcv_saddr;\n\n\t\tif (icsk->icsk_bind2_hash) {\n\t\t\tprev_addr_hashbucket = inet_bhashfn_portaddr(tcp_death_row->hashinfo,\n\t\t\t\t\t\t\t\t     sk, net, inet->inet_num);\n\t\t\tprev_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n\t\t}\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\n\t\tif (prev_addr_hashbucket) {\n\t\t\terr = inet_bhash2_update_saddr(prev_addr_hashbucket, sk);\n\t\t\tif (err) {\n\t\t\t\tsk->sk_v6_rcv_saddr = prev_v6_rcv_saddr;\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\tip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (likely(!tp->repair)) {\n\t\tif (!tp->write_seq)\n\t\t\tWRITE_ONCE(tp->write_seq,\n\t\t\t\t   secure_tcpv6_seq(np->saddr.s6_addr32,\n\t\t\t\t\t\t    sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport));\n\t\ttp->tsoffset = secure_tcpv6_ts_off(net, np->saddr.s6_addr32,\n\t\t\t\t\t\t   sk->sk_v6_daddr.s6_addr32);\n\t}\n\n\tif (tcp_fastopen_defer_connect(sk, &err))\n\t\treturn err;\n\tif (err)\n\t\tgoto late_failure;\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "func": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct inet_timewait_death_row *tcp_death_row;\n\tstruct ipv6_pinfo *np = tcp_inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct dst_entry *dst;\n\tstruct flowi6 fl6;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\tWRITE_ONCE(tp->write_seq, 0);\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tif (ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_mapped);\n\t\tif (sk_is_mptcp(sk))\n\t\t\tmptcpv6_handle_mapped(sk, true);\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_specific);\n\t\t\tif (sk_is_mptcp(sk))\n\t\t\t\tmptcpv6_handle_mapped(sk, false);\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tfl6.flowi6_uid = sk->sk_uid;\n\n\topt = rcu_dereference_protected(np->opt, lockdep_sock_is_held(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\tdst = ip6_dst_lookup_flow(net, sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\ttcp_death_row = &sock_net(sk)->ipv4.tcp_death_row;\n\n\tif (!saddr) {\n\t\tstruct inet_bind_hashbucket *prev_addr_hashbucket = NULL;\n\t\tstruct in6_addr prev_v6_rcv_saddr;\n\n\t\tif (icsk->icsk_bind2_hash) {\n\t\t\tprev_addr_hashbucket = inet_bhashfn_portaddr(tcp_death_row->hashinfo,\n\t\t\t\t\t\t\t\t     sk, net, inet->inet_num);\n\t\t\tprev_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n\t\t}\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\n\t\tif (prev_addr_hashbucket) {\n\t\t\terr = inet_bhash2_update_saddr(prev_addr_hashbucket, sk);\n\t\t\tif (err) {\n\t\t\t\tsk->sk_v6_rcv_saddr = prev_v6_rcv_saddr;\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\tip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (likely(!tp->repair)) {\n\t\tif (!tp->write_seq)\n\t\t\tWRITE_ONCE(tp->write_seq,\n\t\t\t\t   secure_tcpv6_seq(np->saddr.s6_addr32,\n\t\t\t\t\t\t    sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport));\n\t\ttp->tsoffset = secure_tcpv6_ts_off(net, np->saddr.s6_addr32,\n\t\t\t\t\t\t   sk->sk_v6_daddr.s6_addr32);\n\t}\n\n\tif (tcp_fastopen_defer_connect(sk, &err))\n\t\treturn err;\n\tif (err)\n\t\tgoto late_failure;\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -94,7 +94,8 @@\n \t\tsin.sin_port = usin->sin6_port;\n \t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n \n-\t\ticsk->icsk_af_ops = &ipv6_mapped;\n+\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n+\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_mapped);\n \t\tif (sk_is_mptcp(sk))\n \t\t\tmptcpv6_handle_mapped(sk, true);\n \t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n@@ -106,7 +107,8 @@\n \n \t\tif (err) {\n \t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n-\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n+\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n+\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_specific);\n \t\t\tif (sk_is_mptcp(sk))\n \t\t\t\tmptcpv6_handle_mapped(sk, false);\n \t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\ticsk->icsk_af_ops = &ipv6_mapped;",
                "\t\t\ticsk->icsk_af_ops = &ipv6_specific;"
            ],
            "added_lines": [
                "\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */",
                "\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_mapped);",
                "\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */",
                "\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_specific);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3567",
        "func_name": "torvalds/linux/do_ipv6_setsockopt",
        "description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=364f997b5cfe1db0d63a390fe7c801fa2b3115f6",
        "commit_title": "Commit 086d49058cd8 (\"ipv6: annotate some data-races around sk->sk_prot\")",
        "commit_text": "fixed some data-races around sk->sk_prot but it was not enough.  Some functions in inet6_(stream|dgram)_ops still access sk->sk_prot without lock_sock() or rtnl_lock(), so they need READ_ONCE() to avoid load tearing.  ",
        "func_before": "int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (sockptr_is_null(optval))\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tsockopt_lock_sock(sk);\n\n\t/* Another thread has converted the socket into IPv4 with\n\t * IPV6_ADDRFORM concurrently.\n\t */\n\tif (unlikely(sk->sk_family != AF_INET6))\n\t\tgoto unlock;\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tif (sk->sk_prot != &tcpv6_prot) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t__ipv6_sock_mc_close(sk);\n\t\t\t__ipv6_sock_ac_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in net/ipv6/af_inet6.c */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in net/ipv6/af_inet6.c */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, prot);\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\n\t\t\t/* Disable all options not to allocate memory anymore,\n\t\t\t * but there is still a race.  See the lockless path\n\t\t\t * in udpv6_sendmsg() and ipv6_local_rxpmtu().\n\t\t\t */\n\t\t\tnp->rxopt.all = 0;\n\n\t\t\tinet6_cleanup_sock(sk);\n\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~INET_ECN_MASK;\n\t\t\tval |= np->tclass & INET_ECN_MASK;\n\t\t}\n\t\tif (np->tclass != val) {\n\t\t\tnp->tclass = val;\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !sockopt_ns_capable(net->user_ns, CAP_NET_RAW) &&\n\t\t    !sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FREEBIND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we also don't have a separate freebind bit for IPV6 */\n\t\tinet_sk(sk)->freebind = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t\tretv = ipv6_set_opt_hdr(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) ||\n\t\t\t sockptr_is_null(optval))\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_sockptr(&pkt, optval, sizeof(pkt))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!sk_dev_equal_l3scope(sk, pkt.ipi6_ifindex))\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tstruct ipcm6_cookie ipc6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\trefcount_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(opt + 1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\t\tipc6.opt = opt;\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, &ipc6);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\t\t\tint midx;\n\n\t\t\trcu_read_lock();\n\n\t\t\tdev = dev_get_by_index_rcu(net, val);\n\t\t\tif (!dev) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmidx = l3mdev_master_ifindex_rcu(dev);\n\n\t\t\trcu_read_unlock();\n\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != val &&\n\t\t\t    (!midx || midx != sk->sk_bound_dev_if))\n\t\t\t\tgoto e_inval;\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_MULTICAST_ALL:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->mc_all = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t\t    optlen);\n\t\telse\n\t\t\tretv = ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t     optlen);\n\t\tbreak;\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t\tretv = do_ipv6_mcast_group_source(sk, optname, optval, optlen);\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_set_mcast_msfilter(sk, optval,\n\t\t\t\t\t\t\t      optlen);\n\t\telse\n\t\t\tretv = ipv6_set_mcast_msfilter(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT_ISOLATE:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rtalert_isolate = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = __ip6_sock_set_addr_preferences(sk, val);\n\t\tbreak;\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\n\t\tif (val)\n\t\t\tstatic_branch_enable(&ip6_min_hopcount);\n\n\t\t/* tcp_v6_err() and tcp_v6_rcv() might read min_hopcount\n\t\t * while we are changing it.\n\t\t */\n\t\tWRITE_ONCE(np->min_hopcount, val);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tnp->autoflowlabel_set = 1;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVFRAGSIZE:\n\t\tnp->rxopt.bits.recvfragsize = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR_RFC4884:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 1)\n\t\t\tgoto e_inval;\n\t\tnp->recverr_rfc4884 = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\nunlock:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "func": "int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (sockptr_is_null(optval))\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tsockopt_lock_sock(sk);\n\n\t/* Another thread has converted the socket into IPv4 with\n\t * IPV6_ADDRFORM concurrently.\n\t */\n\tif (unlikely(sk->sk_family != AF_INET6))\n\t\tgoto unlock;\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tif (sk->sk_prot != &tcpv6_prot) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t__ipv6_sock_mc_close(sk);\n\t\t\t__ipv6_sock_ac_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_dgram_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, prot);\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\n\t\t\t/* Disable all options not to allocate memory anymore,\n\t\t\t * but there is still a race.  See the lockless path\n\t\t\t * in udpv6_sendmsg() and ipv6_local_rxpmtu().\n\t\t\t */\n\t\t\tnp->rxopt.all = 0;\n\n\t\t\tinet6_cleanup_sock(sk);\n\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~INET_ECN_MASK;\n\t\t\tval |= np->tclass & INET_ECN_MASK;\n\t\t}\n\t\tif (np->tclass != val) {\n\t\t\tnp->tclass = val;\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !sockopt_ns_capable(net->user_ns, CAP_NET_RAW) &&\n\t\t    !sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FREEBIND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we also don't have a separate freebind bit for IPV6 */\n\t\tinet_sk(sk)->freebind = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t\tretv = ipv6_set_opt_hdr(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) ||\n\t\t\t sockptr_is_null(optval))\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_sockptr(&pkt, optval, sizeof(pkt))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!sk_dev_equal_l3scope(sk, pkt.ipi6_ifindex))\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tstruct ipcm6_cookie ipc6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\trefcount_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(opt + 1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\t\tipc6.opt = opt;\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, &ipc6);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\t\t\tint midx;\n\n\t\t\trcu_read_lock();\n\n\t\t\tdev = dev_get_by_index_rcu(net, val);\n\t\t\tif (!dev) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmidx = l3mdev_master_ifindex_rcu(dev);\n\n\t\t\trcu_read_unlock();\n\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != val &&\n\t\t\t    (!midx || midx != sk->sk_bound_dev_if))\n\t\t\t\tgoto e_inval;\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_MULTICAST_ALL:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->mc_all = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t\t    optlen);\n\t\telse\n\t\t\tretv = ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t     optlen);\n\t\tbreak;\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t\tretv = do_ipv6_mcast_group_source(sk, optname, optval, optlen);\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_set_mcast_msfilter(sk, optval,\n\t\t\t\t\t\t\t      optlen);\n\t\telse\n\t\t\tretv = ipv6_set_mcast_msfilter(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT_ISOLATE:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rtalert_isolate = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = __ip6_sock_set_addr_preferences(sk, val);\n\t\tbreak;\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\n\t\tif (val)\n\t\t\tstatic_branch_enable(&ip6_min_hopcount);\n\n\t\t/* tcp_v6_err() and tcp_v6_rcv() might read min_hopcount\n\t\t * while we are changing it.\n\t\t */\n\t\tWRITE_ONCE(np->min_hopcount, val);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tnp->autoflowlabel_set = 1;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVFRAGSIZE:\n\t\tnp->rxopt.bits.recvfragsize = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR_RFC4884:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 1)\n\t\t\tgoto e_inval;\n\t\tnp->recverr_rfc4884 = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\nunlock:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -84,7 +84,7 @@\n \t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n \t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n \n-\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in net/ipv6/af_inet6.c */\n+\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */\n \t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n \t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n \t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n@@ -99,7 +99,7 @@\n \t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n \t\t\t\tsock_prot_inuse_add(net, prot, 1);\n \n-\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in net/ipv6/af_inet6.c */\n+\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_dgram_ops */\n \t\t\t\tWRITE_ONCE(sk->sk_prot, prot);\n \t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n \t\t\t\tsk->sk_family = PF_INET;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in net/ipv6/af_inet6.c */",
                "\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in net/ipv6/af_inet6.c */"
            ],
            "added_lines": [
                "\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */",
                "\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_dgram_ops */"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3567",
        "func_name": "torvalds/linux/sock_common_getsockopt",
        "description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=364f997b5cfe1db0d63a390fe7c801fa2b3115f6",
        "commit_title": "Commit 086d49058cd8 (\"ipv6: annotate some data-races around sk->sk_prot\")",
        "commit_text": "fixed some data-races around sk->sk_prot but it was not enough.  Some functions in inet6_(stream|dgram)_ops still access sk->sk_prot without lock_sock() or rtnl_lock(), so they need READ_ONCE() to avoid load tearing.  ",
        "func_before": "int sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}",
        "func": "int sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\treturn READ_ONCE(sk->sk_prot)->getsockopt(sk, level, optname, optval, optlen);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,5 +3,6 @@\n {\n \tstruct sock *sk = sock->sk;\n \n-\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\treturn READ_ONCE(sk->sk_prot)->getsockopt(sk, level, optname, optval, optlen);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);"
            ],
            "added_lines": [
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\treturn READ_ONCE(sk->sk_prot)->getsockopt(sk, level, optname, optval, optlen);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3567",
        "func_name": "torvalds/linux/sock_common_setsockopt",
        "description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=364f997b5cfe1db0d63a390fe7c801fa2b3115f6",
        "commit_title": "Commit 086d49058cd8 (\"ipv6: annotate some data-races around sk->sk_prot\")",
        "commit_text": "fixed some data-races around sk->sk_prot but it was not enough.  Some functions in inet6_(stream|dgram)_ops still access sk->sk_prot without lock_sock() or rtnl_lock(), so they need READ_ONCE() to avoid load tearing.  ",
        "func_before": "int sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   sockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}",
        "func": "int sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   sockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\treturn READ_ONCE(sk->sk_prot)->setsockopt(sk, level, optname, optval, optlen);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,5 +3,6 @@\n {\n \tstruct sock *sk = sock->sk;\n \n-\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\treturn READ_ONCE(sk->sk_prot)->setsockopt(sk, level, optname, optval, optlen);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);"
            ],
            "added_lines": [
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\treturn READ_ONCE(sk->sk_prot)->setsockopt(sk, level, optname, optval, optlen);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3567",
        "func_name": "torvalds/linux/inet_sendpage",
        "description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=364f997b5cfe1db0d63a390fe7c801fa2b3115f6",
        "commit_title": "Commit 086d49058cd8 (\"ipv6: annotate some data-races around sk->sk_prot\")",
        "commit_text": "fixed some data-races around sk->sk_prot but it was not enough.  Some functions in inet6_(stream|dgram)_ops still access sk->sk_prot without lock_sock() or rtnl_lock(), so they need READ_ONCE() to avoid load tearing.  ",
        "func_before": "ssize_t inet_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t      size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (unlikely(inet_send_prepare(sk)))\n\t\treturn -EAGAIN;\n\n\tif (sk->sk_prot->sendpage)\n\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}",
        "func": "ssize_t inet_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t      size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tconst struct proto *prot;\n\n\tif (unlikely(inet_send_prepare(sk)))\n\t\treturn -EAGAIN;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tprot = READ_ONCE(sk->sk_prot);\n\tif (prot->sendpage)\n\t\treturn prot->sendpage(sk, page, offset, size, flags);\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,11 +2,14 @@\n \t\t      size_t size, int flags)\n {\n \tstruct sock *sk = sock->sk;\n+\tconst struct proto *prot;\n \n \tif (unlikely(inet_send_prepare(sk)))\n \t\treturn -EAGAIN;\n \n-\tif (sk->sk_prot->sendpage)\n-\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);\n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\tprot = READ_ONCE(sk->sk_prot);\n+\tif (prot->sendpage)\n+\t\treturn prot->sendpage(sk, page, offset, size, flags);\n \treturn sock_no_sendpage(sock, page, offset, size, flags);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (sk->sk_prot->sendpage)",
                "\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);"
            ],
            "added_lines": [
                "\tconst struct proto *prot;",
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\tprot = READ_ONCE(sk->sk_prot);",
                "\tif (prot->sendpage)",
                "\t\treturn prot->sendpage(sk, page, offset, size, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3567",
        "func_name": "torvalds/linux/inet_dgram_connect",
        "description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=364f997b5cfe1db0d63a390fe7c801fa2b3115f6",
        "commit_title": "Commit 086d49058cd8 (\"ipv6: annotate some data-races around sk->sk_prot\")",
        "commit_text": "fixed some data-races around sk->sk_prot but it was not enough.  Some functions in inet6_(stream|dgram)_ops still access sk->sk_prot without lock_sock() or rtnl_lock(), so they need READ_ONCE() to avoid load tearing.  ",
        "func_before": "int inet_dgram_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t       int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\tif (uaddr->sa_family == AF_UNSPEC)\n\t\treturn sk->sk_prot->disconnect(sk, flags);\n\n\tif (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) {\n\t\terr = sk->sk_prot->pre_connect(sk, uaddr, addr_len);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (data_race(!inet_sk(sk)->inet_num) && inet_autobind(sk))\n\t\treturn -EAGAIN;\n\treturn sk->sk_prot->connect(sk, uaddr, addr_len);\n}",
        "func": "int inet_dgram_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t       int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tconst struct proto *prot;\n\tint err;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tprot = READ_ONCE(sk->sk_prot);\n\n\tif (uaddr->sa_family == AF_UNSPEC)\n\t\treturn prot->disconnect(sk, flags);\n\n\tif (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) {\n\t\terr = prot->pre_connect(sk, uaddr, addr_len);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (data_race(!inet_sk(sk)->inet_num) && inet_autobind(sk))\n\t\treturn -EAGAIN;\n\treturn prot->connect(sk, uaddr, addr_len);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,20 +2,25 @@\n \t\t       int addr_len, int flags)\n {\n \tstruct sock *sk = sock->sk;\n+\tconst struct proto *prot;\n \tint err;\n \n \tif (addr_len < sizeof(uaddr->sa_family))\n \t\treturn -EINVAL;\n+\n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\tprot = READ_ONCE(sk->sk_prot);\n+\n \tif (uaddr->sa_family == AF_UNSPEC)\n-\t\treturn sk->sk_prot->disconnect(sk, flags);\n+\t\treturn prot->disconnect(sk, flags);\n \n \tif (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) {\n-\t\terr = sk->sk_prot->pre_connect(sk, uaddr, addr_len);\n+\t\terr = prot->pre_connect(sk, uaddr, addr_len);\n \t\tif (err)\n \t\t\treturn err;\n \t}\n \n \tif (data_race(!inet_sk(sk)->inet_num) && inet_autobind(sk))\n \t\treturn -EAGAIN;\n-\treturn sk->sk_prot->connect(sk, uaddr, addr_len);\n+\treturn prot->connect(sk, uaddr, addr_len);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn sk->sk_prot->disconnect(sk, flags);",
                "\t\terr = sk->sk_prot->pre_connect(sk, uaddr, addr_len);",
                "\treturn sk->sk_prot->connect(sk, uaddr, addr_len);"
            ],
            "added_lines": [
                "\tconst struct proto *prot;",
                "",
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\tprot = READ_ONCE(sk->sk_prot);",
                "",
                "\t\treturn prot->disconnect(sk, flags);",
                "\t\terr = prot->pre_connect(sk, uaddr, addr_len);",
                "\treturn prot->connect(sk, uaddr, addr_len);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3567",
        "func_name": "torvalds/linux/inet_accept",
        "description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=364f997b5cfe1db0d63a390fe7c801fa2b3115f6",
        "commit_title": "Commit 086d49058cd8 (\"ipv6: annotate some data-races around sk->sk_prot\")",
        "commit_text": "fixed some data-races around sk->sk_prot but it was not enough.  Some functions in inet6_(stream|dgram)_ops still access sk->sk_prot without lock_sock() or rtnl_lock(), so they need READ_ONCE() to avoid load tearing.  ",
        "func_before": "int inet_accept(struct socket *sock, struct socket *newsock, int flags,\n\t\tbool kern)\n{\n\tstruct sock *sk1 = sock->sk;\n\tint err = -EINVAL;\n\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err, kern);\n\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tsock_rps_record_flow(sk2);\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_SYN_RECV |\n\t\t  TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}",
        "func": "int inet_accept(struct socket *sock, struct socket *newsock, int flags,\n\t\tbool kern)\n{\n\tstruct sock *sk1 = sock->sk, *sk2;\n\tint err = -EINVAL;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tsk2 = READ_ONCE(sk1->sk_prot)->accept(sk1, flags, &err, kern);\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tsock_rps_record_flow(sk2);\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_SYN_RECV |\n\t\t  TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,11 @@\n int inet_accept(struct socket *sock, struct socket *newsock, int flags,\n \t\tbool kern)\n {\n-\tstruct sock *sk1 = sock->sk;\n+\tstruct sock *sk1 = sock->sk, *sk2;\n \tint err = -EINVAL;\n-\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err, kern);\n \n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\tsk2 = READ_ONCE(sk1->sk_prot)->accept(sk1, flags, &err, kern);\n \tif (!sk2)\n \t\tgoto do_err;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct sock *sk1 = sock->sk;",
                "\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err, kern);"
            ],
            "added_lines": [
                "\tstruct sock *sk1 = sock->sk, *sk2;",
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\tsk2 = READ_ONCE(sk1->sk_prot)->accept(sk1, flags, &err, kern);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3623",
        "func_name": "kernel/git/bpf/bpf-next/follow_pmd_mask",
        "description": "A vulnerability was found in Linux Kernel. It has been declared as problematic. Affected by this vulnerability is the function follow_page_pte of the file mm/gup.c of the component BPF. The manipulation leads to race condition. The attack can be launched remotely. It is recommended to apply a patch to fix this issue. The identifier VDB-211921 was assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next.git/commit/?h=fac35ba763ed07ba93154c95ffc0c4a55023707f",
        "commit_title": "On some architectures (like ARM64), it can support CONT-PTE/PMD size",
        "commit_text": "hugetlb, which means it can support not only PMD/PUD size hugetlb (2M and 1G), but also CONT-PTE/PMD size(64K and 32M) if a 4K page size specified.  So when looking up a CONT-PTE size hugetlb page by follow_page(), it will use pte_offset_map_lock() to get the pte entry lock for the CONT-PTE size hugetlb in follow_page_pte().  However this pte entry lock is incorrect for the CONT-PTE size hugetlb, since we should use huge_pte_lock() to get the correct lock, which is mm->page_table_lock.  That means the pte entry of the CONT-PTE size hugetlb under current pte lock is unstable in follow_page_pte(), we can continue to migrate or poison the pte entry of the CONT-PTE size hugetlb, which can cause some potential race issues, even though they are under the 'pte lock'.  For example, suppose thread A is trying to look up a CONT-PTE size hugetlb page by move_pages() syscall under the lock, however antoher thread B can migrate the CONT-PTE hugetlb page at the same time, which will cause thread A to get an incorrect page, if thread A also wants to do page migration, then data inconsistency error occurs.  Moreover we have the same issue for CONT-PMD size hugetlb in follow_huge_pmd().  To fix above issues, rename the follow_huge_pmd() as follow_huge_pmd_pte() to handle PMD and PTE level size hugetlb, which uses huge_pte_lock() to get the correct pte entry lock to make the pte entry stable.  Mike said:  Support for CONT_PMD/_PTE was added with bb9dd3df8ee9 (\"arm64: hugetlb: refactor find_num_contig()\").  Patch series \"Support for contiguous pte hugepages\", v4.  However, I do not believe these code paths were executed until migration support was added with 5480280d3f2d (\"arm64/mm: enable HugeTLB migration for contiguous bit HugeTLB pages\") I would go with 5480280d3f2d for the Fixes: targe.  Link: https://lkml.kernel.org/r/635f43bdd85ac2615a58405da82b4d33c6e5eb05.1662017562.git.baolin.wang@linux.alibaba.com Suggested-by: Mike Kravetz <mike.kravetz@oracle.com> Cc: David Hildenbrand <david@redhat.com> Cc: Muchun Song <songmuchun@bytedance.com> Cc: <stable@vger.kernel.org> ",
        "func_before": "static struct page *follow_pmd_mask(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long address, pud_t *pudp,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct follow_page_context *ctx)\n{\n\tpmd_t *pmd, pmdval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpmd = pmd_offset(pudp, address);\n\t/*\n\t * The READ_ONCE() will stabilize the pmdval in a register or\n\t * on the stack so that it will stop changing under the code.\n\t */\n\tpmdval = READ_ONCE(*pmd);\n\tif (pmd_none(pmdval))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_huge(pmdval) && is_vm_hugetlb_page(vma)) {\n\t\tpage = follow_huge_pmd(mm, address, pmd, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (is_hugepd(__hugepd(pmd_val(pmdval)))) {\n\t\tpage = follow_huge_pd(vma, address,\n\t\t\t\t      __hugepd(pmd_val(pmdval)), flags,\n\t\t\t\t      PMD_SHIFT);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\nretry:\n\tif (!pmd_present(pmdval)) {\n\t\t/*\n\t\t * Should never reach here, if thp migration is not supported;\n\t\t * Otherwise, it must be a thp migration entry.\n\t\t */\n\t\tVM_BUG_ON(!thp_migration_supported() ||\n\t\t\t\t  !is_pmd_migration_entry(pmdval));\n\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tpmdval = READ_ONCE(*pmd);\n\t\t/*\n\t\t * MADV_DONTNEED may convert the pmd to null because\n\t\t * mmap_lock is held in read mode\n\t\t */\n\t\tif (pmd_none(pmdval))\n\t\t\treturn no_page_table(vma, flags);\n\t\tgoto retry;\n\t}\n\tif (pmd_devmap(pmdval)) {\n\t\tptl = pmd_lock(mm, pmd);\n\t\tpage = follow_devmap_pmd(vma, address, pmd, flags, &ctx->pgmap);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (likely(!pmd_trans_huge(pmdval)))\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\n\tif ((flags & FOLL_NUMA) && pmd_protnone(pmdval))\n\t\treturn no_page_table(vma, flags);\n\nretry_locked:\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(pmd_none(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (unlikely(!pmd_present(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tgoto retry_locked;\n\t}\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tif (flags & FOLL_SPLIT_PMD) {\n\t\tint ret;\n\t\tpage = pmd_page(*pmd);\n\t\tif (is_huge_zero_page(page)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tret = 0;\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tif (pmd_trans_unstable(pmd))\n\t\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tspin_unlock(ptl);\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tret = pte_alloc(mm, pmd) ? -ENOMEM : 0;\n\t\t}\n\n\t\treturn ret ? ERR_PTR(ret) :\n\t\t\tfollow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tpage = follow_trans_huge_pmd(vma, address, pmd, flags);\n\tspin_unlock(ptl);\n\tctx->page_mask = HPAGE_PMD_NR - 1;\n\treturn page;\n}",
        "func": "static struct page *follow_pmd_mask(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long address, pud_t *pudp,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct follow_page_context *ctx)\n{\n\tpmd_t *pmd, pmdval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpmd = pmd_offset(pudp, address);\n\t/*\n\t * The READ_ONCE() will stabilize the pmdval in a register or\n\t * on the stack so that it will stop changing under the code.\n\t */\n\tpmdval = READ_ONCE(*pmd);\n\tif (pmd_none(pmdval))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_huge(pmdval) && is_vm_hugetlb_page(vma)) {\n\t\tpage = follow_huge_pmd_pte(vma, address, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (is_hugepd(__hugepd(pmd_val(pmdval)))) {\n\t\tpage = follow_huge_pd(vma, address,\n\t\t\t\t      __hugepd(pmd_val(pmdval)), flags,\n\t\t\t\t      PMD_SHIFT);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\nretry:\n\tif (!pmd_present(pmdval)) {\n\t\t/*\n\t\t * Should never reach here, if thp migration is not supported;\n\t\t * Otherwise, it must be a thp migration entry.\n\t\t */\n\t\tVM_BUG_ON(!thp_migration_supported() ||\n\t\t\t\t  !is_pmd_migration_entry(pmdval));\n\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tpmdval = READ_ONCE(*pmd);\n\t\t/*\n\t\t * MADV_DONTNEED may convert the pmd to null because\n\t\t * mmap_lock is held in read mode\n\t\t */\n\t\tif (pmd_none(pmdval))\n\t\t\treturn no_page_table(vma, flags);\n\t\tgoto retry;\n\t}\n\tif (pmd_devmap(pmdval)) {\n\t\tptl = pmd_lock(mm, pmd);\n\t\tpage = follow_devmap_pmd(vma, address, pmd, flags, &ctx->pgmap);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (likely(!pmd_trans_huge(pmdval)))\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\n\tif ((flags & FOLL_NUMA) && pmd_protnone(pmdval))\n\t\treturn no_page_table(vma, flags);\n\nretry_locked:\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(pmd_none(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (unlikely(!pmd_present(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tgoto retry_locked;\n\t}\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tif (flags & FOLL_SPLIT_PMD) {\n\t\tint ret;\n\t\tpage = pmd_page(*pmd);\n\t\tif (is_huge_zero_page(page)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tret = 0;\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tif (pmd_trans_unstable(pmd))\n\t\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tspin_unlock(ptl);\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tret = pte_alloc(mm, pmd) ? -ENOMEM : 0;\n\t\t}\n\n\t\treturn ret ? ERR_PTR(ret) :\n\t\t\tfollow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tpage = follow_trans_huge_pmd(vma, address, pmd, flags);\n\tspin_unlock(ptl);\n\tctx->page_mask = HPAGE_PMD_NR - 1;\n\treturn page;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,7 +17,7 @@\n \tif (pmd_none(pmdval))\n \t\treturn no_page_table(vma, flags);\n \tif (pmd_huge(pmdval) && is_vm_hugetlb_page(vma)) {\n-\t\tpage = follow_huge_pmd(mm, address, pmd, flags);\n+\t\tpage = follow_huge_pmd_pte(vma, address, flags);\n \t\tif (page)\n \t\t\treturn page;\n \t\treturn no_page_table(vma, flags);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tpage = follow_huge_pmd(mm, address, pmd, flags);"
            ],
            "added_lines": [
                "\t\tpage = follow_huge_pmd_pte(vma, address, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3623",
        "func_name": "kernel/git/bpf/bpf-next/follow_page_pte",
        "description": "A vulnerability was found in Linux Kernel. It has been declared as problematic. Affected by this vulnerability is the function follow_page_pte of the file mm/gup.c of the component BPF. The manipulation leads to race condition. The attack can be launched remotely. It is recommended to apply a patch to fix this issue. The identifier VDB-211921 was assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next.git/commit/?h=fac35ba763ed07ba93154c95ffc0c4a55023707f",
        "commit_title": "On some architectures (like ARM64), it can support CONT-PTE/PMD size",
        "commit_text": "hugetlb, which means it can support not only PMD/PUD size hugetlb (2M and 1G), but also CONT-PTE/PMD size(64K and 32M) if a 4K page size specified.  So when looking up a CONT-PTE size hugetlb page by follow_page(), it will use pte_offset_map_lock() to get the pte entry lock for the CONT-PTE size hugetlb in follow_page_pte().  However this pte entry lock is incorrect for the CONT-PTE size hugetlb, since we should use huge_pte_lock() to get the correct lock, which is mm->page_table_lock.  That means the pte entry of the CONT-PTE size hugetlb under current pte lock is unstable in follow_page_pte(), we can continue to migrate or poison the pte entry of the CONT-PTE size hugetlb, which can cause some potential race issues, even though they are under the 'pte lock'.  For example, suppose thread A is trying to look up a CONT-PTE size hugetlb page by move_pages() syscall under the lock, however antoher thread B can migrate the CONT-PTE hugetlb page at the same time, which will cause thread A to get an incorrect page, if thread A also wants to do page migration, then data inconsistency error occurs.  Moreover we have the same issue for CONT-PMD size hugetlb in follow_huge_pmd().  To fix above issues, rename the follow_huge_pmd() as follow_huge_pmd_pte() to handle PMD and PTE level size hugetlb, which uses huge_pte_lock() to get the correct pte entry lock to make the pte entry stable.  Mike said:  Support for CONT_PMD/_PTE was added with bb9dd3df8ee9 (\"arm64: hugetlb: refactor find_num_contig()\").  Patch series \"Support for contiguous pte hugepages\", v4.  However, I do not believe these code paths were executed until migration support was added with 5480280d3f2d (\"arm64/mm: enable HugeTLB migration for contiguous bit HugeTLB pages\") I would go with 5480280d3f2d for the Fixes: targe.  Link: https://lkml.kernel.org/r/635f43bdd85ac2615a58405da82b4d33c6e5eb05.1662017562.git.baolin.wang@linux.alibaba.com Suggested-by: Mike Kravetz <mike.kravetz@oracle.com> Cc: David Hildenbrand <david@redhat.com> Cc: Muchun Song <songmuchun@bytedance.com> Cc: <stable@vger.kernel.org> ",
        "func_before": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags,\n\t\tstruct dev_pagemap **pgmap)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\tint ret;\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn ERR_PTR(-EINVAL);\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\n\tpage = vm_normal_page(vma, address, pte);\n\n\t/*\n\t * We only care about anon pages in can_follow_write_pte() and don't\n\t * have to worry about pte_devmap() because they are never anon.\n\t */\n\tif ((flags & FOLL_WRITE) &&\n\t    !can_follow_write_pte(pte, page, vma, flags)) {\n\t\tpage = NULL;\n\t\tgoto out;\n\t}\n\n\tif (!page && pte_devmap(pte) && (flags & (FOLL_GET | FOLL_PIN))) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET or FOLL_PIN\n\t\t * case since they are only valid while holding the pgmap\n\t\t * reference.\n\t\t */\n\t\t*pgmap = get_dev_pagemap(pte_pfn(pte), *pgmap);\n\t\tif (*pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!pte_write(pte) && gup_must_unshare(flags, page)) {\n\t\tpage = ERR_PTR(-EMLINK);\n\t\tgoto out;\n\t}\n\n\tVM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) &&\n\t\t       !PageAnonExclusive(page), page);\n\n\t/* try_grab_page() does nothing unless FOLL_GET or FOLL_PIN is set. */\n\tif (unlikely(!try_grab_page(page, flags))) {\n\t\tpage = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\t/*\n\t * We need to make the page accessible if and only if we are going\n\t * to access its content (the FOLL_PIN case).  Please see\n\t * Documentation/core-api/pin_user_pages.rst for details.\n\t */\n\tif (flags & FOLL_PIN) {\n\t\tret = arch_make_page_accessible(page);\n\t\tif (ret) {\n\t\t\tunpin_user_page(page);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
        "func": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags,\n\t\tstruct dev_pagemap **pgmap)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\tint ret;\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Considering PTE level hugetlb, like continuous-PTE hugetlb on\n\t * ARM64 architecture.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tpage = follow_huge_pmd_pte(vma, address, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\n\tpage = vm_normal_page(vma, address, pte);\n\n\t/*\n\t * We only care about anon pages in can_follow_write_pte() and don't\n\t * have to worry about pte_devmap() because they are never anon.\n\t */\n\tif ((flags & FOLL_WRITE) &&\n\t    !can_follow_write_pte(pte, page, vma, flags)) {\n\t\tpage = NULL;\n\t\tgoto out;\n\t}\n\n\tif (!page && pte_devmap(pte) && (flags & (FOLL_GET | FOLL_PIN))) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET or FOLL_PIN\n\t\t * case since they are only valid while holding the pgmap\n\t\t * reference.\n\t\t */\n\t\t*pgmap = get_dev_pagemap(pte_pfn(pte), *pgmap);\n\t\tif (*pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!pte_write(pte) && gup_must_unshare(flags, page)) {\n\t\tpage = ERR_PTR(-EMLINK);\n\t\tgoto out;\n\t}\n\n\tVM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) &&\n\t\t       !PageAnonExclusive(page), page);\n\n\t/* try_grab_page() does nothing unless FOLL_GET or FOLL_PIN is set. */\n\tif (unlikely(!try_grab_page(page, flags))) {\n\t\tpage = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\t/*\n\t * We need to make the page accessible if and only if we are going\n\t * to access its content (the FOLL_PIN case).  Please see\n\t * Documentation/core-api/pin_user_pages.rst for details.\n\t */\n\tif (flags & FOLL_PIN) {\n\t\tret = arch_make_page_accessible(page);\n\t\tif (ret) {\n\t\t\tunpin_user_page(page);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,18 @@\n \tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n \t\t\t (FOLL_PIN | FOLL_GET)))\n \t\treturn ERR_PTR(-EINVAL);\n+\n+\t/*\n+\t * Considering PTE level hugetlb, like continuous-PTE hugetlb on\n+\t * ARM64 architecture.\n+\t */\n+\tif (is_vm_hugetlb_page(vma)) {\n+\t\tpage = follow_huge_pmd_pte(vma, address, flags);\n+\t\tif (page)\n+\t\t\treturn page;\n+\t\treturn no_page_table(vma, flags);\n+\t}\n+\n retry:\n \tif (unlikely(pmd_bad(*pmd)))\n \t\treturn no_page_table(vma, flags);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t/*",
                "\t * Considering PTE level hugetlb, like continuous-PTE hugetlb on",
                "\t * ARM64 architecture.",
                "\t */",
                "\tif (is_vm_hugetlb_page(vma)) {",
                "\t\tpage = follow_huge_pmd_pte(vma, address, flags);",
                "\t\tif (page)",
                "\t\t\treturn page;",
                "\t\treturn no_page_table(vma, flags);",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3635",
        "func_name": "kernel/git/klassert/ipsec-next/idt77252_exit",
        "description": "A vulnerability, which was classified as critical, has been found in Linux Kernel. Affected by this issue is the function tst_timer of the file drivers/atm/idt77252.c of the component IPsec. The manipulation leads to use after free. It is recommended to apply a patch to fix this issue. VDB-211934 is the identifier assigned to this vulnerability.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/klassert/ipsec-next.git/commit/?h=3f4093e2bf4673f218c0bf17d8362337c400e77b",
        "commit_title": "There are use-after-free bugs caused by tst_timer. The root cause",
        "commit_text": "is that there are no functions to stop tst_timer in idt77252_exit(). One of the possible race conditions is shown below:      (thread 1)          |        (thread 2)                         |  idt77252_init_one                         |    init_card                         |      fill_tst                         |        mod_timer(&card->tst_timer, ...) idt77252_exit           |  (wait a time)                         |  tst_timer                         |                         |    ...   kfree(card) // FREE   |                         |    card->soft_tst[e] // USE  The idt77252_dev is deallocated in idt77252_exit() and used in timer handler.  This patch adds del_timer_sync() in idt77252_exit() in order that the timer handler could be stopped before the idt77252_dev is deallocated.  Link: https://lore.kernel.org/r/20220805070008.18007-1-duoming@zju.edu.cn ",
        "func_before": "static void __exit idt77252_exit(void)\n{\n\tstruct idt77252_dev *card;\n\tstruct atm_dev *dev;\n\n\tpci_unregister_driver(&idt77252_driver);\n\n\twhile (idt77252_chain) {\n\t\tcard = idt77252_chain;\n\t\tdev = card->atmdev;\n\t\tidt77252_chain = card->next;\n\n\t\tif (dev->phy->stop)\n\t\t\tdev->phy->stop(dev);\n\t\tdeinit_card(card);\n\t\tpci_disable_device(card->pcidev);\n\t\tkfree(card);\n\t}\n\n\tDIPRINTK(\"idt77252: finished cleanup-module().\\n\");\n}",
        "func": "static void __exit idt77252_exit(void)\n{\n\tstruct idt77252_dev *card;\n\tstruct atm_dev *dev;\n\n\tpci_unregister_driver(&idt77252_driver);\n\n\twhile (idt77252_chain) {\n\t\tcard = idt77252_chain;\n\t\tdev = card->atmdev;\n\t\tidt77252_chain = card->next;\n\t\tdel_timer_sync(&card->tst_timer);\n\n\t\tif (dev->phy->stop)\n\t\t\tdev->phy->stop(dev);\n\t\tdeinit_card(card);\n\t\tpci_disable_device(card->pcidev);\n\t\tkfree(card);\n\t}\n\n\tDIPRINTK(\"idt77252: finished cleanup-module().\\n\");\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,7 @@\n \t\tcard = idt77252_chain;\n \t\tdev = card->atmdev;\n \t\tidt77252_chain = card->next;\n+\t\tdel_timer_sync(&card->tst_timer);\n \n \t\tif (dev->phy->stop)\n \t\t\tdev->phy->stop(dev);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tdel_timer_sync(&card->tst_timer);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-1768",
        "func_name": "torvalds/linux/ip6_tunnel_init",
        "description": "The tunnels implementation in the Linux kernel before 2.6.34, when tunnel functionality is configured as a module, allows remote attackers to cause a denial of service (OOPS) by sending a packet during module loading.",
        "git_url": "https://github.com/torvalds/linux/commit/d5aa407f59f5b83d2c50ec88f5bf56d40f1f8978",
        "commit_title": "tunnels: fix netns vs proto registration ordering",
        "commit_text": " Same stuff as in ip_gre patch: receive hook can be called before netns setup is done, oopsing in net_generic(). ",
        "func_before": "static int __init ip6_tunnel_init(void)\n{\n\tint  err;\n\n\tif (xfrm6_tunnel_register(&ip4ip6_handler, AF_INET)) {\n\t\tprintk(KERN_ERR \"ip6_tunnel init: can't register ip4ip6\\n\");\n\t\terr = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tif (xfrm6_tunnel_register(&ip6ip6_handler, AF_INET6)) {\n\t\tprintk(KERN_ERR \"ip6_tunnel init: can't register ip6ip6\\n\");\n\t\terr = -EAGAIN;\n\t\tgoto unreg_ip4ip6;\n\t}\n\n\terr = register_pernet_device(&ip6_tnl_net_ops);\n\tif (err < 0)\n\t\tgoto err_pernet;\n\treturn 0;\nerr_pernet:\n\txfrm6_tunnel_deregister(&ip6ip6_handler, AF_INET6);\nunreg_ip4ip6:\n\txfrm6_tunnel_deregister(&ip4ip6_handler, AF_INET);\nout:\n\treturn err;\n}",
        "func": "static int __init ip6_tunnel_init(void)\n{\n\tint  err;\n\n\terr = register_pernet_device(&ip6_tnl_net_ops);\n\tif (err < 0)\n\t\tgoto out_pernet;\n\n\terr = xfrm6_tunnel_register(&ip4ip6_handler, AF_INET);\n\tif (err < 0) {\n\t\tprintk(KERN_ERR \"ip6_tunnel init: can't register ip4ip6\\n\");\n\t\tgoto out_ip4ip6;\n\t}\n\n\terr = xfrm6_tunnel_register(&ip6ip6_handler, AF_INET6);\n\tif (err < 0) {\n\t\tprintk(KERN_ERR \"ip6_tunnel init: can't register ip6ip6\\n\");\n\t\tgoto out_ip6ip6;\n\t}\n\n\treturn 0;\n\nout_ip6ip6:\n\txfrm6_tunnel_deregister(&ip4ip6_handler, AF_INET);\nout_ip4ip6:\n\tunregister_pernet_device(&ip6_tnl_net_ops);\nout_pernet:\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,26 +2,28 @@\n {\n \tint  err;\n \n-\tif (xfrm6_tunnel_register(&ip4ip6_handler, AF_INET)) {\n+\terr = register_pernet_device(&ip6_tnl_net_ops);\n+\tif (err < 0)\n+\t\tgoto out_pernet;\n+\n+\terr = xfrm6_tunnel_register(&ip4ip6_handler, AF_INET);\n+\tif (err < 0) {\n \t\tprintk(KERN_ERR \"ip6_tunnel init: can't register ip4ip6\\n\");\n-\t\terr = -EAGAIN;\n-\t\tgoto out;\n+\t\tgoto out_ip4ip6;\n \t}\n \n-\tif (xfrm6_tunnel_register(&ip6ip6_handler, AF_INET6)) {\n+\terr = xfrm6_tunnel_register(&ip6ip6_handler, AF_INET6);\n+\tif (err < 0) {\n \t\tprintk(KERN_ERR \"ip6_tunnel init: can't register ip6ip6\\n\");\n-\t\terr = -EAGAIN;\n-\t\tgoto unreg_ip4ip6;\n+\t\tgoto out_ip6ip6;\n \t}\n \n-\terr = register_pernet_device(&ip6_tnl_net_ops);\n-\tif (err < 0)\n-\t\tgoto err_pernet;\n \treturn 0;\n-err_pernet:\n-\txfrm6_tunnel_deregister(&ip6ip6_handler, AF_INET6);\n-unreg_ip4ip6:\n+\n+out_ip6ip6:\n \txfrm6_tunnel_deregister(&ip4ip6_handler, AF_INET);\n-out:\n+out_ip4ip6:\n+\tunregister_pernet_device(&ip6_tnl_net_ops);\n+out_pernet:\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (xfrm6_tunnel_register(&ip4ip6_handler, AF_INET)) {",
                "\t\terr = -EAGAIN;",
                "\t\tgoto out;",
                "\tif (xfrm6_tunnel_register(&ip6ip6_handler, AF_INET6)) {",
                "\t\terr = -EAGAIN;",
                "\t\tgoto unreg_ip4ip6;",
                "\terr = register_pernet_device(&ip6_tnl_net_ops);",
                "\tif (err < 0)",
                "\t\tgoto err_pernet;",
                "err_pernet:",
                "\txfrm6_tunnel_deregister(&ip6ip6_handler, AF_INET6);",
                "unreg_ip4ip6:",
                "out:"
            ],
            "added_lines": [
                "\terr = register_pernet_device(&ip6_tnl_net_ops);",
                "\tif (err < 0)",
                "\t\tgoto out_pernet;",
                "",
                "\terr = xfrm6_tunnel_register(&ip4ip6_handler, AF_INET);",
                "\tif (err < 0) {",
                "\t\tgoto out_ip4ip6;",
                "\terr = xfrm6_tunnel_register(&ip6ip6_handler, AF_INET6);",
                "\tif (err < 0) {",
                "\t\tgoto out_ip6ip6;",
                "",
                "out_ip6ip6:",
                "out_ip4ip6:",
                "\tunregister_pernet_device(&ip6_tnl_net_ops);",
                "out_pernet:"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-1768",
        "func_name": "torvalds/linux/xfrm6_tunnel_init",
        "description": "The tunnels implementation in the Linux kernel before 2.6.34, when tunnel functionality is configured as a module, allows remote attackers to cause a denial of service (OOPS) by sending a packet during module loading.",
        "git_url": "https://github.com/torvalds/linux/commit/d5aa407f59f5b83d2c50ec88f5bf56d40f1f8978",
        "commit_title": "tunnels: fix netns vs proto registration ordering",
        "commit_text": " Same stuff as in ip_gre patch: receive hook can be called before netns setup is done, oopsing in net_generic(). ",
        "func_before": "static int __init xfrm6_tunnel_init(void)\n{\n\tint rv;\n\n\trv = xfrm_register_type(&xfrm6_tunnel_type, AF_INET6);\n\tif (rv < 0)\n\t\tgoto err;\n\trv = xfrm6_tunnel_register(&xfrm6_tunnel_handler, AF_INET6);\n\tif (rv < 0)\n\t\tgoto unreg;\n\trv = xfrm6_tunnel_register(&xfrm46_tunnel_handler, AF_INET);\n\tif (rv < 0)\n\t\tgoto dereg6;\n\trv = xfrm6_tunnel_spi_init();\n\tif (rv < 0)\n\t\tgoto dereg46;\n\trv = register_pernet_subsys(&xfrm6_tunnel_net_ops);\n\tif (rv < 0)\n\t\tgoto deregspi;\n\treturn 0;\n\nderegspi:\n\txfrm6_tunnel_spi_fini();\ndereg46:\n\txfrm6_tunnel_deregister(&xfrm46_tunnel_handler, AF_INET);\ndereg6:\n\txfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);\nunreg:\n\txfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);\nerr:\n\treturn rv;\n}",
        "func": "static int __init xfrm6_tunnel_init(void)\n{\n\tint rv;\n\n\txfrm6_tunnel_spi_kmem = kmem_cache_create(\"xfrm6_tunnel_spi\",\n\t\t\t\t\t\t  sizeof(struct xfrm6_tunnel_spi),\n\t\t\t\t\t\t  0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t\t  NULL);\n\tif (!xfrm6_tunnel_spi_kmem)\n\t\treturn -ENOMEM;\n\trv = register_pernet_subsys(&xfrm6_tunnel_net_ops);\n\tif (rv < 0)\n\t\tgoto out_pernet;\n\trv = xfrm_register_type(&xfrm6_tunnel_type, AF_INET6);\n\tif (rv < 0)\n\t\tgoto out_type;\n\trv = xfrm6_tunnel_register(&xfrm6_tunnel_handler, AF_INET6);\n\tif (rv < 0)\n\t\tgoto out_xfrm6;\n\trv = xfrm6_tunnel_register(&xfrm46_tunnel_handler, AF_INET);\n\tif (rv < 0)\n\t\tgoto out_xfrm46;\n\treturn 0;\n\nout_xfrm46:\n\txfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);\nout_xfrm6:\n\txfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);\nout_type:\n\tunregister_pernet_subsys(&xfrm6_tunnel_net_ops);\nout_pernet:\n\tkmem_cache_destroy(xfrm6_tunnel_spi_kmem);\n\treturn rv;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,31 +2,33 @@\n {\n \tint rv;\n \n+\txfrm6_tunnel_spi_kmem = kmem_cache_create(\"xfrm6_tunnel_spi\",\n+\t\t\t\t\t\t  sizeof(struct xfrm6_tunnel_spi),\n+\t\t\t\t\t\t  0, SLAB_HWCACHE_ALIGN,\n+\t\t\t\t\t\t  NULL);\n+\tif (!xfrm6_tunnel_spi_kmem)\n+\t\treturn -ENOMEM;\n+\trv = register_pernet_subsys(&xfrm6_tunnel_net_ops);\n+\tif (rv < 0)\n+\t\tgoto out_pernet;\n \trv = xfrm_register_type(&xfrm6_tunnel_type, AF_INET6);\n \tif (rv < 0)\n-\t\tgoto err;\n+\t\tgoto out_type;\n \trv = xfrm6_tunnel_register(&xfrm6_tunnel_handler, AF_INET6);\n \tif (rv < 0)\n-\t\tgoto unreg;\n+\t\tgoto out_xfrm6;\n \trv = xfrm6_tunnel_register(&xfrm46_tunnel_handler, AF_INET);\n \tif (rv < 0)\n-\t\tgoto dereg6;\n-\trv = xfrm6_tunnel_spi_init();\n-\tif (rv < 0)\n-\t\tgoto dereg46;\n-\trv = register_pernet_subsys(&xfrm6_tunnel_net_ops);\n-\tif (rv < 0)\n-\t\tgoto deregspi;\n+\t\tgoto out_xfrm46;\n \treturn 0;\n \n-deregspi:\n-\txfrm6_tunnel_spi_fini();\n-dereg46:\n-\txfrm6_tunnel_deregister(&xfrm46_tunnel_handler, AF_INET);\n-dereg6:\n+out_xfrm46:\n \txfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);\n-unreg:\n+out_xfrm6:\n \txfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);\n-err:\n+out_type:\n+\tunregister_pernet_subsys(&xfrm6_tunnel_net_ops);\n+out_pernet:\n+\tkmem_cache_destroy(xfrm6_tunnel_spi_kmem);\n \treturn rv;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tgoto err;",
                "\t\tgoto unreg;",
                "\t\tgoto dereg6;",
                "\trv = xfrm6_tunnel_spi_init();",
                "\tif (rv < 0)",
                "\t\tgoto dereg46;",
                "\trv = register_pernet_subsys(&xfrm6_tunnel_net_ops);",
                "\tif (rv < 0)",
                "\t\tgoto deregspi;",
                "deregspi:",
                "\txfrm6_tunnel_spi_fini();",
                "dereg46:",
                "\txfrm6_tunnel_deregister(&xfrm46_tunnel_handler, AF_INET);",
                "dereg6:",
                "unreg:",
                "err:"
            ],
            "added_lines": [
                "\txfrm6_tunnel_spi_kmem = kmem_cache_create(\"xfrm6_tunnel_spi\",",
                "\t\t\t\t\t\t  sizeof(struct xfrm6_tunnel_spi),",
                "\t\t\t\t\t\t  0, SLAB_HWCACHE_ALIGN,",
                "\t\t\t\t\t\t  NULL);",
                "\tif (!xfrm6_tunnel_spi_kmem)",
                "\t\treturn -ENOMEM;",
                "\trv = register_pernet_subsys(&xfrm6_tunnel_net_ops);",
                "\tif (rv < 0)",
                "\t\tgoto out_pernet;",
                "\t\tgoto out_type;",
                "\t\tgoto out_xfrm6;",
                "\t\tgoto out_xfrm46;",
                "out_xfrm46:",
                "out_xfrm6:",
                "out_type:",
                "\tunregister_pernet_subsys(&xfrm6_tunnel_net_ops);",
                "out_pernet:",
                "\tkmem_cache_destroy(xfrm6_tunnel_spi_kmem);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-1768",
        "func_name": "torvalds/linux/xfrm6_tunnel_fini",
        "description": "The tunnels implementation in the Linux kernel before 2.6.34, when tunnel functionality is configured as a module, allows remote attackers to cause a denial of service (OOPS) by sending a packet during module loading.",
        "git_url": "https://github.com/torvalds/linux/commit/d5aa407f59f5b83d2c50ec88f5bf56d40f1f8978",
        "commit_title": "tunnels: fix netns vs proto registration ordering",
        "commit_text": " Same stuff as in ip_gre patch: receive hook can be called before netns setup is done, oopsing in net_generic(). ",
        "func_before": "static void __exit xfrm6_tunnel_fini(void)\n{\n\tunregister_pernet_subsys(&xfrm6_tunnel_net_ops);\n\txfrm6_tunnel_spi_fini();\n\txfrm6_tunnel_deregister(&xfrm46_tunnel_handler, AF_INET);\n\txfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);\n\txfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);\n}",
        "func": "static void __exit xfrm6_tunnel_fini(void)\n{\n\txfrm6_tunnel_deregister(&xfrm46_tunnel_handler, AF_INET);\n\txfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);\n\txfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);\n\tunregister_pernet_subsys(&xfrm6_tunnel_net_ops);\n\tkmem_cache_destroy(xfrm6_tunnel_spi_kmem);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,8 @@\n static void __exit xfrm6_tunnel_fini(void)\n {\n-\tunregister_pernet_subsys(&xfrm6_tunnel_net_ops);\n-\txfrm6_tunnel_spi_fini();\n \txfrm6_tunnel_deregister(&xfrm46_tunnel_handler, AF_INET);\n \txfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);\n \txfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);\n+\tunregister_pernet_subsys(&xfrm6_tunnel_net_ops);\n+\tkmem_cache_destroy(xfrm6_tunnel_spi_kmem);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tunregister_pernet_subsys(&xfrm6_tunnel_net_ops);",
                "\txfrm6_tunnel_spi_fini();"
            ],
            "added_lines": [
                "\tunregister_pernet_subsys(&xfrm6_tunnel_net_ops);",
                "\tkmem_cache_destroy(xfrm6_tunnel_spi_kmem);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-1768",
        "func_name": "torvalds/linux/ipip_init",
        "description": "The tunnels implementation in the Linux kernel before 2.6.34, when tunnel functionality is configured as a module, allows remote attackers to cause a denial of service (OOPS) by sending a packet during module loading.",
        "git_url": "https://github.com/torvalds/linux/commit/d5aa407f59f5b83d2c50ec88f5bf56d40f1f8978",
        "commit_title": "tunnels: fix netns vs proto registration ordering",
        "commit_text": " Same stuff as in ip_gre patch: receive hook can be called before netns setup is done, oopsing in net_generic(). ",
        "func_before": "static int __init ipip_init(void)\n{\n\tint err;\n\n\tprintk(banner);\n\n\tif (xfrm4_tunnel_register(&ipip_handler, AF_INET)) {\n\t\tprintk(KERN_INFO \"ipip init: can't register tunnel\\n\");\n\t\treturn -EAGAIN;\n\t}\n\n\terr = register_pernet_device(&ipip_net_ops);\n\tif (err)\n\t\txfrm4_tunnel_deregister(&ipip_handler, AF_INET);\n\n\treturn err;\n}",
        "func": "static int __init ipip_init(void)\n{\n\tint err;\n\n\tprintk(banner);\n\n\terr = register_pernet_device(&ipip_net_ops);\n\tif (err < 0)\n\t\treturn err;\n\terr = xfrm4_tunnel_register(&ipip_handler, AF_INET);\n\tif (err < 0) {\n\t\tunregister_pernet_device(&ipip_net_ops);\n\t\tprintk(KERN_INFO \"ipip init: can't register tunnel\\n\");\n\t}\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,14 +4,13 @@\n \n \tprintk(banner);\n \n-\tif (xfrm4_tunnel_register(&ipip_handler, AF_INET)) {\n+\terr = register_pernet_device(&ipip_net_ops);\n+\tif (err < 0)\n+\t\treturn err;\n+\terr = xfrm4_tunnel_register(&ipip_handler, AF_INET);\n+\tif (err < 0) {\n+\t\tunregister_pernet_device(&ipip_net_ops);\n \t\tprintk(KERN_INFO \"ipip init: can't register tunnel\\n\");\n-\t\treturn -EAGAIN;\n \t}\n-\n-\terr = register_pernet_device(&ipip_net_ops);\n-\tif (err)\n-\t\txfrm4_tunnel_deregister(&ipip_handler, AF_INET);\n-\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (xfrm4_tunnel_register(&ipip_handler, AF_INET)) {",
                "\t\treturn -EAGAIN;",
                "",
                "\terr = register_pernet_device(&ipip_net_ops);",
                "\tif (err)",
                "\t\txfrm4_tunnel_deregister(&ipip_handler, AF_INET);",
                ""
            ],
            "added_lines": [
                "\terr = register_pernet_device(&ipip_net_ops);",
                "\tif (err < 0)",
                "\t\treturn err;",
                "\terr = xfrm4_tunnel_register(&ipip_handler, AF_INET);",
                "\tif (err < 0) {",
                "\t\tunregister_pernet_device(&ipip_net_ops);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-1768",
        "func_name": "torvalds/linux/sit_init",
        "description": "The tunnels implementation in the Linux kernel before 2.6.34, when tunnel functionality is configured as a module, allows remote attackers to cause a denial of service (OOPS) by sending a packet during module loading.",
        "git_url": "https://github.com/torvalds/linux/commit/d5aa407f59f5b83d2c50ec88f5bf56d40f1f8978",
        "commit_title": "tunnels: fix netns vs proto registration ordering",
        "commit_text": " Same stuff as in ip_gre patch: receive hook can be called before netns setup is done, oopsing in net_generic(). ",
        "func_before": "static int __init sit_init(void)\n{\n\tint err;\n\n\tprintk(KERN_INFO \"IPv6 over IPv4 tunneling driver\\n\");\n\n\tif (xfrm4_tunnel_register(&sit_handler, AF_INET6) < 0) {\n\t\tprintk(KERN_INFO \"sit init: Can't add protocol\\n\");\n\t\treturn -EAGAIN;\n\t}\n\n\terr = register_pernet_device(&sit_net_ops);\n\tif (err < 0)\n\t\txfrm4_tunnel_deregister(&sit_handler, AF_INET6);\n\n\treturn err;\n}",
        "func": "static int __init sit_init(void)\n{\n\tint err;\n\n\tprintk(KERN_INFO \"IPv6 over IPv4 tunneling driver\\n\");\n\n\terr = register_pernet_device(&sit_net_ops);\n\tif (err < 0)\n\t\treturn err;\n\terr = xfrm4_tunnel_register(&sit_handler, AF_INET6);\n\tif (err < 0) {\n\t\tunregister_pernet_device(&sit_net_ops);\n\t\tprintk(KERN_INFO \"sit init: Can't add protocol\\n\");\n\t}\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,14 +4,13 @@\n \n \tprintk(KERN_INFO \"IPv6 over IPv4 tunneling driver\\n\");\n \n-\tif (xfrm4_tunnel_register(&sit_handler, AF_INET6) < 0) {\n-\t\tprintk(KERN_INFO \"sit init: Can't add protocol\\n\");\n-\t\treturn -EAGAIN;\n-\t}\n-\n \terr = register_pernet_device(&sit_net_ops);\n \tif (err < 0)\n-\t\txfrm4_tunnel_deregister(&sit_handler, AF_INET6);\n-\n+\t\treturn err;\n+\terr = xfrm4_tunnel_register(&sit_handler, AF_INET6);\n+\tif (err < 0) {\n+\t\tunregister_pernet_device(&sit_net_ops);\n+\t\tprintk(KERN_INFO \"sit init: Can't add protocol\\n\");\n+\t}\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (xfrm4_tunnel_register(&sit_handler, AF_INET6) < 0) {",
                "\t\tprintk(KERN_INFO \"sit init: Can't add protocol\\n\");",
                "\t\treturn -EAGAIN;",
                "\t}",
                "",
                "\t\txfrm4_tunnel_deregister(&sit_handler, AF_INET6);",
                ""
            ],
            "added_lines": [
                "\t\treturn err;",
                "\terr = xfrm4_tunnel_register(&sit_handler, AF_INET6);",
                "\tif (err < 0) {",
                "\t\tunregister_pernet_device(&sit_net_ops);",
                "\t\tprintk(KERN_INFO \"sit init: Can't add protocol\\n\");",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2183",
        "func_name": "torvalds/linux/scan_get_next_rmap_item",
        "description": "Race condition in the scan_get_next_rmap_item function in mm/ksm.c in the Linux kernel before 2.6.39.3, when Kernel SamePage Merging (KSM) is enabled, allows local users to cause a denial of service (NULL pointer dereference) or possibly have unspecified other impact via a crafted application.",
        "git_url": "https://github.com/torvalds/linux/commit/2b472611a32a72f4a118c069c2d62a1a3f087afd",
        "commit_title": "ksm: fix NULL pointer dereference in scan_get_next_rmap_item()",
        "commit_text": " Andrea Righi reported a case where an exiting task can race against ksmd::scan_get_next_rmap_item (http://lkml.org/lkml/2011/6/1/742) easily triggering a NULL pointer dereference in ksmd.  ksm_scan.mm_slot == &ksm_mm_head with only one registered mm  CPU 1 (__ksm_exit)\t\tCPU 2 (scan_get_next_rmap_item)  \t\t\t\tlist_empty() is false lock\t\t\t\tslot == &ksm_mm_head list_del(slot->mm_list) (list now empty) unlock \t\t\t\tlock \t\t\t\tslot = list_entry(slot->mm_list.next) \t\t\t\t(list is empty, so slot is still ksm_mm_head) \t\t\t\tunlock \t\t\t\tslot->mm == NULL ... Oops  Close this race by revalidating that the new slot is not simply the list head again.  Andrea's test case:  #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <sys/mman.h>  #define BUFSIZE getpagesize()  int main(int argc, char **argv) { \tvoid *ptr;  \tif (posix_memalign(&ptr, getpagesize(), BUFSIZE) < 0) { \t\tperror(\"posix_memalign\"); \t\texit(1); \t} \tif (madvise(ptr, BUFSIZE, MADV_MERGEABLE) < 0) { \t\tperror(\"madvise\"); \t\texit(1); \t} \t*(char *)NULL = 0;  \treturn 0; }  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: <stable@kernel.org>",
        "func_before": "static struct rmap_item *scan_get_next_rmap_item(struct page **page)\n{\n\tstruct mm_struct *mm;\n\tstruct mm_slot *slot;\n\tstruct vm_area_struct *vma;\n\tstruct rmap_item *rmap_item;\n\n\tif (list_empty(&ksm_mm_head.mm_list))\n\t\treturn NULL;\n\n\tslot = ksm_scan.mm_slot;\n\tif (slot == &ksm_mm_head) {\n\t\t/*\n\t\t * A number of pages can hang around indefinitely on per-cpu\n\t\t * pagevecs, raised page count preventing write_protect_page\n\t\t * from merging them.  Though it doesn't really matter much,\n\t\t * it is puzzling to see some stuck in pages_volatile until\n\t\t * other activity jostles them out, and they also prevented\n\t\t * LTP's KSM test from succeeding deterministically; so drain\n\t\t * them here (here rather than on entry to ksm_do_scan(),\n\t\t * so we don't IPI too often when pages_to_scan is set low).\n\t\t */\n\t\tlru_add_drain_all();\n\n\t\troot_unstable_tree = RB_ROOT;\n\n\t\tspin_lock(&ksm_mmlist_lock);\n\t\tslot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);\n\t\tksm_scan.mm_slot = slot;\n\t\tspin_unlock(&ksm_mmlist_lock);\nnext_mm:\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &slot->rmap_list;\n\t}\n\n\tmm = slot->mm;\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tvma = NULL;\n\telse\n\t\tvma = find_vma(mm, ksm_scan.address);\n\n\tfor (; vma; vma = vma->vm_next) {\n\t\tif (!(vma->vm_flags & VM_MERGEABLE))\n\t\t\tcontinue;\n\t\tif (ksm_scan.address < vma->vm_start)\n\t\t\tksm_scan.address = vma->vm_start;\n\t\tif (!vma->anon_vma)\n\t\t\tksm_scan.address = vma->vm_end;\n\n\t\twhile (ksm_scan.address < vma->vm_end) {\n\t\t\tif (ksm_test_exit(mm))\n\t\t\t\tbreak;\n\t\t\t*page = follow_page(vma, ksm_scan.address, FOLL_GET);\n\t\t\tif (IS_ERR_OR_NULL(*page)) {\n\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (PageAnon(*page) ||\n\t\t\t    page_trans_compound_anon(*page)) {\n\t\t\t\tflush_anon_page(vma, *page, ksm_scan.address);\n\t\t\t\tflush_dcache_page(*page);\n\t\t\t\trmap_item = get_next_rmap_item(slot,\n\t\t\t\t\tksm_scan.rmap_list, ksm_scan.address);\n\t\t\t\tif (rmap_item) {\n\t\t\t\t\tksm_scan.rmap_list =\n\t\t\t\t\t\t\t&rmap_item->rmap_list;\n\t\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\t} else\n\t\t\t\t\tput_page(*page);\n\t\t\t\tup_read(&mm->mmap_sem);\n\t\t\t\treturn rmap_item;\n\t\t\t}\n\t\t\tput_page(*page);\n\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tif (ksm_test_exit(mm)) {\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &slot->rmap_list;\n\t}\n\t/*\n\t * Nuke all the rmap_items that are above this current rmap:\n\t * because there were no VM_MERGEABLE vmas with such addresses.\n\t */\n\tremove_trailing_rmap_items(slot, ksm_scan.rmap_list);\n\n\tspin_lock(&ksm_mmlist_lock);\n\tksm_scan.mm_slot = list_entry(slot->mm_list.next,\n\t\t\t\t\t\tstruct mm_slot, mm_list);\n\tif (ksm_scan.address == 0) {\n\t\t/*\n\t\t * We've completed a full scan of all vmas, holding mmap_sem\n\t\t * throughout, and found no VM_MERGEABLE: so do the same as\n\t\t * __ksm_exit does to remove this mm from all our lists now.\n\t\t * This applies either when cleaning up after __ksm_exit\n\t\t * (but beware: we can reach here even before __ksm_exit),\n\t\t * or when all VM_MERGEABLE areas have been unmapped (and\n\t\t * mmap_sem then protects against race with MADV_MERGEABLE).\n\t\t */\n\t\thlist_del(&slot->link);\n\t\tlist_del(&slot->mm_list);\n\t\tspin_unlock(&ksm_mmlist_lock);\n\n\t\tfree_mm_slot(slot);\n\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\tup_read(&mm->mmap_sem);\n\t\tmmdrop(mm);\n\t} else {\n\t\tspin_unlock(&ksm_mmlist_lock);\n\t\tup_read(&mm->mmap_sem);\n\t}\n\n\t/* Repeat until we've completed scanning the whole list */\n\tslot = ksm_scan.mm_slot;\n\tif (slot != &ksm_mm_head)\n\t\tgoto next_mm;\n\n\tksm_scan.seqnr++;\n\treturn NULL;\n}",
        "func": "static struct rmap_item *scan_get_next_rmap_item(struct page **page)\n{\n\tstruct mm_struct *mm;\n\tstruct mm_slot *slot;\n\tstruct vm_area_struct *vma;\n\tstruct rmap_item *rmap_item;\n\n\tif (list_empty(&ksm_mm_head.mm_list))\n\t\treturn NULL;\n\n\tslot = ksm_scan.mm_slot;\n\tif (slot == &ksm_mm_head) {\n\t\t/*\n\t\t * A number of pages can hang around indefinitely on per-cpu\n\t\t * pagevecs, raised page count preventing write_protect_page\n\t\t * from merging them.  Though it doesn't really matter much,\n\t\t * it is puzzling to see some stuck in pages_volatile until\n\t\t * other activity jostles them out, and they also prevented\n\t\t * LTP's KSM test from succeeding deterministically; so drain\n\t\t * them here (here rather than on entry to ksm_do_scan(),\n\t\t * so we don't IPI too often when pages_to_scan is set low).\n\t\t */\n\t\tlru_add_drain_all();\n\n\t\troot_unstable_tree = RB_ROOT;\n\n\t\tspin_lock(&ksm_mmlist_lock);\n\t\tslot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);\n\t\tksm_scan.mm_slot = slot;\n\t\tspin_unlock(&ksm_mmlist_lock);\n\t\t/*\n\t\t * Although we tested list_empty() above, a racing __ksm_exit\n\t\t * of the last mm on the list may have removed it since then.\n\t\t */\n\t\tif (slot == &ksm_mm_head)\n\t\t\treturn NULL;\nnext_mm:\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &slot->rmap_list;\n\t}\n\n\tmm = slot->mm;\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tvma = NULL;\n\telse\n\t\tvma = find_vma(mm, ksm_scan.address);\n\n\tfor (; vma; vma = vma->vm_next) {\n\t\tif (!(vma->vm_flags & VM_MERGEABLE))\n\t\t\tcontinue;\n\t\tif (ksm_scan.address < vma->vm_start)\n\t\t\tksm_scan.address = vma->vm_start;\n\t\tif (!vma->anon_vma)\n\t\t\tksm_scan.address = vma->vm_end;\n\n\t\twhile (ksm_scan.address < vma->vm_end) {\n\t\t\tif (ksm_test_exit(mm))\n\t\t\t\tbreak;\n\t\t\t*page = follow_page(vma, ksm_scan.address, FOLL_GET);\n\t\t\tif (IS_ERR_OR_NULL(*page)) {\n\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (PageAnon(*page) ||\n\t\t\t    page_trans_compound_anon(*page)) {\n\t\t\t\tflush_anon_page(vma, *page, ksm_scan.address);\n\t\t\t\tflush_dcache_page(*page);\n\t\t\t\trmap_item = get_next_rmap_item(slot,\n\t\t\t\t\tksm_scan.rmap_list, ksm_scan.address);\n\t\t\t\tif (rmap_item) {\n\t\t\t\t\tksm_scan.rmap_list =\n\t\t\t\t\t\t\t&rmap_item->rmap_list;\n\t\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\t} else\n\t\t\t\t\tput_page(*page);\n\t\t\t\tup_read(&mm->mmap_sem);\n\t\t\t\treturn rmap_item;\n\t\t\t}\n\t\t\tput_page(*page);\n\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tif (ksm_test_exit(mm)) {\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &slot->rmap_list;\n\t}\n\t/*\n\t * Nuke all the rmap_items that are above this current rmap:\n\t * because there were no VM_MERGEABLE vmas with such addresses.\n\t */\n\tremove_trailing_rmap_items(slot, ksm_scan.rmap_list);\n\n\tspin_lock(&ksm_mmlist_lock);\n\tksm_scan.mm_slot = list_entry(slot->mm_list.next,\n\t\t\t\t\t\tstruct mm_slot, mm_list);\n\tif (ksm_scan.address == 0) {\n\t\t/*\n\t\t * We've completed a full scan of all vmas, holding mmap_sem\n\t\t * throughout, and found no VM_MERGEABLE: so do the same as\n\t\t * __ksm_exit does to remove this mm from all our lists now.\n\t\t * This applies either when cleaning up after __ksm_exit\n\t\t * (but beware: we can reach here even before __ksm_exit),\n\t\t * or when all VM_MERGEABLE areas have been unmapped (and\n\t\t * mmap_sem then protects against race with MADV_MERGEABLE).\n\t\t */\n\t\thlist_del(&slot->link);\n\t\tlist_del(&slot->mm_list);\n\t\tspin_unlock(&ksm_mmlist_lock);\n\n\t\tfree_mm_slot(slot);\n\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\tup_read(&mm->mmap_sem);\n\t\tmmdrop(mm);\n\t} else {\n\t\tspin_unlock(&ksm_mmlist_lock);\n\t\tup_read(&mm->mmap_sem);\n\t}\n\n\t/* Repeat until we've completed scanning the whole list */\n\tslot = ksm_scan.mm_slot;\n\tif (slot != &ksm_mm_head)\n\t\tgoto next_mm;\n\n\tksm_scan.seqnr++;\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,6 +28,12 @@\n \t\tslot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);\n \t\tksm_scan.mm_slot = slot;\n \t\tspin_unlock(&ksm_mmlist_lock);\n+\t\t/*\n+\t\t * Although we tested list_empty() above, a racing __ksm_exit\n+\t\t * of the last mm on the list may have removed it since then.\n+\t\t */\n+\t\tif (slot == &ksm_mm_head)\n+\t\t\treturn NULL;\n next_mm:\n \t\tksm_scan.address = 0;\n \t\tksm_scan.rmap_list = &slot->rmap_list;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t/*",
                "\t\t * Although we tested list_empty() above, a racing __ksm_exit",
                "\t\t * of the last mm on the list may have removed it since then.",
                "\t\t */",
                "\t\tif (slot == &ksm_mm_head)",
                "\t\t\treturn NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-4029",
        "func_name": "xorg/xserver/LockServer",
        "description": "The LockServer function in os/utils.c in X.Org xserver before 1.11.2 allows local users to change the permissions of arbitrary files to 444, read those files, and possibly cause a denial of service (removed execution permission) via a symlink attack on a temporary lock file.",
        "git_url": "http://cgit.freedesktop.org/xorg/xserver/commit/?id=b67581cf825940fdf52bf2e0af4330e695d724a4",
        "commit_title": "Use fchmod() to change permissions of the lock file instead",
        "commit_text": "of chmod(), thus avoid the race that can be exploited to set a symbolic link to any file or directory in the system.  ",
        "func_before": "void\nLockServer(void)\n{\n  char tmp[PATH_MAX], pid_str[12];\n  int lfd, i, haslock, l_pid, t;\n  char *tmppath = NULL;\n  int len;\n  char port[20];\n\n  if (nolock) return;\n  /*\n   * Path names\n   */\n  tmppath = LOCK_DIR;\n\n  sprintf(port, \"%d\", atoi(display));\n  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :\n\t\t\t\t\t\tstrlen(LOCK_TMP_PREFIX);\n  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;\n  if (len > sizeof(LockFile))\n    FatalError(\"Display name `%s' is too long\\n\", port);\n  (void)sprintf(tmp, \"%s\" LOCK_TMP_PREFIX \"%s\" LOCK_SUFFIX, tmppath, port);\n  (void)sprintf(LockFile, \"%s\" LOCK_PREFIX \"%s\" LOCK_SUFFIX, tmppath, port);\n\n  /*\n   * Create a temporary file containing our PID.  Attempt three times\n   * to create the file.\n   */\n  StillLocking = TRUE;\n  i = 0;\n  do {\n    i++;\n    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);\n    if (lfd < 0)\n       sleep(2);\n    else\n       break;\n  } while (i < 3);\n  if (lfd < 0) {\n    unlink(tmp);\n    i = 0;\n    do {\n      i++;\n      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);\n      if (lfd < 0)\n         sleep(2);\n      else\n         break;\n    } while (i < 3);\n  }\n  if (lfd < 0)\n    FatalError(\"Could not create lock file in %s\\n\", tmp);\n  (void) sprintf(pid_str, \"%10ld\\n\", (long)getpid());\n  (void) write(lfd, pid_str, 11);\n  (void) chmod(tmp, 0444);\n  (void) close(lfd);\n\n  /*\n   * OK.  Now the tmp file exists.  Try three times to move it in place\n   * for the lock.\n   */\n  i = 0;\n  haslock = 0;\n  while ((!haslock) && (i++ < 3)) {\n    haslock = (link(tmp,LockFile) == 0);\n    if (haslock) {\n      /*\n       * We're done.\n       */\n      break;\n    }\n    else {\n      /*\n       * Read the pid from the existing file\n       */\n      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);\n      if (lfd < 0) {\n        unlink(tmp);\n        FatalError(\"Can't read lock file %s\\n\", LockFile);\n      }\n      pid_str[0] = '\\0';\n      if (read(lfd, pid_str, 11) != 11) {\n        /*\n         * Bogus lock file.\n         */\n        unlink(LockFile);\n        close(lfd);\n        continue;\n      }\n      pid_str[11] = '\\0';\n      sscanf(pid_str, \"%d\", &l_pid);\n      close(lfd);\n\n      /*\n       * Now try to kill the PID to see if it exists.\n       */\n      errno = 0;\n      t = kill(l_pid, 0);\n      if ((t< 0) && (errno == ESRCH)) {\n        /*\n         * Stale lock file.\n         */\n        unlink(LockFile);\n        continue;\n      }\n      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {\n        /*\n         * Process is still active.\n         */\n        unlink(tmp);\n\tFatalError(\"Server is already active for display %s\\n%s %s\\n%s\\n\",\n\t\t   port, \"\\tIf this server is no longer running, remove\",\n\t\t   LockFile, \"\\tand start again.\");\n      }\n    }\n  }\n  unlink(tmp);\n  if (!haslock)\n    FatalError(\"Could not create server lock file: %s\\n\", LockFile);\n  StillLocking = FALSE;\n}",
        "func": "void\nLockServer(void)\n{\n  char tmp[PATH_MAX], pid_str[12];\n  int lfd, i, haslock, l_pid, t;\n  char *tmppath = NULL;\n  int len;\n  char port[20];\n\n  if (nolock) return;\n  /*\n   * Path names\n   */\n  tmppath = LOCK_DIR;\n\n  sprintf(port, \"%d\", atoi(display));\n  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :\n\t\t\t\t\t\tstrlen(LOCK_TMP_PREFIX);\n  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;\n  if (len > sizeof(LockFile))\n    FatalError(\"Display name `%s' is too long\\n\", port);\n  (void)sprintf(tmp, \"%s\" LOCK_TMP_PREFIX \"%s\" LOCK_SUFFIX, tmppath, port);\n  (void)sprintf(LockFile, \"%s\" LOCK_PREFIX \"%s\" LOCK_SUFFIX, tmppath, port);\n\n  /*\n   * Create a temporary file containing our PID.  Attempt three times\n   * to create the file.\n   */\n  StillLocking = TRUE;\n  i = 0;\n  do {\n    i++;\n    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);\n    if (lfd < 0)\n       sleep(2);\n    else\n       break;\n  } while (i < 3);\n  if (lfd < 0) {\n    unlink(tmp);\n    i = 0;\n    do {\n      i++;\n      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);\n      if (lfd < 0)\n         sleep(2);\n      else\n         break;\n    } while (i < 3);\n  }\n  if (lfd < 0)\n    FatalError(\"Could not create lock file in %s\\n\", tmp);\n  (void) sprintf(pid_str, \"%10ld\\n\", (long)getpid());\n  (void) write(lfd, pid_str, 11);\n  (void) fchmod(lfd, 0444);\n  (void) close(lfd);\n\n  /*\n   * OK.  Now the tmp file exists.  Try three times to move it in place\n   * for the lock.\n   */\n  i = 0;\n  haslock = 0;\n  while ((!haslock) && (i++ < 3)) {\n    haslock = (link(tmp,LockFile) == 0);\n    if (haslock) {\n      /*\n       * We're done.\n       */\n      break;\n    }\n    else {\n      /*\n       * Read the pid from the existing file\n       */\n      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);\n      if (lfd < 0) {\n        unlink(tmp);\n        FatalError(\"Can't read lock file %s\\n\", LockFile);\n      }\n      pid_str[0] = '\\0';\n      if (read(lfd, pid_str, 11) != 11) {\n        /*\n         * Bogus lock file.\n         */\n        unlink(LockFile);\n        close(lfd);\n        continue;\n      }\n      pid_str[11] = '\\0';\n      sscanf(pid_str, \"%d\", &l_pid);\n      close(lfd);\n\n      /*\n       * Now try to kill the PID to see if it exists.\n       */\n      errno = 0;\n      t = kill(l_pid, 0);\n      if ((t< 0) && (errno == ESRCH)) {\n        /*\n         * Stale lock file.\n         */\n        unlink(LockFile);\n        continue;\n      }\n      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {\n        /*\n         * Process is still active.\n         */\n        unlink(tmp);\n\tFatalError(\"Server is already active for display %s\\n%s %s\\n%s\\n\",\n\t\t   port, \"\\tIf this server is no longer running, remove\",\n\t\t   LockFile, \"\\tand start again.\");\n      }\n    }\n  }\n  unlink(tmp);\n  if (!haslock)\n    FatalError(\"Could not create server lock file: %s\\n\", LockFile);\n  StillLocking = FALSE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -52,7 +52,7 @@\n     FatalError(\"Could not create lock file in %s\\n\", tmp);\n   (void) sprintf(pid_str, \"%10ld\\n\", (long)getpid());\n   (void) write(lfd, pid_str, 11);\n-  (void) chmod(tmp, 0444);\n+  (void) fchmod(lfd, 0444);\n   (void) close(lfd);\n \n   /*",
        "diff_line_info": {
            "deleted_lines": [
                "  (void) chmod(tmp, 0444);"
            ],
            "added_lines": [
                "  (void) fchmod(lfd, 0444);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-1174",
        "func_name": "systemd/rm_rf_children",
        "description": "The rm_rf_children function in util.c in the systemd-logind login manager in systemd before 44, when logging out, allows local users to delete arbitrary files via a symlink attack on unspecified files, related to \"particular records related with user session.\"",
        "git_url": "http://cgit.freedesktop.org/systemd/systemd/commit/?id=5ebff5337594d690b322078c512eb222d34aaa82",
        "commit_title": "The function checks if the entry is a directory before recursing, but",
        "commit_text": "there is a window between the check and the open, during which the directory could be replaced with a symlink.  CVE-2012-1174 https://bugzilla.redhat.com/show_bug.cgi?id=803358 ",
        "func_before": "static int rm_rf_children(int fd, bool only_dirs, bool honour_sticky) {\n        DIR *d;\n        int ret = 0;\n\n        assert(fd >= 0);\n\n        /* This returns the first error we run into, but nevertheless\n         * tries to go on */\n\n        if (!(d = fdopendir(fd))) {\n                close_nointr_nofail(fd);\n\n                return errno == ENOENT ? 0 : -errno;\n        }\n\n        for (;;) {\n                struct dirent buf, *de;\n                bool is_dir, keep_around = false;\n                int r;\n\n                if ((r = readdir_r(d, &buf, &de)) != 0) {\n                        if (ret == 0)\n                                ret = -r;\n                        break;\n                }\n\n                if (!de)\n                        break;\n\n                if (streq(de->d_name, \".\") || streq(de->d_name, \"..\"))\n                        continue;\n\n                if (de->d_type == DT_UNKNOWN) {\n                        struct stat st;\n\n                        if (fstatat(fd, de->d_name, &st, AT_SYMLINK_NOFOLLOW) < 0) {\n                                if (ret == 0 && errno != ENOENT)\n                                        ret = -errno;\n                                continue;\n                        }\n\n                        if (honour_sticky)\n                                keep_around =\n                                        (st.st_uid == 0 || st.st_uid == getuid()) &&\n                                        (st.st_mode & S_ISVTX);\n\n                        is_dir = S_ISDIR(st.st_mode);\n\n                } else {\n                        if (honour_sticky) {\n                                struct stat st;\n\n                                if (fstatat(fd, de->d_name, &st, AT_SYMLINK_NOFOLLOW) < 0) {\n                                        if (ret == 0 && errno != ENOENT)\n                                                ret = -errno;\n                                        continue;\n                                }\n\n                                keep_around =\n                                        (st.st_uid == 0 || st.st_uid == getuid()) &&\n                                        (st.st_mode & S_ISVTX);\n                        }\n\n                        is_dir = de->d_type == DT_DIR;\n                }\n\n                if (is_dir) {\n                        int subdir_fd;\n\n                        if ((subdir_fd = openat(fd, de->d_name, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC)) < 0) {\n                                if (ret == 0 && errno != ENOENT)\n                                        ret = -errno;\n                                continue;\n                        }\n\n                        if ((r = rm_rf_children(subdir_fd, only_dirs, honour_sticky)) < 0) {\n                                if (ret == 0)\n                                        ret = r;\n                        }\n\n                        if (!keep_around)\n                                if (unlinkat(fd, de->d_name, AT_REMOVEDIR) < 0) {\n                                        if (ret == 0 && errno != ENOENT)\n                                                ret = -errno;\n                                }\n\n                } else if (!only_dirs && !keep_around) {\n\n                        if (unlinkat(fd, de->d_name, 0) < 0) {\n                                if (ret == 0 && errno != ENOENT)\n                                        ret = -errno;\n                        }\n                }\n        }\n\n        closedir(d);\n\n        return ret;\n}",
        "func": "static int rm_rf_children(int fd, bool only_dirs, bool honour_sticky) {\n        DIR *d;\n        int ret = 0;\n\n        assert(fd >= 0);\n\n        /* This returns the first error we run into, but nevertheless\n         * tries to go on */\n\n        if (!(d = fdopendir(fd))) {\n                close_nointr_nofail(fd);\n\n                return errno == ENOENT ? 0 : -errno;\n        }\n\n        for (;;) {\n                struct dirent buf, *de;\n                bool is_dir, keep_around = false;\n                int r;\n\n                if ((r = readdir_r(d, &buf, &de)) != 0) {\n                        if (ret == 0)\n                                ret = -r;\n                        break;\n                }\n\n                if (!de)\n                        break;\n\n                if (streq(de->d_name, \".\") || streq(de->d_name, \"..\"))\n                        continue;\n\n                if (de->d_type == DT_UNKNOWN) {\n                        struct stat st;\n\n                        if (fstatat(fd, de->d_name, &st, AT_SYMLINK_NOFOLLOW) < 0) {\n                                if (ret == 0 && errno != ENOENT)\n                                        ret = -errno;\n                                continue;\n                        }\n\n                        if (honour_sticky)\n                                keep_around =\n                                        (st.st_uid == 0 || st.st_uid == getuid()) &&\n                                        (st.st_mode & S_ISVTX);\n\n                        is_dir = S_ISDIR(st.st_mode);\n\n                } else {\n                        if (honour_sticky) {\n                                struct stat st;\n\n                                if (fstatat(fd, de->d_name, &st, AT_SYMLINK_NOFOLLOW) < 0) {\n                                        if (ret == 0 && errno != ENOENT)\n                                                ret = -errno;\n                                        continue;\n                                }\n\n                                keep_around =\n                                        (st.st_uid == 0 || st.st_uid == getuid()) &&\n                                        (st.st_mode & S_ISVTX);\n                        }\n\n                        is_dir = de->d_type == DT_DIR;\n                }\n\n                if (is_dir) {\n                        int subdir_fd;\n\n                        subdir_fd = openat(fd, de->d_name, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC|O_NOFOLLOW);\n                        if (subdir_fd < 0) {\n                                if (ret == 0 && errno != ENOENT)\n                                        ret = -errno;\n                                continue;\n                        }\n\n                        if ((r = rm_rf_children(subdir_fd, only_dirs, honour_sticky)) < 0) {\n                                if (ret == 0)\n                                        ret = r;\n                        }\n\n                        if (!keep_around)\n                                if (unlinkat(fd, de->d_name, AT_REMOVEDIR) < 0) {\n                                        if (ret == 0 && errno != ENOENT)\n                                                ret = -errno;\n                                }\n\n                } else if (!only_dirs && !keep_around) {\n\n                        if (unlinkat(fd, de->d_name, 0) < 0) {\n                                if (ret == 0 && errno != ENOENT)\n                                        ret = -errno;\n                        }\n                }\n        }\n\n        closedir(d);\n\n        return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -67,7 +67,8 @@\n                 if (is_dir) {\n                         int subdir_fd;\n \n-                        if ((subdir_fd = openat(fd, de->d_name, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC)) < 0) {\n+                        subdir_fd = openat(fd, de->d_name, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC|O_NOFOLLOW);\n+                        if (subdir_fd < 0) {\n                                 if (ret == 0 && errno != ENOENT)\n                                         ret = -errno;\n                                 continue;",
        "diff_line_info": {
            "deleted_lines": [
                "                        if ((subdir_fd = openat(fd, de->d_name, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC)) < 0) {"
            ],
            "added_lines": [
                "                        subdir_fd = openat(fd, de->d_name, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC|O_NOFOLLOW);",
                "                        if (subdir_fd < 0) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2737",
        "func_name": "accountsservice/user_set_location",
        "description": "The user_change_icon_file_authorized_cb function in /usr/libexec/accounts-daemon in AccountsService before 0.6.22 does not properly check the UID when copying an icon file to the system cache directory, which allows local users to read arbitrary files via a race condition.",
        "git_url": "http://cgit.freedesktop.org/accountsservice/commit/?id=bd51aa4cdac380f55d607f4ffdf2ab3c00d08721",
        "commit_title": "The previous commit changed the SetIconFile call to identify",
        "commit_text": "the uid of the calling process via cached peer credentials stored by the bus daemon.  This commit fixes other similar cases where we try to figure out process identity on our own instead of through the bus daemon. ",
        "func_before": "static gboolean\nuser_set_location (AccountsUser          *auser,\n                   GDBusMethodInvocation *context,\n                   const gchar           *location)\n{\n        User *user = (User*)auser;\n        uid_t uid;\n        const gchar *action_id;\n\n        uid = method_invocation_get_uid (context);\n        if (user->uid == uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_location_authorized_cb,\n                                 context,\n                                 g_strdup (location),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "func": "static gboolean\nuser_set_location (AccountsUser          *auser,\n                   GDBusMethodInvocation *context,\n                   const gchar           *location)\n{\n        User *user = (User*)auser;\n        int uid;\n        const gchar *action_id;\n\n        if (!get_caller_uid (context, &uid)) {\n                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n                return FALSE;\n        }\n\n        if (user->uid == (uid_t) uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_location_authorized_cb,\n                                 context,\n                                 g_strdup (location),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,11 +4,15 @@\n                    const gchar           *location)\n {\n         User *user = (User*)auser;\n-        uid_t uid;\n+        int uid;\n         const gchar *action_id;\n \n-        uid = method_invocation_get_uid (context);\n-        if (user->uid == uid)\n+        if (!get_caller_uid (context, &uid)) {\n+                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n+                return FALSE;\n+        }\n+\n+        if (user->uid == (uid_t) uid)\n                 action_id = \"org.freedesktop.accounts.change-own-user-data\";\n         else\n                 action_id = \"org.freedesktop.accounts.user-administration\";",
        "diff_line_info": {
            "deleted_lines": [
                "        uid_t uid;",
                "        uid = method_invocation_get_uid (context);",
                "        if (user->uid == uid)"
            ],
            "added_lines": [
                "        int uid;",
                "        if (!get_caller_uid (context, &uid)) {",
                "                throw_error (context, ERROR_FAILED, \"identifying caller failed\");",
                "                return FALSE;",
                "        }",
                "",
                "        if (user->uid == (uid_t) uid)"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2737",
        "func_name": "accountsservice/user_set_icon_file",
        "description": "The user_change_icon_file_authorized_cb function in /usr/libexec/accounts-daemon in AccountsService before 0.6.22 does not properly check the UID when copying an icon file to the system cache directory, which allows local users to read arbitrary files via a race condition.",
        "git_url": "http://cgit.freedesktop.org/accountsservice/commit/?id=bd51aa4cdac380f55d607f4ffdf2ab3c00d08721",
        "commit_title": "The previous commit changed the SetIconFile call to identify",
        "commit_text": "the uid of the calling process via cached peer credentials stored by the bus daemon.  This commit fixes other similar cases where we try to figure out process identity on our own instead of through the bus daemon. ",
        "func_before": "static gboolean\nuser_set_icon_file (AccountsUser          *auser,\n                    GDBusMethodInvocation *context,\n                    const gchar           *filename)\n{\n        User *user = (User*)auser;\n        uid_t uid;\n        const gchar *action_id;\n\n        uid = method_invocation_get_uid (context);\n        if (user->uid == uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_icon_file_authorized_cb,\n                                 context,\n                                 g_strdup (filename),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "func": "static gboolean\nuser_set_icon_file (AccountsUser          *auser,\n                    GDBusMethodInvocation *context,\n                    const gchar           *filename)\n{\n        User *user = (User*)auser;\n        int uid;\n        const gchar *action_id;\n\n        if (!get_caller_uid (context, &uid)) {\n                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n                return FALSE;\n        }\n\n        if (user->uid == (uid_t) uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_icon_file_authorized_cb,\n                                 context,\n                                 g_strdup (filename),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,11 +4,15 @@\n                     const gchar           *filename)\n {\n         User *user = (User*)auser;\n-        uid_t uid;\n+        int uid;\n         const gchar *action_id;\n \n-        uid = method_invocation_get_uid (context);\n-        if (user->uid == uid)\n+        if (!get_caller_uid (context, &uid)) {\n+                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n+                return FALSE;\n+        }\n+\n+        if (user->uid == (uid_t) uid)\n                 action_id = \"org.freedesktop.accounts.change-own-user-data\";\n         else\n                 action_id = \"org.freedesktop.accounts.user-administration\";",
        "diff_line_info": {
            "deleted_lines": [
                "        uid_t uid;",
                "        uid = method_invocation_get_uid (context);",
                "        if (user->uid == uid)"
            ],
            "added_lines": [
                "        int uid;",
                "        if (!get_caller_uid (context, &uid)) {",
                "                throw_error (context, ERROR_FAILED, \"identifying caller failed\");",
                "                return FALSE;",
                "        }",
                "",
                "        if (user->uid == (uid_t) uid)"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2737",
        "func_name": "accountsservice/user_set_real_name",
        "description": "The user_change_icon_file_authorized_cb function in /usr/libexec/accounts-daemon in AccountsService before 0.6.22 does not properly check the UID when copying an icon file to the system cache directory, which allows local users to read arbitrary files via a race condition.",
        "git_url": "http://cgit.freedesktop.org/accountsservice/commit/?id=bd51aa4cdac380f55d607f4ffdf2ab3c00d08721",
        "commit_title": "The previous commit changed the SetIconFile call to identify",
        "commit_text": "the uid of the calling process via cached peer credentials stored by the bus daemon.  This commit fixes other similar cases where we try to figure out process identity on our own instead of through the bus daemon. ",
        "func_before": "static gboolean\nuser_set_real_name (AccountsUser          *auser,\n                    GDBusMethodInvocation *context,\n                    const gchar           *real_name)\n{\n        User *user = (User*)auser;\n        uid_t uid;\n        const gchar *action_id;\n\n        uid = method_invocation_get_uid (context);\n        if (user->uid == uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_real_name_authorized_cb,\n                                 context,\n                                 g_strdup (real_name),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "func": "static gboolean\nuser_set_real_name (AccountsUser          *auser,\n                    GDBusMethodInvocation *context,\n                    const gchar           *real_name)\n{\n        User *user = (User*)auser;\n        int uid;\n        const gchar *action_id;\n\n        if (!get_caller_uid (context, &uid)) {\n                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n                return FALSE;\n        }\n\n        if (user->uid == (uid_t) uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_real_name_authorized_cb,\n                                 context,\n                                 g_strdup (real_name),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,11 +4,15 @@\n                     const gchar           *real_name)\n {\n         User *user = (User*)auser;\n-        uid_t uid;\n+        int uid;\n         const gchar *action_id;\n \n-        uid = method_invocation_get_uid (context);\n-        if (user->uid == uid)\n+        if (!get_caller_uid (context, &uid)) {\n+                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n+                return FALSE;\n+        }\n+\n+        if (user->uid == (uid_t) uid)\n                 action_id = \"org.freedesktop.accounts.change-own-user-data\";\n         else\n                 action_id = \"org.freedesktop.accounts.user-administration\";",
        "diff_line_info": {
            "deleted_lines": [
                "        uid_t uid;",
                "        uid = method_invocation_get_uid (context);",
                "        if (user->uid == uid)"
            ],
            "added_lines": [
                "        int uid;",
                "        if (!get_caller_uid (context, &uid)) {",
                "                throw_error (context, ERROR_FAILED, \"identifying caller failed\");",
                "                return FALSE;",
                "        }",
                "",
                "        if (user->uid == (uid_t) uid)"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2737",
        "func_name": "accountsservice/user_set_x_session",
        "description": "The user_change_icon_file_authorized_cb function in /usr/libexec/accounts-daemon in AccountsService before 0.6.22 does not properly check the UID when copying an icon file to the system cache directory, which allows local users to read arbitrary files via a race condition.",
        "git_url": "http://cgit.freedesktop.org/accountsservice/commit/?id=bd51aa4cdac380f55d607f4ffdf2ab3c00d08721",
        "commit_title": "The previous commit changed the SetIconFile call to identify",
        "commit_text": "the uid of the calling process via cached peer credentials stored by the bus daemon.  This commit fixes other similar cases where we try to figure out process identity on our own instead of through the bus daemon. ",
        "func_before": "static gboolean\nuser_set_x_session (AccountsUser          *auser,\n                    GDBusMethodInvocation *context,\n                    const gchar           *x_session)\n{\n        User *user = (User*)auser;\n        uid_t uid;\n        const gchar *action_id;\n\n        uid = method_invocation_get_uid (context);\n        if (user->uid == uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_x_session_authorized_cb,\n                                 context,\n                                 g_strdup (x_session),\n                                 (GDestroyNotify) g_free);\n\n        return TRUE;\n}",
        "func": "static gboolean\nuser_set_x_session (AccountsUser          *auser,\n                    GDBusMethodInvocation *context,\n                    const gchar           *x_session)\n{\n        User *user = (User*)auser;\n        int uid;\n        const gchar *action_id;\n\n        if (!get_caller_uid (context, &uid)) {\n                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n                return FALSE;\n        }\n\n        if (user->uid == (uid_t) uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_x_session_authorized_cb,\n                                 context,\n                                 g_strdup (x_session),\n                                 (GDestroyNotify) g_free);\n\n        return TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,11 +4,15 @@\n                     const gchar           *x_session)\n {\n         User *user = (User*)auser;\n-        uid_t uid;\n+        int uid;\n         const gchar *action_id;\n \n-        uid = method_invocation_get_uid (context);\n-        if (user->uid == uid)\n+        if (!get_caller_uid (context, &uid)) {\n+                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n+                return FALSE;\n+        }\n+\n+        if (user->uid == (uid_t) uid)\n                 action_id = \"org.freedesktop.accounts.change-own-user-data\";\n         else\n                 action_id = \"org.freedesktop.accounts.user-administration\";",
        "diff_line_info": {
            "deleted_lines": [
                "        uid_t uid;",
                "        uid = method_invocation_get_uid (context);",
                "        if (user->uid == uid)"
            ],
            "added_lines": [
                "        int uid;",
                "        if (!get_caller_uid (context, &uid)) {",
                "                throw_error (context, ERROR_FAILED, \"identifying caller failed\");",
                "                return FALSE;",
                "        }",
                "",
                "        if (user->uid == (uid_t) uid)"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2737",
        "func_name": "accountsservice/user_set_language",
        "description": "The user_change_icon_file_authorized_cb function in /usr/libexec/accounts-daemon in AccountsService before 0.6.22 does not properly check the UID when copying an icon file to the system cache directory, which allows local users to read arbitrary files via a race condition.",
        "git_url": "http://cgit.freedesktop.org/accountsservice/commit/?id=bd51aa4cdac380f55d607f4ffdf2ab3c00d08721",
        "commit_title": "The previous commit changed the SetIconFile call to identify",
        "commit_text": "the uid of the calling process via cached peer credentials stored by the bus daemon.  This commit fixes other similar cases where we try to figure out process identity on our own instead of through the bus daemon. ",
        "func_before": "static gboolean\nuser_set_language (AccountsUser          *auser,\n                   GDBusMethodInvocation *context,\n                   const gchar           *language)\n{\n        User *user = (User*)auser;\n        uid_t uid;\n        const gchar *action_id;\n\n        uid = method_invocation_get_uid (context);\n        if (user->uid == uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_language_authorized_cb,\n                                 context,\n                                 g_strdup (language),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "func": "static gboolean\nuser_set_language (AccountsUser          *auser,\n                   GDBusMethodInvocation *context,\n                   const gchar           *language)\n{\n        User *user = (User*)auser;\n        int uid;\n        const gchar *action_id;\n\n        if (!get_caller_uid (context, &uid)) {\n                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n                return FALSE;\n        }\n\n        if (user->uid == (uid_t) uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_language_authorized_cb,\n                                 context,\n                                 g_strdup (language),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,11 +4,15 @@\n                    const gchar           *language)\n {\n         User *user = (User*)auser;\n-        uid_t uid;\n+        int uid;\n         const gchar *action_id;\n \n-        uid = method_invocation_get_uid (context);\n-        if (user->uid == uid)\n+        if (!get_caller_uid (context, &uid)) {\n+                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n+                return FALSE;\n+        }\n+\n+        if (user->uid == (uid_t) uid)\n                 action_id = \"org.freedesktop.accounts.change-own-user-data\";\n         else\n                 action_id = \"org.freedesktop.accounts.user-administration\";",
        "diff_line_info": {
            "deleted_lines": [
                "        uid_t uid;",
                "        uid = method_invocation_get_uid (context);",
                "        if (user->uid == uid)"
            ],
            "added_lines": [
                "        int uid;",
                "        if (!get_caller_uid (context, &uid)) {",
                "                throw_error (context, ERROR_FAILED, \"identifying caller failed\");",
                "                return FALSE;",
                "        }",
                "",
                "        if (user->uid == (uid_t) uid)"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2737",
        "func_name": "accountsservice/user_set_email",
        "description": "The user_change_icon_file_authorized_cb function in /usr/libexec/accounts-daemon in AccountsService before 0.6.22 does not properly check the UID when copying an icon file to the system cache directory, which allows local users to read arbitrary files via a race condition.",
        "git_url": "http://cgit.freedesktop.org/accountsservice/commit/?id=bd51aa4cdac380f55d607f4ffdf2ab3c00d08721",
        "commit_title": "The previous commit changed the SetIconFile call to identify",
        "commit_text": "the uid of the calling process via cached peer credentials stored by the bus daemon.  This commit fixes other similar cases where we try to figure out process identity on our own instead of through the bus daemon. ",
        "func_before": "static gboolean\nuser_set_email (AccountsUser          *auser,\n                GDBusMethodInvocation *context,\n                const gchar           *email)\n{\n        User *user = (User*)auser;\n        uid_t uid;\n        const gchar *action_id;\n\n        uid = method_invocation_get_uid (context);\n        if (user->uid == uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_email_authorized_cb,\n                                 context,\n                                 g_strdup (email),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "func": "static gboolean\nuser_set_email (AccountsUser          *auser,\n                GDBusMethodInvocation *context,\n                const gchar           *email)\n{\n        User *user = (User*)auser;\n        int uid;\n        const gchar *action_id;\n\n        if (!get_caller_uid (context, &uid)) {\n                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n                return FALSE;\n        }\n\n        if (user->uid == (uid_t) uid)\n                action_id = \"org.freedesktop.accounts.change-own-user-data\";\n        else\n                action_id = \"org.freedesktop.accounts.user-administration\";\n\n        daemon_local_check_auth (user->daemon,\n                                 user,\n                                 action_id,\n                                 TRUE,\n                                 user_change_email_authorized_cb,\n                                 context,\n                                 g_strdup (email),\n                                 (GDestroyNotify)g_free);\n\n        return TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,11 +4,15 @@\n                 const gchar           *email)\n {\n         User *user = (User*)auser;\n-        uid_t uid;\n+        int uid;\n         const gchar *action_id;\n \n-        uid = method_invocation_get_uid (context);\n-        if (user->uid == uid)\n+        if (!get_caller_uid (context, &uid)) {\n+                throw_error (context, ERROR_FAILED, \"identifying caller failed\");\n+                return FALSE;\n+        }\n+\n+        if (user->uid == (uid_t) uid)\n                 action_id = \"org.freedesktop.accounts.change-own-user-data\";\n         else\n                 action_id = \"org.freedesktop.accounts.user-administration\";",
        "diff_line_info": {
            "deleted_lines": [
                "        uid_t uid;",
                "        uid = method_invocation_get_uid (context);",
                "        if (user->uid == uid)"
            ],
            "added_lines": [
                "        int uid;",
                "        if (!get_caller_uid (context, &uid)) {",
                "                throw_error (context, ERROR_FAILED, \"identifying caller failed\");",
                "                return FALSE;",
                "        }",
                "",
                "        if (user->uid == (uid_t) uid)"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2737",
        "func_name": "accountsservice/get_caller_uid",
        "description": "The user_change_icon_file_authorized_cb function in /usr/libexec/accounts-daemon in AccountsService before 0.6.22 does not properly check the UID when copying an icon file to the system cache directory, which allows local users to read arbitrary files via a race condition.",
        "git_url": "http://cgit.freedesktop.org/accountsservice/commit/?id=26213aa0e0d8dca5f36cc23f6942525224cbe9f5",
        "commit_title": "The AccountsService SetIconFile call associates an icon",
        "commit_text": "with a user.  SetIconFile allows users to have icons visible at the login screen that don't necessarily originate in globally readable or always available locations. This is accomplished by copying the originating icon to the local disk in /var.  Since AccountsService runs with with root privileges, the implemention of the SetIconFile method queries the uid of the method caller, forks, switches to that uid and performs the image copy as if it were the user.  Unfortunately, the uid lookup peformed is done \"just in time\" instead of looking at peer credentials from the time the call was initiated. There is a race condition that means a caller could invoke the method call, quickly exec a setuid binary, and then cause the copy to be performed as the uid of the setuid process.  This commit changes the uid lookup logic to query the system bus daemon for the peer credentials that were cached from the caller at the time of the initial connection. ",
        "func_before": "gboolean\nget_caller_uid (GDBusMethodInvocation *context, gint *uid)\n{\n        PolkitSubject *subject;\n        PolkitSubject *process;\n\n        subject = polkit_system_bus_name_new (g_dbus_method_invocation_get_sender (context));\n        process = polkit_system_bus_name_get_process_sync (POLKIT_SYSTEM_BUS_NAME (subject), NULL, NULL);\n        if (!process) {\n                g_object_unref (subject);\n                return FALSE;\n        }\n\n        *uid = polkit_unix_process_get_uid (POLKIT_UNIX_PROCESS (process));\n\n        g_object_unref (subject);\n        g_object_unref (process);\n\n        return TRUE;\n}",
        "func": "gboolean\nget_caller_uid (GDBusMethodInvocation *context,\n                gint                  *uid)\n{\n        GVariant      *reply;\n        GError        *error;\n\n        error = NULL;\n        reply = g_dbus_connection_call_sync (g_dbus_method_invocation_get_connection (context),\n                                             \"org.freedesktop.DBus\",\n                                             \"/org/freedesktop/DBus\",\n                                             \"org.freedesktop.DBus\",\n                                             \"GetConnectionUnixUser\",\n                                             g_variant_new (\"(s)\",\n                                                            g_dbus_method_invocation_get_sender (context)),\n                                             G_VARIANT_TYPE (\"(u)\"),\n                                             G_DBUS_CALL_FLAGS_NONE,\n                                             -1,\n                                             NULL,\n                                             &error);\n\n        if (reply == NULL) {\n                g_warning (\"Could not talk to message bus to find uid of sender %s: %s\",\n                           g_dbus_method_invocation_get_sender (context),\n                           error->message);\n                g_error_free (error);\n\n                return FALSE;\n        }\n\n        g_variant_get (reply, \"(u)\", uid);\n        g_variant_unref (reply);\n\n        return TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,20 +1,35 @@\n gboolean\n-get_caller_uid (GDBusMethodInvocation *context, gint *uid)\n+get_caller_uid (GDBusMethodInvocation *context,\n+                gint                  *uid)\n {\n-        PolkitSubject *subject;\n-        PolkitSubject *process;\n+        GVariant      *reply;\n+        GError        *error;\n \n-        subject = polkit_system_bus_name_new (g_dbus_method_invocation_get_sender (context));\n-        process = polkit_system_bus_name_get_process_sync (POLKIT_SYSTEM_BUS_NAME (subject), NULL, NULL);\n-        if (!process) {\n-                g_object_unref (subject);\n+        error = NULL;\n+        reply = g_dbus_connection_call_sync (g_dbus_method_invocation_get_connection (context),\n+                                             \"org.freedesktop.DBus\",\n+                                             \"/org/freedesktop/DBus\",\n+                                             \"org.freedesktop.DBus\",\n+                                             \"GetConnectionUnixUser\",\n+                                             g_variant_new (\"(s)\",\n+                                                            g_dbus_method_invocation_get_sender (context)),\n+                                             G_VARIANT_TYPE (\"(u)\"),\n+                                             G_DBUS_CALL_FLAGS_NONE,\n+                                             -1,\n+                                             NULL,\n+                                             &error);\n+\n+        if (reply == NULL) {\n+                g_warning (\"Could not talk to message bus to find uid of sender %s: %s\",\n+                           g_dbus_method_invocation_get_sender (context),\n+                           error->message);\n+                g_error_free (error);\n+\n                 return FALSE;\n         }\n \n-        *uid = polkit_unix_process_get_uid (POLKIT_UNIX_PROCESS (process));\n-\n-        g_object_unref (subject);\n-        g_object_unref (process);\n+        g_variant_get (reply, \"(u)\", uid);\n+        g_variant_unref (reply);\n \n         return TRUE;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "get_caller_uid (GDBusMethodInvocation *context, gint *uid)",
                "        PolkitSubject *subject;",
                "        PolkitSubject *process;",
                "        subject = polkit_system_bus_name_new (g_dbus_method_invocation_get_sender (context));",
                "        process = polkit_system_bus_name_get_process_sync (POLKIT_SYSTEM_BUS_NAME (subject), NULL, NULL);",
                "        if (!process) {",
                "                g_object_unref (subject);",
                "        *uid = polkit_unix_process_get_uid (POLKIT_UNIX_PROCESS (process));",
                "",
                "        g_object_unref (subject);",
                "        g_object_unref (process);"
            ],
            "added_lines": [
                "get_caller_uid (GDBusMethodInvocation *context,",
                "                gint                  *uid)",
                "        GVariant      *reply;",
                "        GError        *error;",
                "        error = NULL;",
                "        reply = g_dbus_connection_call_sync (g_dbus_method_invocation_get_connection (context),",
                "                                             \"org.freedesktop.DBus\",",
                "                                             \"/org/freedesktop/DBus\",",
                "                                             \"org.freedesktop.DBus\",",
                "                                             \"GetConnectionUnixUser\",",
                "                                             g_variant_new (\"(s)\",",
                "                                                            g_dbus_method_invocation_get_sender (context)),",
                "                                             G_VARIANT_TYPE (\"(u)\"),",
                "                                             G_DBUS_CALL_FLAGS_NONE,",
                "                                             -1,",
                "                                             NULL,",
                "                                             &error);",
                "",
                "        if (reply == NULL) {",
                "                g_warning (\"Could not talk to message bus to find uid of sender %s: %s\",",
                "                           g_dbus_method_invocation_get_sender (context),",
                "                           error->message);",
                "                g_error_free (error);",
                "",
                "        g_variant_get (reply, \"(u)\", uid);",
                "        g_variant_unref (reply);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2737",
        "func_name": "accountsservice/get_caller_loginuid",
        "description": "The user_change_icon_file_authorized_cb function in /usr/libexec/accounts-daemon in AccountsService before 0.6.22 does not properly check the UID when copying an icon file to the system cache directory, which allows local users to read arbitrary files via a race condition.",
        "git_url": "http://cgit.freedesktop.org/accountsservice/commit/?id=4c5b12e363410e490e776e4b4a86dcce157a543d",
        "commit_title": "_polkit_subject_get_cmdline is a function copy and pasted",
        "commit_text": "from the polkit code that returns the command line, uid, and pid of a particular polkit subject.  It's used for helping to generate log entries that detail what processes are invoking methods on the accounts service.  It's also used, on older kernels, for setting up the the loginuid of subprocesses that are run on behalf of AccountsService clients, so the audit trail leads back to the user initiating a request.  _polkit_subject_get_cmdline directly looks up the uid of the caller, instead of querying the system bus.  As such, it's vulnerable to the same race condition discussed in the previous two commits.  This commit guts _polkit_subject_get_cmdline, keeping only the part that reads /proc/pid/cmdline. We now get the uid and pid from the bus daemon. ",
        "func_before": "static void\nget_caller_loginuid (GDBusMethodInvocation *context, gchar *loginuid, gint size)\n{\n        PolkitSubject *subject;\n        gchar *cmdline;\n        gint pid;\n        gint uid;\n        gchar *path;\n        gchar *buf;\n\n        subject = polkit_system_bus_name_new (g_dbus_method_invocation_get_sender (context));\n        cmdline = _polkit_subject_get_cmdline (subject, &pid, &uid);\n        g_free (cmdline);\n        g_object_unref (subject);\n\n        path = g_strdup_printf (\"/proc/%d/loginuid\", pid);\n        if (g_file_get_contents (path, &buf, NULL, NULL)) {\n                strncpy (loginuid, buf, size);\n                g_free (buf);\n        }\n        else {\n                g_snprintf (loginuid, size, \"%d\", uid);\n        }\n\n        g_free (path);\n}",
        "func": "static void\nget_caller_loginuid (GDBusMethodInvocation *context, gchar *loginuid, gint size)\n{\n        GPid pid;\n        gint uid;\n        gchar *path;\n        gchar *buf;\n\n        if (!get_caller_uid (context, &uid)) {\n                uid = getuid ();\n        }\n\n        if (get_caller_pid (context, &pid)) {\n                path = g_strdup_printf (\"/proc/%d/loginuid\", (int) pid);\n        } else {\n                path = NULL;\n        }\n\n        if (path != NULL && g_file_get_contents (path, &buf, NULL, NULL)) {\n                strncpy (loginuid, buf, size);\n                g_free (buf);\n        }\n        else {\n                g_snprintf (loginuid, size, \"%d\", uid);\n        }\n\n        g_free (path);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,20 +1,22 @@\n static void\n get_caller_loginuid (GDBusMethodInvocation *context, gchar *loginuid, gint size)\n {\n-        PolkitSubject *subject;\n-        gchar *cmdline;\n-        gint pid;\n+        GPid pid;\n         gint uid;\n         gchar *path;\n         gchar *buf;\n \n-        subject = polkit_system_bus_name_new (g_dbus_method_invocation_get_sender (context));\n-        cmdline = _polkit_subject_get_cmdline (subject, &pid, &uid);\n-        g_free (cmdline);\n-        g_object_unref (subject);\n+        if (!get_caller_uid (context, &uid)) {\n+                uid = getuid ();\n+        }\n \n-        path = g_strdup_printf (\"/proc/%d/loginuid\", pid);\n-        if (g_file_get_contents (path, &buf, NULL, NULL)) {\n+        if (get_caller_pid (context, &pid)) {\n+                path = g_strdup_printf (\"/proc/%d/loginuid\", (int) pid);\n+        } else {\n+                path = NULL;\n+        }\n+\n+        if (path != NULL && g_file_get_contents (path, &buf, NULL, NULL)) {\n                 strncpy (loginuid, buf, size);\n                 g_free (buf);\n         }",
        "diff_line_info": {
            "deleted_lines": [
                "        PolkitSubject *subject;",
                "        gchar *cmdline;",
                "        gint pid;",
                "        subject = polkit_system_bus_name_new (g_dbus_method_invocation_get_sender (context));",
                "        cmdline = _polkit_subject_get_cmdline (subject, &pid, &uid);",
                "        g_free (cmdline);",
                "        g_object_unref (subject);",
                "        path = g_strdup_printf (\"/proc/%d/loginuid\", pid);",
                "        if (g_file_get_contents (path, &buf, NULL, NULL)) {"
            ],
            "added_lines": [
                "        GPid pid;",
                "        if (!get_caller_uid (context, &uid)) {",
                "                uid = getuid ();",
                "        }",
                "        if (get_caller_pid (context, &pid)) {",
                "                path = g_strdup_printf (\"/proc/%d/loginuid\", (int) pid);",
                "        } else {",
                "                path = NULL;",
                "        }",
                "",
                "        if (path != NULL && g_file_get_contents (path, &buf, NULL, NULL)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2737",
        "func_name": "accountsservice/sys_log",
        "description": "The user_change_icon_file_authorized_cb function in /usr/libexec/accounts-daemon in AccountsService before 0.6.22 does not properly check the UID when copying an icon file to the system cache directory, which allows local users to read arbitrary files via a race condition.",
        "git_url": "http://cgit.freedesktop.org/accountsservice/commit/?id=4c5b12e363410e490e776e4b4a86dcce157a543d",
        "commit_title": "_polkit_subject_get_cmdline is a function copy and pasted",
        "commit_text": "from the polkit code that returns the command line, uid, and pid of a particular polkit subject.  It's used for helping to generate log entries that detail what processes are invoking methods on the accounts service.  It's also used, on older kernels, for setting up the the loginuid of subprocesses that are run on behalf of AccountsService clients, so the audit trail leads back to the user initiating a request.  _polkit_subject_get_cmdline directly looks up the uid of the caller, instead of querying the system bus.  As such, it's vulnerable to the same race condition discussed in the previous two commits.  This commit guts _polkit_subject_get_cmdline, keeping only the part that reads /proc/pid/cmdline. We now get the uid and pid from the bus daemon. ",
        "func_before": "void\nsys_log (GDBusMethodInvocation *context,\n         const gchar           *format,\n                                ...)\n{\n        va_list args;\n        gchar *msg;\n\n        va_start (args, format);\n        msg = g_strdup_vprintf (format, args);\n        va_end (args);\n\n        if (context) {\n                PolkitSubject *subject;\n                gchar *cmdline;\n                gchar *id;\n                gint pid = 0;\n                gint uid = 0;\n                gchar *tmp;\n\n                subject = polkit_system_bus_name_new (g_dbus_method_invocation_get_sender (context));\n                id = polkit_subject_to_string (subject);\n                cmdline = _polkit_subject_get_cmdline (subject, &pid, &uid);\n\n                if (cmdline == NULL) {\n                        tmp = g_strdup_printf (\"request by %s: %s\", id, msg);\n                }\n                else {\n                        tmp = g_strdup_printf (\"request by %s [%s pid:%d uid:%d]: %s\", id, cmdline, pid, uid, msg);\n                }\n\n                g_free (msg);\n                msg = tmp;\n\n                g_free (id);\n                g_free (cmdline);\n                g_object_unref (subject);\n        }\n\n        syslog (LOG_NOTICE, \"%s\", msg);\n\n        g_free (msg);\n}",
        "func": "void\nsys_log (GDBusMethodInvocation *context,\n         const gchar           *format,\n                                ...)\n{\n        va_list args;\n        gchar *msg;\n\n        va_start (args, format);\n        msg = g_strdup_vprintf (format, args);\n        va_end (args);\n\n        if (context) {\n                PolkitSubject *subject;\n                gchar *cmdline = NULL;\n                gchar *id;\n                GPid pid = 0;\n                gint uid = -1;\n                gchar *tmp;\n\n                subject = polkit_system_bus_name_new (g_dbus_method_invocation_get_sender (context));\n                id = polkit_subject_to_string (subject);\n\n                if (get_caller_pid (context, &pid)) {\n                        cmdline = get_cmdline_of_pid (pid);\n                } else {\n                        pid = 0;\n                        cmdline = NULL;\n                }\n\n                if (cmdline != NULL) {\n                        if (get_caller_uid (context, &uid)) {\n                                tmp = g_strdup_printf (\"request by %s [%s pid:%d uid:%d]: %s\", id, cmdline, (int) pid, uid, msg);\n                        } else {\n                                tmp = g_strdup_printf (\"request by %s [%s pid:%d]: %s\", id, cmdline, (int) pid, msg);\n                        }\n                } else {\n                        if (get_caller_uid (context, &uid) && pid != 0) {\n                                tmp = g_strdup_printf (\"request by %s [pid:%d uid:%d]: %s\", id, (int) pid, uid, msg);\n                        } else if (pid != 0) {\n                                tmp = g_strdup_printf (\"request by %s [pid:%d]: %s\", id, (int) pid, msg);\n                        } else {\n                                tmp = g_strdup_printf (\"request by %s: %s\", id, msg);\n                        }\n                }\n\n                g_free (msg);\n                msg = tmp;\n\n                g_free (id);\n                g_free (cmdline);\n                g_object_unref (subject);\n        }\n\n        syslog (LOG_NOTICE, \"%s\", msg);\n\n        g_free (msg);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,21 +12,36 @@\n \n         if (context) {\n                 PolkitSubject *subject;\n-                gchar *cmdline;\n+                gchar *cmdline = NULL;\n                 gchar *id;\n-                gint pid = 0;\n-                gint uid = 0;\n+                GPid pid = 0;\n+                gint uid = -1;\n                 gchar *tmp;\n \n                 subject = polkit_system_bus_name_new (g_dbus_method_invocation_get_sender (context));\n                 id = polkit_subject_to_string (subject);\n-                cmdline = _polkit_subject_get_cmdline (subject, &pid, &uid);\n \n-                if (cmdline == NULL) {\n-                        tmp = g_strdup_printf (\"request by %s: %s\", id, msg);\n+                if (get_caller_pid (context, &pid)) {\n+                        cmdline = get_cmdline_of_pid (pid);\n+                } else {\n+                        pid = 0;\n+                        cmdline = NULL;\n                 }\n-                else {\n-                        tmp = g_strdup_printf (\"request by %s [%s pid:%d uid:%d]: %s\", id, cmdline, pid, uid, msg);\n+\n+                if (cmdline != NULL) {\n+                        if (get_caller_uid (context, &uid)) {\n+                                tmp = g_strdup_printf (\"request by %s [%s pid:%d uid:%d]: %s\", id, cmdline, (int) pid, uid, msg);\n+                        } else {\n+                                tmp = g_strdup_printf (\"request by %s [%s pid:%d]: %s\", id, cmdline, (int) pid, msg);\n+                        }\n+                } else {\n+                        if (get_caller_uid (context, &uid) && pid != 0) {\n+                                tmp = g_strdup_printf (\"request by %s [pid:%d uid:%d]: %s\", id, (int) pid, uid, msg);\n+                        } else if (pid != 0) {\n+                                tmp = g_strdup_printf (\"request by %s [pid:%d]: %s\", id, (int) pid, msg);\n+                        } else {\n+                                tmp = g_strdup_printf (\"request by %s: %s\", id, msg);\n+                        }\n                 }\n \n                 g_free (msg);",
        "diff_line_info": {
            "deleted_lines": [
                "                gchar *cmdline;",
                "                gint pid = 0;",
                "                gint uid = 0;",
                "                cmdline = _polkit_subject_get_cmdline (subject, &pid, &uid);",
                "                if (cmdline == NULL) {",
                "                        tmp = g_strdup_printf (\"request by %s: %s\", id, msg);",
                "                else {",
                "                        tmp = g_strdup_printf (\"request by %s [%s pid:%d uid:%d]: %s\", id, cmdline, pid, uid, msg);"
            ],
            "added_lines": [
                "                gchar *cmdline = NULL;",
                "                GPid pid = 0;",
                "                gint uid = -1;",
                "                if (get_caller_pid (context, &pid)) {",
                "                        cmdline = get_cmdline_of_pid (pid);",
                "                } else {",
                "                        pid = 0;",
                "                        cmdline = NULL;",
                "",
                "                if (cmdline != NULL) {",
                "                        if (get_caller_uid (context, &uid)) {",
                "                                tmp = g_strdup_printf (\"request by %s [%s pid:%d uid:%d]: %s\", id, cmdline, (int) pid, uid, msg);",
                "                        } else {",
                "                                tmp = g_strdup_printf (\"request by %s [%s pid:%d]: %s\", id, cmdline, (int) pid, msg);",
                "                        }",
                "                } else {",
                "                        if (get_caller_uid (context, &uid) && pid != 0) {",
                "                                tmp = g_strdup_printf (\"request by %s [pid:%d uid:%d]: %s\", id, (int) pid, uid, msg);",
                "                        } else if (pid != 0) {",
                "                                tmp = g_strdup_printf (\"request by %s [pid:%d]: %s\", id, (int) pid, msg);",
                "                        } else {",
                "                                tmp = g_strdup_printf (\"request by %s: %s\", id, msg);",
                "                        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-2373",
        "func_name": "torvalds/linux/pmd_none_or_trans_huge_or_clear_bad",
        "description": "The Linux kernel before 3.4.5 on the x86 platform, when Physical Address Extension (PAE) is enabled, does not properly use the Page Middle Directory (PMD), which allows local users to cause a denial of service (panic) via a crafted application that triggers a race condition.",
        "git_url": "https://github.com/torvalds/linux/commit/26c191788f18129af0eb32a358cdaea0c7479626",
        "commit_title": "mm: pmd_read_atomic: fix 32bit PAE pmd walk vs pmd_populate SMP race condition",
        "commit_text": " When holding the mmap_sem for reading, pmd_offset_map_lock should only run on a pmd_t that has been read atomically from the pmdp pointer, otherwise we may read only half of it leading to this crash.  PID: 11679  TASK: f06e8000  CPU: 3   COMMAND: \"do_race_2_panic\"  #0 [f06a9dd8] crash_kexec at c049b5ec  #1 [f06a9e2c] oops_end at c083d1c2  #2 [f06a9e40] no_context at c0433ded  #3 [f06a9e64] bad_area_nosemaphore at c043401a  #4 [f06a9e6c] __do_page_fault at c0434493  #5 [f06a9eec] do_page_fault at c083eb45  #6 [f06a9f04] error_code (via page_fault) at c083c5d5     EAX: 01fb470c EBX: fff35000 ECX: 00000003 EDX: 00000100 EBP:     00000000     DS:  007b     ESI: 9e201000 ES:  007b     EDI: 01fb4700 GS:  00e0     CS:  0060     EIP: c083bc14 ERR: ffffffff EFLAGS: 00010246  #7 [f06a9f38] _spin_lock at c083bc14  #8 [f06a9f44] sys_mincore at c0507b7d  #9 [f06a9fb0] system_call at c083becd                          start           len     EAX: ffffffda  EBX: 9e200000  ECX: 00001000  EDX: 6228537f     DS:  007b      ESI: 00000000  ES:  007b      EDI: 003d0f00     SS:  007b      ESP: 62285354  EBP: 62285388  GS:  0033     CS:  0073      EIP: 00291416  ERR: 000000da  EFLAGS: 00000286  This should be a longstanding bug affecting x86 32bit PAE without THP. Only archs with 64bit large pmd_t and 32bit unsigned long should be affected.  With THP enabled the barrier() in pmd_none_or_trans_huge_or_clear_bad() would partly hide the bug when the pmd transition from none to stable, by forcing a re-read of the *pmd in pmd_offset_map_lock, but when THP is enabled a new set of problem arises by the fact could then transition freely in any of the none, pmd_trans_huge or pmd_trans_stable states. So making the barrier in pmd_none_or_trans_huge_or_clear_bad() unconditional isn't good idea and it would be a flakey solution.  This should be fully fixed by introducing a pmd_read_atomic that reads the pmd in order with THP disabled, or by reading the pmd atomically with cmpxchg8b with THP enabled.  Luckily this new race condition only triggers in the places that must already be covered by pmd_none_or_trans_huge_or_clear_bad() so the fix is localized there but this bug is not related to THP.  NOTE: this can trigger on x86 32bit systems with PAE enabled with more than 4G of ram, otherwise the high part of the pmd will never risk to be truncated because it would be zero at all times, in turn so hiding the SMP race.  This bug was discovered and fully debugged by Ulrich, quote:  ---- [..] pmd_none_or_trans_huge_or_clear_bad() loads the content of edx and eax.      496 static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t     *pmd)     497 {     498         /* depend on compiler for an atomic pmd read */     499         pmd_t pmdval = *pmd;                                  // edi = pmd pointer 0xc0507a74 <sys_mincore+548>:   mov    0x8(%esp),%edi ...                                 // edx = PTE page table high address 0xc0507a84 <sys_mincore+564>:   mov    0x4(%edi),%edx ...                                 // eax = PTE page table low address 0xc0507a8e <sys_mincore+574>:   mov    (%edi),%eax  [..]  Please note that the PMD is not read atomically. These are two \"mov\" instructions where the high order bits of the PMD entry are fetched first. Hence, the above machine code is prone to the following race.  -  The PMD entry {high|low} is 0x0000000000000000.    The \"mov\" at 0xc0507a84 loads 0x00000000 into edx.  -  A page fault (on another CPU) sneaks in between the two \"mov\"    instructions and instantiates the PMD.  -  The PMD entry {high|low} is now 0x00000003fda38067.    The \"mov\" at 0xc0507a8e loads 0xfda38067 into eax. ----  Cc: Mel Gorman <mgorman@suse.de> Cc: Hugh Dickins <hughd@google.com> Cc: Larry Woodman <lwoodman@redhat.com> Cc: Petr Matousek <pmatouse@redhat.com> Cc: Rik van Riel <riel@redhat.com> Cc: <stable@vger.kernel.org>",
        "func_before": "static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)\n{\n\t/* depend on compiler for an atomic pmd read */\n\tpmd_t pmdval = *pmd;\n\t/*\n\t * The barrier will stabilize the pmdval in a register or on\n\t * the stack so that it will stop changing under the code.\n\t */\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tbarrier();\n#endif\n\tif (pmd_none(pmdval))\n\t\treturn 1;\n\tif (unlikely(pmd_bad(pmdval))) {\n\t\tif (!pmd_trans_huge(pmdval))\n\t\t\tpmd_clear_bad(pmd);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "func": "static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)\n{\n\tpmd_t pmdval = pmd_read_atomic(pmd);\n\t/*\n\t * The barrier will stabilize the pmdval in a register or on\n\t * the stack so that it will stop changing under the code.\n\t */\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tbarrier();\n#endif\n\tif (pmd_none(pmdval))\n\t\treturn 1;\n\tif (unlikely(pmd_bad(pmdval))) {\n\t\tif (!pmd_trans_huge(pmdval))\n\t\t\tpmd_clear_bad(pmd);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,6 @@\n static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)\n {\n-\t/* depend on compiler for an atomic pmd read */\n-\tpmd_t pmdval = *pmd;\n+\tpmd_t pmdval = pmd_read_atomic(pmd);\n \t/*\n \t * The barrier will stabilize the pmdval in a register or on\n \t * the stack so that it will stop changing under the code.",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* depend on compiler for an atomic pmd read */",
                "\tpmd_t pmdval = *pmd;"
            ],
            "added_lines": [
                "\tpmd_t pmdval = pmd_read_atomic(pmd);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-1833",
        "func_name": "torvalds/linux/ecryptfs_mount",
        "description": "Race condition in the ecryptfs_mount function in fs/ecryptfs/main.c in the eCryptfs subsystem in the Linux kernel before 3.1 allows local users to bypass intended file permissions via a mount.ecryptfs_private mount with a mismatched uid.",
        "git_url": "https://github.com/torvalds/linux/commit/764355487ea220fdc2faf128d577d7f679b91f97",
        "commit_title": "Ecryptfs: Add mount option to check uid of device being mounted = expect uid",
        "commit_text": " Close a TOCTOU race for mounts done via ecryptfs-mount-private.  The mount source (device) can be raced when the ownership test is done in userspace. Provide Ecryptfs a means to force the uid check at mount time.  Cc: <stable@kernel.org>",
        "func_before": "static struct dentry *ecryptfs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *raw_data)\n{\n\tstruct super_block *s;\n\tstruct ecryptfs_sb_info *sbi;\n\tstruct ecryptfs_dentry_info *root_info;\n\tconst char *err = \"Getting sb failed\";\n\tstruct inode *inode;\n\tstruct path path;\n\tint rc;\n\n\tsbi = kmem_cache_zalloc(ecryptfs_sb_info_cache, GFP_KERNEL);\n\tif (!sbi) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trc = ecryptfs_parse_options(sbi, raw_data);\n\tif (rc) {\n\t\terr = \"Error parsing options\";\n\t\tgoto out;\n\t}\n\n\ts = sget(fs_type, NULL, set_anon_super, NULL);\n\tif (IS_ERR(s)) {\n\t\trc = PTR_ERR(s);\n\t\tgoto out;\n\t}\n\n\ts->s_flags = flags;\n\trc = bdi_setup_and_register(&sbi->bdi, \"ecryptfs\", BDI_CAP_MAP_COPY);\n\tif (rc)\n\t\tgoto out1;\n\n\tecryptfs_set_superblock_private(s, sbi);\n\ts->s_bdi = &sbi->bdi;\n\n\t/* ->kill_sb() will take care of sbi after that point */\n\tsbi = NULL;\n\ts->s_op = &ecryptfs_sops;\n\ts->s_d_op = &ecryptfs_dops;\n\n\terr = \"Reading sb failed\";\n\trc = kern_path(dev_name, LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &path);\n\tif (rc) {\n\t\tecryptfs_printk(KERN_WARNING, \"kern_path() failed\\n\");\n\t\tgoto out1;\n\t}\n\tif (path.dentry->d_sb->s_type == &ecryptfs_fs_type) {\n\t\trc = -EINVAL;\n\t\tprintk(KERN_ERR \"Mount on filesystem of type \"\n\t\t\t\"eCryptfs explicitly disallowed due to \"\n\t\t\t\"known incompatibilities\\n\");\n\t\tgoto out_free;\n\t}\n\tecryptfs_set_superblock_lower(s, path.dentry->d_sb);\n\ts->s_maxbytes = path.dentry->d_sb->s_maxbytes;\n\ts->s_blocksize = path.dentry->d_sb->s_blocksize;\n\ts->s_magic = ECRYPTFS_SUPER_MAGIC;\n\n\tinode = ecryptfs_get_inode(path.dentry->d_inode, s);\n\trc = PTR_ERR(inode);\n\tif (IS_ERR(inode))\n\t\tgoto out_free;\n\n\ts->s_root = d_alloc_root(inode);\n\tif (!s->s_root) {\n\t\tiput(inode);\n\t\trc = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\trc = -ENOMEM;\n\troot_info = kmem_cache_zalloc(ecryptfs_dentry_info_cache, GFP_KERNEL);\n\tif (!root_info)\n\t\tgoto out_free;\n\n\t/* ->kill_sb() will take care of root_info */\n\tecryptfs_set_dentry_private(s->s_root, root_info);\n\tecryptfs_set_dentry_lower(s->s_root, path.dentry);\n\tecryptfs_set_dentry_lower_mnt(s->s_root, path.mnt);\n\n\ts->s_flags |= MS_ACTIVE;\n\treturn dget(s->s_root);\n\nout_free:\n\tpath_put(&path);\nout1:\n\tdeactivate_locked_super(s);\nout:\n\tif (sbi) {\n\t\tecryptfs_destroy_mount_crypt_stat(&sbi->mount_crypt_stat);\n\t\tkmem_cache_free(ecryptfs_sb_info_cache, sbi);\n\t}\n\tprintk(KERN_ERR \"%s; rc = [%d]\\n\", err, rc);\n\treturn ERR_PTR(rc);\n}",
        "func": "static struct dentry *ecryptfs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *raw_data)\n{\n\tstruct super_block *s;\n\tstruct ecryptfs_sb_info *sbi;\n\tstruct ecryptfs_dentry_info *root_info;\n\tconst char *err = \"Getting sb failed\";\n\tstruct inode *inode;\n\tstruct path path;\n\tuid_t check_ruid;\n\tint rc;\n\n\tsbi = kmem_cache_zalloc(ecryptfs_sb_info_cache, GFP_KERNEL);\n\tif (!sbi) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trc = ecryptfs_parse_options(sbi, raw_data, &check_ruid);\n\tif (rc) {\n\t\terr = \"Error parsing options\";\n\t\tgoto out;\n\t}\n\n\ts = sget(fs_type, NULL, set_anon_super, NULL);\n\tif (IS_ERR(s)) {\n\t\trc = PTR_ERR(s);\n\t\tgoto out;\n\t}\n\n\ts->s_flags = flags;\n\trc = bdi_setup_and_register(&sbi->bdi, \"ecryptfs\", BDI_CAP_MAP_COPY);\n\tif (rc)\n\t\tgoto out1;\n\n\tecryptfs_set_superblock_private(s, sbi);\n\ts->s_bdi = &sbi->bdi;\n\n\t/* ->kill_sb() will take care of sbi after that point */\n\tsbi = NULL;\n\ts->s_op = &ecryptfs_sops;\n\ts->s_d_op = &ecryptfs_dops;\n\n\terr = \"Reading sb failed\";\n\trc = kern_path(dev_name, LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &path);\n\tif (rc) {\n\t\tecryptfs_printk(KERN_WARNING, \"kern_path() failed\\n\");\n\t\tgoto out1;\n\t}\n\tif (path.dentry->d_sb->s_type == &ecryptfs_fs_type) {\n\t\trc = -EINVAL;\n\t\tprintk(KERN_ERR \"Mount on filesystem of type \"\n\t\t\t\"eCryptfs explicitly disallowed due to \"\n\t\t\t\"known incompatibilities\\n\");\n\t\tgoto out_free;\n\t}\n\n\tif (check_ruid && path.dentry->d_inode->i_uid != current_uid()) {\n\t\trc = -EPERM;\n\t\tprintk(KERN_ERR \"Mount of device (uid: %d) not owned by \"\n\t\t       \"requested user (uid: %d)\\n\",\n\t\t       path.dentry->d_inode->i_uid, current_uid());\n\t\tgoto out_free;\n\t}\n\n\tecryptfs_set_superblock_lower(s, path.dentry->d_sb);\n\ts->s_maxbytes = path.dentry->d_sb->s_maxbytes;\n\ts->s_blocksize = path.dentry->d_sb->s_blocksize;\n\ts->s_magic = ECRYPTFS_SUPER_MAGIC;\n\n\tinode = ecryptfs_get_inode(path.dentry->d_inode, s);\n\trc = PTR_ERR(inode);\n\tif (IS_ERR(inode))\n\t\tgoto out_free;\n\n\ts->s_root = d_alloc_root(inode);\n\tif (!s->s_root) {\n\t\tiput(inode);\n\t\trc = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\trc = -ENOMEM;\n\troot_info = kmem_cache_zalloc(ecryptfs_dentry_info_cache, GFP_KERNEL);\n\tif (!root_info)\n\t\tgoto out_free;\n\n\t/* ->kill_sb() will take care of root_info */\n\tecryptfs_set_dentry_private(s->s_root, root_info);\n\tecryptfs_set_dentry_lower(s->s_root, path.dentry);\n\tecryptfs_set_dentry_lower_mnt(s->s_root, path.mnt);\n\n\ts->s_flags |= MS_ACTIVE;\n\treturn dget(s->s_root);\n\nout_free:\n\tpath_put(&path);\nout1:\n\tdeactivate_locked_super(s);\nout:\n\tif (sbi) {\n\t\tecryptfs_destroy_mount_crypt_stat(&sbi->mount_crypt_stat);\n\t\tkmem_cache_free(ecryptfs_sb_info_cache, sbi);\n\t}\n\tprintk(KERN_ERR \"%s; rc = [%d]\\n\", err, rc);\n\treturn ERR_PTR(rc);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,7 @@\n \tconst char *err = \"Getting sb failed\";\n \tstruct inode *inode;\n \tstruct path path;\n+\tuid_t check_ruid;\n \tint rc;\n \n \tsbi = kmem_cache_zalloc(ecryptfs_sb_info_cache, GFP_KERNEL);\n@@ -15,7 +16,7 @@\n \t\tgoto out;\n \t}\n \n-\trc = ecryptfs_parse_options(sbi, raw_data);\n+\trc = ecryptfs_parse_options(sbi, raw_data, &check_ruid);\n \tif (rc) {\n \t\terr = \"Error parsing options\";\n \t\tgoto out;\n@@ -53,6 +54,15 @@\n \t\t\t\"known incompatibilities\\n\");\n \t\tgoto out_free;\n \t}\n+\n+\tif (check_ruid && path.dentry->d_inode->i_uid != current_uid()) {\n+\t\trc = -EPERM;\n+\t\tprintk(KERN_ERR \"Mount of device (uid: %d) not owned by \"\n+\t\t       \"requested user (uid: %d)\\n\",\n+\t\t       path.dentry->d_inode->i_uid, current_uid());\n+\t\tgoto out_free;\n+\t}\n+\n \tecryptfs_set_superblock_lower(s, path.dentry->d_sb);\n \ts->s_maxbytes = path.dentry->d_sb->s_maxbytes;\n \ts->s_blocksize = path.dentry->d_sb->s_blocksize;",
        "diff_line_info": {
            "deleted_lines": [
                "\trc = ecryptfs_parse_options(sbi, raw_data);"
            ],
            "added_lines": [
                "\tuid_t check_ruid;",
                "\trc = ecryptfs_parse_options(sbi, raw_data, &check_ruid);",
                "",
                "\tif (check_ruid && path.dentry->d_inode->i_uid != current_uid()) {",
                "\t\trc = -EPERM;",
                "\t\tprintk(KERN_ERR \"Mount of device (uid: %d) not owned by \"",
                "\t\t       \"requested user (uid: %d)\\n\",",
                "\t\t       path.dentry->d_inode->i_uid, current_uid());",
                "\t\tgoto out_free;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2011-1833",
        "func_name": "torvalds/linux/ecryptfs_parse_options",
        "description": "Race condition in the ecryptfs_mount function in fs/ecryptfs/main.c in the eCryptfs subsystem in the Linux kernel before 3.1 allows local users to bypass intended file permissions via a mount.ecryptfs_private mount with a mismatched uid.",
        "git_url": "https://github.com/torvalds/linux/commit/764355487ea220fdc2faf128d577d7f679b91f97",
        "commit_title": "Ecryptfs: Add mount option to check uid of device being mounted = expect uid",
        "commit_text": " Close a TOCTOU race for mounts done via ecryptfs-mount-private.  The mount source (device) can be raced when the ownership test is done in userspace. Provide Ecryptfs a means to force the uid check at mount time.  Cc: <stable@kernel.org>",
        "func_before": "static int ecryptfs_parse_options(struct ecryptfs_sb_info *sbi, char *options)\n{\n\tchar *p;\n\tint rc = 0;\n\tint sig_set = 0;\n\tint cipher_name_set = 0;\n\tint fn_cipher_name_set = 0;\n\tint cipher_key_bytes;\n\tint cipher_key_bytes_set = 0;\n\tint fn_cipher_key_bytes;\n\tint fn_cipher_key_bytes_set = 0;\n\tstruct ecryptfs_mount_crypt_stat *mount_crypt_stat =\n\t\t&sbi->mount_crypt_stat;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint token;\n\tchar *sig_src;\n\tchar *cipher_name_dst;\n\tchar *cipher_name_src;\n\tchar *fn_cipher_name_dst;\n\tchar *fn_cipher_name_src;\n\tchar *fnek_dst;\n\tchar *fnek_src;\n\tchar *cipher_key_bytes_src;\n\tchar *fn_cipher_key_bytes_src;\n\n\tif (!options) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\tecryptfs_init_mount_crypt_stat(mount_crypt_stat);\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tif (!*p)\n\t\t\tcontinue;\n\t\ttoken = match_token(p, tokens, args);\n\t\tswitch (token) {\n\t\tcase ecryptfs_opt_sig:\n\t\tcase ecryptfs_opt_ecryptfs_sig:\n\t\t\tsig_src = args[0].from;\n\t\t\trc = ecryptfs_add_global_auth_tok(mount_crypt_stat,\n\t\t\t\t\t\t\t  sig_src, 0);\n\t\t\tif (rc) {\n\t\t\t\tprintk(KERN_ERR \"Error attempting to register \"\n\t\t\t\t       \"global sig; rc = [%d]\\n\", rc);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsig_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_cipher:\n\t\tcase ecryptfs_opt_ecryptfs_cipher:\n\t\t\tcipher_name_src = args[0].from;\n\t\t\tcipher_name_dst =\n\t\t\t\tmount_crypt_stat->\n\t\t\t\tglobal_default_cipher_name;\n\t\t\tstrncpy(cipher_name_dst, cipher_name_src,\n\t\t\t\tECRYPTFS_MAX_CIPHER_NAME_SIZE);\n\t\t\tcipher_name_dst[ECRYPTFS_MAX_CIPHER_NAME_SIZE] = '\\0';\n\t\t\tcipher_name_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_ecryptfs_key_bytes:\n\t\t\tcipher_key_bytes_src = args[0].from;\n\t\t\tcipher_key_bytes =\n\t\t\t\t(int)simple_strtol(cipher_key_bytes_src,\n\t\t\t\t\t\t   &cipher_key_bytes_src, 0);\n\t\t\tmount_crypt_stat->global_default_cipher_key_size =\n\t\t\t\tcipher_key_bytes;\n\t\t\tcipher_key_bytes_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_passthrough:\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_PLAINTEXT_PASSTHROUGH_ENABLED;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_xattr_metadata:\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_XATTR_METADATA_ENABLED;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_encrypted_view:\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_XATTR_METADATA_ENABLED;\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_ENCRYPTED_VIEW_ENABLED;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_fnek_sig:\n\t\t\tfnek_src = args[0].from;\n\t\t\tfnek_dst =\n\t\t\t\tmount_crypt_stat->global_default_fnek_sig;\n\t\t\tstrncpy(fnek_dst, fnek_src, ECRYPTFS_SIG_SIZE_HEX);\n\t\t\tmount_crypt_stat->global_default_fnek_sig[\n\t\t\t\tECRYPTFS_SIG_SIZE_HEX] = '\\0';\n\t\t\trc = ecryptfs_add_global_auth_tok(\n\t\t\t\tmount_crypt_stat,\n\t\t\t\tmount_crypt_stat->global_default_fnek_sig,\n\t\t\t\tECRYPTFS_AUTH_TOK_FNEK);\n\t\t\tif (rc) {\n\t\t\t\tprintk(KERN_ERR \"Error attempting to register \"\n\t\t\t\t       \"global fnek sig [%s]; rc = [%d]\\n\",\n\t\t\t\t       mount_crypt_stat->global_default_fnek_sig,\n\t\t\t\t       rc);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\t(ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES\n\t\t\t\t | ECRYPTFS_GLOBAL_ENCFN_USE_MOUNT_FNEK);\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_fn_cipher:\n\t\t\tfn_cipher_name_src = args[0].from;\n\t\t\tfn_cipher_name_dst =\n\t\t\t\tmount_crypt_stat->global_default_fn_cipher_name;\n\t\t\tstrncpy(fn_cipher_name_dst, fn_cipher_name_src,\n\t\t\t\tECRYPTFS_MAX_CIPHER_NAME_SIZE);\n\t\t\tmount_crypt_stat->global_default_fn_cipher_name[\n\t\t\t\tECRYPTFS_MAX_CIPHER_NAME_SIZE] = '\\0';\n\t\t\tfn_cipher_name_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_fn_cipher_key_bytes:\n\t\t\tfn_cipher_key_bytes_src = args[0].from;\n\t\t\tfn_cipher_key_bytes =\n\t\t\t\t(int)simple_strtol(fn_cipher_key_bytes_src,\n\t\t\t\t\t\t   &fn_cipher_key_bytes_src, 0);\n\t\t\tmount_crypt_stat->global_default_fn_cipher_key_bytes =\n\t\t\t\tfn_cipher_key_bytes;\n\t\t\tfn_cipher_key_bytes_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_unlink_sigs:\n\t\t\tmount_crypt_stat->flags |= ECRYPTFS_UNLINK_SIGS;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_mount_auth_tok_only:\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_GLOBAL_MOUNT_AUTH_TOK_ONLY;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_err:\n\t\tdefault:\n\t\t\tprintk(KERN_WARNING\n\t\t\t       \"%s: eCryptfs: unrecognized option [%s]\\n\",\n\t\t\t       __func__, p);\n\t\t}\n\t}\n\tif (!sig_set) {\n\t\trc = -EINVAL;\n\t\tecryptfs_printk(KERN_ERR, \"You must supply at least one valid \"\n\t\t\t\t\"auth tok signature as a mount \"\n\t\t\t\t\"parameter; see the eCryptfs README\\n\");\n\t\tgoto out;\n\t}\n\tif (!cipher_name_set) {\n\t\tint cipher_name_len = strlen(ECRYPTFS_DEFAULT_CIPHER);\n\n\t\tBUG_ON(cipher_name_len >= ECRYPTFS_MAX_CIPHER_NAME_SIZE);\n\t\tstrcpy(mount_crypt_stat->global_default_cipher_name,\n\t\t       ECRYPTFS_DEFAULT_CIPHER);\n\t}\n\tif ((mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES)\n\t    && !fn_cipher_name_set)\n\t\tstrcpy(mount_crypt_stat->global_default_fn_cipher_name,\n\t\t       mount_crypt_stat->global_default_cipher_name);\n\tif (!cipher_key_bytes_set)\n\t\tmount_crypt_stat->global_default_cipher_key_size = 0;\n\tif ((mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES)\n\t    && !fn_cipher_key_bytes_set)\n\t\tmount_crypt_stat->global_default_fn_cipher_key_bytes =\n\t\t\tmount_crypt_stat->global_default_cipher_key_size;\n\tmutex_lock(&key_tfm_list_mutex);\n\tif (!ecryptfs_tfm_exists(mount_crypt_stat->global_default_cipher_name,\n\t\t\t\t NULL)) {\n\t\trc = ecryptfs_add_new_key_tfm(\n\t\t\tNULL, mount_crypt_stat->global_default_cipher_name,\n\t\t\tmount_crypt_stat->global_default_cipher_key_size);\n\t\tif (rc) {\n\t\t\tprintk(KERN_ERR \"Error attempting to initialize \"\n\t\t\t       \"cipher with name = [%s] and key size = [%td]; \"\n\t\t\t       \"rc = [%d]\\n\",\n\t\t\t       mount_crypt_stat->global_default_cipher_name,\n\t\t\t       mount_crypt_stat->global_default_cipher_key_size,\n\t\t\t       rc);\n\t\t\trc = -EINVAL;\n\t\t\tmutex_unlock(&key_tfm_list_mutex);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif ((mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES)\n\t    && !ecryptfs_tfm_exists(\n\t\t    mount_crypt_stat->global_default_fn_cipher_name, NULL)) {\n\t\trc = ecryptfs_add_new_key_tfm(\n\t\t\tNULL, mount_crypt_stat->global_default_fn_cipher_name,\n\t\t\tmount_crypt_stat->global_default_fn_cipher_key_bytes);\n\t\tif (rc) {\n\t\t\tprintk(KERN_ERR \"Error attempting to initialize \"\n\t\t\t       \"cipher with name = [%s] and key size = [%td]; \"\n\t\t\t       \"rc = [%d]\\n\",\n\t\t\t       mount_crypt_stat->global_default_fn_cipher_name,\n\t\t\t       mount_crypt_stat->global_default_fn_cipher_key_bytes,\n\t\t\t       rc);\n\t\t\trc = -EINVAL;\n\t\t\tmutex_unlock(&key_tfm_list_mutex);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tmutex_unlock(&key_tfm_list_mutex);\n\trc = ecryptfs_init_global_auth_toks(mount_crypt_stat);\n\tif (rc)\n\t\tprintk(KERN_WARNING \"One or more global auth toks could not \"\n\t\t       \"properly register; rc = [%d]\\n\", rc);\nout:\n\treturn rc;\n}",
        "func": "static int ecryptfs_parse_options(struct ecryptfs_sb_info *sbi, char *options,\n\t\t\t\t  uid_t *check_ruid)\n{\n\tchar *p;\n\tint rc = 0;\n\tint sig_set = 0;\n\tint cipher_name_set = 0;\n\tint fn_cipher_name_set = 0;\n\tint cipher_key_bytes;\n\tint cipher_key_bytes_set = 0;\n\tint fn_cipher_key_bytes;\n\tint fn_cipher_key_bytes_set = 0;\n\tstruct ecryptfs_mount_crypt_stat *mount_crypt_stat =\n\t\t&sbi->mount_crypt_stat;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint token;\n\tchar *sig_src;\n\tchar *cipher_name_dst;\n\tchar *cipher_name_src;\n\tchar *fn_cipher_name_dst;\n\tchar *fn_cipher_name_src;\n\tchar *fnek_dst;\n\tchar *fnek_src;\n\tchar *cipher_key_bytes_src;\n\tchar *fn_cipher_key_bytes_src;\n\n\t*check_ruid = 0;\n\n\tif (!options) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\tecryptfs_init_mount_crypt_stat(mount_crypt_stat);\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tif (!*p)\n\t\t\tcontinue;\n\t\ttoken = match_token(p, tokens, args);\n\t\tswitch (token) {\n\t\tcase ecryptfs_opt_sig:\n\t\tcase ecryptfs_opt_ecryptfs_sig:\n\t\t\tsig_src = args[0].from;\n\t\t\trc = ecryptfs_add_global_auth_tok(mount_crypt_stat,\n\t\t\t\t\t\t\t  sig_src, 0);\n\t\t\tif (rc) {\n\t\t\t\tprintk(KERN_ERR \"Error attempting to register \"\n\t\t\t\t       \"global sig; rc = [%d]\\n\", rc);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsig_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_cipher:\n\t\tcase ecryptfs_opt_ecryptfs_cipher:\n\t\t\tcipher_name_src = args[0].from;\n\t\t\tcipher_name_dst =\n\t\t\t\tmount_crypt_stat->\n\t\t\t\tglobal_default_cipher_name;\n\t\t\tstrncpy(cipher_name_dst, cipher_name_src,\n\t\t\t\tECRYPTFS_MAX_CIPHER_NAME_SIZE);\n\t\t\tcipher_name_dst[ECRYPTFS_MAX_CIPHER_NAME_SIZE] = '\\0';\n\t\t\tcipher_name_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_ecryptfs_key_bytes:\n\t\t\tcipher_key_bytes_src = args[0].from;\n\t\t\tcipher_key_bytes =\n\t\t\t\t(int)simple_strtol(cipher_key_bytes_src,\n\t\t\t\t\t\t   &cipher_key_bytes_src, 0);\n\t\t\tmount_crypt_stat->global_default_cipher_key_size =\n\t\t\t\tcipher_key_bytes;\n\t\t\tcipher_key_bytes_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_passthrough:\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_PLAINTEXT_PASSTHROUGH_ENABLED;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_xattr_metadata:\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_XATTR_METADATA_ENABLED;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_encrypted_view:\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_XATTR_METADATA_ENABLED;\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_ENCRYPTED_VIEW_ENABLED;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_fnek_sig:\n\t\t\tfnek_src = args[0].from;\n\t\t\tfnek_dst =\n\t\t\t\tmount_crypt_stat->global_default_fnek_sig;\n\t\t\tstrncpy(fnek_dst, fnek_src, ECRYPTFS_SIG_SIZE_HEX);\n\t\t\tmount_crypt_stat->global_default_fnek_sig[\n\t\t\t\tECRYPTFS_SIG_SIZE_HEX] = '\\0';\n\t\t\trc = ecryptfs_add_global_auth_tok(\n\t\t\t\tmount_crypt_stat,\n\t\t\t\tmount_crypt_stat->global_default_fnek_sig,\n\t\t\t\tECRYPTFS_AUTH_TOK_FNEK);\n\t\t\tif (rc) {\n\t\t\t\tprintk(KERN_ERR \"Error attempting to register \"\n\t\t\t\t       \"global fnek sig [%s]; rc = [%d]\\n\",\n\t\t\t\t       mount_crypt_stat->global_default_fnek_sig,\n\t\t\t\t       rc);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\t(ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES\n\t\t\t\t | ECRYPTFS_GLOBAL_ENCFN_USE_MOUNT_FNEK);\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_fn_cipher:\n\t\t\tfn_cipher_name_src = args[0].from;\n\t\t\tfn_cipher_name_dst =\n\t\t\t\tmount_crypt_stat->global_default_fn_cipher_name;\n\t\t\tstrncpy(fn_cipher_name_dst, fn_cipher_name_src,\n\t\t\t\tECRYPTFS_MAX_CIPHER_NAME_SIZE);\n\t\t\tmount_crypt_stat->global_default_fn_cipher_name[\n\t\t\t\tECRYPTFS_MAX_CIPHER_NAME_SIZE] = '\\0';\n\t\t\tfn_cipher_name_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_fn_cipher_key_bytes:\n\t\t\tfn_cipher_key_bytes_src = args[0].from;\n\t\t\tfn_cipher_key_bytes =\n\t\t\t\t(int)simple_strtol(fn_cipher_key_bytes_src,\n\t\t\t\t\t\t   &fn_cipher_key_bytes_src, 0);\n\t\t\tmount_crypt_stat->global_default_fn_cipher_key_bytes =\n\t\t\t\tfn_cipher_key_bytes;\n\t\t\tfn_cipher_key_bytes_set = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_unlink_sigs:\n\t\t\tmount_crypt_stat->flags |= ECRYPTFS_UNLINK_SIGS;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_mount_auth_tok_only:\n\t\t\tmount_crypt_stat->flags |=\n\t\t\t\tECRYPTFS_GLOBAL_MOUNT_AUTH_TOK_ONLY;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_check_dev_ruid:\n\t\t\t*check_ruid = 1;\n\t\t\tbreak;\n\t\tcase ecryptfs_opt_err:\n\t\tdefault:\n\t\t\tprintk(KERN_WARNING\n\t\t\t       \"%s: eCryptfs: unrecognized option [%s]\\n\",\n\t\t\t       __func__, p);\n\t\t}\n\t}\n\tif (!sig_set) {\n\t\trc = -EINVAL;\n\t\tecryptfs_printk(KERN_ERR, \"You must supply at least one valid \"\n\t\t\t\t\"auth tok signature as a mount \"\n\t\t\t\t\"parameter; see the eCryptfs README\\n\");\n\t\tgoto out;\n\t}\n\tif (!cipher_name_set) {\n\t\tint cipher_name_len = strlen(ECRYPTFS_DEFAULT_CIPHER);\n\n\t\tBUG_ON(cipher_name_len >= ECRYPTFS_MAX_CIPHER_NAME_SIZE);\n\t\tstrcpy(mount_crypt_stat->global_default_cipher_name,\n\t\t       ECRYPTFS_DEFAULT_CIPHER);\n\t}\n\tif ((mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES)\n\t    && !fn_cipher_name_set)\n\t\tstrcpy(mount_crypt_stat->global_default_fn_cipher_name,\n\t\t       mount_crypt_stat->global_default_cipher_name);\n\tif (!cipher_key_bytes_set)\n\t\tmount_crypt_stat->global_default_cipher_key_size = 0;\n\tif ((mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES)\n\t    && !fn_cipher_key_bytes_set)\n\t\tmount_crypt_stat->global_default_fn_cipher_key_bytes =\n\t\t\tmount_crypt_stat->global_default_cipher_key_size;\n\tmutex_lock(&key_tfm_list_mutex);\n\tif (!ecryptfs_tfm_exists(mount_crypt_stat->global_default_cipher_name,\n\t\t\t\t NULL)) {\n\t\trc = ecryptfs_add_new_key_tfm(\n\t\t\tNULL, mount_crypt_stat->global_default_cipher_name,\n\t\t\tmount_crypt_stat->global_default_cipher_key_size);\n\t\tif (rc) {\n\t\t\tprintk(KERN_ERR \"Error attempting to initialize \"\n\t\t\t       \"cipher with name = [%s] and key size = [%td]; \"\n\t\t\t       \"rc = [%d]\\n\",\n\t\t\t       mount_crypt_stat->global_default_cipher_name,\n\t\t\t       mount_crypt_stat->global_default_cipher_key_size,\n\t\t\t       rc);\n\t\t\trc = -EINVAL;\n\t\t\tmutex_unlock(&key_tfm_list_mutex);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif ((mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES)\n\t    && !ecryptfs_tfm_exists(\n\t\t    mount_crypt_stat->global_default_fn_cipher_name, NULL)) {\n\t\trc = ecryptfs_add_new_key_tfm(\n\t\t\tNULL, mount_crypt_stat->global_default_fn_cipher_name,\n\t\t\tmount_crypt_stat->global_default_fn_cipher_key_bytes);\n\t\tif (rc) {\n\t\t\tprintk(KERN_ERR \"Error attempting to initialize \"\n\t\t\t       \"cipher with name = [%s] and key size = [%td]; \"\n\t\t\t       \"rc = [%d]\\n\",\n\t\t\t       mount_crypt_stat->global_default_fn_cipher_name,\n\t\t\t       mount_crypt_stat->global_default_fn_cipher_key_bytes,\n\t\t\t       rc);\n\t\t\trc = -EINVAL;\n\t\t\tmutex_unlock(&key_tfm_list_mutex);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tmutex_unlock(&key_tfm_list_mutex);\n\trc = ecryptfs_init_global_auth_toks(mount_crypt_stat);\n\tif (rc)\n\t\tprintk(KERN_WARNING \"One or more global auth toks could not \"\n\t\t       \"properly register; rc = [%d]\\n\", rc);\nout:\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n-static int ecryptfs_parse_options(struct ecryptfs_sb_info *sbi, char *options)\n+static int ecryptfs_parse_options(struct ecryptfs_sb_info *sbi, char *options,\n+\t\t\t\t  uid_t *check_ruid)\n {\n \tchar *p;\n \tint rc = 0;\n@@ -23,6 +24,8 @@\n \tchar *cipher_key_bytes_src;\n \tchar *fn_cipher_key_bytes_src;\n \n+\t*check_ruid = 0;\n+\n \tif (!options) {\n \t\trc = -EINVAL;\n \t\tgoto out;\n@@ -126,6 +129,9 @@\n \t\tcase ecryptfs_opt_mount_auth_tok_only:\n \t\t\tmount_crypt_stat->flags |=\n \t\t\t\tECRYPTFS_GLOBAL_MOUNT_AUTH_TOK_ONLY;\n+\t\t\tbreak;\n+\t\tcase ecryptfs_opt_check_dev_ruid:\n+\t\t\t*check_ruid = 1;\n \t\t\tbreak;\n \t\tcase ecryptfs_opt_err:\n \t\tdefault:",
        "diff_line_info": {
            "deleted_lines": [
                "static int ecryptfs_parse_options(struct ecryptfs_sb_info *sbi, char *options)"
            ],
            "added_lines": [
                "static int ecryptfs_parse_options(struct ecryptfs_sb_info *sbi, char *options,",
                "\t\t\t\t  uid_t *check_ruid)",
                "\t*check_ruid = 0;",
                "",
                "\t\t\tbreak;",
                "\t\tcase ecryptfs_opt_check_dev_ruid:",
                "\t\t\t*check_ruid = 1;"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-3511",
        "func_name": "torvalds/linux/madvise_remove",
        "description": "Multiple race conditions in the madvise_remove function in mm/madvise.c in the Linux kernel before 3.4.5 allow local users to cause a denial of service (use-after-free and system crash) via vectors involving a (1) munmap or (2) close system call.",
        "git_url": "https://github.com/torvalds/linux/commit/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb",
        "commit_title": "mm: Hold a file reference in madvise_remove",
        "commit_text": " Otherwise the code races with munmap (causing a use-after-free of the vma) or with close (causing a use-after-free of the struct file).  The bug was introduced by commit 90ed52ebe481 (\"[PATCH] holepunch: fix mmap_sem i_mutex deadlock\")  Cc: Hugh Dickins <hugh@veritas.com> Cc: Miklos Szeredi <mszeredi@suse.cz> Cc: Badari Pulavarty <pbadari@us.ibm.com> Cc: Nick Piggin <npiggin@suse.de> Cc: stable@vger.kernel.org",
        "func_before": "static long madvise_remove(struct vm_area_struct *vma,\n\t\t\t\tstruct vm_area_struct **prev,\n\t\t\t\tunsigned long start, unsigned long end)\n{\n\tloff_t offset;\n\tint error;\n\n\t*prev = NULL;\t/* tell sys_madvise we drop mmap_sem */\n\n\tif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\n\t\treturn -EINVAL;\n\n\tif (!vma->vm_file || !vma->vm_file->f_mapping\n\t\t|| !vma->vm_file->f_mapping->host) {\n\t\t\treturn -EINVAL;\n\t}\n\n\tif ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))\n\t\treturn -EACCES;\n\n\toffset = (loff_t)(start - vma->vm_start)\n\t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n\n\t/* filesystem's fallocate may need to take i_mutex */\n\tup_read(&current->mm->mmap_sem);\n\terror = do_fallocate(vma->vm_file,\n\t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n\t\t\t\toffset, end - start);\n\tdown_read(&current->mm->mmap_sem);\n\treturn error;\n}",
        "func": "static long madvise_remove(struct vm_area_struct *vma,\n\t\t\t\tstruct vm_area_struct **prev,\n\t\t\t\tunsigned long start, unsigned long end)\n{\n\tloff_t offset;\n\tint error;\n\tstruct file *f;\n\n\t*prev = NULL;\t/* tell sys_madvise we drop mmap_sem */\n\n\tif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\n\t\treturn -EINVAL;\n\n\tf = vma->vm_file;\n\n\tif (!f || !f->f_mapping || !f->f_mapping->host) {\n\t\t\treturn -EINVAL;\n\t}\n\n\tif ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))\n\t\treturn -EACCES;\n\n\toffset = (loff_t)(start - vma->vm_start)\n\t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n\n\t/*\n\t * Filesystem's fallocate may need to take i_mutex.  We need to\n\t * explicitly grab a reference because the vma (and hence the\n\t * vma's reference to the file) can go away as soon as we drop\n\t * mmap_sem.\n\t */\n\tget_file(f);\n\tup_read(&current->mm->mmap_sem);\n\terror = do_fallocate(f,\n\t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n\t\t\t\toffset, end - start);\n\tfput(f);\n\tdown_read(&current->mm->mmap_sem);\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,14 +4,16 @@\n {\n \tloff_t offset;\n \tint error;\n+\tstruct file *f;\n \n \t*prev = NULL;\t/* tell sys_madvise we drop mmap_sem */\n \n \tif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\n \t\treturn -EINVAL;\n \n-\tif (!vma->vm_file || !vma->vm_file->f_mapping\n-\t\t|| !vma->vm_file->f_mapping->host) {\n+\tf = vma->vm_file;\n+\n+\tif (!f || !f->f_mapping || !f->f_mapping->host) {\n \t\t\treturn -EINVAL;\n \t}\n \n@@ -21,11 +23,18 @@\n \toffset = (loff_t)(start - vma->vm_start)\n \t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n \n-\t/* filesystem's fallocate may need to take i_mutex */\n+\t/*\n+\t * Filesystem's fallocate may need to take i_mutex.  We need to\n+\t * explicitly grab a reference because the vma (and hence the\n+\t * vma's reference to the file) can go away as soon as we drop\n+\t * mmap_sem.\n+\t */\n+\tget_file(f);\n \tup_read(&current->mm->mmap_sem);\n-\terror = do_fallocate(vma->vm_file,\n+\terror = do_fallocate(f,\n \t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n \t\t\t\toffset, end - start);\n+\tfput(f);\n \tdown_read(&current->mm->mmap_sem);\n \treturn error;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!vma->vm_file || !vma->vm_file->f_mapping",
                "\t\t|| !vma->vm_file->f_mapping->host) {",
                "\t/* filesystem's fallocate may need to take i_mutex */",
                "\terror = do_fallocate(vma->vm_file,"
            ],
            "added_lines": [
                "\tstruct file *f;",
                "\tf = vma->vm_file;",
                "",
                "\tif (!f || !f->f_mapping || !f->f_mapping->host) {",
                "\t/*",
                "\t * Filesystem's fallocate may need to take i_mutex.  We need to",
                "\t * explicitly grab a reference because the vma (and hence the",
                "\t * vma's reference to the file) can go away as soon as we drop",
                "\t * mmap_sem.",
                "\t */",
                "\tget_file(f);",
                "\terror = do_fallocate(f,",
                "\tfput(f);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-4508",
        "func_name": "torvalds/linux/ext4_split_unwritten_extents",
        "description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized.",
        "git_url": "https://github.com/torvalds/linux/commit/dee1f973ca341c266229faa5a1a5bb268bed3531",
        "commit_title": "ext4: race-condition protection for ext4_convert_unwritten_extents_endio",
        "commit_text": " We assumed that at the time we call ext4_convert_unwritten_extents_endio() extent in question is fully inside [map.m_lblk, map->m_len] because it was already split during submission.  But this may not be true due to a race between writeback vs fallocate.  If extent in question is larger than requested we will split it again. Special precautions should being done if zeroout required because [map.m_lblk, map->m_len] already contains valid data.  Cc: stable@vger.kernel.org",
        "func_before": "static int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tint flags)\n{\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\tsplit_flag |= EXT4_EXT_MARK_UNINIT2;\n\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, path, map, split_flag, flags);\n}",
        "func": "static int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tint flags)\n{\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\tsplit_flag |= EXT4_EXT_MARK_UNINIT2;\n\tif (flags & EXT4_GET_BLOCKS_CONVERT)\n\t\tsplit_flag |= EXT4_EXT_DATA_VALID2;\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, path, map, split_flag, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -29,7 +29,8 @@\n \n \tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n \tsplit_flag |= EXT4_EXT_MARK_UNINIT2;\n-\n+\tif (flags & EXT4_GET_BLOCKS_CONVERT)\n+\t\tsplit_flag |= EXT4_EXT_DATA_VALID2;\n \tflags |= EXT4_GET_BLOCKS_PRE_IO;\n \treturn ext4_split_extent(handle, inode, path, map, split_flag, flags);\n }",
        "diff_line_info": {
            "deleted_lines": [
                ""
            ],
            "added_lines": [
                "\tif (flags & EXT4_GET_BLOCKS_CONVERT)",
                "\t\tsplit_flag |= EXT4_EXT_DATA_VALID2;"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-4508",
        "func_name": "torvalds/linux/ext4_ext_handle_uninitialized_extents",
        "description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized.",
        "git_url": "https://github.com/torvalds/linux/commit/dee1f973ca341c266229faa5a1a5bb268bed3531",
        "commit_title": "ext4: race-condition protection for ext4_convert_unwritten_extents_endio",
        "commit_text": " We assumed that at the time we call ext4_convert_unwritten_extents_endio() extent in question is fully inside [map.m_lblk, map->m_len] because it was already split during submission.  But this may not be true due to a race between writeback vs fallocate.  If extent in question is larger than requested we will split it again. Special precautions should being done if zeroout required because [map.m_lblk, map->m_len] already contains valid data.  Cc: stable@vger.kernel.org",
        "func_before": "static int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\ttrace_ext4_ext_handle_uninitialized_extents(inode, map, allocated,\n\t\t\t\t\t\t    newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}",
        "func": "static int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\ttrace_ext4_ext_handle_uninitialized_extents(inode, map, allocated,\n\t\t\t\t\t\t    newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,7 +38,7 @@\n \t}\n \t/* IO end_io complete, convert the filled extent to written */\n \tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n-\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n+\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n \t\t\t\t\t\t\tpath);\n \t\tif (ret >= 0) {\n \t\t\text4_update_inode_fsync_trans(handle, inode, 1);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,"
            ],
            "added_lines": [
                "\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-4508",
        "func_name": "torvalds/linux/ext4_convert_unwritten_extents_endio",
        "description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized.",
        "git_url": "https://github.com/torvalds/linux/commit/dee1f973ca341c266229faa5a1a5bb268bed3531",
        "commit_title": "ext4: race-condition protection for ext4_convert_unwritten_extents_endio",
        "commit_text": " We assumed that at the time we call ext4_convert_unwritten_extents_endio() extent in question is fully inside [map.m_lblk, map->m_len] because it was already split during submission.  But this may not be true due to a race between writeback vs fallocate.  If extent in question is larger than requested we will split it again. Special precautions should being done if zeroout required because [map.m_lblk, map->m_len] already contains valid data.  Cc: stable@vger.kernel.org",
        "func_before": "static int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t      struct inode *inode,\n\t\t\t\t\t      struct ext4_ext_path *path)\n{\n\tstruct ext4_extent *ex;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)le32_to_cpu(ex->ee_block),\n\t\text4_ext_get_actual_len(ex));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}",
        "func": "static int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\t/* If extent is larger than requested then split is required */\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n\t\terr = ext4_split_unwritten_extents(handle, inode, map, path,\n\t\t\t\t\t\t   EXT4_GET_BLOCKS_CONVERT);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,18 +1,38 @@\n static int ext4_convert_unwritten_extents_endio(handle_t *handle,\n-\t\t\t\t\t      struct inode *inode,\n-\t\t\t\t\t      struct ext4_ext_path *path)\n+\t\t\t\t\t\tstruct inode *inode,\n+\t\t\t\t\t\tstruct ext4_map_blocks *map,\n+\t\t\t\t\t\tstruct ext4_ext_path *path)\n {\n \tstruct ext4_extent *ex;\n+\text4_lblk_t ee_block;\n+\tunsigned int ee_len;\n \tint depth;\n \tint err = 0;\n \n \tdepth = ext_depth(inode);\n \tex = path[depth].p_ext;\n+\tee_block = le32_to_cpu(ex->ee_block);\n+\tee_len = ext4_ext_get_actual_len(ex);\n \n \text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n \t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n-\t\t(unsigned long long)le32_to_cpu(ex->ee_block),\n-\t\text4_ext_get_actual_len(ex));\n+\t\t  (unsigned long long)ee_block, ee_len);\n+\n+\t/* If extent is larger than requested then split is required */\n+\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n+\t\terr = ext4_split_unwritten_extents(handle, inode, map, path,\n+\t\t\t\t\t\t   EXT4_GET_BLOCKS_CONVERT);\n+\t\tif (err < 0)\n+\t\t\tgoto out;\n+\t\text4_ext_drop_refs(path);\n+\t\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n+\t\tif (IS_ERR(path)) {\n+\t\t\terr = PTR_ERR(path);\n+\t\t\tgoto out;\n+\t\t}\n+\t\tdepth = ext_depth(inode);\n+\t\tex = path[depth].p_ext;\n+\t}\n \n \terr = ext4_ext_get_access(handle, inode, path + depth);\n \tif (err)",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t\t      struct inode *inode,",
                "\t\t\t\t\t      struct ext4_ext_path *path)",
                "\t\t(unsigned long long)le32_to_cpu(ex->ee_block),",
                "\t\text4_ext_get_actual_len(ex));"
            ],
            "added_lines": [
                "\t\t\t\t\t\tstruct inode *inode,",
                "\t\t\t\t\t\tstruct ext4_map_blocks *map,",
                "\t\t\t\t\t\tstruct ext4_ext_path *path)",
                "\text4_lblk_t ee_block;",
                "\tunsigned int ee_len;",
                "\tee_block = le32_to_cpu(ex->ee_block);",
                "\tee_len = ext4_ext_get_actual_len(ex);",
                "\t\t  (unsigned long long)ee_block, ee_len);",
                "",
                "\t/* If extent is larger than requested then split is required */",
                "\tif (ee_block != map->m_lblk || ee_len > map->m_len) {",
                "\t\terr = ext4_split_unwritten_extents(handle, inode, map, path,",
                "\t\t\t\t\t\t   EXT4_GET_BLOCKS_CONVERT);",
                "\t\tif (err < 0)",
                "\t\t\tgoto out;",
                "\t\text4_ext_drop_refs(path);",
                "\t\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);",
                "\t\tif (IS_ERR(path)) {",
                "\t\t\terr = PTR_ERR(path);",
                "\t\t\tgoto out;",
                "\t\t}",
                "\t\tdepth = ext_depth(inode);",
                "\t\tex = path[depth].p_ext;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-4508",
        "func_name": "torvalds/linux/ext4_split_extent",
        "description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized.",
        "git_url": "https://github.com/torvalds/linux/commit/dee1f973ca341c266229faa5a1a5bb268bed3531",
        "commit_title": "ext4: race-condition protection for ext4_convert_unwritten_extents_endio",
        "commit_text": " We assumed that at the time we call ext4_convert_unwritten_extents_endio() extent in question is fully inside [map.m_lblk, map->m_len] because it was already split during submission.  But this may not be true due to a race between writeback vs fallocate.  If extent in question is larger than requested we will split it again. Special precautions should being done if zeroout required because [map.m_lblk, map->m_len] already contains valid data.  Cc: stable@vger.kernel.org",
        "func_before": "static int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint uninitialized;\n\tint split_flag1, flags1;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tuninitialized = ext4_ext_is_uninitialized(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_drop_refs(path);\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : map->m_len;\n}",
        "func": "static int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint uninitialized;\n\tint split_flag1, flags1;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tuninitialized = ext4_ext_is_uninitialized(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n\t\tif (split_flag & EXT4_EXT_DATA_VALID2)\n\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_drop_refs(path);\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t\t\t    EXT4_EXT_DATA_VALID2);\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : map->m_len;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,12 +19,13 @@\n \tuninitialized = ext4_ext_is_uninitialized(ex);\n \n \tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n-\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n-\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n+\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;\n \t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n \t\tif (uninitialized)\n \t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n \t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n+\t\tif (split_flag & EXT4_EXT_DATA_VALID2)\n+\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;\n \t\terr = ext4_split_extent_at(handle, inode, path,\n \t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n \t\tif (err)\n@@ -37,8 +38,8 @@\n \t\treturn PTR_ERR(path);\n \n \tif (map->m_lblk >= ee_block) {\n-\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n-\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n+\t\tsplit_flag1 = split_flag & (EXT4_EXT_MAY_ZEROOUT |\n+\t\t\t\t\t    EXT4_EXT_DATA_VALID2);\n \t\tif (uninitialized)\n \t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n \t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?",
                "\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;",
                "\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?",
                "\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;"
            ],
            "added_lines": [
                "\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;",
                "\t\tif (split_flag & EXT4_EXT_DATA_VALID2)",
                "\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;",
                "\t\tsplit_flag1 = split_flag & (EXT4_EXT_MAY_ZEROOUT |",
                "\t\t\t\t\t    EXT4_EXT_DATA_VALID2);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-4508",
        "func_name": "torvalds/linux/ext4_split_extent_at",
        "description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized.",
        "git_url": "https://github.com/torvalds/linux/commit/dee1f973ca341c266229faa5a1a5bb268bed3531",
        "commit_title": "ext4: race-condition protection for ext4_convert_unwritten_extents_endio",
        "commit_text": " We assumed that at the time we call ext4_convert_unwritten_extents_endio() extent in question is fully inside [map.m_lblk, map->m_len] because it was already split during submission.  But this may not be true due to a race between writeback vs fallocate.  If extent in question is larger than requested we will split it again. Special precautions should being done if zeroout required because [map.m_lblk, map->m_len] already contains valid data.  Cc: stable@vger.kernel.org",
        "func_before": "static int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}",
        "func": "static int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1)\n\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n\t\t\telse\n\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n\t\t} else\n\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,6 +11,9 @@\n \tstruct ext4_extent *ex2 = NULL;\n \tunsigned int ee_len, depth;\n \tint err = 0;\n+\n+\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n+\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n \n \text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n \t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n@@ -70,7 +73,14 @@\n \n \terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n \tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n-\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n+\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n+\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1)\n+\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n+\t\t\telse\n+\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n+\t\t} else\n+\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n+\n \t\tif (err)\n \t\t\tgoto fix_extent_len;\n \t\t/* update the extent length and mark as initialized */",
        "diff_line_info": {
            "deleted_lines": [
                "\t\terr = ext4_ext_zeroout(inode, &orig_ex);"
            ],
            "added_lines": [
                "",
                "\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==",
                "\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));",
                "\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {",
                "\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1)",
                "\t\t\t\terr = ext4_ext_zeroout(inode, ex2);",
                "\t\t\telse",
                "\t\t\t\terr = ext4_ext_zeroout(inode, ex);",
                "\t\t} else",
                "\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15265",
        "func_name": "torvalds/linux/snd_seq_ioctl_create_port",
        "description": "Race condition in the ALSA subsystem in the Linux kernel before 4.13.8 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via crafted /dev/snd/seq ioctl calls, related to sound/core/seq/seq_clientmgr.c and sound/core/seq/seq_ports.c.",
        "git_url": "https://github.com/torvalds/linux/commit/71105998845fb012937332fe2e806d443c09e026",
        "commit_title": "ALSA: seq: Fix use-after-free at creating a port",
        "commit_text": " There is a potential race window opened at creating and deleting a port via ioctl, as spotted by fuzzing.  snd_seq_create_port() creates a port object and returns its pointer, but it doesn't take the refcount, thus it can be deleted immediately by another thread. Meanwhile, snd_seq_ioctl_create_port() still calls the function snd_seq_system_client_ev_port_start() with the created port object that is being deleted, and this triggers use-after-free like:   BUG: KASAN: use-after-free in snd_seq_ioctl_create_port+0x504/0x630 [snd_seq] at addr ffff8801f2241cb1  =============================================================================  BUG kmalloc-512 (Tainted: G    B          ): kasan: bad access detected  -----------------------------------------------------------------------------  INFO: Allocated in snd_seq_create_port+0x94/0x9b0 [snd_seq] age=1 cpu=3 pid=4511  \t___slab_alloc+0x425/0x460  \t__slab_alloc+0x20/0x40   \tkmem_cache_alloc_trace+0x150/0x190 \tsnd_seq_create_port+0x94/0x9b0 [snd_seq] \tsnd_seq_ioctl_create_port+0xd1/0x630 [snd_seq]  \tsnd_seq_do_ioctl+0x11c/0x190 [snd_seq]  \tsnd_seq_ioctl+0x40/0x80 [snd_seq]  \tdo_vfs_ioctl+0x54b/0xda0  \tSyS_ioctl+0x79/0x90  \tentry_SYSCALL_64_fastpath+0x16/0x75  INFO: Freed in port_delete+0x136/0x1a0 [snd_seq] age=1 cpu=2 pid=4717  \t__slab_free+0x204/0x310  \tkfree+0x15f/0x180  \tport_delete+0x136/0x1a0 [snd_seq]  \tsnd_seq_delete_port+0x235/0x350 [snd_seq]  \tsnd_seq_ioctl_delete_port+0xc8/0x180 [snd_seq]  \tsnd_seq_do_ioctl+0x11c/0x190 [snd_seq]  \tsnd_seq_ioctl+0x40/0x80 [snd_seq]  \tdo_vfs_ioctl+0x54b/0xda0  \tSyS_ioctl+0x79/0x90  \tentry_SYSCALL_64_fastpath+0x16/0x75  Call Trace:   [<ffffffff81b03781>] dump_stack+0x63/0x82   [<ffffffff81531b3b>] print_trailer+0xfb/0x160   [<ffffffff81536db4>] object_err+0x34/0x40   [<ffffffff815392d3>] kasan_report.part.2+0x223/0x520   [<ffffffffa07aadf4>] ? snd_seq_ioctl_create_port+0x504/0x630 [snd_seq]   [<ffffffff815395fe>] __asan_report_load1_noabort+0x2e/0x30   [<ffffffffa07aadf4>] snd_seq_ioctl_create_port+0x504/0x630 [snd_seq]   [<ffffffffa07aa8f0>] ? snd_seq_ioctl_delete_port+0x180/0x180 [snd_seq]   [<ffffffff8136be50>] ? taskstats_exit+0xbc0/0xbc0   [<ffffffffa07abc5c>] snd_seq_do_ioctl+0x11c/0x190 [snd_seq]   [<ffffffffa07abd10>] snd_seq_ioctl+0x40/0x80 [snd_seq]   [<ffffffff8136d433>] ? acct_account_cputime+0x63/0x80   [<ffffffff815b515b>] do_vfs_ioctl+0x54b/0xda0   .....  We may fix this in a few different ways, and in this patch, it's fixed simply by taking the refcount properly at snd_seq_create_port() and letting the caller unref the object after use.  Also, there is another potential use-after-free by sprintf() call in snd_seq_create_port(), and this is moved inside the lock.  This fix covers CVE-2017-15265.  Reported-and-tested-by: Michael23 Yu <ycqzsy@gmail.com> Suggested-by: Linus Torvalds <torvalds@linux-foundation.org> Cc: <stable@vger.kernel.org>",
        "func_before": "static int snd_seq_ioctl_create_port(struct snd_seq_client *client, void *arg)\n{\n\tstruct snd_seq_port_info *info = arg;\n\tstruct snd_seq_client_port *port;\n\tstruct snd_seq_port_callback *callback;\n\n\t/* it is not allowed to create the port for an another client */\n\tif (info->addr.client != client->number)\n\t\treturn -EPERM;\n\n\tport = snd_seq_create_port(client, (info->flags & SNDRV_SEQ_PORT_FLG_GIVEN_PORT) ? info->addr.port : -1);\n\tif (port == NULL)\n\t\treturn -ENOMEM;\n\n\tif (client->type == USER_CLIENT && info->kernel) {\n\t\tsnd_seq_delete_port(client, port->addr.port);\n\t\treturn -EINVAL;\n\t}\n\tif (client->type == KERNEL_CLIENT) {\n\t\tif ((callback = info->kernel) != NULL) {\n\t\t\tif (callback->owner)\n\t\t\t\tport->owner = callback->owner;\n\t\t\tport->private_data = callback->private_data;\n\t\t\tport->private_free = callback->private_free;\n\t\t\tport->event_input = callback->event_input;\n\t\t\tport->c_src.open = callback->subscribe;\n\t\t\tport->c_src.close = callback->unsubscribe;\n\t\t\tport->c_dest.open = callback->use;\n\t\t\tport->c_dest.close = callback->unuse;\n\t\t}\n\t}\n\n\tinfo->addr = port->addr;\n\n\tsnd_seq_set_port_info(port, info);\n\tsnd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);\n\n\treturn 0;\n}",
        "func": "static int snd_seq_ioctl_create_port(struct snd_seq_client *client, void *arg)\n{\n\tstruct snd_seq_port_info *info = arg;\n\tstruct snd_seq_client_port *port;\n\tstruct snd_seq_port_callback *callback;\n\tint port_idx;\n\n\t/* it is not allowed to create the port for an another client */\n\tif (info->addr.client != client->number)\n\t\treturn -EPERM;\n\n\tport = snd_seq_create_port(client, (info->flags & SNDRV_SEQ_PORT_FLG_GIVEN_PORT) ? info->addr.port : -1);\n\tif (port == NULL)\n\t\treturn -ENOMEM;\n\n\tif (client->type == USER_CLIENT && info->kernel) {\n\t\tport_idx = port->addr.port;\n\t\tsnd_seq_port_unlock(port);\n\t\tsnd_seq_delete_port(client, port_idx);\n\t\treturn -EINVAL;\n\t}\n\tif (client->type == KERNEL_CLIENT) {\n\t\tif ((callback = info->kernel) != NULL) {\n\t\t\tif (callback->owner)\n\t\t\t\tport->owner = callback->owner;\n\t\t\tport->private_data = callback->private_data;\n\t\t\tport->private_free = callback->private_free;\n\t\t\tport->event_input = callback->event_input;\n\t\t\tport->c_src.open = callback->subscribe;\n\t\t\tport->c_src.close = callback->unsubscribe;\n\t\t\tport->c_dest.open = callback->use;\n\t\t\tport->c_dest.close = callback->unuse;\n\t\t}\n\t}\n\n\tinfo->addr = port->addr;\n\n\tsnd_seq_set_port_info(port, info);\n\tsnd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);\n\tsnd_seq_port_unlock(port);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,7 @@\n \tstruct snd_seq_port_info *info = arg;\n \tstruct snd_seq_client_port *port;\n \tstruct snd_seq_port_callback *callback;\n+\tint port_idx;\n \n \t/* it is not allowed to create the port for an another client */\n \tif (info->addr.client != client->number)\n@@ -13,7 +14,9 @@\n \t\treturn -ENOMEM;\n \n \tif (client->type == USER_CLIENT && info->kernel) {\n-\t\tsnd_seq_delete_port(client, port->addr.port);\n+\t\tport_idx = port->addr.port;\n+\t\tsnd_seq_port_unlock(port);\n+\t\tsnd_seq_delete_port(client, port_idx);\n \t\treturn -EINVAL;\n \t}\n \tif (client->type == KERNEL_CLIENT) {\n@@ -34,6 +37,7 @@\n \n \tsnd_seq_set_port_info(port, info);\n \tsnd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);\n+\tsnd_seq_port_unlock(port);\n \n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tsnd_seq_delete_port(client, port->addr.port);"
            ],
            "added_lines": [
                "\tint port_idx;",
                "\t\tport_idx = port->addr.port;",
                "\t\tsnd_seq_port_unlock(port);",
                "\t\tsnd_seq_delete_port(client, port_idx);",
                "\tsnd_seq_port_unlock(port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15265",
        "func_name": "torvalds/linux/snd_seq_create_port",
        "description": "Race condition in the ALSA subsystem in the Linux kernel before 4.13.8 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via crafted /dev/snd/seq ioctl calls, related to sound/core/seq/seq_clientmgr.c and sound/core/seq/seq_ports.c.",
        "git_url": "https://github.com/torvalds/linux/commit/71105998845fb012937332fe2e806d443c09e026",
        "commit_title": "ALSA: seq: Fix use-after-free at creating a port",
        "commit_text": " There is a potential race window opened at creating and deleting a port via ioctl, as spotted by fuzzing.  snd_seq_create_port() creates a port object and returns its pointer, but it doesn't take the refcount, thus it can be deleted immediately by another thread. Meanwhile, snd_seq_ioctl_create_port() still calls the function snd_seq_system_client_ev_port_start() with the created port object that is being deleted, and this triggers use-after-free like:   BUG: KASAN: use-after-free in snd_seq_ioctl_create_port+0x504/0x630 [snd_seq] at addr ffff8801f2241cb1  =============================================================================  BUG kmalloc-512 (Tainted: G    B          ): kasan: bad access detected  -----------------------------------------------------------------------------  INFO: Allocated in snd_seq_create_port+0x94/0x9b0 [snd_seq] age=1 cpu=3 pid=4511  \t___slab_alloc+0x425/0x460  \t__slab_alloc+0x20/0x40   \tkmem_cache_alloc_trace+0x150/0x190 \tsnd_seq_create_port+0x94/0x9b0 [snd_seq] \tsnd_seq_ioctl_create_port+0xd1/0x630 [snd_seq]  \tsnd_seq_do_ioctl+0x11c/0x190 [snd_seq]  \tsnd_seq_ioctl+0x40/0x80 [snd_seq]  \tdo_vfs_ioctl+0x54b/0xda0  \tSyS_ioctl+0x79/0x90  \tentry_SYSCALL_64_fastpath+0x16/0x75  INFO: Freed in port_delete+0x136/0x1a0 [snd_seq] age=1 cpu=2 pid=4717  \t__slab_free+0x204/0x310  \tkfree+0x15f/0x180  \tport_delete+0x136/0x1a0 [snd_seq]  \tsnd_seq_delete_port+0x235/0x350 [snd_seq]  \tsnd_seq_ioctl_delete_port+0xc8/0x180 [snd_seq]  \tsnd_seq_do_ioctl+0x11c/0x190 [snd_seq]  \tsnd_seq_ioctl+0x40/0x80 [snd_seq]  \tdo_vfs_ioctl+0x54b/0xda0  \tSyS_ioctl+0x79/0x90  \tentry_SYSCALL_64_fastpath+0x16/0x75  Call Trace:   [<ffffffff81b03781>] dump_stack+0x63/0x82   [<ffffffff81531b3b>] print_trailer+0xfb/0x160   [<ffffffff81536db4>] object_err+0x34/0x40   [<ffffffff815392d3>] kasan_report.part.2+0x223/0x520   [<ffffffffa07aadf4>] ? snd_seq_ioctl_create_port+0x504/0x630 [snd_seq]   [<ffffffff815395fe>] __asan_report_load1_noabort+0x2e/0x30   [<ffffffffa07aadf4>] snd_seq_ioctl_create_port+0x504/0x630 [snd_seq]   [<ffffffffa07aa8f0>] ? snd_seq_ioctl_delete_port+0x180/0x180 [snd_seq]   [<ffffffff8136be50>] ? taskstats_exit+0xbc0/0xbc0   [<ffffffffa07abc5c>] snd_seq_do_ioctl+0x11c/0x190 [snd_seq]   [<ffffffffa07abd10>] snd_seq_ioctl+0x40/0x80 [snd_seq]   [<ffffffff8136d433>] ? acct_account_cputime+0x63/0x80   [<ffffffff815b515b>] do_vfs_ioctl+0x54b/0xda0   .....  We may fix this in a few different ways, and in this patch, it's fixed simply by taking the refcount properly at snd_seq_create_port() and letting the caller unref the object after use.  Also, there is another potential use-after-free by sprintf() call in snd_seq_create_port(), and this is moved inside the lock.  This fix covers CVE-2017-15265.  Reported-and-tested-by: Michael23 Yu <ycqzsy@gmail.com> Suggested-by: Linus Torvalds <torvalds@linux-foundation.org> Cc: <stable@vger.kernel.org>",
        "func_before": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\tsprintf(new_port->name, \"port-%d\", num);\n\n\treturn new_port;\n}",
        "func": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\tsnd_use_lock_use(&new_port->use_lock);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\tsprintf(new_port->name, \"port-%d\", num);\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\n\treturn new_port;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,6 +26,7 @@\n \tsnd_use_lock_init(&new_port->use_lock);\n \tport_subs_info_init(&new_port->c_src);\n \tport_subs_info_init(&new_port->c_dest);\n+\tsnd_use_lock_use(&new_port->use_lock);\n \n \tnum = port >= 0 ? port : 0;\n \tmutex_lock(&client->ports_mutex);\n@@ -40,9 +41,9 @@\n \tlist_add_tail(&new_port->list, &p->list);\n \tclient->num_ports++;\n \tnew_port->addr.port = num;\t/* store the port number in the port */\n+\tsprintf(new_port->name, \"port-%d\", num);\n \twrite_unlock_irqrestore(&client->ports_lock, flags);\n \tmutex_unlock(&client->ports_mutex);\n-\tsprintf(new_port->name, \"port-%d\", num);\n \n \treturn new_port;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ],
            "added_lines": [
                "\tsnd_use_lock_use(&new_port->use_lock);",
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15588",
        "func_name": "xen-project/xen/_put_final_page_type",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to execute arbitrary code on the host OS because of a race condition that can cause a stale TLB entry.",
        "git_url": "https://github.com/xen-project/xen/commit/23a183607a427572185fc51c76cc5ab11c00c4cc",
        "commit_title": "x86: don't store possibly stale TLB flush time stamp",
        "commit_text": " While the timing window is extremely narrow, it is theoretically possible for an update to the TLB flush clock and a subsequent flush IPI to happen between the read and write parts of the update of the per-page stamp. Exclude this possibility by disabling interrupts across the update, preventing the IPI to be serviced in the middle.  This is XSA-241.  Suggested-by: George Dunlap <george.dunlap@citrix.com>",
        "func_before": "static int _put_final_page_type(struct page_info *page, unsigned long type,\n                                bool preemptible, struct page_info *ptpg)\n{\n    int rc = free_page_type(page, type, preemptible);\n\n    /* No need for atomic update of type_info here: noone else updates it. */\n    if ( rc == 0 )\n    {\n        if ( ptpg && PGT_type_equal(type, ptpg->u.inuse.type_info) )\n        {\n            dec_linear_uses(page);\n            dec_linear_entries(ptpg);\n        }\n        ASSERT(!page->linear_pt_count || page_get_owner(page)->is_dying);\n        page_set_tlbflush_timestamp(page);\n        smp_wmb();\n        page->u.inuse.type_info--;\n    }\n    else if ( rc == -EINTR )\n    {\n        ASSERT((page->u.inuse.type_info &\n                (PGT_count_mask|PGT_validated|PGT_partial)) == 1);\n        page_set_tlbflush_timestamp(page);\n        smp_wmb();\n        page->u.inuse.type_info |= PGT_validated;\n    }\n    else\n    {\n        BUG_ON(rc != -ERESTART);\n        smp_wmb();\n        get_page_light(page);\n        page->u.inuse.type_info |= PGT_partial;\n    }\n\n    return rc;\n}",
        "func": "static int _put_final_page_type(struct page_info *page, unsigned long type,\n                                bool preemptible, struct page_info *ptpg)\n{\n    int rc = free_page_type(page, type, preemptible);\n\n    /* No need for atomic update of type_info here: noone else updates it. */\n    if ( rc == 0 )\n    {\n        if ( ptpg && PGT_type_equal(type, ptpg->u.inuse.type_info) )\n        {\n            dec_linear_uses(page);\n            dec_linear_entries(ptpg);\n        }\n        ASSERT(!page->linear_pt_count || page_get_owner(page)->is_dying);\n        set_tlbflush_timestamp(page);\n        smp_wmb();\n        page->u.inuse.type_info--;\n    }\n    else if ( rc == -EINTR )\n    {\n        ASSERT((page->u.inuse.type_info &\n                (PGT_count_mask|PGT_validated|PGT_partial)) == 1);\n        set_tlbflush_timestamp(page);\n        smp_wmb();\n        page->u.inuse.type_info |= PGT_validated;\n    }\n    else\n    {\n        BUG_ON(rc != -ERESTART);\n        smp_wmb();\n        get_page_light(page);\n        page->u.inuse.type_info |= PGT_partial;\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,7 @@\n             dec_linear_entries(ptpg);\n         }\n         ASSERT(!page->linear_pt_count || page_get_owner(page)->is_dying);\n-        page_set_tlbflush_timestamp(page);\n+        set_tlbflush_timestamp(page);\n         smp_wmb();\n         page->u.inuse.type_info--;\n     }\n@@ -20,7 +20,7 @@\n     {\n         ASSERT((page->u.inuse.type_info &\n                 (PGT_count_mask|PGT_validated|PGT_partial)) == 1);\n-        page_set_tlbflush_timestamp(page);\n+        set_tlbflush_timestamp(page);\n         smp_wmb();\n         page->u.inuse.type_info |= PGT_validated;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "        page_set_tlbflush_timestamp(page);",
                "        page_set_tlbflush_timestamp(page);"
            ],
            "added_lines": [
                "        set_tlbflush_timestamp(page);",
                "        set_tlbflush_timestamp(page);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15588",
        "func_name": "xen-project/xen/_put_page_type",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to execute arbitrary code on the host OS because of a race condition that can cause a stale TLB entry.",
        "git_url": "https://github.com/xen-project/xen/commit/23a183607a427572185fc51c76cc5ab11c00c4cc",
        "commit_title": "x86: don't store possibly stale TLB flush time stamp",
        "commit_text": " While the timing window is extremely narrow, it is theoretically possible for an update to the TLB flush clock and a subsequent flush IPI to happen between the read and write parts of the update of the per-page stamp. Exclude this possibility by disabling interrupts across the update, preventing the IPI to be serviced in the middle.  This is XSA-241.  Suggested-by: George Dunlap <george.dunlap@citrix.com>",
        "func_before": "static int _put_page_type(struct page_info *page, bool preemptible,\n                          struct page_info *ptpg)\n{\n    unsigned long nx, x, y = page->u.inuse.type_info;\n    int rc = 0;\n\n    for ( ; ; )\n    {\n        x  = y;\n        nx = x - 1;\n\n        ASSERT((x & PGT_count_mask) != 0);\n\n        if ( unlikely((nx & PGT_count_mask) == 0) )\n        {\n            if ( unlikely((nx & PGT_type_mask) <= PGT_l4_page_table) &&\n                 likely(nx & (PGT_validated|PGT_partial)) )\n            {\n                /*\n                 * Page-table pages must be unvalidated when count is zero. The\n                 * 'free' is safe because the refcnt is non-zero and validated\n                 * bit is clear => other ops will spin or fail.\n                 */\n                nx = x & ~(PGT_validated|PGT_partial);\n                if ( unlikely((y = cmpxchg(&page->u.inuse.type_info,\n                                           x, nx)) != x) )\n                    continue;\n                /* We cleared the 'valid bit' so we do the clean up. */\n                rc = _put_final_page_type(page, x, preemptible, ptpg);\n                ptpg = NULL;\n                if ( x & PGT_partial )\n                    put_page(page);\n                break;\n            }\n\n            if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )\n            {\n                /*\n                 * page_set_tlbflush_timestamp() accesses the same union\n                 * linear_pt_count lives in. Unvalidated page table pages,\n                 * however, should occur during domain destruction only\n                 * anyway.  Updating of linear_pt_count luckily is not\n                 * necessary anymore for a dying domain.\n                 */\n                ASSERT(page_get_owner(page)->is_dying);\n                ASSERT(page->linear_pt_count < 0);\n                ASSERT(ptpg->linear_pt_count > 0);\n                ptpg = NULL;\n            }\n\n            page_set_tlbflush_timestamp(page);\n        }\n\n        if ( likely((y = cmpxchg(&page->u.inuse.type_info, x, nx)) == x) )\n            break;\n\n        if ( preemptible && hypercall_preempt_check() )\n            return -EINTR;\n    }\n\n    if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )\n    {\n        ASSERT(!rc);\n        dec_linear_uses(page);\n        dec_linear_entries(ptpg);\n    }\n\n    return rc;\n}",
        "func": "static int _put_page_type(struct page_info *page, bool preemptible,\n                          struct page_info *ptpg)\n{\n    unsigned long nx, x, y = page->u.inuse.type_info;\n    int rc = 0;\n\n    for ( ; ; )\n    {\n        x  = y;\n        nx = x - 1;\n\n        ASSERT((x & PGT_count_mask) != 0);\n\n        if ( unlikely((nx & PGT_count_mask) == 0) )\n        {\n            if ( unlikely((nx & PGT_type_mask) <= PGT_l4_page_table) &&\n                 likely(nx & (PGT_validated|PGT_partial)) )\n            {\n                /*\n                 * Page-table pages must be unvalidated when count is zero. The\n                 * 'free' is safe because the refcnt is non-zero and validated\n                 * bit is clear => other ops will spin or fail.\n                 */\n                nx = x & ~(PGT_validated|PGT_partial);\n                if ( unlikely((y = cmpxchg(&page->u.inuse.type_info,\n                                           x, nx)) != x) )\n                    continue;\n                /* We cleared the 'valid bit' so we do the clean up. */\n                rc = _put_final_page_type(page, x, preemptible, ptpg);\n                ptpg = NULL;\n                if ( x & PGT_partial )\n                    put_page(page);\n                break;\n            }\n\n            if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )\n            {\n                /*\n                 * set_tlbflush_timestamp() accesses the same union\n                 * linear_pt_count lives in. Unvalidated page table pages,\n                 * however, should occur during domain destruction only\n                 * anyway.  Updating of linear_pt_count luckily is not\n                 * necessary anymore for a dying domain.\n                 */\n                ASSERT(page_get_owner(page)->is_dying);\n                ASSERT(page->linear_pt_count < 0);\n                ASSERT(ptpg->linear_pt_count > 0);\n                ptpg = NULL;\n            }\n\n            set_tlbflush_timestamp(page);\n        }\n\n        if ( likely((y = cmpxchg(&page->u.inuse.type_info, x, nx)) == x) )\n            break;\n\n        if ( preemptible && hypercall_preempt_check() )\n            return -EINTR;\n    }\n\n    if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )\n    {\n        ASSERT(!rc);\n        dec_linear_uses(page);\n        dec_linear_entries(ptpg);\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,7 +36,7 @@\n             if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )\n             {\n                 /*\n-                 * page_set_tlbflush_timestamp() accesses the same union\n+                 * set_tlbflush_timestamp() accesses the same union\n                  * linear_pt_count lives in. Unvalidated page table pages,\n                  * however, should occur during domain destruction only\n                  * anyway.  Updating of linear_pt_count luckily is not\n@@ -48,7 +48,7 @@\n                 ptpg = NULL;\n             }\n \n-            page_set_tlbflush_timestamp(page);\n+            set_tlbflush_timestamp(page);\n         }\n \n         if ( likely((y = cmpxchg(&page->u.inuse.type_info, x, nx)) == x) )",
        "diff_line_info": {
            "deleted_lines": [
                "                 * page_set_tlbflush_timestamp() accesses the same union",
                "            page_set_tlbflush_timestamp(page);"
            ],
            "added_lines": [
                "                 * set_tlbflush_timestamp() accesses the same union",
                "            set_tlbflush_timestamp(page);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15588",
        "func_name": "xen-project/xen/shadow_free",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to execute arbitrary code on the host OS because of a race condition that can cause a stale TLB entry.",
        "git_url": "https://github.com/xen-project/xen/commit/23a183607a427572185fc51c76cc5ab11c00c4cc",
        "commit_title": "x86: don't store possibly stale TLB flush time stamp",
        "commit_text": " While the timing window is extremely narrow, it is theoretically possible for an update to the TLB flush clock and a subsequent flush IPI to happen between the read and write parts of the update of the per-page stamp. Exclude this possibility by disabling interrupts across the update, preventing the IPI to be serviced in the middle.  This is XSA-241.  Suggested-by: George Dunlap <george.dunlap@citrix.com>",
        "func_before": "void shadow_free(struct domain *d, mfn_t smfn)\n{\n    struct page_info *next = NULL, *sp = mfn_to_page(smfn);\n    struct page_list_head *pin_list;\n    unsigned int pages;\n    u32 shadow_type;\n    int i;\n\n    ASSERT(paging_locked_by_me(d));\n    perfc_incr(shadow_free);\n\n    shadow_type = sp->u.sh.type;\n    ASSERT(shadow_type != SH_type_none);\n    ASSERT(sp->u.sh.head || (shadow_type > SH_type_max_shadow));\n    pages = shadow_size(shadow_type);\n    pin_list = &d->arch.paging.shadow.pinned_shadows;\n\n    for ( i = 0; i < pages; i++ )\n    {\n#if SHADOW_OPTIMIZATIONS & (SHOPT_WRITABLE_HEURISTIC | SHOPT_FAST_EMULATION)\n        struct vcpu *v;\n        for_each_vcpu(d, v)\n        {\n#if SHADOW_OPTIMIZATIONS & SHOPT_WRITABLE_HEURISTIC\n            /* No longer safe to look for a writeable mapping in this shadow */\n            if ( v->arch.paging.shadow.last_writeable_pte_smfn\n                 == mfn_x(page_to_mfn(sp)) )\n                v->arch.paging.shadow.last_writeable_pte_smfn = 0;\n#endif\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n            v->arch.paging.last_write_emul_ok = 0;\n#endif\n        }\n#endif\n        /* Get the next page before we overwrite the list header */\n        if ( i < pages - 1 )\n            next = page_list_next(sp, pin_list);\n        /* Strip out the type: this is now a free shadow page */\n        sp->u.sh.type = sp->u.sh.head = 0;\n        /* Remember the TLB timestamp so we will know whether to flush\n         * TLBs when we reuse the page.  Because the destructors leave the\n         * contents of the pages in place, we can delay TLB flushes until\n         * just before the allocator hands the page out again. */\n        sp->tlbflush_timestamp = tlbflush_current_time();\n        perfc_decr(shadow_alloc_count);\n        page_list_add_tail(sp, &d->arch.paging.shadow.freelist);\n        sp = next;\n    }\n\n    d->arch.paging.shadow.free_pages += pages;\n}",
        "func": "void shadow_free(struct domain *d, mfn_t smfn)\n{\n    struct page_info *next = NULL, *sp = mfn_to_page(smfn);\n    struct page_list_head *pin_list;\n    unsigned int pages;\n    u32 shadow_type;\n    int i;\n\n    ASSERT(paging_locked_by_me(d));\n    perfc_incr(shadow_free);\n\n    shadow_type = sp->u.sh.type;\n    ASSERT(shadow_type != SH_type_none);\n    ASSERT(sp->u.sh.head || (shadow_type > SH_type_max_shadow));\n    pages = shadow_size(shadow_type);\n    pin_list = &d->arch.paging.shadow.pinned_shadows;\n\n    for ( i = 0; i < pages; i++ )\n    {\n#if SHADOW_OPTIMIZATIONS & (SHOPT_WRITABLE_HEURISTIC | SHOPT_FAST_EMULATION)\n        struct vcpu *v;\n        for_each_vcpu(d, v)\n        {\n#if SHADOW_OPTIMIZATIONS & SHOPT_WRITABLE_HEURISTIC\n            /* No longer safe to look for a writeable mapping in this shadow */\n            if ( v->arch.paging.shadow.last_writeable_pte_smfn\n                 == mfn_x(page_to_mfn(sp)) )\n                v->arch.paging.shadow.last_writeable_pte_smfn = 0;\n#endif\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n            v->arch.paging.last_write_emul_ok = 0;\n#endif\n        }\n#endif\n        /* Get the next page before we overwrite the list header */\n        if ( i < pages - 1 )\n            next = page_list_next(sp, pin_list);\n        /* Strip out the type: this is now a free shadow page */\n        sp->u.sh.type = sp->u.sh.head = 0;\n        /* Remember the TLB timestamp so we will know whether to flush\n         * TLBs when we reuse the page.  Because the destructors leave the\n         * contents of the pages in place, we can delay TLB flushes until\n         * just before the allocator hands the page out again. */\n        page_set_tlbflush_timestamp(sp);\n        perfc_decr(shadow_alloc_count);\n        page_list_add_tail(sp, &d->arch.paging.shadow.freelist);\n        sp = next;\n    }\n\n    d->arch.paging.shadow.free_pages += pages;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -41,7 +41,7 @@\n          * TLBs when we reuse the page.  Because the destructors leave the\n          * contents of the pages in place, we can delay TLB flushes until\n          * just before the allocator hands the page out again. */\n-        sp->tlbflush_timestamp = tlbflush_current_time();\n+        page_set_tlbflush_timestamp(sp);\n         perfc_decr(shadow_alloc_count);\n         page_list_add_tail(sp, &d->arch.paging.shadow.freelist);\n         sp = next;",
        "diff_line_info": {
            "deleted_lines": [
                "        sp->tlbflush_timestamp = tlbflush_current_time();"
            ],
            "added_lines": [
                "        page_set_tlbflush_timestamp(sp);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15588",
        "func_name": "xen-project/xen/free_heap_pages",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to execute arbitrary code on the host OS because of a race condition that can cause a stale TLB entry.",
        "git_url": "https://github.com/xen-project/xen/commit/23a183607a427572185fc51c76cc5ab11c00c4cc",
        "commit_title": "x86: don't store possibly stale TLB flush time stamp",
        "commit_text": " While the timing window is extremely narrow, it is theoretically possible for an update to the TLB flush clock and a subsequent flush IPI to happen between the read and write parts of the update of the per-page stamp. Exclude this possibility by disabling interrupts across the update, preventing the IPI to be serviced in the middle.  This is XSA-241.  Suggested-by: George Dunlap <george.dunlap@citrix.com>",
        "func_before": "static void free_heap_pages(\n    struct page_info *pg, unsigned int order, bool need_scrub)\n{\n    unsigned long mask, mfn = page_to_mfn(pg);\n    unsigned int i, node = phys_to_nid(page_to_maddr(pg)), tainted = 0;\n    unsigned int zone = page_to_zone(pg);\n\n    ASSERT(order <= MAX_ORDER);\n    ASSERT(node >= 0);\n\n    spin_lock(&heap_lock);\n\n    for ( i = 0; i < (1 << order); i++ )\n    {\n        /*\n         * Cannot assume that count_info == 0, as there are some corner cases\n         * where it isn't the case and yet it isn't a bug:\n         *  1. page_get_owner() is NULL\n         *  2. page_get_owner() is a domain that was never accessible by\n         *     its domid (e.g., failed to fully construct the domain).\n         *  3. page was never addressable by the guest (e.g., it's an\n         *     auto-translate-physmap guest and the page was never included\n         *     in its pseudophysical address space).\n         * In all the above cases there can be no guest mappings of this page.\n         */\n        ASSERT(!page_state_is(&pg[i], offlined));\n        pg[i].count_info =\n            ((pg[i].count_info & PGC_broken) |\n             (page_state_is(&pg[i], offlining)\n              ? PGC_state_offlined : PGC_state_free));\n        if ( page_state_is(&pg[i], offlined) )\n            tainted = 1;\n\n        /* If a page has no owner it will need no safety TLB flush. */\n        pg[i].u.free.need_tlbflush = (page_get_owner(&pg[i]) != NULL);\n        if ( pg[i].u.free.need_tlbflush )\n            pg[i].tlbflush_timestamp = tlbflush_current_time();\n\n        /* This page is not a guest frame any more. */\n        page_set_owner(&pg[i], NULL); /* set_gpfn_from_mfn snoops pg owner */\n        set_gpfn_from_mfn(mfn + i, INVALID_M2P_ENTRY);\n\n        if ( need_scrub )\n        {\n            pg[i].count_info |= PGC_need_scrub;\n            poison_one_page(&pg[i]);\n        }\n    }\n\n    avail[node][zone] += 1 << order;\n    total_avail_pages += 1 << order;\n    if ( need_scrub )\n    {\n        node_need_scrub[node] += 1 << order;\n        pg->u.free.first_dirty = 0;\n    }\n    else\n        pg->u.free.first_dirty = INVALID_DIRTY_IDX;\n\n    if ( tmem_enabled() )\n        midsize_alloc_zone_pages = max(\n            midsize_alloc_zone_pages, total_avail_pages / MIDSIZE_ALLOC_FRAC);\n\n    /* Merge chunks as far as possible. */\n    while ( order < MAX_ORDER )\n    {\n        mask = 1UL << order;\n\n        if ( (page_to_mfn(pg) & mask) )\n        {\n            struct page_info *predecessor = pg - mask;\n\n            /* Merge with predecessor block? */\n            if ( !mfn_valid(_mfn(page_to_mfn(predecessor))) ||\n                 !page_state_is(predecessor, free) ||\n                 (PFN_ORDER(predecessor) != order) ||\n                 (phys_to_nid(page_to_maddr(predecessor)) != node) )\n                break;\n\n            check_and_stop_scrub(predecessor);\n\n            page_list_del(predecessor, &heap(node, zone, order));\n\n            /* Keep predecessor's first_dirty if it is already set. */\n            if ( predecessor->u.free.first_dirty == INVALID_DIRTY_IDX &&\n                 pg->u.free.first_dirty != INVALID_DIRTY_IDX )\n                predecessor->u.free.first_dirty = (1U << order) +\n                                                  pg->u.free.first_dirty;\n\n            pg = predecessor;\n        }\n        else\n        {\n            struct page_info *successor = pg + mask;\n\n            /* Merge with successor block? */\n            if ( !mfn_valid(_mfn(page_to_mfn(successor))) ||\n                 !page_state_is(successor, free) ||\n                 (PFN_ORDER(successor) != order) ||\n                 (phys_to_nid(page_to_maddr(successor)) != node) )\n                break;\n\n            check_and_stop_scrub(successor);\n\n            page_list_del(successor, &heap(node, zone, order));\n        }\n\n        order++;\n    }\n\n    page_list_add_scrub(pg, node, zone, order, pg->u.free.first_dirty);\n\n    if ( tainted )\n        reserve_offlined_page(pg);\n\n    spin_unlock(&heap_lock);\n}",
        "func": "static void free_heap_pages(\n    struct page_info *pg, unsigned int order, bool need_scrub)\n{\n    unsigned long mask, mfn = page_to_mfn(pg);\n    unsigned int i, node = phys_to_nid(page_to_maddr(pg)), tainted = 0;\n    unsigned int zone = page_to_zone(pg);\n\n    ASSERT(order <= MAX_ORDER);\n    ASSERT(node >= 0);\n\n    spin_lock(&heap_lock);\n\n    for ( i = 0; i < (1 << order); i++ )\n    {\n        /*\n         * Cannot assume that count_info == 0, as there are some corner cases\n         * where it isn't the case and yet it isn't a bug:\n         *  1. page_get_owner() is NULL\n         *  2. page_get_owner() is a domain that was never accessible by\n         *     its domid (e.g., failed to fully construct the domain).\n         *  3. page was never addressable by the guest (e.g., it's an\n         *     auto-translate-physmap guest and the page was never included\n         *     in its pseudophysical address space).\n         * In all the above cases there can be no guest mappings of this page.\n         */\n        ASSERT(!page_state_is(&pg[i], offlined));\n        pg[i].count_info =\n            ((pg[i].count_info & PGC_broken) |\n             (page_state_is(&pg[i], offlining)\n              ? PGC_state_offlined : PGC_state_free));\n        if ( page_state_is(&pg[i], offlined) )\n            tainted = 1;\n\n        /* If a page has no owner it will need no safety TLB flush. */\n        pg[i].u.free.need_tlbflush = (page_get_owner(&pg[i]) != NULL);\n        if ( pg[i].u.free.need_tlbflush )\n            page_set_tlbflush_timestamp(&pg[i]);\n\n        /* This page is not a guest frame any more. */\n        page_set_owner(&pg[i], NULL); /* set_gpfn_from_mfn snoops pg owner */\n        set_gpfn_from_mfn(mfn + i, INVALID_M2P_ENTRY);\n\n        if ( need_scrub )\n        {\n            pg[i].count_info |= PGC_need_scrub;\n            poison_one_page(&pg[i]);\n        }\n    }\n\n    avail[node][zone] += 1 << order;\n    total_avail_pages += 1 << order;\n    if ( need_scrub )\n    {\n        node_need_scrub[node] += 1 << order;\n        pg->u.free.first_dirty = 0;\n    }\n    else\n        pg->u.free.first_dirty = INVALID_DIRTY_IDX;\n\n    if ( tmem_enabled() )\n        midsize_alloc_zone_pages = max(\n            midsize_alloc_zone_pages, total_avail_pages / MIDSIZE_ALLOC_FRAC);\n\n    /* Merge chunks as far as possible. */\n    while ( order < MAX_ORDER )\n    {\n        mask = 1UL << order;\n\n        if ( (page_to_mfn(pg) & mask) )\n        {\n            struct page_info *predecessor = pg - mask;\n\n            /* Merge with predecessor block? */\n            if ( !mfn_valid(_mfn(page_to_mfn(predecessor))) ||\n                 !page_state_is(predecessor, free) ||\n                 (PFN_ORDER(predecessor) != order) ||\n                 (phys_to_nid(page_to_maddr(predecessor)) != node) )\n                break;\n\n            check_and_stop_scrub(predecessor);\n\n            page_list_del(predecessor, &heap(node, zone, order));\n\n            /* Keep predecessor's first_dirty if it is already set. */\n            if ( predecessor->u.free.first_dirty == INVALID_DIRTY_IDX &&\n                 pg->u.free.first_dirty != INVALID_DIRTY_IDX )\n                predecessor->u.free.first_dirty = (1U << order) +\n                                                  pg->u.free.first_dirty;\n\n            pg = predecessor;\n        }\n        else\n        {\n            struct page_info *successor = pg + mask;\n\n            /* Merge with successor block? */\n            if ( !mfn_valid(_mfn(page_to_mfn(successor))) ||\n                 !page_state_is(successor, free) ||\n                 (PFN_ORDER(successor) != order) ||\n                 (phys_to_nid(page_to_maddr(successor)) != node) )\n                break;\n\n            check_and_stop_scrub(successor);\n\n            page_list_del(successor, &heap(node, zone, order));\n        }\n\n        order++;\n    }\n\n    page_list_add_scrub(pg, node, zone, order, pg->u.free.first_dirty);\n\n    if ( tainted )\n        reserve_offlined_page(pg);\n\n    spin_unlock(&heap_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -34,7 +34,7 @@\n         /* If a page has no owner it will need no safety TLB flush. */\n         pg[i].u.free.need_tlbflush = (page_get_owner(&pg[i]) != NULL);\n         if ( pg[i].u.free.need_tlbflush )\n-            pg[i].tlbflush_timestamp = tlbflush_current_time();\n+            page_set_tlbflush_timestamp(&pg[i]);\n \n         /* This page is not a guest frame any more. */\n         page_set_owner(&pg[i], NULL); /* set_gpfn_from_mfn snoops pg owner */",
        "diff_line_info": {
            "deleted_lines": [
                "            pg[i].tlbflush_timestamp = tlbflush_current_time();"
            ],
            "added_lines": [
                "            page_set_tlbflush_timestamp(&pg[i]);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15649",
        "func_name": "torvalds/linux/fanout_add",
        "description": "net/packet/af_packet.c in the Linux kernel before 4.13.6 allows local users to gain privileges via crafted system calls that trigger mishandling of packet_fanout data structures, because of a race condition (involving fanout_add and packet_do_bind) that leads to a use-after-free, a different vulnerability than CVE-2017-6346.",
        "git_url": "https://github.com/torvalds/linux/commit/008ba2a13f2d04c947adc536d19debb8fe66f110",
        "commit_title": "packet: hold bind lock when rebinding to fanout hook",
        "commit_text": " Packet socket bind operations must hold the po->bind_lock. This keeps po->running consistent with whether the socket is actually on a ptype list to receive packets.  fanout_add unbinds a socket and its packet_rcv/tpacket_rcv call, then binds the fanout object to receive through packet_rcv_fanout.  Make it hold the po->bind_lock when testing po->running and rebinding. Else, it can race with other rebind operations, such as that in packet_set_ring from packet_rcv to tpacket_rcv. Concurrent updates can result in a socket being added to a fanout group twice, causing use-after-free KASAN bug reports, among others.  Reported independently by both trinity and syzkaller. Verified that the syzkaller reproducer passes after this patch. ",
        "func_before": "static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_rollover *rollover = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\n\terr = -EINVAL;\n\tif (!po->running)\n\t\tgoto out;\n\n\terr = -EALREADY;\n\tif (po->fanout)\n\t\tgoto out;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\terr = -ENOMEM;\n\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n\t\tif (!rollover)\n\t\t\tgoto out;\n\t\tatomic_long_set(&rollover->num, 0);\n\t\tatomic_long_set(&rollover->num_huge, 0);\n\t\tatomic_long_set(&rollover->num_failed, 0);\n\t\tpo->rollover = rollover;\n\t}\n\n\tif (type_flags & PACKET_FANOUT_FLAG_UNIQUEID) {\n\t\tif (id != 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!fanout_find_new_id(sk, &id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t/* ephemeral flag for the first socket in the group: drop it */\n\t\tflags &= ~(PACKET_FANOUT_FLAG_UNIQUEID >> 8);\n\t}\n\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\trefcount_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\tif (match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (refcount_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\trefcount_set(&match->sk_ref, refcount_read(&match->sk_ref) + 1);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\nout:\n\tif (err && rollover) {\n\t\tkfree(rollover);\n\t\tpo->rollover = NULL;\n\t}\n\tmutex_unlock(&fanout_mutex);\n\treturn err;\n}",
        "func": "static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_rollover *rollover = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\n\terr = -EALREADY;\n\tif (po->fanout)\n\t\tgoto out;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\terr = -ENOMEM;\n\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n\t\tif (!rollover)\n\t\t\tgoto out;\n\t\tatomic_long_set(&rollover->num, 0);\n\t\tatomic_long_set(&rollover->num_huge, 0);\n\t\tatomic_long_set(&rollover->num_failed, 0);\n\t\tpo->rollover = rollover;\n\t}\n\n\tif (type_flags & PACKET_FANOUT_FLAG_UNIQUEID) {\n\t\tif (id != 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!fanout_find_new_id(sk, &id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t/* ephemeral flag for the first socket in the group: drop it */\n\t\tflags &= ~(PACKET_FANOUT_FLAG_UNIQUEID >> 8);\n\t}\n\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\trefcount_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\n\tspin_lock(&po->bind_lock);\n\tif (po->running &&\n\t    match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (refcount_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\trefcount_set(&match->sk_ref, refcount_read(&match->sk_ref) + 1);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tif (err && !refcount_read(&match->sk_ref)) {\n\t\tlist_del(&match->list);\n\t\tkfree(match);\n\t}\n\nout:\n\tif (err && rollover) {\n\t\tkfree(rollover);\n\t\tpo->rollover = NULL;\n\t}\n\tmutex_unlock(&fanout_mutex);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,10 +24,6 @@\n \t}\n \n \tmutex_lock(&fanout_mutex);\n-\n-\terr = -EINVAL;\n-\tif (!po->running)\n-\t\tgoto out;\n \n \terr = -EALREADY;\n \tif (po->fanout)\n@@ -90,7 +86,10 @@\n \t\tlist_add(&match->list, &fanout_list);\n \t}\n \terr = -EINVAL;\n-\tif (match->type == type &&\n+\n+\tspin_lock(&po->bind_lock);\n+\tif (po->running &&\n+\t    match->type == type &&\n \t    match->prot_hook.type == po->prot_hook.type &&\n \t    match->prot_hook.dev == po->prot_hook.dev) {\n \t\terr = -ENOSPC;\n@@ -102,6 +101,13 @@\n \t\t\terr = 0;\n \t\t}\n \t}\n+\tspin_unlock(&po->bind_lock);\n+\n+\tif (err && !refcount_read(&match->sk_ref)) {\n+\t\tlist_del(&match->list);\n+\t\tkfree(match);\n+\t}\n+\n out:\n \tif (err && rollover) {\n \t\tkfree(rollover);",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\terr = -EINVAL;",
                "\tif (!po->running)",
                "\t\tgoto out;",
                "\tif (match->type == type &&"
            ],
            "added_lines": [
                "",
                "\tspin_lock(&po->bind_lock);",
                "\tif (po->running &&",
                "\t    match->type == type &&",
                "\tspin_unlock(&po->bind_lock);",
                "",
                "\tif (err && !refcount_read(&match->sk_ref)) {",
                "\t\tlist_del(&match->list);",
                "\t\tkfree(match);",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15649",
        "func_name": "torvalds/linux/packet_do_bind",
        "description": "net/packet/af_packet.c in the Linux kernel before 4.13.6 allows local users to gain privileges via crafted system calls that trigger mishandling of packet_fanout data structures, because of a race condition (involving fanout_add and packet_do_bind) that leads to a use-after-free, a different vulnerability than CVE-2017-6346.",
        "git_url": "https://github.com/torvalds/linux/commit/4971613c1639d8e5f102c4e797c3bf8f83a5a69e",
        "commit_title": "packet: in packet_do_bind, test fanout with bind_lock held",
        "commit_text": " Once a socket has po->fanout set, it remains a member of the group until it is destroyed. The prot_hook must be constant and identical across sockets in the group.  If fanout_add races with packet_do_bind between the test of po->fanout and taking the lock, the bind call may make type or dev inconsistent with that of the fanout group.  Hold po->bind_lock when testing po->fanout to avoid this race.  I had to introduce artificial delay (local_bh_enable) to actually observe the race. ",
        "func_before": "static int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n\t\t\t  __be16 proto)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct net_device *dev_curr;\n\t__be16 proto_curr;\n\tbool need_rehook;\n\tstruct net_device *dev = NULL;\n\tint ret = 0;\n\tbool unlisted = false;\n\n\tif (po->fanout)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tspin_lock(&po->bind_lock);\n\trcu_read_lock();\n\n\tif (name) {\n\t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (ifindex) {\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (dev)\n\t\tdev_hold(dev);\n\n\tproto_curr = po->prot_hook.type;\n\tdev_curr = po->prot_hook.dev;\n\n\tneed_rehook = proto_curr != proto || dev_curr != dev;\n\n\tif (need_rehook) {\n\t\tif (po->running) {\n\t\t\trcu_read_unlock();\n\t\t\t__unregister_prot_hook(sk, true);\n\t\t\trcu_read_lock();\n\t\t\tdev_curr = po->prot_hook.dev;\n\t\t\tif (dev)\n\t\t\t\tunlisted = !dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t\t\t dev->ifindex);\n\t\t}\n\n\t\tpo->num = proto;\n\t\tpo->prot_hook.type = proto;\n\n\t\tif (unlikely(unlisted)) {\n\t\t\tdev_put(dev);\n\t\t\tpo->prot_hook.dev = NULL;\n\t\t\tpo->ifindex = -1;\n\t\t\tpacket_cached_dev_reset(po);\n\t\t} else {\n\t\t\tpo->prot_hook.dev = dev;\n\t\t\tpo->ifindex = dev ? dev->ifindex : 0;\n\t\t\tpacket_cached_dev_assign(po, dev);\n\t\t}\n\t}\n\tif (dev_curr)\n\t\tdev_put(dev_curr);\n\n\tif (proto == 0 || !need_rehook)\n\t\tgoto out_unlock;\n\n\tif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\n\t\tregister_prot_hook(sk);\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn ret;\n}",
        "func": "static int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n\t\t\t  __be16 proto)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct net_device *dev_curr;\n\t__be16 proto_curr;\n\tbool need_rehook;\n\tstruct net_device *dev = NULL;\n\tint ret = 0;\n\tbool unlisted = false;\n\n\tlock_sock(sk);\n\tspin_lock(&po->bind_lock);\n\trcu_read_lock();\n\n\tif (po->fanout) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (name) {\n\t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (ifindex) {\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (dev)\n\t\tdev_hold(dev);\n\n\tproto_curr = po->prot_hook.type;\n\tdev_curr = po->prot_hook.dev;\n\n\tneed_rehook = proto_curr != proto || dev_curr != dev;\n\n\tif (need_rehook) {\n\t\tif (po->running) {\n\t\t\trcu_read_unlock();\n\t\t\t__unregister_prot_hook(sk, true);\n\t\t\trcu_read_lock();\n\t\t\tdev_curr = po->prot_hook.dev;\n\t\t\tif (dev)\n\t\t\t\tunlisted = !dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t\t\t dev->ifindex);\n\t\t}\n\n\t\tpo->num = proto;\n\t\tpo->prot_hook.type = proto;\n\n\t\tif (unlikely(unlisted)) {\n\t\t\tdev_put(dev);\n\t\t\tpo->prot_hook.dev = NULL;\n\t\t\tpo->ifindex = -1;\n\t\t\tpacket_cached_dev_reset(po);\n\t\t} else {\n\t\t\tpo->prot_hook.dev = dev;\n\t\t\tpo->ifindex = dev ? dev->ifindex : 0;\n\t\t\tpacket_cached_dev_assign(po, dev);\n\t\t}\n\t}\n\tif (dev_curr)\n\t\tdev_put(dev_curr);\n\n\tif (proto == 0 || !need_rehook)\n\t\tgoto out_unlock;\n\n\tif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\n\t\tregister_prot_hook(sk);\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,12 +9,14 @@\n \tint ret = 0;\n \tbool unlisted = false;\n \n-\tif (po->fanout)\n-\t\treturn -EINVAL;\n-\n \tlock_sock(sk);\n \tspin_lock(&po->bind_lock);\n \trcu_read_lock();\n+\n+\tif (po->fanout) {\n+\t\tret = -EINVAL;\n+\t\tgoto out_unlock;\n+\t}\n \n \tif (name) {\n \t\tdev = dev_get_by_name_rcu(sock_net(sk), name);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (po->fanout)",
                "\t\treturn -EINVAL;",
                ""
            ],
            "added_lines": [
                "",
                "\tif (po->fanout) {",
                "\t\tret = -EINVAL;",
                "\t\tgoto out_unlock;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-12146",
        "func_name": "torvalds/linux/driver_override_store",
        "description": "The driver_override implementation in drivers/base/platform.c in the Linux kernel before 4.12.1 allows local users to gain privileges by leveraging a race condition between a read operation and a store operation that involve different overrides.",
        "git_url": "https://github.com/torvalds/linux/commit/6265539776a0810b7ce6398c27866ddb9c6bd154",
        "commit_title": "driver core: platform: fix race condition with driver_override",
        "commit_text": " The driver_override implementation is susceptible to race condition when different threads are reading vs storing a different driver override. Add locking to avoid race condition.  Cc: stable@vger.kernel.org",
        "func_before": "static ssize_t driver_override_store(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tchar *driver_override, *old = pdev->driver_override, *cp;\n\n\tif (count > PATH_MAX)\n\t\treturn -EINVAL;\n\n\tdriver_override = kstrndup(buf, count, GFP_KERNEL);\n\tif (!driver_override)\n\t\treturn -ENOMEM;\n\n\tcp = strchr(driver_override, '\\n');\n\tif (cp)\n\t\t*cp = '\\0';\n\n\tif (strlen(driver_override)) {\n\t\tpdev->driver_override = driver_override;\n\t} else {\n\t\tkfree(driver_override);\n\t\tpdev->driver_override = NULL;\n\t}\n\n\tkfree(old);\n\n\treturn count;\n}",
        "func": "static ssize_t driver_override_store(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tchar *driver_override, *old, *cp;\n\n\tif (count > PATH_MAX)\n\t\treturn -EINVAL;\n\n\tdriver_override = kstrndup(buf, count, GFP_KERNEL);\n\tif (!driver_override)\n\t\treturn -ENOMEM;\n\n\tcp = strchr(driver_override, '\\n');\n\tif (cp)\n\t\t*cp = '\\0';\n\n\tdevice_lock(dev);\n\told = pdev->driver_override;\n\tif (strlen(driver_override)) {\n\t\tpdev->driver_override = driver_override;\n\t} else {\n\t\tkfree(driver_override);\n\t\tpdev->driver_override = NULL;\n\t}\n\tdevice_unlock(dev);\n\n\tkfree(old);\n\n\treturn count;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n \t\t\t\t     const char *buf, size_t count)\n {\n \tstruct platform_device *pdev = to_platform_device(dev);\n-\tchar *driver_override, *old = pdev->driver_override, *cp;\n+\tchar *driver_override, *old, *cp;\n \n \tif (count > PATH_MAX)\n \t\treturn -EINVAL;\n@@ -16,12 +16,15 @@\n \tif (cp)\n \t\t*cp = '\\0';\n \n+\tdevice_lock(dev);\n+\told = pdev->driver_override;\n \tif (strlen(driver_override)) {\n \t\tpdev->driver_override = driver_override;\n \t} else {\n \t\tkfree(driver_override);\n \t\tpdev->driver_override = NULL;\n \t}\n+\tdevice_unlock(dev);\n \n \tkfree(old);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tchar *driver_override, *old = pdev->driver_override, *cp;"
            ],
            "added_lines": [
                "\tchar *driver_override, *old, *cp;",
                "\tdevice_lock(dev);",
                "\told = pdev->driver_override;",
                "\tdevice_unlock(dev);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-12146",
        "func_name": "torvalds/linux/driver_override_show",
        "description": "The driver_override implementation in drivers/base/platform.c in the Linux kernel before 4.12.1 allows local users to gain privileges by leveraging a race condition between a read operation and a store operation that involve different overrides.",
        "git_url": "https://github.com/torvalds/linux/commit/6265539776a0810b7ce6398c27866ddb9c6bd154",
        "commit_title": "driver core: platform: fix race condition with driver_override",
        "commit_text": " The driver_override implementation is susceptible to race condition when different threads are reading vs storing a different driver override. Add locking to avoid race condition.  Cc: stable@vger.kernel.org",
        "func_before": "static ssize_t driver_override_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\n\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);\n}",
        "func": "static ssize_t driver_override_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tssize_t len;\n\n\tdevice_lock(dev);\n\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);\n\tdevice_unlock(dev);\n\treturn len;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,10 @@\n \t\t\t\t    struct device_attribute *attr, char *buf)\n {\n \tstruct platform_device *pdev = to_platform_device(dev);\n+\tssize_t len;\n \n-\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);\n+\tdevice_lock(dev);\n+\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);\n+\tdevice_unlock(dev);\n+\treturn len;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);"
            ],
            "added_lines": [
                "\tssize_t len;",
                "\tdevice_lock(dev);",
                "\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);",
                "\tdevice_unlock(dev);",
                "\treturn len;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-14317",
        "func_name": "xen-project/xen/domain_cleanup",
        "description": "A domain cleanup issue was discovered in the C xenstore daemon (aka cxenstored) in Xen through 4.9.x. When shutting down a VM with a stubdomain, a race in cxenstored may cause a double-free. The xenstored daemon may crash, resulting in a DoS of any parts of the system relying on it (including domain creation / destruction, ballooning, device changes, etc.).",
        "git_url": "https://github.com/xen-project/xen/commit/562a1c0f7ef3fbf3c122c3dfa4f2ad9dd51da9fe",
        "commit_title": "tools/xenstore: dont unlink connection object twice",
        "commit_text": " A connection object of a domain with associated stubdom has two parents: the domain and the stubdom. When cleaning up the list of active domains in domain_cleanup() make sure not to unlink the connection twice from the same domain. This could happen when the domain and its stubdom are being destroyed at the same time leading to the domain loop being entered twice.  Additionally don't use talloc_free() in this case as it will remove a random parent link, leading eventually to a memory leak. Use talloc_unlink() instead specifying the context from which the connection object should be removed.  This is CVE-2017-14317 / XSA-233. ",
        "func_before": "static void domain_cleanup(void)\n{\n\txc_dominfo_t dominfo;\n\tstruct domain *domain, *tmp;\n\tint notify = 0;\n\n\tlist_for_each_entry_safe(domain, tmp, &domains, list) {\n\t\tif (xc_domain_getinfo(*xc_handle, domain->domid, 1,\n\t\t\t\t      &dominfo) == 1 &&\n\t\t    dominfo.domid == domain->domid) {\n\t\t\tif ((dominfo.crashed || dominfo.shutdown)\n\t\t\t    && !domain->shutdown) {\n\t\t\t\tdomain->shutdown = 1;\n\t\t\t\tnotify = 1;\n\t\t\t}\n\t\t\tif (!dominfo.dying)\n\t\t\t\tcontinue;\n\t\t}\n\t\ttalloc_free(domain->conn);\n\t\tnotify = 0; /* destroy_domain() fires the watch */\n\t}\n\n\tif (notify)\n\t\tfire_watches(NULL, NULL, \"@releaseDomain\", false);\n}",
        "func": "static void domain_cleanup(void)\n{\n\txc_dominfo_t dominfo;\n\tstruct domain *domain;\n\tint notify = 0;\n\n again:\n\tlist_for_each_entry(domain, &domains, list) {\n\t\tif (xc_domain_getinfo(*xc_handle, domain->domid, 1,\n\t\t\t\t      &dominfo) == 1 &&\n\t\t    dominfo.domid == domain->domid) {\n\t\t\tif ((dominfo.crashed || dominfo.shutdown)\n\t\t\t    && !domain->shutdown) {\n\t\t\t\tdomain->shutdown = 1;\n\t\t\t\tnotify = 1;\n\t\t\t}\n\t\t\tif (!dominfo.dying)\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (domain->conn) {\n\t\t\ttalloc_unlink(talloc_autofree_context(), domain->conn);\n\t\t\tdomain->conn = NULL;\n\t\t\tnotify = 0; /* destroy_domain() fires the watch */\n\t\t\tgoto again;\n\t\t}\n\t}\n\n\tif (notify)\n\t\tfire_watches(NULL, NULL, \"@releaseDomain\", false);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,11 @@\n static void domain_cleanup(void)\n {\n \txc_dominfo_t dominfo;\n-\tstruct domain *domain, *tmp;\n+\tstruct domain *domain;\n \tint notify = 0;\n \n-\tlist_for_each_entry_safe(domain, tmp, &domains, list) {\n+ again:\n+\tlist_for_each_entry(domain, &domains, list) {\n \t\tif (xc_domain_getinfo(*xc_handle, domain->domid, 1,\n \t\t\t\t      &dominfo) == 1 &&\n \t\t    dominfo.domid == domain->domid) {\n@@ -16,8 +17,12 @@\n \t\t\tif (!dominfo.dying)\n \t\t\t\tcontinue;\n \t\t}\n-\t\ttalloc_free(domain->conn);\n-\t\tnotify = 0; /* destroy_domain() fires the watch */\n+\t\tif (domain->conn) {\n+\t\t\ttalloc_unlink(talloc_autofree_context(), domain->conn);\n+\t\t\tdomain->conn = NULL;\n+\t\t\tnotify = 0; /* destroy_domain() fires the watch */\n+\t\t\tgoto again;\n+\t\t}\n \t}\n \n \tif (notify)",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct domain *domain, *tmp;",
                "\tlist_for_each_entry_safe(domain, tmp, &domains, list) {",
                "\t\ttalloc_free(domain->conn);",
                "\t\tnotify = 0; /* destroy_domain() fires the watch */"
            ],
            "added_lines": [
                "\tstruct domain *domain;",
                " again:",
                "\tlist_for_each_entry(domain, &domains, list) {",
                "\t\tif (domain->conn) {",
                "\t\t\ttalloc_unlink(talloc_autofree_context(), domain->conn);",
                "\t\t\tdomain->conn = NULL;",
                "\t\t\tnotify = 0; /* destroy_domain() fires the watch */",
                "\t\t\tgoto again;",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2013-0871",
        "func_name": "torvalds/linux/ptrace_check_attach",
        "description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death.",
        "git_url": "https://github.com/torvalds/linux/commit/9899d11f654474d2d54ea52ceaa2a1f4db3abd68",
        "commit_title": "ptrace: ensure arch_ptrace/ptrace_request can never race with SIGKILL",
        "commit_text": " putreg() assumes that the tracee is not running and pt_regs_access() can safely play with its stack.  However a killed tracee can return from ptrace_stop() to the low-level asm code and do RESTORE_REST, this means that debugger can actually read/modify the kernel stack until the tracee does SAVE_REST again.  set_task_blockstep() can race with SIGKILL too and in some sense this race is even worse, the very fact the tracee can be woken up breaks the logic.  As Linus suggested we can clear TASK_WAKEKILL around the arch_ptrace() call, this ensures that nobody can ever wakeup the tracee while the debugger looks at it.  Not only this fixes the mentioned problems, we can do some cleanups/simplifications in arch_ptrace() paths.  Probably ptrace_unfreeze_traced() needs more callers, for example it makes sense to make the tracee killable for oom-killer before access_process_vm().  While at it, add the comment into may_ptrace_stop() to explain why ptrace_stop() still can't rely on SIGKILL and signal_pending_state().  Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "func_before": "static int ptrace_check_attach(struct task_struct *child, bool ignore_state)\n{\n\tint ret = -ESRCH;\n\n\t/*\n\t * We take the read lock around doing both checks to close a\n\t * possible race where someone else was tracing our child and\n\t * detached between these two checks.  After this locked check,\n\t * we are sure that this is our traced child and that can only\n\t * be changed by us so it's not changing right after this.\n\t */\n\tread_lock(&tasklist_lock);\n\tif ((child->ptrace & PT_PTRACED) && child->parent == current) {\n\t\t/*\n\t\t * child->sighand can't be NULL, release_task()\n\t\t * does ptrace_unlink() before __exit_signal().\n\t\t */\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tWARN_ON_ONCE(task_is_stopped(child));\n\t\tif (ignore_state || (task_is_traced(child) &&\n\t\t\t\t     !(child->jobctl & JOBCTL_LISTENING)))\n\t\t\tret = 0;\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tif (!ret && !ignore_state)\n\t\tret = wait_task_inactive(child, TASK_TRACED) ? 0 : -ESRCH;\n\n\t/* All systems go.. */\n\treturn ret;\n}",
        "func": "static int ptrace_check_attach(struct task_struct *child, bool ignore_state)\n{\n\tint ret = -ESRCH;\n\n\t/*\n\t * We take the read lock around doing both checks to close a\n\t * possible race where someone else was tracing our child and\n\t * detached between these two checks.  After this locked check,\n\t * we are sure that this is our traced child and that can only\n\t * be changed by us so it's not changing right after this.\n\t */\n\tread_lock(&tasklist_lock);\n\tif (child->ptrace && child->parent == current) {\n\t\tWARN_ON(child->state == __TASK_TRACED);\n\t\t/*\n\t\t * child->sighand can't be NULL, release_task()\n\t\t * does ptrace_unlink() before __exit_signal().\n\t\t */\n\t\tif (ignore_state || ptrace_freeze_traced(child))\n\t\t\tret = 0;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tif (!ret && !ignore_state) {\n\t\tif (!wait_task_inactive(child, __TASK_TRACED)) {\n\t\t\t/*\n\t\t\t * This can only happen if may_ptrace_stop() fails and\n\t\t\t * ptrace_stop() changes ->state back to TASK_RUNNING,\n\t\t\t * so we should not worry about leaking __TASK_TRACED.\n\t\t\t */\n\t\t\tWARN_ON(child->state == __TASK_TRACED);\n\t\t\tret = -ESRCH;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,23 +10,28 @@\n \t * be changed by us so it's not changing right after this.\n \t */\n \tread_lock(&tasklist_lock);\n-\tif ((child->ptrace & PT_PTRACED) && child->parent == current) {\n+\tif (child->ptrace && child->parent == current) {\n+\t\tWARN_ON(child->state == __TASK_TRACED);\n \t\t/*\n \t\t * child->sighand can't be NULL, release_task()\n \t\t * does ptrace_unlink() before __exit_signal().\n \t\t */\n-\t\tspin_lock_irq(&child->sighand->siglock);\n-\t\tWARN_ON_ONCE(task_is_stopped(child));\n-\t\tif (ignore_state || (task_is_traced(child) &&\n-\t\t\t\t     !(child->jobctl & JOBCTL_LISTENING)))\n+\t\tif (ignore_state || ptrace_freeze_traced(child))\n \t\t\tret = 0;\n-\t\tspin_unlock_irq(&child->sighand->siglock);\n \t}\n \tread_unlock(&tasklist_lock);\n \n-\tif (!ret && !ignore_state)\n-\t\tret = wait_task_inactive(child, TASK_TRACED) ? 0 : -ESRCH;\n+\tif (!ret && !ignore_state) {\n+\t\tif (!wait_task_inactive(child, __TASK_TRACED)) {\n+\t\t\t/*\n+\t\t\t * This can only happen if may_ptrace_stop() fails and\n+\t\t\t * ptrace_stop() changes ->state back to TASK_RUNNING,\n+\t\t\t * so we should not worry about leaking __TASK_TRACED.\n+\t\t\t */\n+\t\t\tWARN_ON(child->state == __TASK_TRACED);\n+\t\t\tret = -ESRCH;\n+\t\t}\n+\t}\n \n-\t/* All systems go.. */\n \treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif ((child->ptrace & PT_PTRACED) && child->parent == current) {",
                "\t\tspin_lock_irq(&child->sighand->siglock);",
                "\t\tWARN_ON_ONCE(task_is_stopped(child));",
                "\t\tif (ignore_state || (task_is_traced(child) &&",
                "\t\t\t\t     !(child->jobctl & JOBCTL_LISTENING)))",
                "\t\tspin_unlock_irq(&child->sighand->siglock);",
                "\tif (!ret && !ignore_state)",
                "\t\tret = wait_task_inactive(child, TASK_TRACED) ? 0 : -ESRCH;",
                "\t/* All systems go.. */"
            ],
            "added_lines": [
                "\tif (child->ptrace && child->parent == current) {",
                "\t\tWARN_ON(child->state == __TASK_TRACED);",
                "\t\tif (ignore_state || ptrace_freeze_traced(child))",
                "\tif (!ret && !ignore_state) {",
                "\t\tif (!wait_task_inactive(child, __TASK_TRACED)) {",
                "\t\t\t/*",
                "\t\t\t * This can only happen if may_ptrace_stop() fails and",
                "\t\t\t * ptrace_stop() changes ->state back to TASK_RUNNING,",
                "\t\t\t * so we should not worry about leaking __TASK_TRACED.",
                "\t\t\t */",
                "\t\t\tWARN_ON(child->state == __TASK_TRACED);",
                "\t\t\tret = -ESRCH;",
                "\t\t}",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2013-0871",
        "func_name": "torvalds/linux/compat_sys_ptrace",
        "description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death.",
        "git_url": "https://github.com/torvalds/linux/commit/9899d11f654474d2d54ea52ceaa2a1f4db3abd68",
        "commit_title": "ptrace: ensure arch_ptrace/ptrace_request can never race with SIGKILL",
        "commit_text": " putreg() assumes that the tracee is not running and pt_regs_access() can safely play with its stack.  However a killed tracee can return from ptrace_stop() to the low-level asm code and do RESTORE_REST, this means that debugger can actually read/modify the kernel stack until the tracee does SAVE_REST again.  set_task_blockstep() can race with SIGKILL too and in some sense this race is even worse, the very fact the tracee can be woken up breaks the logic.  As Linus suggested we can clear TASK_WAKEKILL around the arch_ptrace() call, this ensures that nobody can ever wakeup the tracee while the debugger looks at it.  Not only this fixes the mentioned problems, we can do some cleanups/simplifications in arch_ptrace() paths.  Probably ptrace_unfreeze_traced() needs more callers, for example it makes sense to make the tracee killable for oom-killer before access_process_vm().  While at it, add the comment into may_ptrace_stop() to explain why ptrace_stop() still can't rely on SIGKILL and signal_pending_state().  Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "func_before": "asmlinkage long compat_sys_ptrace(compat_long_t request, compat_long_t pid,\n\t\t\t\t  compat_long_t addr, compat_long_t data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (!ret)\n\t\tret = compat_arch_ptrace(child, request, addr, data);\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}",
        "func": "asmlinkage long compat_sys_ptrace(compat_long_t request, compat_long_t pid,\n\t\t\t\t  compat_long_t addr, compat_long_t data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (!ret) {\n\t\tret = compat_arch_ptrace(child, request, addr, data);\n\t\tif (ret || request != PTRACE_DETACH)\n\t\t\tptrace_unfreeze_traced(child);\n\t}\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,8 +28,11 @@\n \n \tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n \t\t\t\t  request == PTRACE_INTERRUPT);\n-\tif (!ret)\n+\tif (!ret) {\n \t\tret = compat_arch_ptrace(child, request, addr, data);\n+\t\tif (ret || request != PTRACE_DETACH)\n+\t\t\tptrace_unfreeze_traced(child);\n+\t}\n \n  out_put_task_struct:\n \tput_task_struct(child);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!ret)"
            ],
            "added_lines": [
                "\tif (!ret) {",
                "\t\tif (ret || request != PTRACE_DETACH)",
                "\t\t\tptrace_unfreeze_traced(child);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2013-0871",
        "func_name": "torvalds/linux/may_ptrace_stop",
        "description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death.",
        "git_url": "https://github.com/torvalds/linux/commit/9899d11f654474d2d54ea52ceaa2a1f4db3abd68",
        "commit_title": "ptrace: ensure arch_ptrace/ptrace_request can never race with SIGKILL",
        "commit_text": " putreg() assumes that the tracee is not running and pt_regs_access() can safely play with its stack.  However a killed tracee can return from ptrace_stop() to the low-level asm code and do RESTORE_REST, this means that debugger can actually read/modify the kernel stack until the tracee does SAVE_REST again.  set_task_blockstep() can race with SIGKILL too and in some sense this race is even worse, the very fact the tracee can be woken up breaks the logic.  As Linus suggested we can clear TASK_WAKEKILL around the arch_ptrace() call, this ensures that nobody can ever wakeup the tracee while the debugger looks at it.  Not only this fixes the mentioned problems, we can do some cleanups/simplifications in arch_ptrace() paths.  Probably ptrace_unfreeze_traced() needs more callers, for example it makes sense to make the tracee killable for oom-killer before access_process_vm().  While at it, add the comment into may_ptrace_stop() to explain why ptrace_stop() still can't rely on SIGKILL and signal_pending_state().  Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "func_before": "static inline int may_ptrace_stop(void)\n{\n\tif (!likely(current->ptrace))\n\t\treturn 0;\n\t/*\n\t * Are we in the middle of do_coredump?\n\t * If so and our tracer is also part of the coredump stopping\n\t * is a deadlock situation, and pointless because our tracer\n\t * is dead so don't allow us to stop.\n\t * If SIGKILL was already sent before the caller unlocked\n\t * ->siglock we must see ->core_state != NULL. Otherwise it\n\t * is safe to enter schedule().\n\t */\n\tif (unlikely(current->mm->core_state) &&\n\t    unlikely(current->mm == current->parent->mm))\n\t\treturn 0;\n\n\treturn 1;\n}",
        "func": "static inline int may_ptrace_stop(void)\n{\n\tif (!likely(current->ptrace))\n\t\treturn 0;\n\t/*\n\t * Are we in the middle of do_coredump?\n\t * If so and our tracer is also part of the coredump stopping\n\t * is a deadlock situation, and pointless because our tracer\n\t * is dead so don't allow us to stop.\n\t * If SIGKILL was already sent before the caller unlocked\n\t * ->siglock we must see ->core_state != NULL. Otherwise it\n\t * is safe to enter schedule().\n\t *\n\t * This is almost outdated, a task with the pending SIGKILL can't\n\t * block in TASK_TRACED. But PTRACE_EVENT_EXIT can be reported\n\t * after SIGKILL was already dequeued.\n\t */\n\tif (unlikely(current->mm->core_state) &&\n\t    unlikely(current->mm == current->parent->mm))\n\t\treturn 0;\n\n\treturn 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,10 @@\n \t * If SIGKILL was already sent before the caller unlocked\n \t * ->siglock we must see ->core_state != NULL. Otherwise it\n \t * is safe to enter schedule().\n+\t *\n+\t * This is almost outdated, a task with the pending SIGKILL can't\n+\t * block in TASK_TRACED. But PTRACE_EVENT_EXIT can be reported\n+\t * after SIGKILL was already dequeued.\n \t */\n \tif (unlikely(current->mm->core_state) &&\n \t    unlikely(current->mm == current->parent->mm))",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t *",
                "\t * This is almost outdated, a task with the pending SIGKILL can't",
                "\t * block in TASK_TRACED. But PTRACE_EVENT_EXIT can be reported",
                "\t * after SIGKILL was already dequeued."
            ]
        }
    },
    {
        "cve_id": "CVE-2013-0871",
        "func_name": "torvalds/linux/set_task_blockstep",
        "description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death.",
        "git_url": "https://github.com/torvalds/linux/commit/9899d11f654474d2d54ea52ceaa2a1f4db3abd68",
        "commit_title": "ptrace: ensure arch_ptrace/ptrace_request can never race with SIGKILL",
        "commit_text": " putreg() assumes that the tracee is not running and pt_regs_access() can safely play with its stack.  However a killed tracee can return from ptrace_stop() to the low-level asm code and do RESTORE_REST, this means that debugger can actually read/modify the kernel stack until the tracee does SAVE_REST again.  set_task_blockstep() can race with SIGKILL too and in some sense this race is even worse, the very fact the tracee can be woken up breaks the logic.  As Linus suggested we can clear TASK_WAKEKILL around the arch_ptrace() call, this ensures that nobody can ever wakeup the tracee while the debugger looks at it.  Not only this fixes the mentioned problems, we can do some cleanups/simplifications in arch_ptrace() paths.  Probably ptrace_unfreeze_traced() needs more callers, for example it makes sense to make the tracee killable for oom-killer before access_process_vm().  While at it, add the comment into may_ptrace_stop() to explain why ptrace_stop() still can't rely on SIGKILL and signal_pending_state().  Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "func_before": "void set_task_blockstep(struct task_struct *task, bool on)\n{\n\tunsigned long debugctl;\n\n\t/*\n\t * Ensure irq/preemption can't change debugctl in between.\n\t * Note also that both TIF_BLOCKSTEP and debugctl should\n\t * be changed atomically wrt preemption.\n\t * FIXME: this means that set/clear TIF_BLOCKSTEP is simply\n\t * wrong if task != current, SIGKILL can wakeup the stopped\n\t * tracee and set/clear can play with the running task, this\n\t * can confuse the next __switch_to_xtra().\n\t */\n\tlocal_irq_disable();\n\tdebugctl = get_debugctlmsr();\n\tif (on) {\n\t\tdebugctl |= DEBUGCTLMSR_BTF;\n\t\tset_tsk_thread_flag(task, TIF_BLOCKSTEP);\n\t} else {\n\t\tdebugctl &= ~DEBUGCTLMSR_BTF;\n\t\tclear_tsk_thread_flag(task, TIF_BLOCKSTEP);\n\t}\n\tif (task == current)\n\t\tupdate_debugctlmsr(debugctl);\n\tlocal_irq_enable();\n}",
        "func": "void set_task_blockstep(struct task_struct *task, bool on)\n{\n\tunsigned long debugctl;\n\n\t/*\n\t * Ensure irq/preemption can't change debugctl in between.\n\t * Note also that both TIF_BLOCKSTEP and debugctl should\n\t * be changed atomically wrt preemption.\n\t *\n\t * NOTE: this means that set/clear TIF_BLOCKSTEP is only safe if\n\t * task is current or it can't be running, otherwise we can race\n\t * with __switch_to_xtra(). We rely on ptrace_freeze_traced() but\n\t * PTRACE_KILL is not safe.\n\t */\n\tlocal_irq_disable();\n\tdebugctl = get_debugctlmsr();\n\tif (on) {\n\t\tdebugctl |= DEBUGCTLMSR_BTF;\n\t\tset_tsk_thread_flag(task, TIF_BLOCKSTEP);\n\t} else {\n\t\tdebugctl &= ~DEBUGCTLMSR_BTF;\n\t\tclear_tsk_thread_flag(task, TIF_BLOCKSTEP);\n\t}\n\tif (task == current)\n\t\tupdate_debugctlmsr(debugctl);\n\tlocal_irq_enable();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,10 +6,11 @@\n \t * Ensure irq/preemption can't change debugctl in between.\n \t * Note also that both TIF_BLOCKSTEP and debugctl should\n \t * be changed atomically wrt preemption.\n-\t * FIXME: this means that set/clear TIF_BLOCKSTEP is simply\n-\t * wrong if task != current, SIGKILL can wakeup the stopped\n-\t * tracee and set/clear can play with the running task, this\n-\t * can confuse the next __switch_to_xtra().\n+\t *\n+\t * NOTE: this means that set/clear TIF_BLOCKSTEP is only safe if\n+\t * task is current or it can't be running, otherwise we can race\n+\t * with __switch_to_xtra(). We rely on ptrace_freeze_traced() but\n+\t * PTRACE_KILL is not safe.\n \t */\n \tlocal_irq_disable();\n \tdebugctl = get_debugctlmsr();",
        "diff_line_info": {
            "deleted_lines": [
                "\t * FIXME: this means that set/clear TIF_BLOCKSTEP is simply",
                "\t * wrong if task != current, SIGKILL can wakeup the stopped",
                "\t * tracee and set/clear can play with the running task, this",
                "\t * can confuse the next __switch_to_xtra()."
            ],
            "added_lines": [
                "\t *",
                "\t * NOTE: this means that set/clear TIF_BLOCKSTEP is only safe if",
                "\t * task is current or it can't be running, otherwise we can race",
                "\t * with __switch_to_xtra(). We rely on ptrace_freeze_traced() but",
                "\t * PTRACE_KILL is not safe."
            ]
        }
    },
    {
        "cve_id": "CVE-2013-1792",
        "func_name": "torvalds/linux/install_user_keyrings",
        "description": "Race condition in the install_user_keyrings function in security/keys/process_keys.c in the Linux kernel before 3.8.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) via crafted keyctl system calls that trigger keyring operations in simultaneous threads.",
        "git_url": "https://github.com/torvalds/linux/commit/0da9dfdd2cd9889201bc6f6f43580c99165cd087",
        "commit_title": "keys: fix race with concurrent install_user_keyrings()",
        "commit_text": " This fixes CVE-2013-1792.  There is a race in install_user_keyrings() that can cause a NULL pointer dereference when called concurrently for the same user if the uid and uid-session keyrings are not yet created.  It might be possible for an unprivileged user to trigger this by calling keyctl() from userspace in parallel immediately after logging in.  Assume that we have two threads both executing lookup_user_key(), both looking for KEY_SPEC_USER_SESSION_KEYRING.  \tTHREAD A\t\t\tTHREAD B \t===============================\t=============================== \t\t\t\t\t==>call install_user_keyrings(); \tif (!cred->user->session_keyring) \t==>call install_user_keyrings() \t\t\t\t\t... \t\t\t\t\tuser->uid_keyring = uid_keyring; \tif (user->uid_keyring) \t\treturn 0; \t<== \tkey = cred->user->session_keyring [== NULL] \t\t\t\t\tuser->session_keyring = session_keyring; \tatomic_inc(&key->usage); [oops]  At the point thread A dereferences cred->user->session_keyring, thread B hasn't updated user->session_keyring yet, but thread A assumes it is populated because install_user_keyrings() returned ok.  The race window is really small but can be exploited if, for example, thread B is interrupted or preempted after initializing uid_keyring, but before doing setting session_keyring.  This couldn't be reproduced on a stock kernel.  However, after placing systemtap probe on 'user->session_keyring = session_keyring;' that introduced some delay, the kernel could be crashed reliably.  Fix this by checking both pointers before deciding whether to return. Alternatively, the test could be done away with entirely as it is checked inside the mutex - but since the mutex is global, that may not be the best way.  Cc: <stable@kernel.org>",
        "func_before": "int install_user_keyrings(void)\n{\n\tstruct user_struct *user;\n\tconst struct cred *cred;\n\tstruct key *uid_keyring, *session_keyring;\n\tkey_perm_t user_keyring_perm;\n\tchar buf[20];\n\tint ret;\n\tuid_t uid;\n\n\tuser_keyring_perm = (KEY_POS_ALL & ~KEY_POS_SETATTR) | KEY_USR_ALL;\n\tcred = current_cred();\n\tuser = cred->user;\n\tuid = from_kuid(cred->user_ns, user->uid);\n\n\tkenter(\"%p{%u}\", user, uid);\n\n\tif (user->uid_keyring) {\n\t\tkleave(\" = 0 [exist]\");\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&key_user_keyring_mutex);\n\tret = 0;\n\n\tif (!user->uid_keyring) {\n\t\t/* get the UID-specific keyring\n\t\t * - there may be one in existence already as it may have been\n\t\t *   pinned by a session, but the user_struct pointing to it\n\t\t *   may have been destroyed by setuid */\n\t\tsprintf(buf, \"_uid.%u\", uid);\n\n\t\tuid_keyring = find_keyring_by_name(buf, true);\n\t\tif (IS_ERR(uid_keyring)) {\n\t\t\tuid_keyring = keyring_alloc(buf, user->uid, INVALID_GID,\n\t\t\t\t\t\t    cred, user_keyring_perm,\n\t\t\t\t\t\t    KEY_ALLOC_IN_QUOTA, NULL);\n\t\t\tif (IS_ERR(uid_keyring)) {\n\t\t\t\tret = PTR_ERR(uid_keyring);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\n\t\t/* get a default session keyring (which might also exist\n\t\t * already) */\n\t\tsprintf(buf, \"_uid_ses.%u\", uid);\n\n\t\tsession_keyring = find_keyring_by_name(buf, true);\n\t\tif (IS_ERR(session_keyring)) {\n\t\t\tsession_keyring =\n\t\t\t\tkeyring_alloc(buf, user->uid, INVALID_GID,\n\t\t\t\t\t      cred, user_keyring_perm,\n\t\t\t\t\t      KEY_ALLOC_IN_QUOTA, NULL);\n\t\t\tif (IS_ERR(session_keyring)) {\n\t\t\t\tret = PTR_ERR(session_keyring);\n\t\t\t\tgoto error_release;\n\t\t\t}\n\n\t\t\t/* we install a link from the user session keyring to\n\t\t\t * the user keyring */\n\t\t\tret = key_link(session_keyring, uid_keyring);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error_release_both;\n\t\t}\n\n\t\t/* install the keyrings */\n\t\tuser->uid_keyring = uid_keyring;\n\t\tuser->session_keyring = session_keyring;\n\t}\n\n\tmutex_unlock(&key_user_keyring_mutex);\n\tkleave(\" = 0\");\n\treturn 0;\n\nerror_release_both:\n\tkey_put(session_keyring);\nerror_release:\n\tkey_put(uid_keyring);\nerror:\n\tmutex_unlock(&key_user_keyring_mutex);\n\tkleave(\" = %d\", ret);\n\treturn ret;\n}",
        "func": "int install_user_keyrings(void)\n{\n\tstruct user_struct *user;\n\tconst struct cred *cred;\n\tstruct key *uid_keyring, *session_keyring;\n\tkey_perm_t user_keyring_perm;\n\tchar buf[20];\n\tint ret;\n\tuid_t uid;\n\n\tuser_keyring_perm = (KEY_POS_ALL & ~KEY_POS_SETATTR) | KEY_USR_ALL;\n\tcred = current_cred();\n\tuser = cred->user;\n\tuid = from_kuid(cred->user_ns, user->uid);\n\n\tkenter(\"%p{%u}\", user, uid);\n\n\tif (user->uid_keyring && user->session_keyring) {\n\t\tkleave(\" = 0 [exist]\");\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&key_user_keyring_mutex);\n\tret = 0;\n\n\tif (!user->uid_keyring) {\n\t\t/* get the UID-specific keyring\n\t\t * - there may be one in existence already as it may have been\n\t\t *   pinned by a session, but the user_struct pointing to it\n\t\t *   may have been destroyed by setuid */\n\t\tsprintf(buf, \"_uid.%u\", uid);\n\n\t\tuid_keyring = find_keyring_by_name(buf, true);\n\t\tif (IS_ERR(uid_keyring)) {\n\t\t\tuid_keyring = keyring_alloc(buf, user->uid, INVALID_GID,\n\t\t\t\t\t\t    cred, user_keyring_perm,\n\t\t\t\t\t\t    KEY_ALLOC_IN_QUOTA, NULL);\n\t\t\tif (IS_ERR(uid_keyring)) {\n\t\t\t\tret = PTR_ERR(uid_keyring);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\n\t\t/* get a default session keyring (which might also exist\n\t\t * already) */\n\t\tsprintf(buf, \"_uid_ses.%u\", uid);\n\n\t\tsession_keyring = find_keyring_by_name(buf, true);\n\t\tif (IS_ERR(session_keyring)) {\n\t\t\tsession_keyring =\n\t\t\t\tkeyring_alloc(buf, user->uid, INVALID_GID,\n\t\t\t\t\t      cred, user_keyring_perm,\n\t\t\t\t\t      KEY_ALLOC_IN_QUOTA, NULL);\n\t\t\tif (IS_ERR(session_keyring)) {\n\t\t\t\tret = PTR_ERR(session_keyring);\n\t\t\t\tgoto error_release;\n\t\t\t}\n\n\t\t\t/* we install a link from the user session keyring to\n\t\t\t * the user keyring */\n\t\t\tret = key_link(session_keyring, uid_keyring);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error_release_both;\n\t\t}\n\n\t\t/* install the keyrings */\n\t\tuser->uid_keyring = uid_keyring;\n\t\tuser->session_keyring = session_keyring;\n\t}\n\n\tmutex_unlock(&key_user_keyring_mutex);\n\tkleave(\" = 0\");\n\treturn 0;\n\nerror_release_both:\n\tkey_put(session_keyring);\nerror_release:\n\tkey_put(uid_keyring);\nerror:\n\tmutex_unlock(&key_user_keyring_mutex);\n\tkleave(\" = %d\", ret);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,7 +15,7 @@\n \n \tkenter(\"%p{%u}\", user, uid);\n \n-\tif (user->uid_keyring) {\n+\tif (user->uid_keyring && user->session_keyring) {\n \t\tkleave(\" = 0 [exist]\");\n \t\treturn 0;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (user->uid_keyring) {"
            ],
            "added_lines": [
                "\tif (user->uid_keyring && user->session_keyring) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2013-3302",
        "func_name": "torvalds/linux/smb_send_kvec",
        "description": "Race condition in the smb_send_rqst function in fs/cifs/transport.c in the Linux kernel before 3.7.2 allows local users to cause a denial of service (NULL pointer dereference and OOPS) or possibly have unspecified other impact via vectors involving a reconnection event.",
        "git_url": "https://github.com/torvalds/linux/commit/ea702b80e0bbb2448e201472127288beb82ca2fe",
        "commit_title": "cifs: move check for NULL socket into smb_send_rqst",
        "commit_text": " Cai reported this oops:  [90701.616664] BUG: unable to handle kernel NULL pointer dereference at 0000000000000028 [90701.625438] IP: [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60 [90701.632167] PGD fea319067 PUD 103fda4067 PMD 0 [90701.637255] Oops: 0000 [#1] SMP [90701.640878] Modules linked in: des_generic md4 nls_utf8 cifs dns_resolver binfmt_misc tun sg igb iTCO_wdt iTCO_vendor_support lpc_ich pcspkr i2c_i801 i2c_core i7core_edac edac_core ioatdma dca mfd_core coretemp kvm_intel kvm crc32c_intel microcode sr_mod cdrom ata_generic sd_mod pata_acpi crc_t10dif ata_piix libata megaraid_sas dm_mirror dm_region_hash dm_log dm_mod [90701.677655] CPU 10 [90701.679808] Pid: 9627, comm: ls Tainted: G        W    3.7.1+ #10 QCI QSSC-S4R/QSSC-S4R [90701.688950] RIP: 0010:[<ffffffff814a343e>]  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60 [90701.698383] RSP: 0018:ffff88177b431bb8  EFLAGS: 00010206 [90701.704309] RAX: ffff88177b431fd8 RBX: 00007ffffffff000 RCX: ffff88177b431bec [90701.712271] RDX: 0000000000000003 RSI: 0000000000000006 RDI: 0000000000000000 [90701.720223] RBP: ffff88177b431bc8 R08: 0000000000000004 R09: 0000000000000000 [90701.728185] R10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000001 [90701.736147] R13: ffff88184ef92000 R14: 0000000000000023 R15: ffff88177b431c88 [90701.744109] FS:  00007fd56a1a47c0(0000) GS:ffff88105fc40000(0000) knlGS:0000000000000000 [90701.753137] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b [90701.759550] CR2: 0000000000000028 CR3: 000000104f15f000 CR4: 00000000000007e0 [90701.767512] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 [90701.775465] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400 [90701.783428] Process ls (pid: 9627, threadinfo ffff88177b430000, task ffff88185ca4cb60) [90701.792261] Stack: [90701.794505]  0000000000000023 ffff88177b431c50 ffff88177b431c38 ffffffffa014fcb1 [90701.802809]  ffff88184ef921bc 0000000000000000 00000001ffffffff ffff88184ef921c0 [90701.811123]  ffff88177b431c08 ffffffff815ca3d9 ffff88177b431c18 ffff880857758000 [90701.819433] Call Trace: [90701.822183]  [<ffffffffa014fcb1>] smb_send_rqst+0x71/0x1f0 [cifs] [90701.828991]  [<ffffffff815ca3d9>] ? schedule+0x29/0x70 [90701.834736]  [<ffffffffa014fe6d>] smb_sendv+0x3d/0x40 [cifs] [90701.841062]  [<ffffffffa014fe96>] smb_send+0x26/0x30 [cifs] [90701.847291]  [<ffffffffa015801f>] send_nt_cancel+0x6f/0xd0 [cifs] [90701.854102]  [<ffffffffa015075e>] SendReceive+0x18e/0x360 [cifs] [90701.860814]  [<ffffffffa0134a78>] CIFSFindFirst+0x1a8/0x3f0 [cifs] [90701.867724]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs] [90701.875601]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs] [90701.883477]  [<ffffffffa01578e6>] cifs_query_dir_first+0x26/0x30 [cifs] [90701.890869]  [<ffffffffa015480d>] initiate_cifs_search+0xed/0x250 [cifs] [90701.898354]  [<ffffffff81195970>] ? fillonedir+0x100/0x100 [90701.904486]  [<ffffffffa01554cb>] cifs_readdir+0x45b/0x8f0 [cifs] [90701.911288]  [<ffffffff81195970>] ? fillonedir+0x100/0x100 [90701.917410]  [<ffffffff81195970>] ? fillonedir+0x100/0x100 [90701.923533]  [<ffffffff81195970>] ? fillonedir+0x100/0x100 [90701.929657]  [<ffffffff81195848>] vfs_readdir+0xb8/0xe0 [90701.935490]  [<ffffffff81195b9f>] sys_getdents+0x8f/0x110 [90701.941521]  [<ffffffff815d3b99>] system_call_fastpath+0x16/0x1b [90701.948222] Code: 66 90 55 65 48 8b 04 25 f0 c6 00 00 48 89 e5 53 48 83 ec 08 83 fe 01 48 8b 98 48 e0 ff ff 48 c7 80 48 e0 ff ff ff ff ff ff 74 22 <48> 8b 47 28 ff 50 68 65 48 8b 14 25 f0 c6 00 00 48 89 9a 48 e0 [90701.970313] RIP  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60 [90701.977125]  RSP <ffff88177b431bb8> [90701.981018] CR2: 0000000000000028 [90701.984809] ---[ end trace 24bd602971110a43 ]---  This is likely due to a race vs. a reconnection event.  The current code checks for a NULL socket in smb_send_kvec, but that's too late. By the time that check is done, the socket will already have been passed to kernel_setsockopt. Move the check into smb_send_rqst, so that it's checked earlier.  In truth, this is a bit of a half-assed fix. The -ENOTSOCK error return here looks like it could bubble back up to userspace. The locking rules around the ssocket pointer are really unclear as well. There are cases where the ssocket pointer is changed without holding the srv_mutex, but I'm not clear whether there's a potential race here yet or not.  This code seems like it could benefit from some fundamental re-think of how the socket handling should behave. Until then though, this patch should at least fix the above oops in most cases.  Cc: <stable@vger.kernel.org> # 3.7+ Reported-and-Tested-by: CAI Qian <caiqian@redhat.com>",
        "func_before": "static int\nsmb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,\n\t\tsize_t *sent)\n{\n\tint rc = 0;\n\tint i = 0;\n\tstruct msghdr smb_msg;\n\tunsigned int remaining;\n\tsize_t first_vec = 0;\n\tstruct socket *ssocket = server->ssocket;\n\n\t*sent = 0;\n\n\tif (ssocket == NULL)\n\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */\n\n\tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n\tsmb_msg.msg_namelen = sizeof(struct sockaddr);\n\tsmb_msg.msg_control = NULL;\n\tsmb_msg.msg_controllen = 0;\n\tif (server->noblocksnd)\n\t\tsmb_msg.msg_flags = MSG_DONTWAIT + MSG_NOSIGNAL;\n\telse\n\t\tsmb_msg.msg_flags = MSG_NOSIGNAL;\n\n\tremaining = 0;\n\tfor (i = 0; i < n_vec; i++)\n\t\tremaining += iov[i].iov_len;\n\n\ti = 0;\n\twhile (remaining) {\n\t\t/*\n\t\t * If blocking send, we try 3 times, since each can block\n\t\t * for 5 seconds. For nonblocking  we have to try more\n\t\t * but wait increasing amounts of time allowing time for\n\t\t * socket to clear.  The overall time we wait in either\n\t\t * case to send on the socket is about 15 seconds.\n\t\t * Similarly we wait for 15 seconds for a response from\n\t\t * the server in SendReceive[2] for the server to send\n\t\t * a response back for most types of requests (except\n\t\t * SMB Write past end of file which can be slow, and\n\t\t * blocking lock operations). NFS waits slightly longer\n\t\t * than CIFS, but this can make it take longer for\n\t\t * nonresponsive servers to be detected and 15 seconds\n\t\t * is more than enough time for modern networks to\n\t\t * send a packet.  In most cases if we fail to send\n\t\t * after the retries we will kill the socket and\n\t\t * reconnect which may clear the network problem.\n\t\t */\n\t\trc = kernel_sendmsg(ssocket, &smb_msg, &iov[first_vec],\n\t\t\t\t    n_vec - first_vec, remaining);\n\t\tif (rc == -ENOSPC || rc == -EAGAIN) {\n\t\t\t/*\n\t\t\t * Catch if a low level driver returns -ENOSPC. This\n\t\t\t * WARN_ON will be removed by 3.10 if no one reports\n\t\t\t * seeing this.\n\t\t\t */\n\t\t\tWARN_ON_ONCE(rc == -ENOSPC);\n\t\t\ti++;\n\t\t\tif (i >= 14 || (!server->noblocksnd && (i > 2))) {\n\t\t\t\tcERROR(1, \"sends on sock %p stuck for 15 \"\n\t\t\t\t\t  \"seconds\", ssocket);\n\t\t\t\trc = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmsleep(1 << i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\t/* send was at least partially successful */\n\t\t*sent += rc;\n\n\t\tif (rc == remaining) {\n\t\t\tremaining = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc > remaining) {\n\t\t\tcERROR(1, \"sent %d requested %d\", rc, remaining);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc == 0) {\n\t\t\t/* should never happen, letting socket clear before\n\t\t\t   retrying is our only obvious option here */\n\t\t\tcERROR(1, \"tcp sent no data\");\n\t\t\tmsleep(500);\n\t\t\tcontinue;\n\t\t}\n\n\t\tremaining -= rc;\n\n\t\t/* the line below resets i */\n\t\tfor (i = first_vec; i < n_vec; i++) {\n\t\t\tif (iov[i].iov_len) {\n\t\t\t\tif (rc > iov[i].iov_len) {\n\t\t\t\t\trc -= iov[i].iov_len;\n\t\t\t\t\tiov[i].iov_len = 0;\n\t\t\t\t} else {\n\t\t\t\t\tiov[i].iov_base += rc;\n\t\t\t\t\tiov[i].iov_len -= rc;\n\t\t\t\t\tfirst_vec = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\ti = 0; /* in case we get ENOSPC on the next send */\n\t\trc = 0;\n\t}\n\treturn rc;\n}",
        "func": "static int\nsmb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,\n\t\tsize_t *sent)\n{\n\tint rc = 0;\n\tint i = 0;\n\tstruct msghdr smb_msg;\n\tunsigned int remaining;\n\tsize_t first_vec = 0;\n\tstruct socket *ssocket = server->ssocket;\n\n\t*sent = 0;\n\n\tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n\tsmb_msg.msg_namelen = sizeof(struct sockaddr);\n\tsmb_msg.msg_control = NULL;\n\tsmb_msg.msg_controllen = 0;\n\tif (server->noblocksnd)\n\t\tsmb_msg.msg_flags = MSG_DONTWAIT + MSG_NOSIGNAL;\n\telse\n\t\tsmb_msg.msg_flags = MSG_NOSIGNAL;\n\n\tremaining = 0;\n\tfor (i = 0; i < n_vec; i++)\n\t\tremaining += iov[i].iov_len;\n\n\ti = 0;\n\twhile (remaining) {\n\t\t/*\n\t\t * If blocking send, we try 3 times, since each can block\n\t\t * for 5 seconds. For nonblocking  we have to try more\n\t\t * but wait increasing amounts of time allowing time for\n\t\t * socket to clear.  The overall time we wait in either\n\t\t * case to send on the socket is about 15 seconds.\n\t\t * Similarly we wait for 15 seconds for a response from\n\t\t * the server in SendReceive[2] for the server to send\n\t\t * a response back for most types of requests (except\n\t\t * SMB Write past end of file which can be slow, and\n\t\t * blocking lock operations). NFS waits slightly longer\n\t\t * than CIFS, but this can make it take longer for\n\t\t * nonresponsive servers to be detected and 15 seconds\n\t\t * is more than enough time for modern networks to\n\t\t * send a packet.  In most cases if we fail to send\n\t\t * after the retries we will kill the socket and\n\t\t * reconnect which may clear the network problem.\n\t\t */\n\t\trc = kernel_sendmsg(ssocket, &smb_msg, &iov[first_vec],\n\t\t\t\t    n_vec - first_vec, remaining);\n\t\tif (rc == -ENOSPC || rc == -EAGAIN) {\n\t\t\t/*\n\t\t\t * Catch if a low level driver returns -ENOSPC. This\n\t\t\t * WARN_ON will be removed by 3.10 if no one reports\n\t\t\t * seeing this.\n\t\t\t */\n\t\t\tWARN_ON_ONCE(rc == -ENOSPC);\n\t\t\ti++;\n\t\t\tif (i >= 14 || (!server->noblocksnd && (i > 2))) {\n\t\t\t\tcERROR(1, \"sends on sock %p stuck for 15 \"\n\t\t\t\t\t  \"seconds\", ssocket);\n\t\t\t\trc = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmsleep(1 << i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\t/* send was at least partially successful */\n\t\t*sent += rc;\n\n\t\tif (rc == remaining) {\n\t\t\tremaining = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc > remaining) {\n\t\t\tcERROR(1, \"sent %d requested %d\", rc, remaining);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc == 0) {\n\t\t\t/* should never happen, letting socket clear before\n\t\t\t   retrying is our only obvious option here */\n\t\t\tcERROR(1, \"tcp sent no data\");\n\t\t\tmsleep(500);\n\t\t\tcontinue;\n\t\t}\n\n\t\tremaining -= rc;\n\n\t\t/* the line below resets i */\n\t\tfor (i = first_vec; i < n_vec; i++) {\n\t\t\tif (iov[i].iov_len) {\n\t\t\t\tif (rc > iov[i].iov_len) {\n\t\t\t\t\trc -= iov[i].iov_len;\n\t\t\t\t\tiov[i].iov_len = 0;\n\t\t\t\t} else {\n\t\t\t\t\tiov[i].iov_base += rc;\n\t\t\t\t\tiov[i].iov_len -= rc;\n\t\t\t\t\tfirst_vec = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\ti = 0; /* in case we get ENOSPC on the next send */\n\t\trc = 0;\n\t}\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,9 +10,6 @@\n \tstruct socket *ssocket = server->ssocket;\n \n \t*sent = 0;\n-\n-\tif (ssocket == NULL)\n-\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */\n \n \tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n \tsmb_msg.msg_namelen = sizeof(struct sockaddr);",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\tif (ssocket == NULL)",
                "\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2013-3302",
        "func_name": "torvalds/linux/smb_send_rqst",
        "description": "Race condition in the smb_send_rqst function in fs/cifs/transport.c in the Linux kernel before 3.7.2 allows local users to cause a denial of service (NULL pointer dereference and OOPS) or possibly have unspecified other impact via vectors involving a reconnection event.",
        "git_url": "https://github.com/torvalds/linux/commit/ea702b80e0bbb2448e201472127288beb82ca2fe",
        "commit_title": "cifs: move check for NULL socket into smb_send_rqst",
        "commit_text": " Cai reported this oops:  [90701.616664] BUG: unable to handle kernel NULL pointer dereference at 0000000000000028 [90701.625438] IP: [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60 [90701.632167] PGD fea319067 PUD 103fda4067 PMD 0 [90701.637255] Oops: 0000 [#1] SMP [90701.640878] Modules linked in: des_generic md4 nls_utf8 cifs dns_resolver binfmt_misc tun sg igb iTCO_wdt iTCO_vendor_support lpc_ich pcspkr i2c_i801 i2c_core i7core_edac edac_core ioatdma dca mfd_core coretemp kvm_intel kvm crc32c_intel microcode sr_mod cdrom ata_generic sd_mod pata_acpi crc_t10dif ata_piix libata megaraid_sas dm_mirror dm_region_hash dm_log dm_mod [90701.677655] CPU 10 [90701.679808] Pid: 9627, comm: ls Tainted: G        W    3.7.1+ #10 QCI QSSC-S4R/QSSC-S4R [90701.688950] RIP: 0010:[<ffffffff814a343e>]  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60 [90701.698383] RSP: 0018:ffff88177b431bb8  EFLAGS: 00010206 [90701.704309] RAX: ffff88177b431fd8 RBX: 00007ffffffff000 RCX: ffff88177b431bec [90701.712271] RDX: 0000000000000003 RSI: 0000000000000006 RDI: 0000000000000000 [90701.720223] RBP: ffff88177b431bc8 R08: 0000000000000004 R09: 0000000000000000 [90701.728185] R10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000001 [90701.736147] R13: ffff88184ef92000 R14: 0000000000000023 R15: ffff88177b431c88 [90701.744109] FS:  00007fd56a1a47c0(0000) GS:ffff88105fc40000(0000) knlGS:0000000000000000 [90701.753137] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b [90701.759550] CR2: 0000000000000028 CR3: 000000104f15f000 CR4: 00000000000007e0 [90701.767512] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 [90701.775465] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400 [90701.783428] Process ls (pid: 9627, threadinfo ffff88177b430000, task ffff88185ca4cb60) [90701.792261] Stack: [90701.794505]  0000000000000023 ffff88177b431c50 ffff88177b431c38 ffffffffa014fcb1 [90701.802809]  ffff88184ef921bc 0000000000000000 00000001ffffffff ffff88184ef921c0 [90701.811123]  ffff88177b431c08 ffffffff815ca3d9 ffff88177b431c18 ffff880857758000 [90701.819433] Call Trace: [90701.822183]  [<ffffffffa014fcb1>] smb_send_rqst+0x71/0x1f0 [cifs] [90701.828991]  [<ffffffff815ca3d9>] ? schedule+0x29/0x70 [90701.834736]  [<ffffffffa014fe6d>] smb_sendv+0x3d/0x40 [cifs] [90701.841062]  [<ffffffffa014fe96>] smb_send+0x26/0x30 [cifs] [90701.847291]  [<ffffffffa015801f>] send_nt_cancel+0x6f/0xd0 [cifs] [90701.854102]  [<ffffffffa015075e>] SendReceive+0x18e/0x360 [cifs] [90701.860814]  [<ffffffffa0134a78>] CIFSFindFirst+0x1a8/0x3f0 [cifs] [90701.867724]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs] [90701.875601]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs] [90701.883477]  [<ffffffffa01578e6>] cifs_query_dir_first+0x26/0x30 [cifs] [90701.890869]  [<ffffffffa015480d>] initiate_cifs_search+0xed/0x250 [cifs] [90701.898354]  [<ffffffff81195970>] ? fillonedir+0x100/0x100 [90701.904486]  [<ffffffffa01554cb>] cifs_readdir+0x45b/0x8f0 [cifs] [90701.911288]  [<ffffffff81195970>] ? fillonedir+0x100/0x100 [90701.917410]  [<ffffffff81195970>] ? fillonedir+0x100/0x100 [90701.923533]  [<ffffffff81195970>] ? fillonedir+0x100/0x100 [90701.929657]  [<ffffffff81195848>] vfs_readdir+0xb8/0xe0 [90701.935490]  [<ffffffff81195b9f>] sys_getdents+0x8f/0x110 [90701.941521]  [<ffffffff815d3b99>] system_call_fastpath+0x16/0x1b [90701.948222] Code: 66 90 55 65 48 8b 04 25 f0 c6 00 00 48 89 e5 53 48 83 ec 08 83 fe 01 48 8b 98 48 e0 ff ff 48 c7 80 48 e0 ff ff ff ff ff ff 74 22 <48> 8b 47 28 ff 50 68 65 48 8b 14 25 f0 c6 00 00 48 89 9a 48 e0 [90701.970313] RIP  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60 [90701.977125]  RSP <ffff88177b431bb8> [90701.981018] CR2: 0000000000000028 [90701.984809] ---[ end trace 24bd602971110a43 ]---  This is likely due to a race vs. a reconnection event.  The current code checks for a NULL socket in smb_send_kvec, but that's too late. By the time that check is done, the socket will already have been passed to kernel_setsockopt. Move the check into smb_send_rqst, so that it's checked earlier.  In truth, this is a bit of a half-assed fix. The -ENOTSOCK error return here looks like it could bubble back up to userspace. The locking rules around the ssocket pointer are really unclear as well. There are cases where the ssocket pointer is changed without holding the srv_mutex, but I'm not clear whether there's a potential race here yet or not.  This code seems like it could benefit from some fundamental re-think of how the socket handling should behave. Until then though, this patch should at least fix the above oops in most cases.  Cc: <stable@vger.kernel.org> # 3.7+ Reported-and-Tested-by: CAI Qian <caiqian@redhat.com>",
        "func_before": "static int\nsmb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)\n{\n\tint rc;\n\tstruct kvec *iov = rqst->rq_iov;\n\tint n_vec = rqst->rq_nvec;\n\tunsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);\n\tunsigned int i;\n\tsize_t total_len = 0, sent;\n\tstruct socket *ssocket = server->ssocket;\n\tint val = 1;\n\n\tcFYI(1, \"Sending smb: smb_len=%u\", smb_buf_length);\n\tdump_smb(iov[0].iov_base, iov[0].iov_len);\n\n\t/* cork the socket */\n\tkernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,\n\t\t\t\t(char *)&val, sizeof(val));\n\n\trc = smb_send_kvec(server, iov, n_vec, &sent);\n\tif (rc < 0)\n\t\tgoto uncork;\n\n\ttotal_len += sent;\n\n\t/* now walk the page array and send each page in it */\n\tfor (i = 0; i < rqst->rq_npages; i++) {\n\t\tstruct kvec p_iov;\n\n\t\tcifs_rqst_page_to_kvec(rqst, i, &p_iov);\n\t\trc = smb_send_kvec(server, &p_iov, 1, &sent);\n\t\tkunmap(rqst->rq_pages[i]);\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\ttotal_len += sent;\n\t}\n\nuncork:\n\t/* uncork it */\n\tval = 0;\n\tkernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,\n\t\t\t\t(char *)&val, sizeof(val));\n\n\tif ((total_len > 0) && (total_len != smb_buf_length + 4)) {\n\t\tcFYI(1, \"partial send (wanted=%u sent=%zu): terminating \"\n\t\t\t\"session\", smb_buf_length + 4, total_len);\n\t\t/*\n\t\t * If we have only sent part of an SMB then the next SMB could\n\t\t * be taken as the remainder of this one. We need to kill the\n\t\t * socket so the server throws away the partial SMB\n\t\t */\n\t\tserver->tcpStatus = CifsNeedReconnect;\n\t}\n\n\tif (rc < 0 && rc != -EINTR)\n\t\tcERROR(1, \"Error %d sending data on socket to server\", rc);\n\telse\n\t\trc = 0;\n\n\treturn rc;\n}",
        "func": "static int\nsmb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)\n{\n\tint rc;\n\tstruct kvec *iov = rqst->rq_iov;\n\tint n_vec = rqst->rq_nvec;\n\tunsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);\n\tunsigned int i;\n\tsize_t total_len = 0, sent;\n\tstruct socket *ssocket = server->ssocket;\n\tint val = 1;\n\n\tif (ssocket == NULL)\n\t\treturn -ENOTSOCK;\n\n\tcFYI(1, \"Sending smb: smb_len=%u\", smb_buf_length);\n\tdump_smb(iov[0].iov_base, iov[0].iov_len);\n\n\t/* cork the socket */\n\tkernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,\n\t\t\t\t(char *)&val, sizeof(val));\n\n\trc = smb_send_kvec(server, iov, n_vec, &sent);\n\tif (rc < 0)\n\t\tgoto uncork;\n\n\ttotal_len += sent;\n\n\t/* now walk the page array and send each page in it */\n\tfor (i = 0; i < rqst->rq_npages; i++) {\n\t\tstruct kvec p_iov;\n\n\t\tcifs_rqst_page_to_kvec(rqst, i, &p_iov);\n\t\trc = smb_send_kvec(server, &p_iov, 1, &sent);\n\t\tkunmap(rqst->rq_pages[i]);\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\ttotal_len += sent;\n\t}\n\nuncork:\n\t/* uncork it */\n\tval = 0;\n\tkernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,\n\t\t\t\t(char *)&val, sizeof(val));\n\n\tif ((total_len > 0) && (total_len != smb_buf_length + 4)) {\n\t\tcFYI(1, \"partial send (wanted=%u sent=%zu): terminating \"\n\t\t\t\"session\", smb_buf_length + 4, total_len);\n\t\t/*\n\t\t * If we have only sent part of an SMB then the next SMB could\n\t\t * be taken as the remainder of this one. We need to kill the\n\t\t * socket so the server throws away the partial SMB\n\t\t */\n\t\tserver->tcpStatus = CifsNeedReconnect;\n\t}\n\n\tif (rc < 0 && rc != -EINTR)\n\t\tcERROR(1, \"Error %d sending data on socket to server\", rc);\n\telse\n\t\trc = 0;\n\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,9 @@\n \tsize_t total_len = 0, sent;\n \tstruct socket *ssocket = server->ssocket;\n \tint val = 1;\n+\n+\tif (ssocket == NULL)\n+\t\treturn -ENOTSOCK;\n \n \tcFYI(1, \"Sending smb: smb_len=%u\", smb_buf_length);\n \tdump_smb(iov[0].iov_base, iov[0].iov_len);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (ssocket == NULL)",
                "\t\treturn -ENOTSOCK;"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-4348",
        "func_name": "torvalds/linux/sctp_rcv",
        "description": "Race condition in the sctp_rcv function in net/sctp/input.c in the Linux kernel before 2.6.29 allows remote attackers to cause a denial of service (system hang) via SCTP packets.  NOTE: in some environments, this issue exists because of an incomplete fix for CVE-2011-2482.",
        "git_url": "https://github.com/torvalds/linux/commit/ae53b5bd77719fed58086c5be60ce4f22bffe1c6",
        "commit_title": "sctp: Fix another socket race during accept/peeloff",
        "commit_text": " There is a race between sctp_rcv() and sctp_accept() where we have moved the association from the listening socket to the accepted socket, but sctp_rcv() processing cached the old socket and continues to use it.  The easy solution is to check for the socket mismatch once we've grabed the socket lock.  If we hit a mis-match, that means that were are currently holding the lock on the listening socket, but the association is refrencing a newly accepted socket.  We need to drop the lock on the old socket and grab the lock on the new one.  A more proper solution might be to create accepted sockets when the new association is established, similar to TCP.  That would eliminate the race for 1-to-1 style sockets, but it would still existing for 1-to-many sockets where a user wished to peeloff an association.  For now, we'll live with this easy solution as it addresses the problem. ",
        "func_before": "int sctp_rcv(struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tstruct sctp_association *asoc;\n\tstruct sctp_endpoint *ep = NULL;\n\tstruct sctp_ep_common *rcvr;\n\tstruct sctp_transport *transport = NULL;\n\tstruct sctp_chunk *chunk;\n\tstruct sctphdr *sh;\n\tunion sctp_addr src;\n\tunion sctp_addr dest;\n\tint family;\n\tstruct sctp_af *af;\n\n\tif (skb->pkt_type!=PACKET_HOST)\n\t\tgoto discard_it;\n\n\tSCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);\n\n\tif (skb_linearize(skb))\n\t\tgoto discard_it;\n\n\tsh = sctp_hdr(skb);\n\n\t/* Pull up the IP and SCTP headers. */\n\t__skb_pull(skb, skb_transport_offset(skb));\n\tif (skb->len < sizeof(struct sctphdr))\n\t\tgoto discard_it;\n\tif (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)\n\t\tgoto discard_it;\n\n\tskb_pull(skb, sizeof(struct sctphdr));\n\n\t/* Make sure we at least have chunk headers worth of data left. */\n\tif (skb->len < sizeof(struct sctp_chunkhdr))\n\t\tgoto discard_it;\n\n\tfamily = ipver2af(ip_hdr(skb)->version);\n\taf = sctp_get_af_specific(family);\n\tif (unlikely(!af))\n\t\tgoto discard_it;\n\n\t/* Initialize local addresses for lookups. */\n\taf->from_skb(&src, skb, 1);\n\taf->from_skb(&dest, skb, 0);\n\n\t/* If the packet is to or from a non-unicast address,\n\t * silently discard the packet.\n\t *\n\t * This is not clearly defined in the RFC except in section\n\t * 8.4 - OOTB handling.  However, based on the book \"Stream Control\n\t * Transmission Protocol\" 2.1, \"It is important to note that the\n\t * IP address of an SCTP transport address must be a routable\n\t * unicast address.  In other words, IP multicast addresses and\n\t * IP broadcast addresses cannot be used in an SCTP transport\n\t * address.\"\n\t */\n\tif (!af->addr_valid(&src, NULL, skb) ||\n\t    !af->addr_valid(&dest, NULL, skb))\n\t\tgoto discard_it;\n\n\tasoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);\n\n\tif (!asoc)\n\t\tep = __sctp_rcv_lookup_endpoint(&dest);\n\n\t/* Retrieve the common input handling substructure. */\n\trcvr = asoc ? &asoc->base : &ep->base;\n\tsk = rcvr->sk;\n\n\t/*\n\t * If a frame arrives on an interface and the receiving socket is\n\t * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB\n\t */\n\tif (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))\n\t{\n\t\tif (asoc) {\n\t\t\tsctp_association_put(asoc);\n\t\t\tasoc = NULL;\n\t\t} else {\n\t\t\tsctp_endpoint_put(ep);\n\t\t\tep = NULL;\n\t\t}\n\t\tsk = sctp_get_ctl_sock();\n\t\tep = sctp_sk(sk)->ep;\n\t\tsctp_endpoint_hold(ep);\n\t\trcvr = &ep->base;\n\t}\n\n\t/*\n\t * RFC 2960, 8.4 - Handle \"Out of the blue\" Packets.\n\t * An SCTP packet is called an \"out of the blue\" (OOTB)\n\t * packet if it is correctly formed, i.e., passed the\n\t * receiver's checksum check, but the receiver is not\n\t * able to identify the association to which this\n\t * packet belongs.\n\t */\n\tif (!asoc) {\n\t\tif (sctp_rcv_ootb(skb)) {\n\t\t\tSCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);\n\t\t\tgoto discard_release;\n\t\t}\n\t}\n\n\tif (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))\n\t\tgoto discard_release;\n\tnf_reset(skb);\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_release;\n\n\t/* Create an SCTP packet structure. */\n\tchunk = sctp_chunkify(skb, asoc, sk);\n\tif (!chunk)\n\t\tgoto discard_release;\n\tSCTP_INPUT_CB(skb)->chunk = chunk;\n\n\t/* Remember what endpoint is to handle this packet. */\n\tchunk->rcvr = rcvr;\n\n\t/* Remember the SCTP header. */\n\tchunk->sctp_hdr = sh;\n\n\t/* Set the source and destination addresses of the incoming chunk.  */\n\tsctp_init_addrs(chunk, &src, &dest);\n\n\t/* Remember where we came from.  */\n\tchunk->transport = transport;\n\n\t/* Acquire access to the sock lock. Note: We are safe from other\n\t * bottom halves on this lock, but a user may be in the lock too,\n\t * so check if it is busy.\n\t */\n\tsctp_bh_lock_sock(sk);\n\n\tif (sock_owned_by_user(sk)) {\n\t\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);\n\t\tsctp_add_backlog(sk, skb);\n\t} else {\n\t\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);\n\t\tsctp_inq_push(&chunk->rcvr->inqueue, chunk);\n\t}\n\n\tsctp_bh_unlock_sock(sk);\n\n\t/* Release the asoc/ep ref we took in the lookup calls. */\n\tif (asoc)\n\t\tsctp_association_put(asoc);\n\telse\n\t\tsctp_endpoint_put(ep);\n\n\treturn 0;\n\ndiscard_it:\n\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_release:\n\t/* Release the asoc/ep ref we took in the lookup calls. */\n\tif (asoc)\n\t\tsctp_association_put(asoc);\n\telse\n\t\tsctp_endpoint_put(ep);\n\n\tgoto discard_it;\n}",
        "func": "int sctp_rcv(struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tstruct sctp_association *asoc;\n\tstruct sctp_endpoint *ep = NULL;\n\tstruct sctp_ep_common *rcvr;\n\tstruct sctp_transport *transport = NULL;\n\tstruct sctp_chunk *chunk;\n\tstruct sctphdr *sh;\n\tunion sctp_addr src;\n\tunion sctp_addr dest;\n\tint family;\n\tstruct sctp_af *af;\n\n\tif (skb->pkt_type!=PACKET_HOST)\n\t\tgoto discard_it;\n\n\tSCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);\n\n\tif (skb_linearize(skb))\n\t\tgoto discard_it;\n\n\tsh = sctp_hdr(skb);\n\n\t/* Pull up the IP and SCTP headers. */\n\t__skb_pull(skb, skb_transport_offset(skb));\n\tif (skb->len < sizeof(struct sctphdr))\n\t\tgoto discard_it;\n\tif (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)\n\t\tgoto discard_it;\n\n\tskb_pull(skb, sizeof(struct sctphdr));\n\n\t/* Make sure we at least have chunk headers worth of data left. */\n\tif (skb->len < sizeof(struct sctp_chunkhdr))\n\t\tgoto discard_it;\n\n\tfamily = ipver2af(ip_hdr(skb)->version);\n\taf = sctp_get_af_specific(family);\n\tif (unlikely(!af))\n\t\tgoto discard_it;\n\n\t/* Initialize local addresses for lookups. */\n\taf->from_skb(&src, skb, 1);\n\taf->from_skb(&dest, skb, 0);\n\n\t/* If the packet is to or from a non-unicast address,\n\t * silently discard the packet.\n\t *\n\t * This is not clearly defined in the RFC except in section\n\t * 8.4 - OOTB handling.  However, based on the book \"Stream Control\n\t * Transmission Protocol\" 2.1, \"It is important to note that the\n\t * IP address of an SCTP transport address must be a routable\n\t * unicast address.  In other words, IP multicast addresses and\n\t * IP broadcast addresses cannot be used in an SCTP transport\n\t * address.\"\n\t */\n\tif (!af->addr_valid(&src, NULL, skb) ||\n\t    !af->addr_valid(&dest, NULL, skb))\n\t\tgoto discard_it;\n\n\tasoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);\n\n\tif (!asoc)\n\t\tep = __sctp_rcv_lookup_endpoint(&dest);\n\n\t/* Retrieve the common input handling substructure. */\n\trcvr = asoc ? &asoc->base : &ep->base;\n\tsk = rcvr->sk;\n\n\t/*\n\t * If a frame arrives on an interface and the receiving socket is\n\t * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB\n\t */\n\tif (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))\n\t{\n\t\tif (asoc) {\n\t\t\tsctp_association_put(asoc);\n\t\t\tasoc = NULL;\n\t\t} else {\n\t\t\tsctp_endpoint_put(ep);\n\t\t\tep = NULL;\n\t\t}\n\t\tsk = sctp_get_ctl_sock();\n\t\tep = sctp_sk(sk)->ep;\n\t\tsctp_endpoint_hold(ep);\n\t\trcvr = &ep->base;\n\t}\n\n\t/*\n\t * RFC 2960, 8.4 - Handle \"Out of the blue\" Packets.\n\t * An SCTP packet is called an \"out of the blue\" (OOTB)\n\t * packet if it is correctly formed, i.e., passed the\n\t * receiver's checksum check, but the receiver is not\n\t * able to identify the association to which this\n\t * packet belongs.\n\t */\n\tif (!asoc) {\n\t\tif (sctp_rcv_ootb(skb)) {\n\t\t\tSCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);\n\t\t\tgoto discard_release;\n\t\t}\n\t}\n\n\tif (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))\n\t\tgoto discard_release;\n\tnf_reset(skb);\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_release;\n\n\t/* Create an SCTP packet structure. */\n\tchunk = sctp_chunkify(skb, asoc, sk);\n\tif (!chunk)\n\t\tgoto discard_release;\n\tSCTP_INPUT_CB(skb)->chunk = chunk;\n\n\t/* Remember what endpoint is to handle this packet. */\n\tchunk->rcvr = rcvr;\n\n\t/* Remember the SCTP header. */\n\tchunk->sctp_hdr = sh;\n\n\t/* Set the source and destination addresses of the incoming chunk.  */\n\tsctp_init_addrs(chunk, &src, &dest);\n\n\t/* Remember where we came from.  */\n\tchunk->transport = transport;\n\n\t/* Acquire access to the sock lock. Note: We are safe from other\n\t * bottom halves on this lock, but a user may be in the lock too,\n\t * so check if it is busy.\n\t */\n\tsctp_bh_lock_sock(sk);\n\n\tif (sk != rcvr->sk) {\n\t\t/* Our cached sk is different from the rcvr->sk.  This is\n\t\t * because migrate()/accept() may have moved the association\n\t\t * to a new socket and released all the sockets.  So now we\n\t\t * are holding a lock on the old socket while the user may\n\t\t * be doing something with the new socket.  Switch our veiw\n\t\t * of the current sk.\n\t\t */\n\t\tsctp_bh_unlock_sock(sk);\n\t\tsk = rcvr->sk;\n\t\tsctp_bh_lock_sock(sk);\n\t}\n\n\tif (sock_owned_by_user(sk)) {\n\t\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);\n\t\tsctp_add_backlog(sk, skb);\n\t} else {\n\t\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);\n\t\tsctp_inq_push(&chunk->rcvr->inqueue, chunk);\n\t}\n\n\tsctp_bh_unlock_sock(sk);\n\n\t/* Release the asoc/ep ref we took in the lookup calls. */\n\tif (asoc)\n\t\tsctp_association_put(asoc);\n\telse\n\t\tsctp_endpoint_put(ep);\n\n\treturn 0;\n\ndiscard_it:\n\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_release:\n\t/* Release the asoc/ep ref we took in the lookup calls. */\n\tif (asoc)\n\t\tsctp_association_put(asoc);\n\telse\n\t\tsctp_endpoint_put(ep);\n\n\tgoto discard_it;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -133,6 +133,19 @@\n \t */\n \tsctp_bh_lock_sock(sk);\n \n+\tif (sk != rcvr->sk) {\n+\t\t/* Our cached sk is different from the rcvr->sk.  This is\n+\t\t * because migrate()/accept() may have moved the association\n+\t\t * to a new socket and released all the sockets.  So now we\n+\t\t * are holding a lock on the old socket while the user may\n+\t\t * be doing something with the new socket.  Switch our veiw\n+\t\t * of the current sk.\n+\t\t */\n+\t\tsctp_bh_unlock_sock(sk);\n+\t\tsk = rcvr->sk;\n+\t\tsctp_bh_lock_sock(sk);\n+\t}\n+\n \tif (sock_owned_by_user(sk)) {\n \t\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);\n \t\tsctp_add_backlog(sk, skb);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (sk != rcvr->sk) {",
                "\t\t/* Our cached sk is different from the rcvr->sk.  This is",
                "\t\t * because migrate()/accept() may have moved the association",
                "\t\t * to a new socket and released all the sockets.  So now we",
                "\t\t * are holding a lock on the old socket while the user may",
                "\t\t * be doing something with the new socket.  Switch our veiw",
                "\t\t * of the current sk.",
                "\t\t */",
                "\t\tsctp_bh_unlock_sock(sk);",
                "\t\tsk = rcvr->sk;",
                "\t\tsctp_bh_lock_sock(sk);",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2013-7026",
        "func_name": "torvalds/linux/do_shmat",
        "description": "Multiple race conditions in ipc/shm.c in the Linux kernel before 3.12.2 allow local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via a crafted application that uses shmctl IPC_RMID operations in conjunction with other shm system calls.",
        "git_url": "https://github.com/torvalds/linux/commit/a399b29dfbaaaf91162b2dc5a5875dd51bbfa2a1",
        "commit_title": "ipc,shm: fix shm_file deletion races",
        "commit_text": " When IPC_RMID races with other shm operations there's potential for use-after-free of the shm object's associated file (shm_file).  Here's the race before this patch:    TASK 1                     TASK 2   ------                     ------   shm_rmid()     ipc_lock_object()                              shmctl()                              shp = shm_obtain_object_check()      shm_destroy()       shum_unlock()       fput(shp->shm_file)                              ipc_lock_object()                              shmem_lock(shp->shm_file)                              <OOPS>  The oops is caused because shm_destroy() calls fput() after dropping the ipc_lock.  fput() clears the file's f_inode, f_path.dentry, and f_path.mnt, which causes various NULL pointer references in task 2.  I reliably see the oops in task 2 if with shmlock, shmu  This patch fixes the races by: 1) set shm_file=NULL in shm_destroy() while holding ipc_object_lock(). 2) modify at risk operations to check shm_file while holding    ipc_object_lock().  Example workloads, which each trigger oops...  Workload 1:   while true; do     id=$(shmget 1 4096)     shm_rmid $id &     shmlock $id &     wait   done    The oops stack shows accessing NULL f_inode due to racing fput:     _raw_spin_lock     shmem_lock     SyS_shmctl  Workload 2:   while true; do     id=$(shmget 1 4096)     shmat $id 4096 &     shm_rmid $id &     wait   done    The oops stack is similar to workload 1 due to NULL f_inode:     touch_atime     shmem_mmap     shm_mmap     mmap_region     do_mmap_pgoff     do_shmat     SyS_shmat  Workload 3:   while true; do     id=$(shmget 1 4096)     shmlock $id     shm_rmid $id &     shmunlock $id &     wait   done    The oops stack shows second fput tripping on an NULL f_inode.  The   first fput() completed via from shm_destroy(), but a racing thread did   a get_file() and queued this fput():     locks_remove_flock     __fput     ____fput     task_work_run     do_notify_resume     int_signal  Cc: Davidlohr Bueso <davidlohr@hp.com> Cc: Rik van Riel <riel@redhat.com> Cc: Manfred Spraul <manfred@colorfullife.com> Cc: <stable@vger.kernel.org>  # 3.10.17+ 3.11.6+",
        "func_before": "long do_shmat(int shmid, char __user *shmaddr, int shmflg, ulong *raddr,\n\t      unsigned long shmlba)\n{\n\tstruct shmid_kernel *shp;\n\tunsigned long addr;\n\tunsigned long size;\n\tstruct file * file;\n\tint    err;\n\tunsigned long flags;\n\tunsigned long prot;\n\tint acc_mode;\n\tstruct ipc_namespace *ns;\n\tstruct shm_file_data *sfd;\n\tstruct path path;\n\tfmode_t f_mode;\n\tunsigned long populate = 0;\n\n\terr = -EINVAL;\n\tif (shmid < 0)\n\t\tgoto out;\n\telse if ((addr = (ulong)shmaddr)) {\n\t\tif (addr & (shmlba - 1)) {\n\t\t\tif (shmflg & SHM_RND)\n\t\t\t\taddr &= ~(shmlba - 1);\t   /* round down */\n\t\t\telse\n#ifndef __ARCH_FORCE_SHMLBA\n\t\t\t\tif (addr & ~PAGE_MASK)\n#endif\n\t\t\t\t\tgoto out;\n\t\t}\n\t\tflags = MAP_SHARED | MAP_FIXED;\n\t} else {\n\t\tif ((shmflg & SHM_REMAP))\n\t\t\tgoto out;\n\n\t\tflags = MAP_SHARED;\n\t}\n\n\tif (shmflg & SHM_RDONLY) {\n\t\tprot = PROT_READ;\n\t\tacc_mode = S_IRUGO;\n\t\tf_mode = FMODE_READ;\n\t} else {\n\t\tprot = PROT_READ | PROT_WRITE;\n\t\tacc_mode = S_IRUGO | S_IWUGO;\n\t\tf_mode = FMODE_READ | FMODE_WRITE;\n\t}\n\tif (shmflg & SHM_EXEC) {\n\t\tprot |= PROT_EXEC;\n\t\tacc_mode |= S_IXUGO;\n\t}\n\n\t/*\n\t * We cannot rely on the fs check since SYSV IPC does have an\n\t * additional creator id...\n\t */\n\tns = current->nsproxy->ipc_ns;\n\trcu_read_lock();\n\tshp = shm_obtain_object_check(ns, shmid);\n\tif (IS_ERR(shp)) {\n\t\terr = PTR_ERR(shp);\n\t\tgoto out_unlock;\n\t}\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &shp->shm_perm, acc_mode))\n\t\tgoto out_unlock;\n\n\terr = security_shm_shmat(shp, shmaddr, shmflg);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tipc_lock_object(&shp->shm_perm);\n\tpath = shp->shm_file->f_path;\n\tpath_get(&path);\n\tshp->shm_nattch++;\n\tsize = i_size_read(path.dentry->d_inode);\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\n\terr = -ENOMEM;\n\tsfd = kzalloc(sizeof(*sfd), GFP_KERNEL);\n\tif (!sfd) {\n\t\tpath_put(&path);\n\t\tgoto out_nattch;\n\t}\n\n\tfile = alloc_file(&path, f_mode,\n\t\t\t  is_file_hugepages(shp->shm_file) ?\n\t\t\t\t&shm_file_operations_huge :\n\t\t\t\t&shm_file_operations);\n\terr = PTR_ERR(file);\n\tif (IS_ERR(file)) {\n\t\tkfree(sfd);\n\t\tpath_put(&path);\n\t\tgoto out_nattch;\n\t}\n\n\tfile->private_data = sfd;\n\tfile->f_mapping = shp->shm_file->f_mapping;\n\tsfd->id = shp->shm_perm.id;\n\tsfd->ns = get_ipc_ns(ns);\n\tsfd->file = shp->shm_file;\n\tsfd->vm_ops = NULL;\n\n\terr = security_mmap_file(file, prot, flags);\n\tif (err)\n\t\tgoto out_fput;\n\n\tdown_write(&current->mm->mmap_sem);\n\tif (addr && !(shmflg & SHM_REMAP)) {\n\t\terr = -EINVAL;\n\t\tif (find_vma_intersection(current->mm, addr, addr + size))\n\t\t\tgoto invalid;\n\t\t/*\n\t\t * If shm segment goes below stack, make sure there is some\n\t\t * space left for the stack to grow (at least 4 pages).\n\t\t */\n\t\tif (addr < current->mm->start_stack &&\n\t\t    addr > current->mm->start_stack - size - PAGE_SIZE * 5)\n\t\t\tgoto invalid;\n\t}\n\n\taddr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &populate);\n\t*raddr = addr;\n\terr = 0;\n\tif (IS_ERR_VALUE(addr))\n\t\terr = (long)addr;\ninvalid:\n\tup_write(&current->mm->mmap_sem);\n\tif (populate)\n\t\tmm_populate(addr, populate);\n\nout_fput:\n\tfput(file);\n\nout_nattch:\n\tdown_write(&shm_ids(ns).rwsem);\n\tshp = shm_lock(ns, shmid);\n\tBUG_ON(IS_ERR(shp));\n\tshp->shm_nattch--;\n\tif (shm_may_destroy(ns, shp))\n\t\tshm_destroy(ns, shp);\n\telse\n\t\tshm_unlock(shp);\n\tup_write(&shm_ids(ns).rwsem);\n\treturn err;\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\treturn err;\n}",
        "func": "long do_shmat(int shmid, char __user *shmaddr, int shmflg, ulong *raddr,\n\t      unsigned long shmlba)\n{\n\tstruct shmid_kernel *shp;\n\tunsigned long addr;\n\tunsigned long size;\n\tstruct file * file;\n\tint    err;\n\tunsigned long flags;\n\tunsigned long prot;\n\tint acc_mode;\n\tstruct ipc_namespace *ns;\n\tstruct shm_file_data *sfd;\n\tstruct path path;\n\tfmode_t f_mode;\n\tunsigned long populate = 0;\n\n\terr = -EINVAL;\n\tif (shmid < 0)\n\t\tgoto out;\n\telse if ((addr = (ulong)shmaddr)) {\n\t\tif (addr & (shmlba - 1)) {\n\t\t\tif (shmflg & SHM_RND)\n\t\t\t\taddr &= ~(shmlba - 1);\t   /* round down */\n\t\t\telse\n#ifndef __ARCH_FORCE_SHMLBA\n\t\t\t\tif (addr & ~PAGE_MASK)\n#endif\n\t\t\t\t\tgoto out;\n\t\t}\n\t\tflags = MAP_SHARED | MAP_FIXED;\n\t} else {\n\t\tif ((shmflg & SHM_REMAP))\n\t\t\tgoto out;\n\n\t\tflags = MAP_SHARED;\n\t}\n\n\tif (shmflg & SHM_RDONLY) {\n\t\tprot = PROT_READ;\n\t\tacc_mode = S_IRUGO;\n\t\tf_mode = FMODE_READ;\n\t} else {\n\t\tprot = PROT_READ | PROT_WRITE;\n\t\tacc_mode = S_IRUGO | S_IWUGO;\n\t\tf_mode = FMODE_READ | FMODE_WRITE;\n\t}\n\tif (shmflg & SHM_EXEC) {\n\t\tprot |= PROT_EXEC;\n\t\tacc_mode |= S_IXUGO;\n\t}\n\n\t/*\n\t * We cannot rely on the fs check since SYSV IPC does have an\n\t * additional creator id...\n\t */\n\tns = current->nsproxy->ipc_ns;\n\trcu_read_lock();\n\tshp = shm_obtain_object_check(ns, shmid);\n\tif (IS_ERR(shp)) {\n\t\terr = PTR_ERR(shp);\n\t\tgoto out_unlock;\n\t}\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &shp->shm_perm, acc_mode))\n\t\tgoto out_unlock;\n\n\terr = security_shm_shmat(shp, shmaddr, shmflg);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tipc_lock_object(&shp->shm_perm);\n\n\t/* check if shm_destroy() is tearing down shp */\n\tif (shp->shm_file == NULL) {\n\t\tipc_unlock_object(&shp->shm_perm);\n\t\terr = -EIDRM;\n\t\tgoto out_unlock;\n\t}\n\n\tpath = shp->shm_file->f_path;\n\tpath_get(&path);\n\tshp->shm_nattch++;\n\tsize = i_size_read(path.dentry->d_inode);\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\n\terr = -ENOMEM;\n\tsfd = kzalloc(sizeof(*sfd), GFP_KERNEL);\n\tif (!sfd) {\n\t\tpath_put(&path);\n\t\tgoto out_nattch;\n\t}\n\n\tfile = alloc_file(&path, f_mode,\n\t\t\t  is_file_hugepages(shp->shm_file) ?\n\t\t\t\t&shm_file_operations_huge :\n\t\t\t\t&shm_file_operations);\n\terr = PTR_ERR(file);\n\tif (IS_ERR(file)) {\n\t\tkfree(sfd);\n\t\tpath_put(&path);\n\t\tgoto out_nattch;\n\t}\n\n\tfile->private_data = sfd;\n\tfile->f_mapping = shp->shm_file->f_mapping;\n\tsfd->id = shp->shm_perm.id;\n\tsfd->ns = get_ipc_ns(ns);\n\tsfd->file = shp->shm_file;\n\tsfd->vm_ops = NULL;\n\n\terr = security_mmap_file(file, prot, flags);\n\tif (err)\n\t\tgoto out_fput;\n\n\tdown_write(&current->mm->mmap_sem);\n\tif (addr && !(shmflg & SHM_REMAP)) {\n\t\terr = -EINVAL;\n\t\tif (find_vma_intersection(current->mm, addr, addr + size))\n\t\t\tgoto invalid;\n\t\t/*\n\t\t * If shm segment goes below stack, make sure there is some\n\t\t * space left for the stack to grow (at least 4 pages).\n\t\t */\n\t\tif (addr < current->mm->start_stack &&\n\t\t    addr > current->mm->start_stack - size - PAGE_SIZE * 5)\n\t\t\tgoto invalid;\n\t}\n\n\taddr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &populate);\n\t*raddr = addr;\n\terr = 0;\n\tif (IS_ERR_VALUE(addr))\n\t\terr = (long)addr;\ninvalid:\n\tup_write(&current->mm->mmap_sem);\n\tif (populate)\n\t\tmm_populate(addr, populate);\n\nout_fput:\n\tfput(file);\n\nout_nattch:\n\tdown_write(&shm_ids(ns).rwsem);\n\tshp = shm_lock(ns, shmid);\n\tBUG_ON(IS_ERR(shp));\n\tshp->shm_nattch--;\n\tif (shm_may_destroy(ns, shp))\n\t\tshm_destroy(ns, shp);\n\telse\n\t\tshm_unlock(shp);\n\tup_write(&shm_ids(ns).rwsem);\n\treturn err;\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -71,6 +71,14 @@\n \t\tgoto out_unlock;\n \n \tipc_lock_object(&shp->shm_perm);\n+\n+\t/* check if shm_destroy() is tearing down shp */\n+\tif (shp->shm_file == NULL) {\n+\t\tipc_unlock_object(&shp->shm_perm);\n+\t\terr = -EIDRM;\n+\t\tgoto out_unlock;\n+\t}\n+\n \tpath = shp->shm_file->f_path;\n \tpath_get(&path);\n \tshp->shm_nattch++;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t/* check if shm_destroy() is tearing down shp */",
                "\tif (shp->shm_file == NULL) {",
                "\t\tipc_unlock_object(&shp->shm_perm);",
                "\t\terr = -EIDRM;",
                "\t\tgoto out_unlock;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2013-7026",
        "func_name": "torvalds/linux/shm_destroy",
        "description": "Multiple race conditions in ipc/shm.c in the Linux kernel before 3.12.2 allow local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via a crafted application that uses shmctl IPC_RMID operations in conjunction with other shm system calls.",
        "git_url": "https://github.com/torvalds/linux/commit/a399b29dfbaaaf91162b2dc5a5875dd51bbfa2a1",
        "commit_title": "ipc,shm: fix shm_file deletion races",
        "commit_text": " When IPC_RMID races with other shm operations there's potential for use-after-free of the shm object's associated file (shm_file).  Here's the race before this patch:    TASK 1                     TASK 2   ------                     ------   shm_rmid()     ipc_lock_object()                              shmctl()                              shp = shm_obtain_object_check()      shm_destroy()       shum_unlock()       fput(shp->shm_file)                              ipc_lock_object()                              shmem_lock(shp->shm_file)                              <OOPS>  The oops is caused because shm_destroy() calls fput() after dropping the ipc_lock.  fput() clears the file's f_inode, f_path.dentry, and f_path.mnt, which causes various NULL pointer references in task 2.  I reliably see the oops in task 2 if with shmlock, shmu  This patch fixes the races by: 1) set shm_file=NULL in shm_destroy() while holding ipc_object_lock(). 2) modify at risk operations to check shm_file while holding    ipc_object_lock().  Example workloads, which each trigger oops...  Workload 1:   while true; do     id=$(shmget 1 4096)     shm_rmid $id &     shmlock $id &     wait   done    The oops stack shows accessing NULL f_inode due to racing fput:     _raw_spin_lock     shmem_lock     SyS_shmctl  Workload 2:   while true; do     id=$(shmget 1 4096)     shmat $id 4096 &     shm_rmid $id &     wait   done    The oops stack is similar to workload 1 due to NULL f_inode:     touch_atime     shmem_mmap     shm_mmap     mmap_region     do_mmap_pgoff     do_shmat     SyS_shmat  Workload 3:   while true; do     id=$(shmget 1 4096)     shmlock $id     shm_rmid $id &     shmunlock $id &     wait   done    The oops stack shows second fput tripping on an NULL f_inode.  The   first fput() completed via from shm_destroy(), but a racing thread did   a get_file() and queued this fput():     locks_remove_flock     __fput     ____fput     task_work_run     do_notify_resume     int_signal  Cc: Davidlohr Bueso <davidlohr@hp.com> Cc: Rik van Riel <riel@redhat.com> Cc: Manfred Spraul <manfred@colorfullife.com> Cc: <stable@vger.kernel.org>  # 3.10.17+ 3.11.6+",
        "func_before": "static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)\n{\n\tns->shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tshm_rmid(ns, shp);\n\tshm_unlock(shp);\n\tif (!is_file_hugepages(shp->shm_file))\n\t\tshmem_lock(shp->shm_file, 0, shp->mlock_user);\n\telse if (shp->mlock_user)\n\t\tuser_shm_unlock(file_inode(shp->shm_file)->i_size,\n\t\t\t\t\t\tshp->mlock_user);\n\tfput (shp->shm_file);\n\tipc_rcu_putref(shp, shm_rcu_free);\n}",
        "func": "static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)\n{\n\tstruct file *shm_file;\n\n\tshm_file = shp->shm_file;\n\tshp->shm_file = NULL;\n\tns->shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tshm_rmid(ns, shp);\n\tshm_unlock(shp);\n\tif (!is_file_hugepages(shm_file))\n\t\tshmem_lock(shm_file, 0, shp->mlock_user);\n\telse if (shp->mlock_user)\n\t\tuser_shm_unlock(file_inode(shm_file)->i_size, shp->mlock_user);\n\tfput(shm_file);\n\tipc_rcu_putref(shp, shm_rcu_free);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,13 +1,16 @@\n static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)\n {\n+\tstruct file *shm_file;\n+\n+\tshm_file = shp->shm_file;\n+\tshp->shm_file = NULL;\n \tns->shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;\n \tshm_rmid(ns, shp);\n \tshm_unlock(shp);\n-\tif (!is_file_hugepages(shp->shm_file))\n-\t\tshmem_lock(shp->shm_file, 0, shp->mlock_user);\n+\tif (!is_file_hugepages(shm_file))\n+\t\tshmem_lock(shm_file, 0, shp->mlock_user);\n \telse if (shp->mlock_user)\n-\t\tuser_shm_unlock(file_inode(shp->shm_file)->i_size,\n-\t\t\t\t\t\tshp->mlock_user);\n-\tfput (shp->shm_file);\n+\t\tuser_shm_unlock(file_inode(shm_file)->i_size, shp->mlock_user);\n+\tfput(shm_file);\n \tipc_rcu_putref(shp, shm_rcu_free);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!is_file_hugepages(shp->shm_file))",
                "\t\tshmem_lock(shp->shm_file, 0, shp->mlock_user);",
                "\t\tuser_shm_unlock(file_inode(shp->shm_file)->i_size,",
                "\t\t\t\t\t\tshp->mlock_user);",
                "\tfput (shp->shm_file);"
            ],
            "added_lines": [
                "\tstruct file *shm_file;",
                "",
                "\tshm_file = shp->shm_file;",
                "\tshp->shm_file = NULL;",
                "\tif (!is_file_hugepages(shm_file))",
                "\t\tshmem_lock(shm_file, 0, shp->mlock_user);",
                "\t\tuser_shm_unlock(file_inode(shm_file)->i_size, shp->mlock_user);",
                "\tfput(shm_file);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-2672",
        "func_name": "torvalds/linux/ath_tx_aggr_sleep",
        "description": "Race condition in the ath_tx_aggr_sleep function in drivers/net/wireless/ath/ath9k/xmit.c in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via a large amount of network traffic that triggers certain list deletions.",
        "git_url": "https://github.com/torvalds/linux/commit/21f8aaee0c62708654988ce092838aa7df4d25d8",
        "commit_title": "ath9k: protect tid->sched check",
        "commit_text": " We check tid->sched without a lock taken on ath_tx_aggr_sleep(). That is race condition which can result of doing list_del(&tid->list) twice (second time with poisoned list node) and cause crash like shown below:  [424271.637220] BUG: unable to handle kernel paging request at 00100104 [424271.637328] IP: [<f90fc072>] ath_tx_aggr_sleep+0x62/0xe0 [ath9k] ... [424271.639953] Call Trace: [424271.639998]  [<f90f6900>] ? ath9k_get_survey+0x110/0x110 [ath9k] [424271.640083]  [<f90f6942>] ath9k_sta_notify+0x42/0x50 [ath9k] [424271.640177]  [<f809cfef>] sta_ps_start+0x8f/0x1c0 [mac80211] [424271.640258]  [<c10f730e>] ? free_compound_page+0x2e/0x40 [424271.640346]  [<f809e915>] ieee80211_rx_handlers+0x9d5/0x2340 [mac80211] [424271.640437]  [<c112f048>] ? kmem_cache_free+0x1d8/0x1f0 [424271.640510]  [<c1345a84>] ? kfree_skbmem+0x34/0x90 [424271.640578]  [<c10fc23c>] ? put_page+0x2c/0x40 [424271.640640]  [<c1345a84>] ? kfree_skbmem+0x34/0x90 [424271.640706]  [<c1345a84>] ? kfree_skbmem+0x34/0x90 [424271.640787]  [<f809dde3>] ? ieee80211_rx_handlers_result+0x73/0x1d0 [mac80211] [424271.640897]  [<f80a07a0>] ieee80211_prepare_and_rx_handle+0x520/0xad0 [mac80211] [424271.641009]  [<f809e22d>] ? ieee80211_rx_handlers+0x2ed/0x2340 [mac80211] [424271.641104]  [<c13846ce>] ? ip_output+0x7e/0xd0 [424271.641182]  [<f80a1057>] ieee80211_rx+0x307/0x7c0 [mac80211] [424271.641266]  [<f90fa6ee>] ath_rx_tasklet+0x88e/0xf70 [ath9k] [424271.641358]  [<f80a0f2c>] ? ieee80211_rx+0x1dc/0x7c0 [mac80211] [424271.641445]  [<f90f82db>] ath9k_tasklet+0xcb/0x130 [ath9k]  Bug report: https://bugzilla.kernel.org/show_bug.cgi?id=70551  Reported-and-tested-by: Max Sydorenko <maxim.stargazer@gmail.com> Cc: stable@vger.kernel.org",
        "func_before": "void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,\n\t\t       struct ath_node *an)\n{\n\tstruct ath_atx_tid *tid;\n\tstruct ath_atx_ac *ac;\n\tstruct ath_txq *txq;\n\tbool buffered;\n\tint tidno;\n\n\tfor (tidno = 0, tid = &an->tid[tidno];\n\t     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {\n\n\t\tif (!tid->sched)\n\t\t\tcontinue;\n\n\t\tac = tid->ac;\n\t\ttxq = ac->txq;\n\n\t\tath_txq_lock(sc, txq);\n\n\t\tbuffered = ath_tid_has_buffered(tid);\n\n\t\ttid->sched = false;\n\t\tlist_del(&tid->list);\n\n\t\tif (ac->sched) {\n\t\t\tac->sched = false;\n\t\t\tlist_del(&ac->list);\n\t\t}\n\n\t\tath_txq_unlock(sc, txq);\n\n\t\tieee80211_sta_set_buffered(sta, tidno, buffered);\n\t}\n}",
        "func": "void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,\n\t\t       struct ath_node *an)\n{\n\tstruct ath_atx_tid *tid;\n\tstruct ath_atx_ac *ac;\n\tstruct ath_txq *txq;\n\tbool buffered;\n\tint tidno;\n\n\tfor (tidno = 0, tid = &an->tid[tidno];\n\t     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {\n\n\t\tac = tid->ac;\n\t\ttxq = ac->txq;\n\n\t\tath_txq_lock(sc, txq);\n\n\t\tif (!tid->sched) {\n\t\t\tath_txq_unlock(sc, txq);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbuffered = ath_tid_has_buffered(tid);\n\n\t\ttid->sched = false;\n\t\tlist_del(&tid->list);\n\n\t\tif (ac->sched) {\n\t\t\tac->sched = false;\n\t\t\tlist_del(&ac->list);\n\t\t}\n\n\t\tath_txq_unlock(sc, txq);\n\n\t\tieee80211_sta_set_buffered(sta, tidno, buffered);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,13 +10,15 @@\n \tfor (tidno = 0, tid = &an->tid[tidno];\n \t     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {\n \n-\t\tif (!tid->sched)\n-\t\t\tcontinue;\n-\n \t\tac = tid->ac;\n \t\ttxq = ac->txq;\n \n \t\tath_txq_lock(sc, txq);\n+\n+\t\tif (!tid->sched) {\n+\t\t\tath_txq_unlock(sc, txq);\n+\t\t\tcontinue;\n+\t\t}\n \n \t\tbuffered = ath_tid_has_buffered(tid);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (!tid->sched)",
                "\t\t\tcontinue;",
                ""
            ],
            "added_lines": [
                "",
                "\t\tif (!tid->sched) {",
                "\t\t\tath_txq_unlock(sc, txq);",
                "\t\t\tcontinue;",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-2706",
        "func_name": "torvalds/linux/sta_info_alloc",
        "description": "Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c.",
        "git_url": "https://github.com/torvalds/linux/commit/1d147bfa64293b2723c4fec50922168658e613ba",
        "commit_title": "mac80211: fix AP powersave TX vs. wakeup race",
        "commit_text": " There is a race between the TX path and the STA wakeup: while a station is sleeping, mac80211 buffers frames until it wakes up, then the frames are transmitted. However, the RX and TX path are concurrent, so the packet indicating wakeup can be processed while a packet is being transmitted.  This can lead to a situation where the buffered frames list is emptied on the one side, while a frame is being added on the other side, as the station is still seen as sleeping in the TX path.  As a result, the newly added frame will not be send anytime soon. It might be sent much later (and out of order) when the station goes to sleep and wakes up the next time.  Additionally, it can lead to the crash below.  Fix all this by synchronising both paths with a new lock. Both path are not fastpath since they handle PS situations.  In a later patch we'll remove the extra skb queue locks to reduce locking overhead.  BUG: unable to handle kernel NULL pointer dereference at 000000b0 IP: [<ff6f1791>] ieee80211_report_used_skb+0x11/0x3e0 [mac80211] *pde = 00000000 Oops: 0000 [#1] SMP DEBUG_PAGEALLOC EIP: 0060:[<ff6f1791>] EFLAGS: 00210282 CPU: 1 EIP is at ieee80211_report_used_skb+0x11/0x3e0 [mac80211] EAX: e5900da0 EBX: 00000000 ECX: 00000001 EDX: 00000000 ESI: e41d00c0 EDI: e5900da0 EBP: ebe458e4 ESP: ebe458b0  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068 CR0: 8005003b CR2: 000000b0 CR3: 25a78000 CR4: 000407d0 DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000 DR6: ffff0ff0 DR7: 00000400 Process iperf (pid: 3934, ti=ebe44000 task=e757c0b0 task.ti=ebe44000) iwlwifi 0000:02:00.0: I iwl_pcie_enqueue_hcmd Sending command LQ_CMD (#4e), seq: 0x0903, 92 bytes at 3[3]:9 Stack:  e403b32c ebe458c4 00200002 00200286 e403b338 ebe458cc c10960bb e5900da0  ff76a6ec ebe458d8 00000000 e41d00c0 e5900da0 ebe458f0 ff6f1b75 e403b210  ebe4598c ff723dc1 00000000 ff76a6ec e597c978 e403b758 00000002 00000002 Call Trace:  [<ff6f1b75>] ieee80211_free_txskb+0x15/0x20 [mac80211]  [<ff723dc1>] invoke_tx_handlers+0x1661/0x1780 [mac80211]  [<ff7248a5>] ieee80211_tx+0x75/0x100 [mac80211]  [<ff7249bf>] ieee80211_xmit+0x8f/0xc0 [mac80211]  [<ff72550e>] ieee80211_subif_start_xmit+0x4fe/0xe20 [mac80211]  [<c149ef70>] dev_hard_start_xmit+0x450/0x950  [<c14b9aa9>] sch_direct_xmit+0xa9/0x250  [<c14b9c9b>] __qdisc_run+0x4b/0x150  [<c149f732>] dev_queue_xmit+0x2c2/0xca0  Cc: stable@vger.kernel.org [reword commit log, use a separate lock]",
        "func_before": "struct sta_info *sta_info_alloc(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tconst u8 *addr, gfp_t gfp)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\tstruct timespec uptime;\n\tstruct ieee80211_tx_latency_bin_ranges *tx_latency;\n\tint i;\n\n\tsta = kzalloc(sizeof(*sta) + local->hw.sta_data_size, gfp);\n\tif (!sta)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttx_latency = rcu_dereference(local->tx_latency);\n\t/* init stations Tx latency statistics && TID bins */\n\tif (tx_latency) {\n\t\tsta->tx_lat = kzalloc(IEEE80211_NUM_TIDS *\n\t\t\t\t      sizeof(struct ieee80211_tx_latency_stat),\n\t\t\t\t      GFP_ATOMIC);\n\t\tif (!sta->tx_lat) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto free;\n\t\t}\n\n\t\tif (tx_latency->n_ranges) {\n\t\t\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++) {\n\t\t\t\t/* size of bins is size of the ranges +1 */\n\t\t\t\tsta->tx_lat[i].bin_count =\n\t\t\t\t\ttx_latency->n_ranges + 1;\n\t\t\t\tsta->tx_lat[i].bins =\n\t\t\t\t\tkcalloc(sta->tx_lat[i].bin_count,\n\t\t\t\t\t\tsizeof(u32), GFP_ATOMIC);\n\t\t\t\tif (!sta->tx_lat[i].bins) {\n\t\t\t\t\trcu_read_unlock();\n\t\t\t\t\tgoto free;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tspin_lock_init(&sta->lock);\n\tINIT_WORK(&sta->drv_unblock_wk, sta_unblock);\n\tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n\tmutex_init(&sta->ampdu_mlme.mtx);\n#ifdef CONFIG_MAC80211_MESH\n\tif (ieee80211_vif_is_mesh(&sdata->vif) &&\n\t    !sdata->u.mesh.user_mpm)\n\t\tinit_timer(&sta->plink_timer);\n\tsta->nonpeer_pm = NL80211_MESH_POWER_ACTIVE;\n#endif\n\n\tmemcpy(sta->sta.addr, addr, ETH_ALEN);\n\tsta->local = local;\n\tsta->sdata = sdata;\n\tsta->last_rx = jiffies;\n\n\tsta->sta_state = IEEE80211_STA_NONE;\n\n\tdo_posix_clock_monotonic_gettime(&uptime);\n\tsta->last_connected = uptime.tv_sec;\n\tewma_init(&sta->avg_signal, 1024, 8);\n\tfor (i = 0; i < ARRAY_SIZE(sta->chain_signal_avg); i++)\n\t\tewma_init(&sta->chain_signal_avg[i], 1024, 8);\n\n\tif (sta_prepare_rate_control(local, sta, gfp))\n\t\tgoto free;\n\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++) {\n\t\t/*\n\t\t * timer_to_tid must be initialized with identity mapping\n\t\t * to enable session_timer's data differentiation. See\n\t\t * sta_rx_agg_session_timer_expired for usage.\n\t\t */\n\t\tsta->timer_to_tid[i] = i;\n\t}\n\tfor (i = 0; i < IEEE80211_NUM_ACS; i++) {\n\t\tskb_queue_head_init(&sta->ps_tx_buf[i]);\n\t\tskb_queue_head_init(&sta->tx_filtered[i]);\n\t}\n\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++)\n\t\tsta->last_seq_ctrl[i] = cpu_to_le16(USHRT_MAX);\n\n\tsta->sta.smps_mode = IEEE80211_SMPS_OFF;\n\tif (sdata->vif.type == NL80211_IFTYPE_AP ||\n\t    sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\tstruct ieee80211_supported_band *sband =\n\t\t\tlocal->hw.wiphy->bands[ieee80211_get_sdata_band(sdata)];\n\t\tu8 smps = (sband->ht_cap.cap & IEEE80211_HT_CAP_SM_PS) >>\n\t\t\t\tIEEE80211_HT_CAP_SM_PS_SHIFT;\n\t\t/*\n\t\t * Assume that hostapd advertises our caps in the beacon and\n\t\t * this is the known_smps_mode for a station that just assciated\n\t\t */\n\t\tswitch (smps) {\n\t\tcase WLAN_HT_SMPS_CONTROL_DISABLED:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_OFF;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_STATIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_STATIC;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_DYNAMIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_DYNAMIC;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON(1);\n\t\t}\n\t}\n\n\tsta_dbg(sdata, \"Allocated STA %pM\\n\", sta->sta.addr);\n\treturn sta;\n\nfree:\n\tif (sta->tx_lat) {\n\t\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++)\n\t\t\tkfree(sta->tx_lat[i].bins);\n\t\tkfree(sta->tx_lat);\n\t}\n\tkfree(sta);\n\treturn NULL;\n}",
        "func": "struct sta_info *sta_info_alloc(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tconst u8 *addr, gfp_t gfp)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\tstruct timespec uptime;\n\tstruct ieee80211_tx_latency_bin_ranges *tx_latency;\n\tint i;\n\n\tsta = kzalloc(sizeof(*sta) + local->hw.sta_data_size, gfp);\n\tif (!sta)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttx_latency = rcu_dereference(local->tx_latency);\n\t/* init stations Tx latency statistics && TID bins */\n\tif (tx_latency) {\n\t\tsta->tx_lat = kzalloc(IEEE80211_NUM_TIDS *\n\t\t\t\t      sizeof(struct ieee80211_tx_latency_stat),\n\t\t\t\t      GFP_ATOMIC);\n\t\tif (!sta->tx_lat) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto free;\n\t\t}\n\n\t\tif (tx_latency->n_ranges) {\n\t\t\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++) {\n\t\t\t\t/* size of bins is size of the ranges +1 */\n\t\t\t\tsta->tx_lat[i].bin_count =\n\t\t\t\t\ttx_latency->n_ranges + 1;\n\t\t\t\tsta->tx_lat[i].bins =\n\t\t\t\t\tkcalloc(sta->tx_lat[i].bin_count,\n\t\t\t\t\t\tsizeof(u32), GFP_ATOMIC);\n\t\t\t\tif (!sta->tx_lat[i].bins) {\n\t\t\t\t\trcu_read_unlock();\n\t\t\t\t\tgoto free;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tspin_lock_init(&sta->lock);\n\tspin_lock_init(&sta->ps_lock);\n\tINIT_WORK(&sta->drv_unblock_wk, sta_unblock);\n\tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n\tmutex_init(&sta->ampdu_mlme.mtx);\n#ifdef CONFIG_MAC80211_MESH\n\tif (ieee80211_vif_is_mesh(&sdata->vif) &&\n\t    !sdata->u.mesh.user_mpm)\n\t\tinit_timer(&sta->plink_timer);\n\tsta->nonpeer_pm = NL80211_MESH_POWER_ACTIVE;\n#endif\n\n\tmemcpy(sta->sta.addr, addr, ETH_ALEN);\n\tsta->local = local;\n\tsta->sdata = sdata;\n\tsta->last_rx = jiffies;\n\n\tsta->sta_state = IEEE80211_STA_NONE;\n\n\tdo_posix_clock_monotonic_gettime(&uptime);\n\tsta->last_connected = uptime.tv_sec;\n\tewma_init(&sta->avg_signal, 1024, 8);\n\tfor (i = 0; i < ARRAY_SIZE(sta->chain_signal_avg); i++)\n\t\tewma_init(&sta->chain_signal_avg[i], 1024, 8);\n\n\tif (sta_prepare_rate_control(local, sta, gfp))\n\t\tgoto free;\n\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++) {\n\t\t/*\n\t\t * timer_to_tid must be initialized with identity mapping\n\t\t * to enable session_timer's data differentiation. See\n\t\t * sta_rx_agg_session_timer_expired for usage.\n\t\t */\n\t\tsta->timer_to_tid[i] = i;\n\t}\n\tfor (i = 0; i < IEEE80211_NUM_ACS; i++) {\n\t\tskb_queue_head_init(&sta->ps_tx_buf[i]);\n\t\tskb_queue_head_init(&sta->tx_filtered[i]);\n\t}\n\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++)\n\t\tsta->last_seq_ctrl[i] = cpu_to_le16(USHRT_MAX);\n\n\tsta->sta.smps_mode = IEEE80211_SMPS_OFF;\n\tif (sdata->vif.type == NL80211_IFTYPE_AP ||\n\t    sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\tstruct ieee80211_supported_band *sband =\n\t\t\tlocal->hw.wiphy->bands[ieee80211_get_sdata_band(sdata)];\n\t\tu8 smps = (sband->ht_cap.cap & IEEE80211_HT_CAP_SM_PS) >>\n\t\t\t\tIEEE80211_HT_CAP_SM_PS_SHIFT;\n\t\t/*\n\t\t * Assume that hostapd advertises our caps in the beacon and\n\t\t * this is the known_smps_mode for a station that just assciated\n\t\t */\n\t\tswitch (smps) {\n\t\tcase WLAN_HT_SMPS_CONTROL_DISABLED:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_OFF;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_STATIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_STATIC;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_DYNAMIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_DYNAMIC;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON(1);\n\t\t}\n\t}\n\n\tsta_dbg(sdata, \"Allocated STA %pM\\n\", sta->sta.addr);\n\treturn sta;\n\nfree:\n\tif (sta->tx_lat) {\n\t\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++)\n\t\t\tkfree(sta->tx_lat[i].bins);\n\t\tkfree(sta->tx_lat);\n\t}\n\tkfree(sta);\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -41,6 +41,7 @@\n \trcu_read_unlock();\n \n \tspin_lock_init(&sta->lock);\n+\tspin_lock_init(&sta->ps_lock);\n \tINIT_WORK(&sta->drv_unblock_wk, sta_unblock);\n \tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n \tmutex_init(&sta->ampdu_mlme.mtx);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tspin_lock_init(&sta->ps_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-2706",
        "func_name": "torvalds/linux/ieee80211_sta_ps_deliver_wakeup",
        "description": "Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c.",
        "git_url": "https://github.com/torvalds/linux/commit/1d147bfa64293b2723c4fec50922168658e613ba",
        "commit_title": "mac80211: fix AP powersave TX vs. wakeup race",
        "commit_text": " There is a race between the TX path and the STA wakeup: while a station is sleeping, mac80211 buffers frames until it wakes up, then the frames are transmitted. However, the RX and TX path are concurrent, so the packet indicating wakeup can be processed while a packet is being transmitted.  This can lead to a situation where the buffered frames list is emptied on the one side, while a frame is being added on the other side, as the station is still seen as sleeping in the TX path.  As a result, the newly added frame will not be send anytime soon. It might be sent much later (and out of order) when the station goes to sleep and wakes up the next time.  Additionally, it can lead to the crash below.  Fix all this by synchronising both paths with a new lock. Both path are not fastpath since they handle PS situations.  In a later patch we'll remove the extra skb queue locks to reduce locking overhead.  BUG: unable to handle kernel NULL pointer dereference at 000000b0 IP: [<ff6f1791>] ieee80211_report_used_skb+0x11/0x3e0 [mac80211] *pde = 00000000 Oops: 0000 [#1] SMP DEBUG_PAGEALLOC EIP: 0060:[<ff6f1791>] EFLAGS: 00210282 CPU: 1 EIP is at ieee80211_report_used_skb+0x11/0x3e0 [mac80211] EAX: e5900da0 EBX: 00000000 ECX: 00000001 EDX: 00000000 ESI: e41d00c0 EDI: e5900da0 EBP: ebe458e4 ESP: ebe458b0  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068 CR0: 8005003b CR2: 000000b0 CR3: 25a78000 CR4: 000407d0 DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000 DR6: ffff0ff0 DR7: 00000400 Process iperf (pid: 3934, ti=ebe44000 task=e757c0b0 task.ti=ebe44000) iwlwifi 0000:02:00.0: I iwl_pcie_enqueue_hcmd Sending command LQ_CMD (#4e), seq: 0x0903, 92 bytes at 3[3]:9 Stack:  e403b32c ebe458c4 00200002 00200286 e403b338 ebe458cc c10960bb e5900da0  ff76a6ec ebe458d8 00000000 e41d00c0 e5900da0 ebe458f0 ff6f1b75 e403b210  ebe4598c ff723dc1 00000000 ff76a6ec e597c978 e403b758 00000002 00000002 Call Trace:  [<ff6f1b75>] ieee80211_free_txskb+0x15/0x20 [mac80211]  [<ff723dc1>] invoke_tx_handlers+0x1661/0x1780 [mac80211]  [<ff7248a5>] ieee80211_tx+0x75/0x100 [mac80211]  [<ff7249bf>] ieee80211_xmit+0x8f/0xc0 [mac80211]  [<ff72550e>] ieee80211_subif_start_xmit+0x4fe/0xe20 [mac80211]  [<c149ef70>] dev_hard_start_xmit+0x450/0x950  [<c14b9aa9>] sch_direct_xmit+0xa9/0x250  [<c14b9c9b>] __qdisc_run+0x4b/0x150  [<c149f732>] dev_queue_xmit+0x2c2/0xca0  Cc: stable@vger.kernel.org [reword commit log, use a separate lock]",
        "func_before": "void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff_head pending;\n\tint filtered = 0, buffered = 0, ac;\n\tunsigned long flags;\n\n\tclear_sta_flag(sta, WLAN_STA_SP);\n\n\tBUILD_BUG_ON(BITS_TO_LONGS(IEEE80211_NUM_TIDS) > 1);\n\tsta->driver_buffered_tids = 0;\n\n\tif (!(local->hw.flags & IEEE80211_HW_AP_LINK_PS))\n\t\tdrv_sta_notify(local, sdata, STA_NOTIFY_AWAKE, &sta->sta);\n\n\tskb_queue_head_init(&pending);\n\n\t/* Send all buffered frames to the station */\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tint count = skb_queue_len(&pending), tmp;\n\n\t\tspin_lock_irqsave(&sta->tx_filtered[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->tx_filtered[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->tx_filtered[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tfiltered += tmp - count;\n\t\tcount = tmp;\n\n\t\tspin_lock_irqsave(&sta->ps_tx_buf[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->ps_tx_buf[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->ps_tx_buf[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tbuffered += tmp - count;\n\t}\n\n\tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n\n\t/* This station just woke up and isn't aware of our SMPS state */\n\tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,\n\t\t\t\t\t   sdata->smps_mode) &&\n\t    sta->known_smps_mode != sdata->bss->req_smps &&\n\t    sta_info_tx_streams(sta) != 1) {\n\t\tht_dbg(sdata,\n\t\t       \"%pM just woke up and MIMO capable - update SMPS\\n\",\n\t\t       sta->sta.addr);\n\t\tieee80211_send_smps_action(sdata, sdata->bss->req_smps,\n\t\t\t\t\t   sta->sta.addr,\n\t\t\t\t\t   sdata->vif.bss_conf.bssid);\n\t}\n\n\tlocal->total_ps_buffered -= buffered;\n\n\tsta_info_recalc_tim(sta);\n\n\tps_dbg(sdata,\n\t       \"STA %pM aid %d sending %d filtered/%d PS frames since STA not sleeping anymore\\n\",\n\t       sta->sta.addr, sta->sta.aid, filtered, buffered);\n}",
        "func": "void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff_head pending;\n\tint filtered = 0, buffered = 0, ac;\n\tunsigned long flags;\n\n\tclear_sta_flag(sta, WLAN_STA_SP);\n\n\tBUILD_BUG_ON(BITS_TO_LONGS(IEEE80211_NUM_TIDS) > 1);\n\tsta->driver_buffered_tids = 0;\n\n\tif (!(local->hw.flags & IEEE80211_HW_AP_LINK_PS))\n\t\tdrv_sta_notify(local, sdata, STA_NOTIFY_AWAKE, &sta->sta);\n\n\tskb_queue_head_init(&pending);\n\n\t/* sync with ieee80211_tx_h_unicast_ps_buf */\n\tspin_lock(&sta->ps_lock);\n\t/* Send all buffered frames to the station */\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tint count = skb_queue_len(&pending), tmp;\n\n\t\tspin_lock_irqsave(&sta->tx_filtered[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->tx_filtered[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->tx_filtered[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tfiltered += tmp - count;\n\t\tcount = tmp;\n\n\t\tspin_lock_irqsave(&sta->ps_tx_buf[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->ps_tx_buf[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->ps_tx_buf[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tbuffered += tmp - count;\n\t}\n\n\tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n\tspin_unlock(&sta->ps_lock);\n\n\t/* This station just woke up and isn't aware of our SMPS state */\n\tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,\n\t\t\t\t\t   sdata->smps_mode) &&\n\t    sta->known_smps_mode != sdata->bss->req_smps &&\n\t    sta_info_tx_streams(sta) != 1) {\n\t\tht_dbg(sdata,\n\t\t       \"%pM just woke up and MIMO capable - update SMPS\\n\",\n\t\t       sta->sta.addr);\n\t\tieee80211_send_smps_action(sdata, sdata->bss->req_smps,\n\t\t\t\t\t   sta->sta.addr,\n\t\t\t\t\t   sdata->vif.bss_conf.bssid);\n\t}\n\n\tlocal->total_ps_buffered -= buffered;\n\n\tsta_info_recalc_tim(sta);\n\n\tps_dbg(sdata,\n\t       \"STA %pM aid %d sending %d filtered/%d PS frames since STA not sleeping anymore\\n\",\n\t       sta->sta.addr, sta->sta.aid, filtered, buffered);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,6 +16,8 @@\n \n \tskb_queue_head_init(&pending);\n \n+\t/* sync with ieee80211_tx_h_unicast_ps_buf */\n+\tspin_lock(&sta->ps_lock);\n \t/* Send all buffered frames to the station */\n \tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n \t\tint count = skb_queue_len(&pending), tmp;\n@@ -35,6 +37,7 @@\n \t}\n \n \tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n+\tspin_unlock(&sta->ps_lock);\n \n \t/* This station just woke up and isn't aware of our SMPS state */\n \tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t/* sync with ieee80211_tx_h_unicast_ps_buf */",
                "\tspin_lock(&sta->ps_lock);",
                "\tspin_unlock(&sta->ps_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-2706",
        "func_name": "torvalds/linux/ieee80211_tx_h_unicast_ps_buf",
        "description": "Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c.",
        "git_url": "https://github.com/torvalds/linux/commit/1d147bfa64293b2723c4fec50922168658e613ba",
        "commit_title": "mac80211: fix AP powersave TX vs. wakeup race",
        "commit_text": " There is a race between the TX path and the STA wakeup: while a station is sleeping, mac80211 buffers frames until it wakes up, then the frames are transmitted. However, the RX and TX path are concurrent, so the packet indicating wakeup can be processed while a packet is being transmitted.  This can lead to a situation where the buffered frames list is emptied on the one side, while a frame is being added on the other side, as the station is still seen as sleeping in the TX path.  As a result, the newly added frame will not be send anytime soon. It might be sent much later (and out of order) when the station goes to sleep and wakes up the next time.  Additionally, it can lead to the crash below.  Fix all this by synchronising both paths with a new lock. Both path are not fastpath since they handle PS situations.  In a later patch we'll remove the extra skb queue locks to reduce locking overhead.  BUG: unable to handle kernel NULL pointer dereference at 000000b0 IP: [<ff6f1791>] ieee80211_report_used_skb+0x11/0x3e0 [mac80211] *pde = 00000000 Oops: 0000 [#1] SMP DEBUG_PAGEALLOC EIP: 0060:[<ff6f1791>] EFLAGS: 00210282 CPU: 1 EIP is at ieee80211_report_used_skb+0x11/0x3e0 [mac80211] EAX: e5900da0 EBX: 00000000 ECX: 00000001 EDX: 00000000 ESI: e41d00c0 EDI: e5900da0 EBP: ebe458e4 ESP: ebe458b0  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068 CR0: 8005003b CR2: 000000b0 CR3: 25a78000 CR4: 000407d0 DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000 DR6: ffff0ff0 DR7: 00000400 Process iperf (pid: 3934, ti=ebe44000 task=e757c0b0 task.ti=ebe44000) iwlwifi 0000:02:00.0: I iwl_pcie_enqueue_hcmd Sending command LQ_CMD (#4e), seq: 0x0903, 92 bytes at 3[3]:9 Stack:  e403b32c ebe458c4 00200002 00200286 e403b338 ebe458cc c10960bb e5900da0  ff76a6ec ebe458d8 00000000 e41d00c0 e5900da0 ebe458f0 ff6f1b75 e403b210  ebe4598c ff723dc1 00000000 ff76a6ec e597c978 e403b758 00000002 00000002 Call Trace:  [<ff6f1b75>] ieee80211_free_txskb+0x15/0x20 [mac80211]  [<ff723dc1>] invoke_tx_handlers+0x1661/0x1780 [mac80211]  [<ff7248a5>] ieee80211_tx+0x75/0x100 [mac80211]  [<ff7249bf>] ieee80211_xmit+0x8f/0xc0 [mac80211]  [<ff72550e>] ieee80211_subif_start_xmit+0x4fe/0xe20 [mac80211]  [<c149ef70>] dev_hard_start_xmit+0x450/0x950  [<c14b9aa9>] sch_direct_xmit+0xa9/0x250  [<c14b9c9b>] __qdisc_run+0x4b/0x150  [<c149f732>] dev_queue_xmit+0x2c2/0xca0  Cc: stable@vger.kernel.org [reword commit log, use a separate lock]",
        "func_before": "static ieee80211_tx_result\nieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct sta_info *sta = tx->sta;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_local *local = tx->local;\n\n\tif (unlikely(!sta))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&\n\t\t     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {\n\t\tint ac = skb_get_queue_mapping(tx->skb);\n\n\t\tps_dbg(sta->sdata, \"STA %pM aid %d: PS buffer for AC %d\\n\",\n\t\t       sta->sta.addr, sta->sta.aid, ac);\n\t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\t\tpurge_old_ps_buffers(tx->local);\n\t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n\t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\tps_dbg(tx->sdata,\n\t\t\t       \"STA %pM TX buffer for AC %d full - dropping oldest frame\\n\",\n\t\t\t       sta->sta.addr, ac);\n\t\t\tieee80211_free_txskb(&local->hw, old);\n\t\t} else\n\t\t\ttx->local->total_ps_buffered++;\n\n\t\tinfo->control.jiffies = jiffies;\n\t\tinfo->control.vif = &tx->sdata->vif;\n\t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n\n\t\tif (!timer_pending(&local->sta_cleanup))\n\t\t\tmod_timer(&local->sta_cleanup,\n\t\t\t\t  round_jiffies(jiffies +\n\t\t\t\t\t\tSTA_INFO_CLEANUP_INTERVAL));\n\n\t\t/*\n\t\t * We queued up some frames, so the TIM bit might\n\t\t * need to be set, recalculate it.\n\t\t */\n\t\tsta_info_recalc_tim(sta);\n\n\t\treturn TX_QUEUED;\n\t} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"STA %pM in PS mode, but polling/in SP -> send frame\\n\",\n\t\t       sta->sta.addr);\n\t}\n\n\treturn TX_CONTINUE;\n}",
        "func": "static ieee80211_tx_result\nieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct sta_info *sta = tx->sta;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_local *local = tx->local;\n\n\tif (unlikely(!sta))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&\n\t\t     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {\n\t\tint ac = skb_get_queue_mapping(tx->skb);\n\n\t\tps_dbg(sta->sdata, \"STA %pM aid %d: PS buffer for AC %d\\n\",\n\t\t       sta->sta.addr, sta->sta.aid, ac);\n\t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\t\tpurge_old_ps_buffers(tx->local);\n\n\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n\t\tspin_lock(&sta->ps_lock);\n\t\t/*\n\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n\t\t * been queued to pending queue. No reordering can happen, go\n\t\t * ahead and Tx the packet.\n\t\t */\n\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n\t\t\tspin_unlock(&sta->ps_lock);\n\t\t\treturn TX_CONTINUE;\n\t\t}\n\n\t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n\t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\tps_dbg(tx->sdata,\n\t\t\t       \"STA %pM TX buffer for AC %d full - dropping oldest frame\\n\",\n\t\t\t       sta->sta.addr, ac);\n\t\t\tieee80211_free_txskb(&local->hw, old);\n\t\t} else\n\t\t\ttx->local->total_ps_buffered++;\n\n\t\tinfo->control.jiffies = jiffies;\n\t\tinfo->control.vif = &tx->sdata->vif;\n\t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n\t\tspin_unlock(&sta->ps_lock);\n\n\t\tif (!timer_pending(&local->sta_cleanup))\n\t\t\tmod_timer(&local->sta_cleanup,\n\t\t\t\t  round_jiffies(jiffies +\n\t\t\t\t\t\tSTA_INFO_CLEANUP_INTERVAL));\n\n\t\t/*\n\t\t * We queued up some frames, so the TIM bit might\n\t\t * need to be set, recalculate it.\n\t\t */\n\t\tsta_info_recalc_tim(sta);\n\n\t\treturn TX_QUEUED;\n\t} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"STA %pM in PS mode, but polling/in SP -> send frame\\n\",\n\t\t       sta->sta.addr);\n\t}\n\n\treturn TX_CONTINUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,6 +17,20 @@\n \t\t       sta->sta.addr, sta->sta.aid, ac);\n \t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n \t\t\tpurge_old_ps_buffers(tx->local);\n+\n+\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n+\t\tspin_lock(&sta->ps_lock);\n+\t\t/*\n+\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n+\t\t * been queued to pending queue. No reordering can happen, go\n+\t\t * ahead and Tx the packet.\n+\t\t */\n+\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n+\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n+\t\t\tspin_unlock(&sta->ps_lock);\n+\t\t\treturn TX_CONTINUE;\n+\t\t}\n+\n \t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n \t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n \t\t\tps_dbg(tx->sdata,\n@@ -31,6 +45,7 @@\n \t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n \t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n \t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n+\t\tspin_unlock(&sta->ps_lock);\n \n \t\tif (!timer_pending(&local->sta_cleanup))\n \t\t\tmod_timer(&local->sta_cleanup,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */",
                "\t\tspin_lock(&sta->ps_lock);",
                "\t\t/*",
                "\t\t * STA woke up the meantime and all the frames on ps_tx_buf have",
                "\t\t * been queued to pending queue. No reordering can happen, go",
                "\t\t * ahead and Tx the packet.",
                "\t\t */",
                "\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&",
                "\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {",
                "\t\t\tspin_unlock(&sta->ps_lock);",
                "\t\t\treturn TX_CONTINUE;",
                "\t\t}",
                "",
                "\t\tspin_unlock(&sta->ps_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-0196",
        "func_name": "torvalds/linux/n_tty_write",
        "description": "The n_tty_write function in drivers/tty/n_tty.c in the Linux kernel through 3.14.3 does not properly manage tty driver access in the \"LECHO & !OPOST\" case, which allows local users to cause a denial of service (memory corruption and system crash) or gain privileges by triggering a race condition involving read and write operations with long strings.",
        "git_url": "https://github.com/torvalds/linux/commit/4291086b1f081b869c6d79e5b7441633dc3ace00",
        "commit_title": "n_tty: Fix n_tty_write crash when echoing in raw mode",
        "commit_text": " The tty atomic_write_lock does not provide an exclusion guarantee for the tty driver if the termios settings are LECHO & !OPOST.  And since it is unexpected and not allowed to call TTY buffer helpers like tty_insert_flip_string concurrently, this may lead to crashes when concurrect writers call pty_write. In that case the following two writers: * the ECHOing from a workqueue and * pty_write from the process race and can overflow the corresponding TTY buffer like follows.  If we look into tty_insert_flip_string_fixed_flag, there is:   int space = __tty_buffer_request_room(port, goal, flags);   struct tty_buffer *tb = port->buf.tail;   ...   memcpy(char_buf_ptr(tb, tb->used), chars, space);   ...   tb->used += space;  so the race of the two can result in something like this:               A                                B __tty_buffer_request_room                                   __tty_buffer_request_room memcpy(buf(tb->used), ...) tb->used += space;                                   memcpy(buf(tb->used), ...) ->BOOM  B's memcpy is past the tty_buffer due to the previous A's tb->used increment.  Since the N_TTY line discipline input processing can output concurrently with a tty write, obtain the N_TTY ldisc output_lock to serialize echo output with normal tty writes.  This ensures the tty buffer helper tty_insert_flip_string is not called concurrently and everything is fine.  Note that this is nicely reproducible by an ordinary user using forkpty and some setup around that (raw termios + ECHO). And it is present in kernels at least after commit d945cb9cce20ac7143c2de8d88b187f62db99bdc (pty: Rework the pty layer to use the normal buffering logic) in 2.6.31-rc3.  js: add more info to the commit log js: switch to bool js: lock unconditionally js: lock only the tty->ops->write call  References: CVE-2014-0196 Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Alan Cox <alan@lxorguk.ukuu.org.uk> Cc: <stable@vger.kernel.org>",
        "func_before": "static ssize_t n_tty_write(struct tty_struct *tty, struct file *file,\n\t\t\t   const unsigned char *buf, size_t nr)\n{\n\tconst unsigned char *b = buf;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint c;\n\tssize_t retval = 0;\n\n\t/* Job control check -- must be done at start (POSIX.1 7.1.1.4). */\n\tif (L_TOSTOP(tty) && file->f_op->write != redirected_tty_write) {\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\tdown_read(&tty->termios_rwsem);\n\n\t/* Write out any echoed characters that are still pending */\n\tprocess_echoes(tty);\n\n\tadd_wait_queue(&tty->write_wait, &wait);\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tretval = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_hung_up_p(file) || (tty->link && !tty->link->count)) {\n\t\t\tretval = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (O_OPOST(tty)) {\n\t\t\twhile (nr > 0) {\n\t\t\t\tssize_t num = process_output_block(tty, b, nr);\n\t\t\t\tif (num < 0) {\n\t\t\t\t\tif (num == -EAGAIN)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tretval = num;\n\t\t\t\t\tgoto break_out;\n\t\t\t\t}\n\t\t\t\tb += num;\n\t\t\t\tnr -= num;\n\t\t\t\tif (nr == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tc = *b;\n\t\t\t\tif (process_output(c, tty) < 0)\n\t\t\t\t\tbreak;\n\t\t\t\tb++; nr--;\n\t\t\t}\n\t\t\tif (tty->ops->flush_chars)\n\t\t\t\ttty->ops->flush_chars(tty);\n\t\t} else {\n\t\t\twhile (nr > 0) {\n\t\t\t\tc = tty->ops->write(tty, b, nr);\n\t\t\t\tif (c < 0) {\n\t\t\t\t\tretval = c;\n\t\t\t\t\tgoto break_out;\n\t\t\t\t}\n\t\t\t\tif (!c)\n\t\t\t\t\tbreak;\n\t\t\t\tb += c;\n\t\t\t\tnr -= c;\n\t\t\t}\n\t\t}\n\t\tif (!nr)\n\t\t\tbreak;\n\t\tif (file->f_flags & O_NONBLOCK) {\n\t\t\tretval = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tup_read(&tty->termios_rwsem);\n\n\t\tschedule();\n\n\t\tdown_read(&tty->termios_rwsem);\n\t}\nbreak_out:\n\t__set_current_state(TASK_RUNNING);\n\tremove_wait_queue(&tty->write_wait, &wait);\n\tif (b - buf != nr && tty->fasync)\n\t\tset_bit(TTY_DO_WRITE_WAKEUP, &tty->flags);\n\tup_read(&tty->termios_rwsem);\n\treturn (b - buf) ? b - buf : retval;\n}",
        "func": "static ssize_t n_tty_write(struct tty_struct *tty, struct file *file,\n\t\t\t   const unsigned char *buf, size_t nr)\n{\n\tconst unsigned char *b = buf;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint c;\n\tssize_t retval = 0;\n\n\t/* Job control check -- must be done at start (POSIX.1 7.1.1.4). */\n\tif (L_TOSTOP(tty) && file->f_op->write != redirected_tty_write) {\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\tdown_read(&tty->termios_rwsem);\n\n\t/* Write out any echoed characters that are still pending */\n\tprocess_echoes(tty);\n\n\tadd_wait_queue(&tty->write_wait, &wait);\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tretval = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_hung_up_p(file) || (tty->link && !tty->link->count)) {\n\t\t\tretval = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (O_OPOST(tty)) {\n\t\t\twhile (nr > 0) {\n\t\t\t\tssize_t num = process_output_block(tty, b, nr);\n\t\t\t\tif (num < 0) {\n\t\t\t\t\tif (num == -EAGAIN)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tretval = num;\n\t\t\t\t\tgoto break_out;\n\t\t\t\t}\n\t\t\t\tb += num;\n\t\t\t\tnr -= num;\n\t\t\t\tif (nr == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tc = *b;\n\t\t\t\tif (process_output(c, tty) < 0)\n\t\t\t\t\tbreak;\n\t\t\t\tb++; nr--;\n\t\t\t}\n\t\t\tif (tty->ops->flush_chars)\n\t\t\t\ttty->ops->flush_chars(tty);\n\t\t} else {\n\t\t\tstruct n_tty_data *ldata = tty->disc_data;\n\n\t\t\twhile (nr > 0) {\n\t\t\t\tmutex_lock(&ldata->output_lock);\n\t\t\t\tc = tty->ops->write(tty, b, nr);\n\t\t\t\tmutex_unlock(&ldata->output_lock);\n\t\t\t\tif (c < 0) {\n\t\t\t\t\tretval = c;\n\t\t\t\t\tgoto break_out;\n\t\t\t\t}\n\t\t\t\tif (!c)\n\t\t\t\t\tbreak;\n\t\t\t\tb += c;\n\t\t\t\tnr -= c;\n\t\t\t}\n\t\t}\n\t\tif (!nr)\n\t\t\tbreak;\n\t\tif (file->f_flags & O_NONBLOCK) {\n\t\t\tretval = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tup_read(&tty->termios_rwsem);\n\n\t\tschedule();\n\n\t\tdown_read(&tty->termios_rwsem);\n\t}\nbreak_out:\n\t__set_current_state(TASK_RUNNING);\n\tremove_wait_queue(&tty->write_wait, &wait);\n\tif (b - buf != nr && tty->fasync)\n\t\tset_bit(TTY_DO_WRITE_WAKEUP, &tty->flags);\n\tup_read(&tty->termios_rwsem);\n\treturn (b - buf) ? b - buf : retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -50,8 +50,12 @@\n \t\t\tif (tty->ops->flush_chars)\n \t\t\t\ttty->ops->flush_chars(tty);\n \t\t} else {\n+\t\t\tstruct n_tty_data *ldata = tty->disc_data;\n+\n \t\t\twhile (nr > 0) {\n+\t\t\t\tmutex_lock(&ldata->output_lock);\n \t\t\t\tc = tty->ops->write(tty, b, nr);\n+\t\t\t\tmutex_unlock(&ldata->output_lock);\n \t\t\t\tif (c < 0) {\n \t\t\t\t\tretval = c;\n \t\t\t\t\tgoto break_out;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tstruct n_tty_data *ldata = tty->disc_data;",
                "",
                "\t\t\t\tmutex_lock(&ldata->output_lock);",
                "\t\t\t\tmutex_unlock(&ldata->output_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-4652",
        "func_name": "torvalds/linux/snd_ctl_elem_user_tlv",
        "description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "git_url": "https://github.com/torvalds/linux/commit/07f4d9d74a04aa7c72c5dae0ef97565f28f17b92",
        "commit_title": "ALSA: control: Protect user controls against concurrent access",
        "commit_text": " The user-control put and get handlers as well as the tlv do not protect against concurrent access from multiple threads. Since the state of the control is not updated atomically it is possible that either two write operations or a write and a read operation race against each other. Both can lead to arbitrary memory disclosure. This patch introduces a new lock that protects user-controls from concurrent access. Since applications typically access controls sequentially than in parallel a single lock per card should be fine.  Cc: <stable@vger.kernel.org>",
        "func_before": "static int snd_ctl_elem_user_tlv(struct snd_kcontrol *kcontrol,\n\t\t\t\t int op_flag,\n\t\t\t\t unsigned int size,\n\t\t\t\t unsigned int __user *tlv)\n{\n\tstruct user_element *ue = kcontrol->private_data;\n\tint change = 0;\n\tvoid *new_data;\n\n\tif (op_flag > 0) {\n\t\tif (size > 1024 * 128)\t/* sane value */\n\t\t\treturn -EINVAL;\n\n\t\tnew_data = memdup_user(tlv, size);\n\t\tif (IS_ERR(new_data))\n\t\t\treturn PTR_ERR(new_data);\n\t\tchange = ue->tlv_data_size != size;\n\t\tif (!change)\n\t\t\tchange = memcmp(ue->tlv_data, new_data, size);\n\t\tkfree(ue->tlv_data);\n\t\tue->tlv_data = new_data;\n\t\tue->tlv_data_size = size;\n\t} else {\n\t\tif (! ue->tlv_data_size || ! ue->tlv_data)\n\t\t\treturn -ENXIO;\n\t\tif (size < ue->tlv_data_size)\n\t\t\treturn -ENOSPC;\n\t\tif (copy_to_user(tlv, ue->tlv_data, ue->tlv_data_size))\n\t\t\treturn -EFAULT;\n\t}\n\treturn change;\n}",
        "func": "static int snd_ctl_elem_user_tlv(struct snd_kcontrol *kcontrol,\n\t\t\t\t int op_flag,\n\t\t\t\t unsigned int size,\n\t\t\t\t unsigned int __user *tlv)\n{\n\tstruct user_element *ue = kcontrol->private_data;\n\tint change = 0;\n\tvoid *new_data;\n\n\tif (op_flag > 0) {\n\t\tif (size > 1024 * 128)\t/* sane value */\n\t\t\treturn -EINVAL;\n\n\t\tnew_data = memdup_user(tlv, size);\n\t\tif (IS_ERR(new_data))\n\t\t\treturn PTR_ERR(new_data);\n\t\tmutex_lock(&ue->card->user_ctl_lock);\n\t\tchange = ue->tlv_data_size != size;\n\t\tif (!change)\n\t\t\tchange = memcmp(ue->tlv_data, new_data, size);\n\t\tkfree(ue->tlv_data);\n\t\tue->tlv_data = new_data;\n\t\tue->tlv_data_size = size;\n\t\tmutex_unlock(&ue->card->user_ctl_lock);\n\t} else {\n\t\tint ret = 0;\n\n\t\tmutex_lock(&ue->card->user_ctl_lock);\n\t\tif (!ue->tlv_data_size || !ue->tlv_data) {\n\t\t\tret = -ENXIO;\n\t\t\tgoto err_unlock;\n\t\t}\n\t\tif (size < ue->tlv_data_size) {\n\t\t\tret = -ENOSPC;\n\t\t\tgoto err_unlock;\n\t\t}\n\t\tif (copy_to_user(tlv, ue->tlv_data, ue->tlv_data_size))\n\t\t\tret = -EFAULT;\nerr_unlock:\n\t\tmutex_unlock(&ue->card->user_ctl_lock);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn change;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,19 +14,32 @@\n \t\tnew_data = memdup_user(tlv, size);\n \t\tif (IS_ERR(new_data))\n \t\t\treturn PTR_ERR(new_data);\n+\t\tmutex_lock(&ue->card->user_ctl_lock);\n \t\tchange = ue->tlv_data_size != size;\n \t\tif (!change)\n \t\t\tchange = memcmp(ue->tlv_data, new_data, size);\n \t\tkfree(ue->tlv_data);\n \t\tue->tlv_data = new_data;\n \t\tue->tlv_data_size = size;\n+\t\tmutex_unlock(&ue->card->user_ctl_lock);\n \t} else {\n-\t\tif (! ue->tlv_data_size || ! ue->tlv_data)\n-\t\t\treturn -ENXIO;\n-\t\tif (size < ue->tlv_data_size)\n-\t\t\treturn -ENOSPC;\n+\t\tint ret = 0;\n+\n+\t\tmutex_lock(&ue->card->user_ctl_lock);\n+\t\tif (!ue->tlv_data_size || !ue->tlv_data) {\n+\t\t\tret = -ENXIO;\n+\t\t\tgoto err_unlock;\n+\t\t}\n+\t\tif (size < ue->tlv_data_size) {\n+\t\t\tret = -ENOSPC;\n+\t\t\tgoto err_unlock;\n+\t\t}\n \t\tif (copy_to_user(tlv, ue->tlv_data, ue->tlv_data_size))\n-\t\t\treturn -EFAULT;\n+\t\t\tret = -EFAULT;\n+err_unlock:\n+\t\tmutex_unlock(&ue->card->user_ctl_lock);\n+\t\tif (ret)\n+\t\t\treturn ret;\n \t}\n \treturn change;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (! ue->tlv_data_size || ! ue->tlv_data)",
                "\t\t\treturn -ENXIO;",
                "\t\tif (size < ue->tlv_data_size)",
                "\t\t\treturn -ENOSPC;",
                "\t\t\treturn -EFAULT;"
            ],
            "added_lines": [
                "\t\tmutex_lock(&ue->card->user_ctl_lock);",
                "\t\tmutex_unlock(&ue->card->user_ctl_lock);",
                "\t\tint ret = 0;",
                "",
                "\t\tmutex_lock(&ue->card->user_ctl_lock);",
                "\t\tif (!ue->tlv_data_size || !ue->tlv_data) {",
                "\t\t\tret = -ENXIO;",
                "\t\t\tgoto err_unlock;",
                "\t\t}",
                "\t\tif (size < ue->tlv_data_size) {",
                "\t\t\tret = -ENOSPC;",
                "\t\t\tgoto err_unlock;",
                "\t\t}",
                "\t\t\tret = -EFAULT;",
                "err_unlock:",
                "\t\tmutex_unlock(&ue->card->user_ctl_lock);",
                "\t\tif (ret)",
                "\t\t\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-4652",
        "func_name": "torvalds/linux/snd_ctl_elem_add",
        "description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "git_url": "https://github.com/torvalds/linux/commit/07f4d9d74a04aa7c72c5dae0ef97565f28f17b92",
        "commit_title": "ALSA: control: Protect user controls against concurrent access",
        "commit_text": " The user-control put and get handlers as well as the tlv do not protect against concurrent access from multiple threads. Since the state of the control is not updated atomically it is possible that either two write operations or a write and a read operation race against each other. Both can lead to arbitrary memory disclosure. This patch introduces a new lock that protects user-controls from concurrent access. Since applications typically access controls sequentially than in parallel a single lock per card should be fine.  Cc: <stable@vger.kernel.org>",
        "func_before": "static int snd_ctl_elem_add(struct snd_ctl_file *file,\n\t\t\t    struct snd_ctl_elem_info *info, int replace)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_kcontrol kctl, *_kctl;\n\tunsigned int access;\n\tlong private_size;\n\tstruct user_element *ue;\n\tint idx, err;\n\n\tif (!replace && card->user_ctl_count >= MAX_USER_CONTROLS)\n\t\treturn -ENOMEM;\n\tif (info->count < 1)\n\t\treturn -EINVAL;\n\taccess = info->access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE :\n\t\t(info->access & (SNDRV_CTL_ELEM_ACCESS_READWRITE|\n\t\t\t\t SNDRV_CTL_ELEM_ACCESS_INACTIVE|\n\t\t\t\t SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE));\n\tinfo->id.numid = 0;\n\tmemset(&kctl, 0, sizeof(kctl));\n\tdown_write(&card->controls_rwsem);\n\t_kctl = snd_ctl_find_id(card, &info->id);\n\terr = 0;\n\tif (_kctl) {\n\t\tif (replace)\n\t\t\terr = snd_ctl_remove(card, _kctl);\n\t\telse\n\t\t\terr = -EBUSY;\n\t} else {\n\t\tif (replace)\n\t\t\terr = -ENOENT;\n\t}\n\tup_write(&card->controls_rwsem);\n\tif (err < 0)\n\t\treturn err;\n\tmemcpy(&kctl.id, &info->id, sizeof(info->id));\n\tkctl.count = info->owner ? info->owner : 1;\n\taccess |= SNDRV_CTL_ELEM_ACCESS_USER;\n\tif (info->type == SNDRV_CTL_ELEM_TYPE_ENUMERATED)\n\t\tkctl.info = snd_ctl_elem_user_enum_info;\n\telse\n\t\tkctl.info = snd_ctl_elem_user_info;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_READ)\n\t\tkctl.get = snd_ctl_elem_user_get;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_WRITE)\n\t\tkctl.put = snd_ctl_elem_user_put;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE) {\n\t\tkctl.tlv.c = snd_ctl_elem_user_tlv;\n\t\taccess |= SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK;\n\t}\n\tswitch (info->type) {\n\tcase SNDRV_CTL_ELEM_TYPE_BOOLEAN:\n\tcase SNDRV_CTL_ELEM_TYPE_INTEGER:\n\t\tprivate_size = sizeof(long);\n\t\tif (info->count > 128)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_INTEGER64:\n\t\tprivate_size = sizeof(long long);\n\t\tif (info->count > 64)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_ENUMERATED:\n\t\tprivate_size = sizeof(unsigned int);\n\t\tif (info->count > 128 || info->value.enumerated.items == 0)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_BYTES:\n\t\tprivate_size = sizeof(unsigned char);\n\t\tif (info->count > 512)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_IEC958:\n\t\tprivate_size = sizeof(struct snd_aes_iec958);\n\t\tif (info->count != 1)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tprivate_size *= info->count;\n\tue = kzalloc(sizeof(struct user_element) + private_size, GFP_KERNEL);\n\tif (ue == NULL)\n\t\treturn -ENOMEM;\n\tue->info = *info;\n\tue->info.access = 0;\n\tue->elem_data = (char *)ue + sizeof(*ue);\n\tue->elem_data_size = private_size;\n\tif (ue->info.type == SNDRV_CTL_ELEM_TYPE_ENUMERATED) {\n\t\terr = snd_ctl_elem_init_enum_names(ue);\n\t\tif (err < 0) {\n\t\t\tkfree(ue);\n\t\t\treturn err;\n\t\t}\n\t}\n\tkctl.private_free = snd_ctl_elem_user_free;\n\t_kctl = snd_ctl_new(&kctl, access);\n\tif (_kctl == NULL) {\n\t\tkfree(ue->priv_data);\n\t\tkfree(ue);\n\t\treturn -ENOMEM;\n\t}\n\t_kctl->private_data = ue;\n\tfor (idx = 0; idx < _kctl->count; idx++)\n\t\t_kctl->vd[idx].owner = file;\n\terr = snd_ctl_add(card, _kctl);\n\tif (err < 0)\n\t\treturn err;\n\n\tdown_write(&card->controls_rwsem);\n\tcard->user_ctl_count++;\n\tup_write(&card->controls_rwsem);\n\n\treturn 0;\n}",
        "func": "static int snd_ctl_elem_add(struct snd_ctl_file *file,\n\t\t\t    struct snd_ctl_elem_info *info, int replace)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_kcontrol kctl, *_kctl;\n\tunsigned int access;\n\tlong private_size;\n\tstruct user_element *ue;\n\tint idx, err;\n\n\tif (!replace && card->user_ctl_count >= MAX_USER_CONTROLS)\n\t\treturn -ENOMEM;\n\tif (info->count < 1)\n\t\treturn -EINVAL;\n\taccess = info->access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE :\n\t\t(info->access & (SNDRV_CTL_ELEM_ACCESS_READWRITE|\n\t\t\t\t SNDRV_CTL_ELEM_ACCESS_INACTIVE|\n\t\t\t\t SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE));\n\tinfo->id.numid = 0;\n\tmemset(&kctl, 0, sizeof(kctl));\n\tdown_write(&card->controls_rwsem);\n\t_kctl = snd_ctl_find_id(card, &info->id);\n\terr = 0;\n\tif (_kctl) {\n\t\tif (replace)\n\t\t\terr = snd_ctl_remove(card, _kctl);\n\t\telse\n\t\t\terr = -EBUSY;\n\t} else {\n\t\tif (replace)\n\t\t\terr = -ENOENT;\n\t}\n\tup_write(&card->controls_rwsem);\n\tif (err < 0)\n\t\treturn err;\n\tmemcpy(&kctl.id, &info->id, sizeof(info->id));\n\tkctl.count = info->owner ? info->owner : 1;\n\taccess |= SNDRV_CTL_ELEM_ACCESS_USER;\n\tif (info->type == SNDRV_CTL_ELEM_TYPE_ENUMERATED)\n\t\tkctl.info = snd_ctl_elem_user_enum_info;\n\telse\n\t\tkctl.info = snd_ctl_elem_user_info;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_READ)\n\t\tkctl.get = snd_ctl_elem_user_get;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_WRITE)\n\t\tkctl.put = snd_ctl_elem_user_put;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE) {\n\t\tkctl.tlv.c = snd_ctl_elem_user_tlv;\n\t\taccess |= SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK;\n\t}\n\tswitch (info->type) {\n\tcase SNDRV_CTL_ELEM_TYPE_BOOLEAN:\n\tcase SNDRV_CTL_ELEM_TYPE_INTEGER:\n\t\tprivate_size = sizeof(long);\n\t\tif (info->count > 128)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_INTEGER64:\n\t\tprivate_size = sizeof(long long);\n\t\tif (info->count > 64)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_ENUMERATED:\n\t\tprivate_size = sizeof(unsigned int);\n\t\tif (info->count > 128 || info->value.enumerated.items == 0)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_BYTES:\n\t\tprivate_size = sizeof(unsigned char);\n\t\tif (info->count > 512)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_IEC958:\n\t\tprivate_size = sizeof(struct snd_aes_iec958);\n\t\tif (info->count != 1)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tprivate_size *= info->count;\n\tue = kzalloc(sizeof(struct user_element) + private_size, GFP_KERNEL);\n\tif (ue == NULL)\n\t\treturn -ENOMEM;\n\tue->card = card;\n\tue->info = *info;\n\tue->info.access = 0;\n\tue->elem_data = (char *)ue + sizeof(*ue);\n\tue->elem_data_size = private_size;\n\tif (ue->info.type == SNDRV_CTL_ELEM_TYPE_ENUMERATED) {\n\t\terr = snd_ctl_elem_init_enum_names(ue);\n\t\tif (err < 0) {\n\t\t\tkfree(ue);\n\t\t\treturn err;\n\t\t}\n\t}\n\tkctl.private_free = snd_ctl_elem_user_free;\n\t_kctl = snd_ctl_new(&kctl, access);\n\tif (_kctl == NULL) {\n\t\tkfree(ue->priv_data);\n\t\tkfree(ue);\n\t\treturn -ENOMEM;\n\t}\n\t_kctl->private_data = ue;\n\tfor (idx = 0; idx < _kctl->count; idx++)\n\t\t_kctl->vd[idx].owner = file;\n\terr = snd_ctl_add(card, _kctl);\n\tif (err < 0)\n\t\treturn err;\n\n\tdown_write(&card->controls_rwsem);\n\tcard->user_ctl_count++;\n\tup_write(&card->controls_rwsem);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -82,6 +82,7 @@\n \tue = kzalloc(sizeof(struct user_element) + private_size, GFP_KERNEL);\n \tif (ue == NULL)\n \t\treturn -ENOMEM;\n+\tue->card = card;\n \tue->info = *info;\n \tue->info.access = 0;\n \tue->elem_data = (char *)ue + sizeof(*ue);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tue->card = card;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-4652",
        "func_name": "torvalds/linux/snd_ctl_elem_user_get",
        "description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "git_url": "https://github.com/torvalds/linux/commit/07f4d9d74a04aa7c72c5dae0ef97565f28f17b92",
        "commit_title": "ALSA: control: Protect user controls against concurrent access",
        "commit_text": " The user-control put and get handlers as well as the tlv do not protect against concurrent access from multiple threads. Since the state of the control is not updated atomically it is possible that either two write operations or a write and a read operation race against each other. Both can lead to arbitrary memory disclosure. This patch introduces a new lock that protects user-controls from concurrent access. Since applications typically access controls sequentially than in parallel a single lock per card should be fine.  Cc: <stable@vger.kernel.org>",
        "func_before": "static int snd_ctl_elem_user_get(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tmemcpy(&ucontrol->value, ue->elem_data, ue->elem_data_size);\n\treturn 0;\n}",
        "func": "static int snd_ctl_elem_user_get(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tmutex_lock(&ue->card->user_ctl_lock);\n\tmemcpy(&ucontrol->value, ue->elem_data, ue->elem_data_size);\n\tmutex_unlock(&ue->card->user_ctl_lock);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,8 @@\n {\n \tstruct user_element *ue = kcontrol->private_data;\n \n+\tmutex_lock(&ue->card->user_ctl_lock);\n \tmemcpy(&ucontrol->value, ue->elem_data, ue->elem_data_size);\n+\tmutex_unlock(&ue->card->user_ctl_lock);\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_lock(&ue->card->user_ctl_lock);",
                "\tmutex_unlock(&ue->card->user_ctl_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-4652",
        "func_name": "torvalds/linux/snd_ctl_elem_user_put",
        "description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "git_url": "https://github.com/torvalds/linux/commit/07f4d9d74a04aa7c72c5dae0ef97565f28f17b92",
        "commit_title": "ALSA: control: Protect user controls against concurrent access",
        "commit_text": " The user-control put and get handlers as well as the tlv do not protect against concurrent access from multiple threads. Since the state of the control is not updated atomically it is possible that either two write operations or a write and a read operation race against each other. Both can lead to arbitrary memory disclosure. This patch introduces a new lock that protects user-controls from concurrent access. Since applications typically access controls sequentially than in parallel a single lock per card should be fine.  Cc: <stable@vger.kernel.org>",
        "func_before": "static int snd_ctl_elem_user_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tint change;\n\tstruct user_element *ue = kcontrol->private_data;\n\t\n\tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n\tif (change)\n\t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n\treturn change;\n}",
        "func": "static int snd_ctl_elem_user_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tint change;\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tmutex_lock(&ue->card->user_ctl_lock);\n\tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n\tif (change)\n\t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n\tmutex_unlock(&ue->card->user_ctl_lock);\n\treturn change;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,9 +3,11 @@\n {\n \tint change;\n \tstruct user_element *ue = kcontrol->private_data;\n-\t\n+\n+\tmutex_lock(&ue->card->user_ctl_lock);\n \tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n \tif (change)\n \t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n+\tmutex_unlock(&ue->card->user_ctl_lock);\n \treturn change;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t"
            ],
            "added_lines": [
                "",
                "\tmutex_lock(&ue->card->user_ctl_lock);",
                "\tmutex_unlock(&ue->card->user_ctl_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-4652",
        "func_name": "torvalds/linux/snd_card_new",
        "description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "git_url": "https://github.com/torvalds/linux/commit/07f4d9d74a04aa7c72c5dae0ef97565f28f17b92",
        "commit_title": "ALSA: control: Protect user controls against concurrent access",
        "commit_text": " The user-control put and get handlers as well as the tlv do not protect against concurrent access from multiple threads. Since the state of the control is not updated atomically it is possible that either two write operations or a write and a read operation race against each other. Both can lead to arbitrary memory disclosure. This patch introduces a new lock that protects user-controls from concurrent access. Since applications typically access controls sequentially than in parallel a single lock per card should be fine.  Cc: <stable@vger.kernel.org>",
        "func_before": "int snd_card_new(struct device *parent, int idx, const char *xid,\n\t\t    struct module *module, int extra_size,\n\t\t    struct snd_card **card_ret)\n{\n\tstruct snd_card *card;\n\tint err;\n\n\tif (snd_BUG_ON(!card_ret))\n\t\treturn -EINVAL;\n\t*card_ret = NULL;\n\n\tif (extra_size < 0)\n\t\textra_size = 0;\n\tcard = kzalloc(sizeof(*card) + extra_size, GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\tif (extra_size > 0)\n\t\tcard->private_data = (char *)card + sizeof(struct snd_card);\n\tif (xid)\n\t\tstrlcpy(card->id, xid, sizeof(card->id));\n\terr = 0;\n\tmutex_lock(&snd_card_mutex);\n\tif (idx < 0) /* first check the matching module-name slot */\n\t\tidx = get_slot_from_bitmask(idx, module_slot_match, module);\n\tif (idx < 0) /* if not matched, assign an empty slot */\n\t\tidx = get_slot_from_bitmask(idx, check_empty_slot, module);\n\tif (idx < 0)\n\t\terr = -ENODEV;\n\telse if (idx < snd_ecards_limit) {\n\t\tif (test_bit(idx, snd_cards_lock))\n\t\t\terr = -EBUSY;\t/* invalid */\n\t} else if (idx >= SNDRV_CARDS)\n\t\terr = -ENODEV;\n\tif (err < 0) {\n\t\tmutex_unlock(&snd_card_mutex);\n\t\tdev_err(parent, \"cannot find the slot for index %d (range 0-%i), error: %d\\n\",\n\t\t\t idx, snd_ecards_limit - 1, err);\n\t\tkfree(card);\n\t\treturn err;\n\t}\n\tset_bit(idx, snd_cards_lock);\t\t/* lock it */\n\tif (idx >= snd_ecards_limit)\n\t\tsnd_ecards_limit = idx + 1; /* increase the limit */\n\tmutex_unlock(&snd_card_mutex);\n\tcard->dev = parent;\n\tcard->number = idx;\n\tcard->module = module;\n\tINIT_LIST_HEAD(&card->devices);\n\tinit_rwsem(&card->controls_rwsem);\n\trwlock_init(&card->ctl_files_rwlock);\n\tINIT_LIST_HEAD(&card->controls);\n\tINIT_LIST_HEAD(&card->ctl_files);\n\tspin_lock_init(&card->files_lock);\n\tINIT_LIST_HEAD(&card->files_list);\n#ifdef CONFIG_PM\n\tmutex_init(&card->power_lock);\n\tinit_waitqueue_head(&card->power_sleep);\n#endif\n\n\tdevice_initialize(&card->card_dev);\n\tcard->card_dev.parent = parent;\n\tcard->card_dev.class = sound_class;\n\tcard->card_dev.release = release_card_device;\n\tcard->card_dev.groups = card_dev_attr_groups;\n\terr = kobject_set_name(&card->card_dev.kobj, \"card%d\", idx);\n\tif (err < 0)\n\t\tgoto __error;\n\n\t/* the control interface cannot be accessed from the user space until */\n\t/* snd_cards_bitmask and snd_cards are set with snd_card_register */\n\terr = snd_ctl_create(card);\n\tif (err < 0) {\n\t\tdev_err(parent, \"unable to register control minors\\n\");\n\t\tgoto __error;\n\t}\n\terr = snd_info_card_create(card);\n\tif (err < 0) {\n\t\tdev_err(parent, \"unable to create card info\\n\");\n\t\tgoto __error_ctl;\n\t}\n\t*card_ret = card;\n\treturn 0;\n\n      __error_ctl:\n\tsnd_device_free_all(card);\n      __error:\n\tput_device(&card->card_dev);\n  \treturn err;\n}",
        "func": "int snd_card_new(struct device *parent, int idx, const char *xid,\n\t\t    struct module *module, int extra_size,\n\t\t    struct snd_card **card_ret)\n{\n\tstruct snd_card *card;\n\tint err;\n\n\tif (snd_BUG_ON(!card_ret))\n\t\treturn -EINVAL;\n\t*card_ret = NULL;\n\n\tif (extra_size < 0)\n\t\textra_size = 0;\n\tcard = kzalloc(sizeof(*card) + extra_size, GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\tif (extra_size > 0)\n\t\tcard->private_data = (char *)card + sizeof(struct snd_card);\n\tif (xid)\n\t\tstrlcpy(card->id, xid, sizeof(card->id));\n\terr = 0;\n\tmutex_lock(&snd_card_mutex);\n\tif (idx < 0) /* first check the matching module-name slot */\n\t\tidx = get_slot_from_bitmask(idx, module_slot_match, module);\n\tif (idx < 0) /* if not matched, assign an empty slot */\n\t\tidx = get_slot_from_bitmask(idx, check_empty_slot, module);\n\tif (idx < 0)\n\t\terr = -ENODEV;\n\telse if (idx < snd_ecards_limit) {\n\t\tif (test_bit(idx, snd_cards_lock))\n\t\t\terr = -EBUSY;\t/* invalid */\n\t} else if (idx >= SNDRV_CARDS)\n\t\terr = -ENODEV;\n\tif (err < 0) {\n\t\tmutex_unlock(&snd_card_mutex);\n\t\tdev_err(parent, \"cannot find the slot for index %d (range 0-%i), error: %d\\n\",\n\t\t\t idx, snd_ecards_limit - 1, err);\n\t\tkfree(card);\n\t\treturn err;\n\t}\n\tset_bit(idx, snd_cards_lock);\t\t/* lock it */\n\tif (idx >= snd_ecards_limit)\n\t\tsnd_ecards_limit = idx + 1; /* increase the limit */\n\tmutex_unlock(&snd_card_mutex);\n\tcard->dev = parent;\n\tcard->number = idx;\n\tcard->module = module;\n\tINIT_LIST_HEAD(&card->devices);\n\tinit_rwsem(&card->controls_rwsem);\n\trwlock_init(&card->ctl_files_rwlock);\n\tmutex_init(&card->user_ctl_lock);\n\tINIT_LIST_HEAD(&card->controls);\n\tINIT_LIST_HEAD(&card->ctl_files);\n\tspin_lock_init(&card->files_lock);\n\tINIT_LIST_HEAD(&card->files_list);\n#ifdef CONFIG_PM\n\tmutex_init(&card->power_lock);\n\tinit_waitqueue_head(&card->power_sleep);\n#endif\n\n\tdevice_initialize(&card->card_dev);\n\tcard->card_dev.parent = parent;\n\tcard->card_dev.class = sound_class;\n\tcard->card_dev.release = release_card_device;\n\tcard->card_dev.groups = card_dev_attr_groups;\n\terr = kobject_set_name(&card->card_dev.kobj, \"card%d\", idx);\n\tif (err < 0)\n\t\tgoto __error;\n\n\t/* the control interface cannot be accessed from the user space until */\n\t/* snd_cards_bitmask and snd_cards are set with snd_card_register */\n\terr = snd_ctl_create(card);\n\tif (err < 0) {\n\t\tdev_err(parent, \"unable to register control minors\\n\");\n\t\tgoto __error;\n\t}\n\terr = snd_info_card_create(card);\n\tif (err < 0) {\n\t\tdev_err(parent, \"unable to create card info\\n\");\n\t\tgoto __error_ctl;\n\t}\n\t*card_ret = card;\n\treturn 0;\n\n      __error_ctl:\n\tsnd_device_free_all(card);\n      __error:\n\tput_device(&card->card_dev);\n  \treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -48,6 +48,7 @@\n \tINIT_LIST_HEAD(&card->devices);\n \tinit_rwsem(&card->controls_rwsem);\n \trwlock_init(&card->ctl_files_rwlock);\n+\tmutex_init(&card->user_ctl_lock);\n \tINIT_LIST_HEAD(&card->controls);\n \tINIT_LIST_HEAD(&card->ctl_files);\n \tspin_lock_init(&card->files_lock);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_init(&card->user_ctl_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15567",
        "func_name": "xen-project/xen/ept_next_level",
        "description": "An issue was discovered in Xen through 4.13.x, allowing Intel guest OS users to gain privileges or cause a denial of service because of non-atomic modification of a live EPT PTE. When mapping guest EPT (nested paging) tables, Xen would in some circumstances use a series of non-atomic bitfield writes. Depending on the compiler version and optimisation flags, Xen might expose a dangerous partially written PTE to the hardware, which an attacker might be able to race to exploit. A guest administrator or perhaps even an unprivileged guest user might be able to cause denial of service, data corruption, or privilege escalation. Only systems using Intel CPUs are vulnerable. Systems using AMD CPUs, and Arm systems, are not vulnerable. Only systems using nested paging (hap, aka nested paging, aka in this case Intel EPT) are vulnerable. Only HVM and PVH guests can exploit the vulnerability. The presence and scope of the vulnerability depends on the precise optimisations performed by the compiler used to build Xen. If the compiler generates (a) a single 64-bit write, or (b) a series of read-modify-write operations in the same order as the source code, the hypervisor is not vulnerable. For example, in one test build using GCC 8.3 with normal settings, the compiler generated multiple (unlocked) read-modify-write operations in source-code order, which did not constitute a vulnerability. We have not been able to survey compilers; consequently we cannot say which compiler(s) might produce vulnerable code (with which code-generation options). The source code clearly violates the C rules, and thus should be considered vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/bc3d9f95d661372b059a5539ae6cb1e79435bb95",
        "commit_title": "x86/ept: atomically modify entries in ept_next_level",
        "commit_text": " ept_next_level was passing a live PTE pointer to ept_set_middle_entry, which was then modified without taking into account that the PTE could be part of a live EPT table. This wasn't a security issue because the pages returned by p2m_alloc_ptp are zeroed, so adding such an entry before actually initializing it didn't allow a guest to access physical memory addresses it wasn't supposed to access.  This is part of XSA-328. ",
        "func_before": "static int ept_next_level(struct p2m_domain *p2m, bool_t read_only,\n                          ept_entry_t **table, unsigned long *gfn_remainder,\n                          int next_level)\n{\n    ept_entry_t *ept_entry, *next = NULL, e;\n    u32 shift, index;\n\n    shift = next_level * EPT_TABLE_ORDER;\n\n    index = *gfn_remainder >> shift;\n\n    /* index must be falling into the page */\n    ASSERT(index < EPT_PAGETABLE_ENTRIES);\n\n    ept_entry = (*table) + index;\n\n    /* ept_next_level() is called (sometimes) without a lock.  Read\n     * the entry once, and act on the \"cached\" entry after that to\n     * avoid races. */\n    e = atomic_read_ept_entry(ept_entry);\n\n    if ( !is_epte_present(&e) )\n    {\n        if ( e.sa_p2mt == p2m_populate_on_demand )\n            return GUEST_TABLE_POD_PAGE;\n\n        if ( read_only )\n            return GUEST_TABLE_MAP_FAILED;\n\n        next = ept_set_middle_entry(p2m, ept_entry);\n        if ( !next )\n            return GUEST_TABLE_MAP_FAILED;\n        /* e is now stale and hence may not be used anymore below. */\n    }\n    /* The only time sp would be set here is if we had hit a superpage */\n    else if ( is_epte_superpage(&e) )\n        return GUEST_TABLE_SUPER_PAGE;\n\n    unmap_domain_page(*table);\n    *table = next ?: map_domain_page(_mfn(e.mfn));\n    *gfn_remainder &= (1UL << shift) - 1;\n    return GUEST_TABLE_NORMAL_PAGE;\n}",
        "func": "static int ept_next_level(struct p2m_domain *p2m, bool_t read_only,\n                          ept_entry_t **table, unsigned long *gfn_remainder,\n                          int next_level)\n{\n    ept_entry_t *ept_entry, *next = NULL, e;\n    u32 shift, index;\n\n    ASSERT(next_level);\n\n    shift = next_level * EPT_TABLE_ORDER;\n\n    index = *gfn_remainder >> shift;\n\n    /* index must be falling into the page */\n    ASSERT(index < EPT_PAGETABLE_ENTRIES);\n\n    ept_entry = (*table) + index;\n\n    /* ept_next_level() is called (sometimes) without a lock.  Read\n     * the entry once, and act on the \"cached\" entry after that to\n     * avoid races. */\n    e = atomic_read_ept_entry(ept_entry);\n\n    if ( !is_epte_present(&e) )\n    {\n        int rc;\n\n        if ( e.sa_p2mt == p2m_populate_on_demand )\n            return GUEST_TABLE_POD_PAGE;\n\n        if ( read_only )\n            return GUEST_TABLE_MAP_FAILED;\n\n        next = ept_set_middle_entry(p2m, &e);\n        if ( !next )\n            return GUEST_TABLE_MAP_FAILED;\n\n        rc = atomic_write_ept_entry(p2m, ept_entry, e, next_level);\n        ASSERT(rc == 0);\n    }\n    /* The only time sp would be set here is if we had hit a superpage */\n    else if ( is_epte_superpage(&e) )\n        return GUEST_TABLE_SUPER_PAGE;\n\n    unmap_domain_page(*table);\n    *table = next ?: map_domain_page(_mfn(e.mfn));\n    *gfn_remainder &= (1UL << shift) - 1;\n    return GUEST_TABLE_NORMAL_PAGE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,8 @@\n {\n     ept_entry_t *ept_entry, *next = NULL, e;\n     u32 shift, index;\n+\n+    ASSERT(next_level);\n \n     shift = next_level * EPT_TABLE_ORDER;\n \n@@ -21,16 +23,20 @@\n \n     if ( !is_epte_present(&e) )\n     {\n+        int rc;\n+\n         if ( e.sa_p2mt == p2m_populate_on_demand )\n             return GUEST_TABLE_POD_PAGE;\n \n         if ( read_only )\n             return GUEST_TABLE_MAP_FAILED;\n \n-        next = ept_set_middle_entry(p2m, ept_entry);\n+        next = ept_set_middle_entry(p2m, &e);\n         if ( !next )\n             return GUEST_TABLE_MAP_FAILED;\n-        /* e is now stale and hence may not be used anymore below. */\n+\n+        rc = atomic_write_ept_entry(p2m, ept_entry, e, next_level);\n+        ASSERT(rc == 0);\n     }\n     /* The only time sp would be set here is if we had hit a superpage */\n     else if ( is_epte_superpage(&e) )",
        "diff_line_info": {
            "deleted_lines": [
                "        next = ept_set_middle_entry(p2m, ept_entry);",
                "        /* e is now stale and hence may not be used anymore below. */"
            ],
            "added_lines": [
                "",
                "    ASSERT(next_level);",
                "        int rc;",
                "",
                "        next = ept_set_middle_entry(p2m, &e);",
                "",
                "        rc = atomic_write_ept_entry(p2m, ept_entry, e, next_level);",
                "        ASSERT(rc == 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15567",
        "func_name": "xen-project/xen/ept_next_level",
        "description": "An issue was discovered in Xen through 4.13.x, allowing Intel guest OS users to gain privileges or cause a denial of service because of non-atomic modification of a live EPT PTE. When mapping guest EPT (nested paging) tables, Xen would in some circumstances use a series of non-atomic bitfield writes. Depending on the compiler version and optimisation flags, Xen might expose a dangerous partially written PTE to the hardware, which an attacker might be able to race to exploit. A guest administrator or perhaps even an unprivileged guest user might be able to cause denial of service, data corruption, or privilege escalation. Only systems using Intel CPUs are vulnerable. Systems using AMD CPUs, and Arm systems, are not vulnerable. Only systems using nested paging (hap, aka nested paging, aka in this case Intel EPT) are vulnerable. Only HVM and PVH guests can exploit the vulnerability. The presence and scope of the vulnerability depends on the precise optimisations performed by the compiler used to build Xen. If the compiler generates (a) a single 64-bit write, or (b) a series of read-modify-write operations in the same order as the source code, the hypervisor is not vulnerable. For example, in one test build using GCC 8.3 with normal settings, the compiler generated multiple (unlocked) read-modify-write operations in source-code order, which did not constitute a vulnerability. We have not been able to survey compilers; consequently we cannot say which compiler(s) might produce vulnerable code (with which code-generation options). The source code clearly violates the C rules, and thus should be considered vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/1104288186ee73a7f9bfa41cbaa5bb7611521028",
        "commit_title": "x86/EPT: ept_set_middle_entry() related adjustments",
        "commit_text": " ept_split_super_page() wants to further modify the newly allocated table, so have ept_set_middle_entry() return the mapped pointer rather than tearing it down and then getting re-established right again.  Similarly ept_next_level() wants to hand back a mapped pointer of the next level page, so re-use the one established by ept_set_middle_entry() in case that path was taken.  Pull the setting of suppress_ve ahead of insertion into the higher level table, and don't have ept_split_super_page() set the field a 2nd time.  This is part of XSA-328. ",
        "func_before": "static int ept_next_level(struct p2m_domain *p2m, bool_t read_only,\n                          ept_entry_t **table, unsigned long *gfn_remainder,\n                          int next_level)\n{\n    unsigned long mfn;\n    ept_entry_t *ept_entry, e;\n    u32 shift, index;\n\n    shift = next_level * EPT_TABLE_ORDER;\n\n    index = *gfn_remainder >> shift;\n\n    /* index must be falling into the page */\n    ASSERT(index < EPT_PAGETABLE_ENTRIES);\n\n    ept_entry = (*table) + index;\n\n    /* ept_next_level() is called (sometimes) without a lock.  Read\n     * the entry once, and act on the \"cached\" entry after that to\n     * avoid races. */\n    e = atomic_read_ept_entry(ept_entry);\n\n    if ( !is_epte_present(&e) )\n    {\n        if ( e.sa_p2mt == p2m_populate_on_demand )\n            return GUEST_TABLE_POD_PAGE;\n\n        if ( read_only )\n            return GUEST_TABLE_MAP_FAILED;\n\n        if ( !ept_set_middle_entry(p2m, ept_entry) )\n            return GUEST_TABLE_MAP_FAILED;\n        else\n            e = atomic_read_ept_entry(ept_entry); /* Refresh */\n    }\n\n    /* The only time sp would be set here is if we had hit a superpage */\n    if ( is_epte_superpage(&e) )\n        return GUEST_TABLE_SUPER_PAGE;\n\n    mfn = e.mfn;\n    unmap_domain_page(*table);\n    *table = map_domain_page(_mfn(mfn));\n    *gfn_remainder &= (1UL << shift) - 1;\n    return GUEST_TABLE_NORMAL_PAGE;\n}",
        "func": "static int ept_next_level(struct p2m_domain *p2m, bool_t read_only,\n                          ept_entry_t **table, unsigned long *gfn_remainder,\n                          int next_level)\n{\n    ept_entry_t *ept_entry, *next = NULL, e;\n    u32 shift, index;\n\n    shift = next_level * EPT_TABLE_ORDER;\n\n    index = *gfn_remainder >> shift;\n\n    /* index must be falling into the page */\n    ASSERT(index < EPT_PAGETABLE_ENTRIES);\n\n    ept_entry = (*table) + index;\n\n    /* ept_next_level() is called (sometimes) without a lock.  Read\n     * the entry once, and act on the \"cached\" entry after that to\n     * avoid races. */\n    e = atomic_read_ept_entry(ept_entry);\n\n    if ( !is_epte_present(&e) )\n    {\n        if ( e.sa_p2mt == p2m_populate_on_demand )\n            return GUEST_TABLE_POD_PAGE;\n\n        if ( read_only )\n            return GUEST_TABLE_MAP_FAILED;\n\n        next = ept_set_middle_entry(p2m, ept_entry);\n        if ( !next )\n            return GUEST_TABLE_MAP_FAILED;\n        /* e is now stale and hence may not be used anymore below. */\n    }\n    /* The only time sp would be set here is if we had hit a superpage */\n    else if ( is_epte_superpage(&e) )\n        return GUEST_TABLE_SUPER_PAGE;\n\n    unmap_domain_page(*table);\n    *table = next ?: map_domain_page(_mfn(e.mfn));\n    *gfn_remainder &= (1UL << shift) - 1;\n    return GUEST_TABLE_NORMAL_PAGE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,8 +2,7 @@\n                           ept_entry_t **table, unsigned long *gfn_remainder,\n                           int next_level)\n {\n-    unsigned long mfn;\n-    ept_entry_t *ept_entry, e;\n+    ept_entry_t *ept_entry, *next = NULL, e;\n     u32 shift, index;\n \n     shift = next_level * EPT_TABLE_ORDER;\n@@ -28,19 +27,17 @@\n         if ( read_only )\n             return GUEST_TABLE_MAP_FAILED;\n \n-        if ( !ept_set_middle_entry(p2m, ept_entry) )\n+        next = ept_set_middle_entry(p2m, ept_entry);\n+        if ( !next )\n             return GUEST_TABLE_MAP_FAILED;\n-        else\n-            e = atomic_read_ept_entry(ept_entry); /* Refresh */\n+        /* e is now stale and hence may not be used anymore below. */\n     }\n-\n     /* The only time sp would be set here is if we had hit a superpage */\n-    if ( is_epte_superpage(&e) )\n+    else if ( is_epte_superpage(&e) )\n         return GUEST_TABLE_SUPER_PAGE;\n \n-    mfn = e.mfn;\n     unmap_domain_page(*table);\n-    *table = map_domain_page(_mfn(mfn));\n+    *table = next ?: map_domain_page(_mfn(e.mfn));\n     *gfn_remainder &= (1UL << shift) - 1;\n     return GUEST_TABLE_NORMAL_PAGE;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    unsigned long mfn;",
                "    ept_entry_t *ept_entry, e;",
                "        if ( !ept_set_middle_entry(p2m, ept_entry) )",
                "        else",
                "            e = atomic_read_ept_entry(ept_entry); /* Refresh */",
                "",
                "    if ( is_epte_superpage(&e) )",
                "    mfn = e.mfn;",
                "    *table = map_domain_page(_mfn(mfn));"
            ],
            "added_lines": [
                "    ept_entry_t *ept_entry, *next = NULL, e;",
                "        next = ept_set_middle_entry(p2m, ept_entry);",
                "        if ( !next )",
                "        /* e is now stale and hence may not be used anymore below. */",
                "    else if ( is_epte_superpage(&e) )",
                "    *table = next ?: map_domain_page(_mfn(e.mfn));"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15567",
        "func_name": "xen-project/xen/ept_split_super_page",
        "description": "An issue was discovered in Xen through 4.13.x, allowing Intel guest OS users to gain privileges or cause a denial of service because of non-atomic modification of a live EPT PTE. When mapping guest EPT (nested paging) tables, Xen would in some circumstances use a series of non-atomic bitfield writes. Depending on the compiler version and optimisation flags, Xen might expose a dangerous partially written PTE to the hardware, which an attacker might be able to race to exploit. A guest administrator or perhaps even an unprivileged guest user might be able to cause denial of service, data corruption, or privilege escalation. Only systems using Intel CPUs are vulnerable. Systems using AMD CPUs, and Arm systems, are not vulnerable. Only systems using nested paging (hap, aka nested paging, aka in this case Intel EPT) are vulnerable. Only HVM and PVH guests can exploit the vulnerability. The presence and scope of the vulnerability depends on the precise optimisations performed by the compiler used to build Xen. If the compiler generates (a) a single 64-bit write, or (b) a series of read-modify-write operations in the same order as the source code, the hypervisor is not vulnerable. For example, in one test build using GCC 8.3 with normal settings, the compiler generated multiple (unlocked) read-modify-write operations in source-code order, which did not constitute a vulnerability. We have not been able to survey compilers; consequently we cannot say which compiler(s) might produce vulnerable code (with which code-generation options). The source code clearly violates the C rules, and thus should be considered vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/1104288186ee73a7f9bfa41cbaa5bb7611521028",
        "commit_title": "x86/EPT: ept_set_middle_entry() related adjustments",
        "commit_text": " ept_split_super_page() wants to further modify the newly allocated table, so have ept_set_middle_entry() return the mapped pointer rather than tearing it down and then getting re-established right again.  Similarly ept_next_level() wants to hand back a mapped pointer of the next level page, so re-use the one established by ept_set_middle_entry() in case that path was taken.  Pull the setting of suppress_ve ahead of insertion into the higher level table, and don't have ept_split_super_page() set the field a 2nd time.  This is part of XSA-328. ",
        "func_before": "static bool_t ept_split_super_page(struct p2m_domain *p2m,\n                                   ept_entry_t *ept_entry,\n                                   unsigned int level, unsigned int target)\n{\n    ept_entry_t new_ept, *table;\n    uint64_t trunk;\n    unsigned int i;\n    bool_t rv = 1;\n\n    /* End if the entry is a leaf entry or reaches the target level. */\n    if ( level <= target )\n        return 1;\n\n    ASSERT(is_epte_superpage(ept_entry));\n\n    if ( !ept_set_middle_entry(p2m, &new_ept) )\n        return 0;\n\n    table = map_domain_page(_mfn(new_ept.mfn));\n    trunk = 1UL << ((level - 1) * EPT_TABLE_ORDER);\n\n    for ( i = 0; i < EPT_PAGETABLE_ENTRIES; i++ )\n    {\n        ept_entry_t *epte = table + i;\n\n        *epte = *ept_entry;\n        epte->sp = (level > 1);\n        epte->mfn += i * trunk;\n        epte->snp = is_iommu_enabled(p2m->domain) && iommu_snoop;\n        epte->suppress_ve = 1;\n\n        ept_p2m_type_to_flags(p2m, epte);\n\n        if ( (level - 1) == target )\n            continue;\n\n        ASSERT(is_epte_superpage(epte));\n\n        if ( !(rv = ept_split_super_page(p2m, epte, level - 1, target)) )\n            break;\n    }\n\n    unmap_domain_page(table);\n\n    /* Even failed we should install the newly allocated ept page. */\n    *ept_entry = new_ept;\n\n    return rv;\n}",
        "func": "static bool_t ept_split_super_page(struct p2m_domain *p2m,\n                                   ept_entry_t *ept_entry,\n                                   unsigned int level, unsigned int target)\n{\n    ept_entry_t new_ept, *table;\n    uint64_t trunk;\n    unsigned int i;\n    bool_t rv = 1;\n\n    /* End if the entry is a leaf entry or reaches the target level. */\n    if ( level <= target )\n        return 1;\n\n    ASSERT(is_epte_superpage(ept_entry));\n\n    table = ept_set_middle_entry(p2m, &new_ept);\n    if ( !table )\n        return 0;\n\n    trunk = 1UL << ((level - 1) * EPT_TABLE_ORDER);\n\n    for ( i = 0; i < EPT_PAGETABLE_ENTRIES; i++ )\n    {\n        ept_entry_t *epte = table + i;\n\n        *epte = *ept_entry;\n        epte->sp = (level > 1);\n        epte->mfn += i * trunk;\n        epte->snp = is_iommu_enabled(p2m->domain) && iommu_snoop;\n\n        ept_p2m_type_to_flags(p2m, epte);\n\n        if ( (level - 1) == target )\n            continue;\n\n        ASSERT(is_epte_superpage(epte));\n\n        if ( !(rv = ept_split_super_page(p2m, epte, level - 1, target)) )\n            break;\n    }\n\n    unmap_domain_page(table);\n\n    /* Even failed we should install the newly allocated ept page. */\n    *ept_entry = new_ept;\n\n    return rv;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,10 +13,10 @@\n \n     ASSERT(is_epte_superpage(ept_entry));\n \n-    if ( !ept_set_middle_entry(p2m, &new_ept) )\n+    table = ept_set_middle_entry(p2m, &new_ept);\n+    if ( !table )\n         return 0;\n \n-    table = map_domain_page(_mfn(new_ept.mfn));\n     trunk = 1UL << ((level - 1) * EPT_TABLE_ORDER);\n \n     for ( i = 0; i < EPT_PAGETABLE_ENTRIES; i++ )\n@@ -27,7 +27,6 @@\n         epte->sp = (level > 1);\n         epte->mfn += i * trunk;\n         epte->snp = is_iommu_enabled(p2m->domain) && iommu_snoop;\n-        epte->suppress_ve = 1;\n \n         ept_p2m_type_to_flags(p2m, epte);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( !ept_set_middle_entry(p2m, &new_ept) )",
                "    table = map_domain_page(_mfn(new_ept.mfn));",
                "        epte->suppress_ve = 1;"
            ],
            "added_lines": [
                "    table = ept_set_middle_entry(p2m, &new_ept);",
                "    if ( !table )"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15567",
        "func_name": "xen-project/xen/ept_set_middle_entry",
        "description": "An issue was discovered in Xen through 4.13.x, allowing Intel guest OS users to gain privileges or cause a denial of service because of non-atomic modification of a live EPT PTE. When mapping guest EPT (nested paging) tables, Xen would in some circumstances use a series of non-atomic bitfield writes. Depending on the compiler version and optimisation flags, Xen might expose a dangerous partially written PTE to the hardware, which an attacker might be able to race to exploit. A guest administrator or perhaps even an unprivileged guest user might be able to cause denial of service, data corruption, or privilege escalation. Only systems using Intel CPUs are vulnerable. Systems using AMD CPUs, and Arm systems, are not vulnerable. Only systems using nested paging (hap, aka nested paging, aka in this case Intel EPT) are vulnerable. Only HVM and PVH guests can exploit the vulnerability. The presence and scope of the vulnerability depends on the precise optimisations performed by the compiler used to build Xen. If the compiler generates (a) a single 64-bit write, or (b) a series of read-modify-write operations in the same order as the source code, the hypervisor is not vulnerable. For example, in one test build using GCC 8.3 with normal settings, the compiler generated multiple (unlocked) read-modify-write operations in source-code order, which did not constitute a vulnerability. We have not been able to survey compilers; consequently we cannot say which compiler(s) might produce vulnerable code (with which code-generation options). The source code clearly violates the C rules, and thus should be considered vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/1104288186ee73a7f9bfa41cbaa5bb7611521028",
        "commit_title": "x86/EPT: ept_set_middle_entry() related adjustments",
        "commit_text": " ept_split_super_page() wants to further modify the newly allocated table, so have ept_set_middle_entry() return the mapped pointer rather than tearing it down and then getting re-established right again.  Similarly ept_next_level() wants to hand back a mapped pointer of the next level page, so re-use the one established by ept_set_middle_entry() in case that path was taken.  Pull the setting of suppress_ve ahead of insertion into the higher level table, and don't have ept_split_super_page() set the field a 2nd time.  This is part of XSA-328. ",
        "func_before": "static int ept_set_middle_entry(struct p2m_domain *p2m, ept_entry_t *ept_entry)\n{\n    mfn_t mfn;\n    ept_entry_t *table;\n    unsigned int i;\n\n    mfn = p2m_alloc_ptp(p2m, 0);\n    if ( mfn_eq(mfn, INVALID_MFN) )\n        return 0;\n\n    ept_entry->epte = 0;\n    ept_entry->mfn = mfn_x(mfn);\n    ept_entry->access = p2m->default_access;\n\n    ept_entry->r = ept_entry->w = ept_entry->x = 1;\n    /* Manually set A bit to avoid overhead of MMU having to write it later. */\n    ept_entry->a = !!cpu_has_vmx_ept_ad;\n\n    ept_entry->suppress_ve = 1;\n\n    table = map_domain_page(mfn);\n\n    for ( i = 0; i < EPT_PAGETABLE_ENTRIES; i++ )\n        table[i].suppress_ve = 1;\n\n    unmap_domain_page(table);\n\n    return 1;\n}",
        "func": "static ept_entry_t *ept_set_middle_entry(struct p2m_domain *p2m,\n                                         ept_entry_t *ept_entry)\n{\n    mfn_t mfn;\n    ept_entry_t *table;\n    unsigned int i;\n\n    mfn = p2m_alloc_ptp(p2m, 0);\n    if ( mfn_eq(mfn, INVALID_MFN) )\n        return NULL;\n\n    table = map_domain_page(mfn);\n\n    for ( i = 0; i < EPT_PAGETABLE_ENTRIES; i++ )\n        table[i].suppress_ve = 1;\n\n    ept_entry->epte = 0;\n    ept_entry->mfn = mfn_x(mfn);\n    ept_entry->access = p2m->default_access;\n\n    ept_entry->r = ept_entry->w = ept_entry->x = 1;\n    /* Manually set A bit to avoid overhead of MMU having to write it later. */\n    ept_entry->a = !!cpu_has_vmx_ept_ad;\n\n    ept_entry->suppress_ve = 1;\n\n    return table;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n-static int ept_set_middle_entry(struct p2m_domain *p2m, ept_entry_t *ept_entry)\n+static ept_entry_t *ept_set_middle_entry(struct p2m_domain *p2m,\n+                                         ept_entry_t *ept_entry)\n {\n     mfn_t mfn;\n     ept_entry_t *table;\n@@ -6,7 +7,12 @@\n \n     mfn = p2m_alloc_ptp(p2m, 0);\n     if ( mfn_eq(mfn, INVALID_MFN) )\n-        return 0;\n+        return NULL;\n+\n+    table = map_domain_page(mfn);\n+\n+    for ( i = 0; i < EPT_PAGETABLE_ENTRIES; i++ )\n+        table[i].suppress_ve = 1;\n \n     ept_entry->epte = 0;\n     ept_entry->mfn = mfn_x(mfn);\n@@ -18,12 +24,5 @@\n \n     ept_entry->suppress_ve = 1;\n \n-    table = map_domain_page(mfn);\n-\n-    for ( i = 0; i < EPT_PAGETABLE_ENTRIES; i++ )\n-        table[i].suppress_ve = 1;\n-\n-    unmap_domain_page(table);\n-\n-    return 1;\n+    return table;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static int ept_set_middle_entry(struct p2m_domain *p2m, ept_entry_t *ept_entry)",
                "        return 0;",
                "    table = map_domain_page(mfn);",
                "",
                "    for ( i = 0; i < EPT_PAGETABLE_ENTRIES; i++ )",
                "        table[i].suppress_ve = 1;",
                "",
                "    unmap_domain_page(table);",
                "",
                "    return 1;"
            ],
            "added_lines": [
                "static ept_entry_t *ept_set_middle_entry(struct p2m_domain *p2m,",
                "                                         ept_entry_t *ept_entry)",
                "        return NULL;",
                "",
                "    table = map_domain_page(mfn);",
                "",
                "    for ( i = 0; i < EPT_PAGETABLE_ENTRIES; i++ )",
                "        table[i].suppress_ve = 1;",
                "    return table;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25285",
        "func_name": "torvalds/linux/hugetlb_sysctl_handler_common",
        "description": "A race condition between hugetlb sysctl handlers in mm/hugetlb.c in the Linux kernel before 5.8.8 could be used by local attackers to corrupt memory, cause a NULL pointer dereference, or possibly have unspecified other impact, aka CID-17743798d812.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17743798d81238ab13050e8e2833699b54e15467",
        "commit_title": "There is a race between the assignment of `table->data` and write value",
        "commit_text": "to the pointer of `table->data` in the __do_proc_doulongvec_minmax() on the other thread.    CPU0:                                 CPU1:                                         proc_sys_write   hugetlb_sysctl_handler                  proc_sys_call_handler   hugetlb_sysctl_handler_common             hugetlb_sysctl_handler     table->data = &tmp;                       hugetlb_sysctl_handler_common                                                 table->data = &tmp;       proc_doulongvec_minmax         do_proc_doulongvec_minmax           sysctl_head_finish           __do_proc_doulongvec_minmax         unuse_table             i = table->data;             *i = val;  // corrupt CPU1's stack  Fix this by duplicating the `table`, and only update the duplicate of it.  And introduce a helper of proc_hugetlb_doulongvec_minmax() to simplify the code.  The following oops was seen:      BUG: kernel NULL pointer dereference, address: 0000000000000000     #PF: supervisor instruction fetch in kernel mode     #PF: error_code(0x0010) - not-present page     Code: Bad RIP value.     ...     Call Trace:      ? set_max_huge_pages+0x3da/0x4f0      ? alloc_pool_huge_page+0x150/0x150      ? proc_doulongvec_minmax+0x46/0x60      ? hugetlb_sysctl_handler_common+0x1c7/0x200      ? nr_hugepages_store+0x20/0x20      ? copy_fd_bitmaps+0x170/0x170      ? hugetlb_sysctl_handler+0x1e/0x20      ? proc_sys_call_handler+0x2f1/0x300      ? unregister_sysctl_table+0xb0/0xb0      ? __fd_install+0x78/0x100      ? proc_sys_write+0x14/0x20      ? __vfs_write+0x4d/0x90      ? vfs_write+0xef/0x240      ? ksys_write+0xc0/0x160      ? __ia32_sys_read+0x50/0x50      ? __close_fd+0x129/0x150      ? __x64_sys_write+0x43/0x50      ? do_syscall_64+0x6c/0x200      ? entry_SYSCALL_64_after_hwframe+0x44/0xa9  Cc: Andi Kleen <ak@linux.intel.com> Link: http://lkml.kernel.org/r/20200828031146.43035-1-songmuchun@bytedance.com ",
        "func_before": "static int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}",
        "func": "static int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,9 +9,8 @@\n \tif (!hugepages_supported())\n \t\treturn -EOPNOTSUPP;\n \n-\ttable->data = &tmp;\n-\ttable->maxlen = sizeof(unsigned long);\n-\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n+\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n+\t\t\t\t\t     &tmp);\n \tif (ret)\n \t\tgoto out;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\ttable->data = &tmp;",
                "\ttable->maxlen = sizeof(unsigned long);",
                "\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);"
            ],
            "added_lines": [
                "\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,",
                "\t\t\t\t\t     &tmp);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25285",
        "func_name": "torvalds/linux/hugetlb_overcommit_handler",
        "description": "A race condition between hugetlb sysctl handlers in mm/hugetlb.c in the Linux kernel before 5.8.8 could be used by local attackers to corrupt memory, cause a NULL pointer dereference, or possibly have unspecified other impact, aka CID-17743798d812.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17743798d81238ab13050e8e2833699b54e15467",
        "commit_title": "There is a race between the assignment of `table->data` and write value",
        "commit_text": "to the pointer of `table->data` in the __do_proc_doulongvec_minmax() on the other thread.    CPU0:                                 CPU1:                                         proc_sys_write   hugetlb_sysctl_handler                  proc_sys_call_handler   hugetlb_sysctl_handler_common             hugetlb_sysctl_handler     table->data = &tmp;                       hugetlb_sysctl_handler_common                                                 table->data = &tmp;       proc_doulongvec_minmax         do_proc_doulongvec_minmax           sysctl_head_finish           __do_proc_doulongvec_minmax         unuse_table             i = table->data;             *i = val;  // corrupt CPU1's stack  Fix this by duplicating the `table`, and only update the duplicate of it.  And introduce a helper of proc_hugetlb_doulongvec_minmax() to simplify the code.  The following oops was seen:      BUG: kernel NULL pointer dereference, address: 0000000000000000     #PF: supervisor instruction fetch in kernel mode     #PF: error_code(0x0010) - not-present page     Code: Bad RIP value.     ...     Call Trace:      ? set_max_huge_pages+0x3da/0x4f0      ? alloc_pool_huge_page+0x150/0x150      ? proc_doulongvec_minmax+0x46/0x60      ? hugetlb_sysctl_handler_common+0x1c7/0x200      ? nr_hugepages_store+0x20/0x20      ? copy_fd_bitmaps+0x170/0x170      ? hugetlb_sysctl_handler+0x1e/0x20      ? proc_sys_call_handler+0x2f1/0x300      ? unregister_sysctl_table+0xb0/0xb0      ? __fd_install+0x78/0x100      ? proc_sys_write+0x14/0x20      ? __vfs_write+0x4d/0x90      ? vfs_write+0xef/0x240      ? ksys_write+0xc0/0x160      ? __ia32_sys_read+0x50/0x50      ? __close_fd+0x129/0x150      ? __x64_sys_write+0x43/0x50      ? do_syscall_64+0x6c/0x200      ? entry_SYSCALL_64_after_hwframe+0x44/0xa9  Cc: Andi Kleen <ak@linux.intel.com> Link: http://lkml.kernel.org/r/20200828031146.43035-1-songmuchun@bytedance.com ",
        "func_before": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
        "func": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,9 +13,8 @@\n \tif (write && hstate_is_gigantic(h))\n \t\treturn -EINVAL;\n \n-\ttable->data = &tmp;\n-\ttable->maxlen = sizeof(unsigned long);\n-\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n+\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n+\t\t\t\t\t     &tmp);\n \tif (ret)\n \t\tgoto out;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\ttable->data = &tmp;",
                "\ttable->maxlen = sizeof(unsigned long);",
                "\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);"
            ],
            "added_lines": [
                "\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,",
                "\t\t\t\t\t     &tmp);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1672",
        "func_name": "latchset/tang/create_new_keys",
        "description": "A race condition exists in the Tang server functionality for key generation and key rotation. This flaw results in a small time window where Tang private keys become readable by other processes on the same host.",
        "git_url": "https://github.com/latchset/tang/commit/8dbbed10870378f1b2c3cf3df2ea7edca7617096",
        "commit_title": "Fix race condition when creating/rotating keys (#123)",
        "commit_text": " When we create/rotate keys using either the tangd-keygen and\r tangd-rotate-keys helpers, there is a small window between the\r keys being created and then the proper ownership permissions being\r set. This also happens when there are no keys and tang creates a\r pair of keys itself.\r \r In certain situations, such as the keys directory having wide open\r permissions, a user with local access could exploit this race\r condition and read the keys before they are set to more restrictive\r permissions.\r \r To prevent this issue, we now set the default umask to 0337 before\r creating the files, so that they are already created with restrictive\r permissions; afterwards, we set the proper ownership as usual.\r \r Issue reported by Brian McDermott of CENSUS labs.\r \r Fixes CVE-2023-1672\r \r \r",
        "func_before": "static int\ncreate_new_keys(const char* jwkdir)\n{\n    const char* alg[] = {\"ES512\", \"ECMR\", NULL};\n    char path[PATH_MAX];\n    for (int i = 0; alg[i] != NULL; i++) {\n        json_auto_t* jwk = jwk_generate(alg[i]);\n        if (!jwk) {\n            return 0;\n        }\n        __attribute__ ((__cleanup__(cleanup_str))) char* thp = jwk_thumbprint(jwk, DEFAULT_THP_HASH);\n        if (!thp) {\n            return 0;\n        }\n        if (snprintf(path, PATH_MAX, \"%s/%s.jwk\", jwkdir, thp) < 0) {\n            fprintf(stderr, \"Unable to prepare variable with file full path (%s)\\n\", thp);\n            return 0;\n        }\n        path[sizeof(path) - 1] = '\\0';\n        if (json_dump_file(jwk, path, 0) == -1) {\n            fprintf(stderr, \"Error saving JWK to file (%s)\\n\", path);\n            return 0;\n        }\n\n        /* Set 0440 permission for the new key. */\n        if (chmod(path, S_IRUSR | S_IRGRP) == -1) {\n            fprintf(stderr, \"Unable to set permissions for JWK file (%s)\\n\", path);\n            return 0;\n        }\n    }\n    return 1;\n}",
        "func": "static int\ncreate_new_keys(const char* jwkdir)\n{\n    const char* alg[] = {\"ES512\", \"ECMR\", NULL};\n    char path[PATH_MAX];\n\n    /* Set default umask for file creation. */\n    umask(0337);\n    for (int i = 0; alg[i] != NULL; i++) {\n        json_auto_t* jwk = jwk_generate(alg[i]);\n        if (!jwk) {\n            return 0;\n        }\n        __attribute__ ((__cleanup__(cleanup_str))) char* thp = jwk_thumbprint(jwk, DEFAULT_THP_HASH);\n        if (!thp) {\n            return 0;\n        }\n        if (snprintf(path, PATH_MAX, \"%s/%s.jwk\", jwkdir, thp) < 0) {\n            fprintf(stderr, \"Unable to prepare variable with file full path (%s)\\n\", thp);\n            return 0;\n        }\n        path[sizeof(path) - 1] = '\\0';\n        if (json_dump_file(jwk, path, 0) == -1) {\n            fprintf(stderr, \"Error saving JWK to file (%s)\\n\", path);\n            return 0;\n        }\n\n        /* Set 0440 permission for the new key. */\n        if (chmod(path, S_IRUSR | S_IRGRP) == -1) {\n            fprintf(stderr, \"Unable to set permissions for JWK file (%s)\\n\", path);\n            return 0;\n        }\n    }\n    return 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,9 @@\n {\n     const char* alg[] = {\"ES512\", \"ECMR\", NULL};\n     char path[PATH_MAX];\n+\n+    /* Set default umask for file creation. */\n+    umask(0337);\n     for (int i = 0; alg[i] != NULL; i++) {\n         json_auto_t* jwk = jwk_generate(alg[i]);\n         if (!jwk) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    /* Set default umask for file creation. */",
                "    umask(0337);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-3108",
        "func_name": "torvalds/linux/af_alg_make_sg",
        "description": "A flaw was found in the subsequent get_user_pages_fast in the Linux kernel’s interface for symmetric key cipher algorithms in the skcipher_recvmsg of crypto/algif_skcipher.c function. This flaw allows a local user to crash the system.",
        "git_url": "https://github.com/torvalds/linux/commit/9399f0c51489ae8c16d6559b82a452fdc1895e91",
        "commit_title": "crypto: fix af_alg_make_sg() conversion to iov_iter",
        "commit_text": " Commit 1d10eb2f156f (\"crypto: switch af_alg_make_sg() to iov_iter\") broke af_alg_make_sg() and skcipher_recvmsg() in the process of moving them to the iov_iter interfaces.  The 'npages' calculation in the formar calculated the number of *bytes* in the pages, and in the latter case the conversion didn't re-read the value of 'ctx->used' after waiting for it to become non-zero.  This reverts to the original code for both these cases.  Cc: Al Viro <viro@zeniv.linux.org.uk> Cc: David Miller <davem@davemloft.net>",
        "func_before": "int af_alg_make_sg(struct af_alg_sgl *sgl, struct iov_iter *iter, int len)\n{\n\tsize_t off;\n\tssize_t n;\n\tint npages, i;\n\n\tn = iov_iter_get_pages(iter, sgl->pages, len, ALG_MAX_PAGES, &off);\n\tif (n < 0)\n\t\treturn n;\n\n\tnpages = PAGE_ALIGN(off + n);\n\tif (WARN_ON(npages == 0))\n\t\treturn -EINVAL;\n\n\tsg_init_table(sgl->sg, npages);\n\n\tfor (i = 0, len = n; i < npages; i++) {\n\t\tint plen = min_t(int, len, PAGE_SIZE - off);\n\n\t\tsg_set_page(sgl->sg + i, sgl->pages[i], plen, off);\n\n\t\toff = 0;\n\t\tlen -= plen;\n\t}\n\treturn n;\n}",
        "func": "int af_alg_make_sg(struct af_alg_sgl *sgl, struct iov_iter *iter, int len)\n{\n\tsize_t off;\n\tssize_t n;\n\tint npages, i;\n\n\tn = iov_iter_get_pages(iter, sgl->pages, len, ALG_MAX_PAGES, &off);\n\tif (n < 0)\n\t\treturn n;\n\n\tnpages = (off + n + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tif (WARN_ON(npages == 0))\n\t\treturn -EINVAL;\n\n\tsg_init_table(sgl->sg, npages);\n\n\tfor (i = 0, len = n; i < npages; i++) {\n\t\tint plen = min_t(int, len, PAGE_SIZE - off);\n\n\t\tsg_set_page(sgl->sg + i, sgl->pages[i], plen, off);\n\n\t\toff = 0;\n\t\tlen -= plen;\n\t}\n\treturn n;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,7 @@\n \tif (n < 0)\n \t\treturn n;\n \n-\tnpages = PAGE_ALIGN(off + n);\n+\tnpages = (off + n + PAGE_SIZE - 1) >> PAGE_SHIFT;\n \tif (WARN_ON(npages == 0))\n \t\treturn -EINVAL;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tnpages = PAGE_ALIGN(off + n);"
            ],
            "added_lines": [
                "\tnpages = (off + n + PAGE_SIZE - 1) >> PAGE_SHIFT;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-3108",
        "func_name": "torvalds/linux/skcipher_recvmsg",
        "description": "A flaw was found in the subsequent get_user_pages_fast in the Linux kernel’s interface for symmetric key cipher algorithms in the skcipher_recvmsg of crypto/algif_skcipher.c function. This flaw allows a local user to crash the system.",
        "git_url": "https://github.com/torvalds/linux/commit/9399f0c51489ae8c16d6559b82a452fdc1895e91",
        "commit_title": "crypto: fix af_alg_make_sg() conversion to iov_iter",
        "commit_text": " Commit 1d10eb2f156f (\"crypto: switch af_alg_make_sg() to iov_iter\") broke af_alg_make_sg() and skcipher_recvmsg() in the process of moving them to the iov_iter interfaces.  The 'npages' calculation in the formar calculated the number of *bytes* in the pages, and in the latter case the conversion didn't re-read the value of 'ctx->used' after waiting for it to become non-zero.  This reverts to the original code for both these cases.  Cc: Al Viro <viro@zeniv.linux.org.uk> Cc: David Miller <davem@davemloft.net>",
        "func_before": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\twhile (iov_iter_count(&msg->msg_iter)) {\n\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t       struct skcipher_sg_list, list);\n\t\tsg = sgl->sg;\n\n\t\twhile (!sg->length)\n\t\t\tsg++;\n\n\t\tused = ctx->used;\n\t\tif (!used) {\n\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t}\n\n\t\tused = min_t(unsigned long, used, iov_iter_count(&msg->msg_iter));\n\n\t\tused = af_alg_make_sg(&ctx->rsgl, &msg->msg_iter, used);\n\t\terr = used;\n\t\tif (err < 0)\n\t\t\tgoto unlock;\n\n\t\tif (ctx->more || used < ctx->used)\n\t\t\tused -= used % bs;\n\n\t\terr = -EINVAL;\n\t\tif (!used)\n\t\t\tgoto free;\n\n\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t     ctx->iv);\n\n\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\tif (err)\n\t\t\tgoto unlock;\n\n\t\tcopied += used;\n\t\tskcipher_pull_sgl(sk, used);\n\t\tiov_iter_advance(&msg->msg_iter, used);\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "func": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\twhile (iov_iter_count(&msg->msg_iter)) {\n\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t       struct skcipher_sg_list, list);\n\t\tsg = sgl->sg;\n\n\t\twhile (!sg->length)\n\t\t\tsg++;\n\n\t\tif (!ctx->used) {\n\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t}\n\n\t\tused = min_t(unsigned long, ctx->used, iov_iter_count(&msg->msg_iter));\n\n\t\tused = af_alg_make_sg(&ctx->rsgl, &msg->msg_iter, used);\n\t\terr = used;\n\t\tif (err < 0)\n\t\t\tgoto unlock;\n\n\t\tif (ctx->more || used < ctx->used)\n\t\t\tused -= used % bs;\n\n\t\terr = -EINVAL;\n\t\tif (!used)\n\t\t\tgoto free;\n\n\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t     ctx->iv);\n\n\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\tif (err)\n\t\t\tgoto unlock;\n\n\t\tcopied += used;\n\t\tskcipher_pull_sgl(sk, used);\n\t\tiov_iter_advance(&msg->msg_iter, used);\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,14 +21,13 @@\n \t\twhile (!sg->length)\n \t\t\tsg++;\n \n-\t\tused = ctx->used;\n-\t\tif (!used) {\n+\t\tif (!ctx->used) {\n \t\t\terr = skcipher_wait_for_data(sk, flags);\n \t\t\tif (err)\n \t\t\t\tgoto unlock;\n \t\t}\n \n-\t\tused = min_t(unsigned long, used, iov_iter_count(&msg->msg_iter));\n+\t\tused = min_t(unsigned long, ctx->used, iov_iter_count(&msg->msg_iter));\n \n \t\tused = af_alg_make_sg(&ctx->rsgl, &msg->msg_iter, used);\n \t\terr = used;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tused = ctx->used;",
                "\t\tif (!used) {",
                "\t\tused = min_t(unsigned long, used, iov_iter_count(&msg->msg_iter));"
            ],
            "added_lines": [
                "\t\tif (!ctx->used) {",
                "\t\tused = min_t(unsigned long, ctx->used, iov_iter_count(&msg->msg_iter));"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-21262",
        "func_name": "android/AudioPolicyService::startInput",
        "description": "In startInput of AudioPolicyInterfaceImpl.cpp, there is a possible way of erroneously displaying the microphone privacy indicator due to a race condition. This could lead to false user expectations. User interaction is needed for exploitation.\n\n",
        "git_url": "https://android.googlesource.com/platform/frameworks/av/+/2c8973c39478cd3c8cf11d9f27cc0556a106d006",
        "commit_title": "Force unsilence record clients on startInput",
        "commit_text": " We call startRecording unconditionally in startInput, so we must update the client state to be unsilenced (since we are treating as such). We subsequently re-update the silence state (with the client marked as active to dispatch ops) in updateUidStates_l.  This fixes an issue where we call startRecording for a silenced client, then call it again when it moves to unsilenced when the client is active. Since startRecording is ref-counted, this leaves the client in the recording state leading to incorrect appop attributions.  Bug: 279905816 Bug: 281485019 Test: Manual verification of repro cases + verbose log analysis (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:e7720b379bfaba648ab6d85c4c2df6f03ec854d3) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:2951ad10a6641f9b3554d674877ad314e8cc011f) Merged-In: I31d50457ca8adae577407a28d4d4c0e8582bac5d ",
        "func_before": "Status AudioPolicyService::startInput(int32_t portIdAidl)\n{\n    audio_port_handle_t portId = VALUE_OR_RETURN_BINDER_STATUS(\n            aidl2legacy_int32_t_audio_port_handle_t(portIdAidl));\n\n    if (mAudioPolicyManager == NULL) {\n        return binderStatusFromStatusT(NO_INIT);\n    }\n    sp<AudioRecordClient> client;\n    {\n        Mutex::Autolock _l(mLock);\n\n        ssize_t index = mAudioRecordClients.indexOfKey(portId);\n        if (index < 0) {\n            return binderStatusFromStatusT(INVALID_OPERATION);\n        }\n        client = mAudioRecordClients.valueAt(index);\n    }\n\n    std::stringstream msg;\n    msg << \"Audio recording on session \" << client->session;\n\n    // check calling permissions\n    if (!(startRecording(client->attributionSource, String16(msg.str().c_str()),\n                         client->attributes.source)\n            || client->attributes.source == AUDIO_SOURCE_FM_TUNER\n            || client->attributes.source == AUDIO_SOURCE_REMOTE_SUBMIX\n            || client->attributes.source == AUDIO_SOURCE_ECHO_REFERENCE)) {\n        ALOGE(\"%s permission denied: recording not allowed for attribution source %s\",\n                __func__, client->attributionSource.toString().c_str());\n        return binderStatusFromStatusT(PERMISSION_DENIED);\n    }\n\n    Mutex::Autolock _l(mLock);\n\n    client->active = true;\n    client->startTimeNs = systemTime();\n    updateUidStates_l();\n\n    status_t status;\n    {\n        AutoCallerClear acc;\n        status = mAudioPolicyManager->startInput(portId);\n\n    }\n\n    // including successes gets very verbose\n    // but once we cut over to statsd, log them all.\n    if (status != NO_ERROR) {\n\n        static constexpr char kAudioPolicy[] = \"audiopolicy\";\n\n        static constexpr char kAudioPolicyStatus[] = \"android.media.audiopolicy.status\";\n        static constexpr char kAudioPolicyRqstSrc[] = \"android.media.audiopolicy.rqst.src\";\n        static constexpr char kAudioPolicyRqstPkg[] = \"android.media.audiopolicy.rqst.pkg\";\n        static constexpr char kAudioPolicyRqstSession[] = \"android.media.audiopolicy.rqst.session\";\n        static constexpr char kAudioPolicyRqstDevice[] =\n                \"android.media.audiopolicy.rqst.device\";\n        static constexpr char kAudioPolicyActiveSrc[] = \"android.media.audiopolicy.active.src\";\n        static constexpr char kAudioPolicyActivePkg[] = \"android.media.audiopolicy.active.pkg\";\n        static constexpr char kAudioPolicyActiveSession[] =\n                \"android.media.audiopolicy.active.session\";\n        static constexpr char kAudioPolicyActiveDevice[] =\n                \"android.media.audiopolicy.active.device\";\n\n        mediametrics::Item *item = mediametrics::Item::create(kAudioPolicy);\n        if (item != NULL) {\n\n            item->setInt32(kAudioPolicyStatus, status);\n\n            item->setCString(kAudioPolicyRqstSrc,\n                             toString(client->attributes.source).c_str());\n            item->setInt32(kAudioPolicyRqstSession, client->session);\n            if (client->attributionSource.packageName.has_value() &&\n                client->attributionSource.packageName.value().size() != 0) {\n                item->setCString(kAudioPolicyRqstPkg,\n                    client->attributionSource.packageName.value().c_str());\n            } else {\n                item->setCString(kAudioPolicyRqstPkg,\n                    std::to_string(client->attributionSource.uid).c_str());\n            }\n            item->setCString(\n                    kAudioPolicyRqstDevice, getDeviceTypeStrForPortId(client->deviceId).c_str());\n\n            int count = mAudioRecordClients.size();\n            for (int i = 0; i < count ; i++) {\n                if (portId == mAudioRecordClients.keyAt(i)) {\n                    continue;\n                }\n                sp<AudioRecordClient> other = mAudioRecordClients.valueAt(i);\n                if (other->active) {\n                    // keeps the last of the clients marked active\n                    item->setCString(kAudioPolicyActiveSrc,\n                                     toString(other->attributes.source).c_str());\n                    item->setInt32(kAudioPolicyActiveSession, other->session);\n                    if (other->attributionSource.packageName.has_value() &&\n                        other->attributionSource.packageName.value().size() != 0) {\n                        item->setCString(kAudioPolicyActivePkg,\n                            other->attributionSource.packageName.value().c_str());\n                    } else {\n                        item->setCString(kAudioPolicyRqstPkg, std::to_string(\n                            other->attributionSource.uid).c_str());\n                    }\n                    item->setCString(kAudioPolicyActiveDevice,\n                                     getDeviceTypeStrForPortId(other->deviceId).c_str());\n                }\n            }\n            item->selfrecord();\n            delete item;\n            item = NULL;\n        }\n    }\n\n    if (status != NO_ERROR) {\n        client->active = false;\n        client->startTimeNs = 0;\n        updateUidStates_l();\n        finishRecording(client->attributionSource, client->attributes.source);\n    }\n\n    return binderStatusFromStatusT(status);\n}",
        "func": "Status AudioPolicyService::startInput(int32_t portIdAidl)\n{\n    audio_port_handle_t portId = VALUE_OR_RETURN_BINDER_STATUS(\n            aidl2legacy_int32_t_audio_port_handle_t(portIdAidl));\n\n    if (mAudioPolicyManager == NULL) {\n        return binderStatusFromStatusT(NO_INIT);\n    }\n    sp<AudioRecordClient> client;\n    {\n        Mutex::Autolock _l(mLock);\n\n        ssize_t index = mAudioRecordClients.indexOfKey(portId);\n        if (index < 0) {\n            return binderStatusFromStatusT(INVALID_OPERATION);\n        }\n        client = mAudioRecordClients.valueAt(index);\n    }\n\n    std::stringstream msg;\n    msg << \"Audio recording on session \" << client->session;\n\n    // check calling permissions\n    if (!(startRecording(client->attributionSource, String16(msg.str().c_str()),\n                         client->attributes.source)\n            || client->attributes.source == AUDIO_SOURCE_FM_TUNER\n            || client->attributes.source == AUDIO_SOURCE_REMOTE_SUBMIX\n            || client->attributes.source == AUDIO_SOURCE_ECHO_REFERENCE)) {\n        ALOGE(\"%s permission denied: recording not allowed for attribution source %s\",\n                __func__, client->attributionSource.toString().c_str());\n        return binderStatusFromStatusT(PERMISSION_DENIED);\n    }\n\n    Mutex::Autolock _l(mLock);\n\n    ALOGW_IF(client->silenced, \"startInput on silenced input for port %d, uid %d. Unsilencing.\",\n            portIdAidl,\n            client->attributionSource.uid);\n\n    if (client->active) {\n        ALOGE(\"Client should never be active before startInput. Uid %d port %d\",\n                client->attributionSource.uid, portId);\n        finishRecording(client->attributionSource, client->attributes.source);\n        return binderStatusFromStatusT(INVALID_OPERATION);\n    }\n\n    // Force the possibly silenced client to be unsilenced since we just called\n    // startRecording (i.e. we have assumed it is unsilenced).\n    // At this point in time, the client is inactive, so no calls to appops are sent in\n    // setAppState_l.\n    // This ensures existing clients have the same behavior as new clients (starting unsilenced).\n    // TODO(b/282076713)\n    setAppState_l(client, APP_STATE_TOP);\n\n    client->active = true;\n    client->startTimeNs = systemTime();\n    // This call updates the silenced state, and since we are active, appropriately notifies appops\n    // if we silence the track.\n    updateUidStates_l();\n\n    status_t status;\n    {\n        AutoCallerClear acc;\n        status = mAudioPolicyManager->startInput(portId);\n\n    }\n\n    // including successes gets very verbose\n    // but once we cut over to statsd, log them all.\n    if (status != NO_ERROR) {\n\n        static constexpr char kAudioPolicy[] = \"audiopolicy\";\n\n        static constexpr char kAudioPolicyStatus[] = \"android.media.audiopolicy.status\";\n        static constexpr char kAudioPolicyRqstSrc[] = \"android.media.audiopolicy.rqst.src\";\n        static constexpr char kAudioPolicyRqstPkg[] = \"android.media.audiopolicy.rqst.pkg\";\n        static constexpr char kAudioPolicyRqstSession[] = \"android.media.audiopolicy.rqst.session\";\n        static constexpr char kAudioPolicyRqstDevice[] =\n                \"android.media.audiopolicy.rqst.device\";\n        static constexpr char kAudioPolicyActiveSrc[] = \"android.media.audiopolicy.active.src\";\n        static constexpr char kAudioPolicyActivePkg[] = \"android.media.audiopolicy.active.pkg\";\n        static constexpr char kAudioPolicyActiveSession[] =\n                \"android.media.audiopolicy.active.session\";\n        static constexpr char kAudioPolicyActiveDevice[] =\n                \"android.media.audiopolicy.active.device\";\n\n        mediametrics::Item *item = mediametrics::Item::create(kAudioPolicy);\n        if (item != NULL) {\n\n            item->setInt32(kAudioPolicyStatus, status);\n\n            item->setCString(kAudioPolicyRqstSrc,\n                             toString(client->attributes.source).c_str());\n            item->setInt32(kAudioPolicyRqstSession, client->session);\n            if (client->attributionSource.packageName.has_value() &&\n                client->attributionSource.packageName.value().size() != 0) {\n                item->setCString(kAudioPolicyRqstPkg,\n                    client->attributionSource.packageName.value().c_str());\n            } else {\n                item->setCString(kAudioPolicyRqstPkg,\n                    std::to_string(client->attributionSource.uid).c_str());\n            }\n            item->setCString(\n                    kAudioPolicyRqstDevice, getDeviceTypeStrForPortId(client->deviceId).c_str());\n\n            int count = mAudioRecordClients.size();\n            for (int i = 0; i < count ; i++) {\n                if (portId == mAudioRecordClients.keyAt(i)) {\n                    continue;\n                }\n                sp<AudioRecordClient> other = mAudioRecordClients.valueAt(i);\n                if (other->active) {\n                    // keeps the last of the clients marked active\n                    item->setCString(kAudioPolicyActiveSrc,\n                                     toString(other->attributes.source).c_str());\n                    item->setInt32(kAudioPolicyActiveSession, other->session);\n                    if (other->attributionSource.packageName.has_value() &&\n                        other->attributionSource.packageName.value().size() != 0) {\n                        item->setCString(kAudioPolicyActivePkg,\n                            other->attributionSource.packageName.value().c_str());\n                    } else {\n                        item->setCString(kAudioPolicyRqstPkg, std::to_string(\n                            other->attributionSource.uid).c_str());\n                    }\n                    item->setCString(kAudioPolicyActiveDevice,\n                                     getDeviceTypeStrForPortId(other->deviceId).c_str());\n                }\n            }\n            item->selfrecord();\n            delete item;\n            item = NULL;\n        }\n    }\n\n    if (status != NO_ERROR) {\n        client->active = false;\n        client->startTimeNs = 0;\n        updateUidStates_l();\n        finishRecording(client->attributionSource, client->attributes.source);\n    }\n\n    return binderStatusFromStatusT(status);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -33,8 +33,29 @@\n \n     Mutex::Autolock _l(mLock);\n \n+    ALOGW_IF(client->silenced, \"startInput on silenced input for port %d, uid %d. Unsilencing.\",\n+            portIdAidl,\n+            client->attributionSource.uid);\n+\n+    if (client->active) {\n+        ALOGE(\"Client should never be active before startInput. Uid %d port %d\",\n+                client->attributionSource.uid, portId);\n+        finishRecording(client->attributionSource, client->attributes.source);\n+        return binderStatusFromStatusT(INVALID_OPERATION);\n+    }\n+\n+    // Force the possibly silenced client to be unsilenced since we just called\n+    // startRecording (i.e. we have assumed it is unsilenced).\n+    // At this point in time, the client is inactive, so no calls to appops are sent in\n+    // setAppState_l.\n+    // This ensures existing clients have the same behavior as new clients (starting unsilenced).\n+    // TODO(b/282076713)\n+    setAppState_l(client, APP_STATE_TOP);\n+\n     client->active = true;\n     client->startTimeNs = systemTime();\n+    // This call updates the silenced state, and since we are active, appropriately notifies appops\n+    // if we silence the track.\n     updateUidStates_l();\n \n     status_t status;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    ALOGW_IF(client->silenced, \"startInput on silenced input for port %d, uid %d. Unsilencing.\",",
                "            portIdAidl,",
                "            client->attributionSource.uid);",
                "",
                "    if (client->active) {",
                "        ALOGE(\"Client should never be active before startInput. Uid %d port %d\",",
                "                client->attributionSource.uid, portId);",
                "        finishRecording(client->attributionSource, client->attributes.source);",
                "        return binderStatusFromStatusT(INVALID_OPERATION);",
                "    }",
                "",
                "    // Force the possibly silenced client to be unsilenced since we just called",
                "    // startRecording (i.e. we have assumed it is unsilenced).",
                "    // At this point in time, the client is inactive, so no calls to appops are sent in",
                "    // setAppState_l.",
                "    // This ensures existing clients have the same behavior as new clients (starting unsilenced).",
                "    // TODO(b/282076713)",
                "    setAppState_l(client, APP_STATE_TOP);",
                "",
                "    // This call updates the silenced state, and since we are active, appropriately notifies appops",
                "    // if we silence the track."
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2069",
        "func_name": "torvalds/linux/switch_mm",
        "description": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU.",
        "git_url": "https://github.com/torvalds/linux/commit/71b3c126e61177eb693423f2e18a1914205b165e",
        "commit_title": "x86/mm: Add barriers and document switch_mm()-vs-flush synchronization",
        "commit_text": " When switch_mm() activates a new PGD, it also sets a bit that tells other CPUs that the PGD is in use so that TLB flush IPIs will be sent.  In order for that to work correctly, the bit needs to be visible prior to loading the PGD and therefore starting to fill the local TLB.  Document all the barriers that make this work correctly and add a couple that were missing.  Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Andy Lutomirski <luto@amacapital.net> Cc: Borislav Petkov <bp@alien8.de> Cc: Brian Gerst <brgerst@gmail.com> Cc: Dave Hansen <dave.hansen@linux.intel.com> Cc: Denys Vlasenko <dvlasenk@redhat.com> Cc: H. Peter Anvin <hpa@zytor.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Rik van Riel <riel@redhat.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: linux-mm@kvack.org Cc: stable@vger.kernel.org",
        "func_before": "static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t\t\t     struct task_struct *tsk)\n{\n\tunsigned cpu = smp_processor_id();\n\n\tif (likely(prev != next)) {\n#ifdef CONFIG_SMP\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tthis_cpu_write(cpu_tlbstate.active_mm, next);\n#endif\n\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t/* Re-load page tables */\n\t\tload_cr3(next->pgd);\n\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\n\t\t/* Stop flush ipis for the previous mm */\n\t\tcpumask_clear_cpu(cpu, mm_cpumask(prev));\n\n\t\t/* Load per-mm CR4 state */\n\t\tload_mm_cr4(next);\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\t\t/*\n\t\t * Load the LDT, if the LDT is different.\n\t\t *\n\t\t * It's possible that prev->context.ldt doesn't match\n\t\t * the LDT register.  This can happen if leave_mm(prev)\n\t\t * was called and then modify_ldt changed\n\t\t * prev->context.ldt but suppressed an IPI to this CPU.\n\t\t * In this case, prev->context.ldt != NULL, because we\n\t\t * never set context.ldt to NULL while the mm still\n\t\t * exists.  That means that next->context.ldt !=\n\t\t * prev->context.ldt, because mms never share an LDT.\n\t\t */\n\t\tif (unlikely(prev->context.ldt != next->context.ldt))\n\t\t\tload_mm_ldt(next);\n#endif\n\t}\n#ifdef CONFIG_SMP\n\t  else {\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tBUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);\n\n\t\tif (!cpumask_test_cpu(cpu, mm_cpumask(next))) {\n\t\t\t/*\n\t\t\t * On established mms, the mm_cpumask is only changed\n\t\t\t * from irq context, from ptep_clear_flush() while in\n\t\t\t * lazy tlb mode, and here. Irqs are blocked during\n\t\t\t * schedule, protecting us from simultaneous changes.\n\t\t\t */\n\t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\t\t\t/*\n\t\t\t * We were in lazy tlb mode and leave_mm disabled\n\t\t\t * tlb flush IPI delivery. We must reload CR3\n\t\t\t * to make sure to use no freed page tables.\n\t\t\t */\n\t\t\tload_cr3(next->pgd);\n\t\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\t\t\tload_mm_cr4(next);\n\t\t\tload_mm_ldt(next);\n\t\t}\n\t}\n#endif\n}",
        "func": "static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t\t\t     struct task_struct *tsk)\n{\n\tunsigned cpu = smp_processor_id();\n\n\tif (likely(prev != next)) {\n#ifdef CONFIG_SMP\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tthis_cpu_write(cpu_tlbstate.active_mm, next);\n#endif\n\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t/*\n\t\t * Re-load page tables.\n\t\t *\n\t\t * This logic has an ordering constraint:\n\t\t *\n\t\t *  CPU 0: Write to a PTE for 'next'\n\t\t *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.\n\t\t *  CPU 1: set bit 1 in next's mm_cpumask\n\t\t *  CPU 1: load from the PTE that CPU 0 writes (implicit)\n\t\t *\n\t\t * We need to prevent an outcome in which CPU 1 observes\n\t\t * the new PTE value and CPU 0 observes bit 1 clear in\n\t\t * mm_cpumask.  (If that occurs, then the IPI will never\n\t\t * be sent, and CPU 0's TLB will contain a stale entry.)\n\t\t *\n\t\t * The bad outcome can occur if either CPU's load is\n\t\t * reordered before that CPU's store, so both CPUs much\n\t\t * execute full barriers to prevent this from happening.\n\t\t *\n\t\t * Thus, switch_mm needs a full barrier between the\n\t\t * store to mm_cpumask and any operation that could load\n\t\t * from next->pgd.  This barrier synchronizes with\n\t\t * remote TLB flushers.  Fortunately, load_cr3 is\n\t\t * serializing and thus acts as a full barrier.\n\t\t *\n\t\t */\n\t\tload_cr3(next->pgd);\n\n\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\n\t\t/* Stop flush ipis for the previous mm */\n\t\tcpumask_clear_cpu(cpu, mm_cpumask(prev));\n\n\t\t/* Load per-mm CR4 state */\n\t\tload_mm_cr4(next);\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\t\t/*\n\t\t * Load the LDT, if the LDT is different.\n\t\t *\n\t\t * It's possible that prev->context.ldt doesn't match\n\t\t * the LDT register.  This can happen if leave_mm(prev)\n\t\t * was called and then modify_ldt changed\n\t\t * prev->context.ldt but suppressed an IPI to this CPU.\n\t\t * In this case, prev->context.ldt != NULL, because we\n\t\t * never set context.ldt to NULL while the mm still\n\t\t * exists.  That means that next->context.ldt !=\n\t\t * prev->context.ldt, because mms never share an LDT.\n\t\t */\n\t\tif (unlikely(prev->context.ldt != next->context.ldt))\n\t\t\tload_mm_ldt(next);\n#endif\n\t}\n#ifdef CONFIG_SMP\n\t  else {\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tBUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);\n\n\t\tif (!cpumask_test_cpu(cpu, mm_cpumask(next))) {\n\t\t\t/*\n\t\t\t * On established mms, the mm_cpumask is only changed\n\t\t\t * from irq context, from ptep_clear_flush() while in\n\t\t\t * lazy tlb mode, and here. Irqs are blocked during\n\t\t\t * schedule, protecting us from simultaneous changes.\n\t\t\t */\n\t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t\t/*\n\t\t\t * We were in lazy tlb mode and leave_mm disabled\n\t\t\t * tlb flush IPI delivery. We must reload CR3\n\t\t\t * to make sure to use no freed page tables.\n\t\t\t *\n\t\t\t * As above, this is a barrier that forces\n\t\t\t * TLB repopulation to be ordered after the\n\t\t\t * store to mm_cpumask.\n\t\t\t */\n\t\t\tload_cr3(next->pgd);\n\t\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\t\t\tload_mm_cr4(next);\n\t\t\tload_mm_ldt(next);\n\t\t}\n\t}\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,8 +10,34 @@\n #endif\n \t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n \n-\t\t/* Re-load page tables */\n+\t\t/*\n+\t\t * Re-load page tables.\n+\t\t *\n+\t\t * This logic has an ordering constraint:\n+\t\t *\n+\t\t *  CPU 0: Write to a PTE for 'next'\n+\t\t *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.\n+\t\t *  CPU 1: set bit 1 in next's mm_cpumask\n+\t\t *  CPU 1: load from the PTE that CPU 0 writes (implicit)\n+\t\t *\n+\t\t * We need to prevent an outcome in which CPU 1 observes\n+\t\t * the new PTE value and CPU 0 observes bit 1 clear in\n+\t\t * mm_cpumask.  (If that occurs, then the IPI will never\n+\t\t * be sent, and CPU 0's TLB will contain a stale entry.)\n+\t\t *\n+\t\t * The bad outcome can occur if either CPU's load is\n+\t\t * reordered before that CPU's store, so both CPUs much\n+\t\t * execute full barriers to prevent this from happening.\n+\t\t *\n+\t\t * Thus, switch_mm needs a full barrier between the\n+\t\t * store to mm_cpumask and any operation that could load\n+\t\t * from next->pgd.  This barrier synchronizes with\n+\t\t * remote TLB flushers.  Fortunately, load_cr3 is\n+\t\t * serializing and thus acts as a full barrier.\n+\t\t *\n+\t\t */\n \t\tload_cr3(next->pgd);\n+\n \t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n \n \t\t/* Stop flush ipis for the previous mm */\n@@ -50,10 +76,15 @@\n \t\t\t * schedule, protecting us from simultaneous changes.\n \t\t\t */\n \t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n+\n \t\t\t/*\n \t\t\t * We were in lazy tlb mode and leave_mm disabled\n \t\t\t * tlb flush IPI delivery. We must reload CR3\n \t\t\t * to make sure to use no freed page tables.\n+\t\t\t *\n+\t\t\t * As above, this is a barrier that forces\n+\t\t\t * TLB repopulation to be ordered after the\n+\t\t\t * store to mm_cpumask.\n \t\t\t */\n \t\t\tload_cr3(next->pgd);\n \t\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t/* Re-load page tables */"
            ],
            "added_lines": [
                "\t\t/*",
                "\t\t * Re-load page tables.",
                "\t\t *",
                "\t\t * This logic has an ordering constraint:",
                "\t\t *",
                "\t\t *  CPU 0: Write to a PTE for 'next'",
                "\t\t *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.",
                "\t\t *  CPU 1: set bit 1 in next's mm_cpumask",
                "\t\t *  CPU 1: load from the PTE that CPU 0 writes (implicit)",
                "\t\t *",
                "\t\t * We need to prevent an outcome in which CPU 1 observes",
                "\t\t * the new PTE value and CPU 0 observes bit 1 clear in",
                "\t\t * mm_cpumask.  (If that occurs, then the IPI will never",
                "\t\t * be sent, and CPU 0's TLB will contain a stale entry.)",
                "\t\t *",
                "\t\t * The bad outcome can occur if either CPU's load is",
                "\t\t * reordered before that CPU's store, so both CPUs much",
                "\t\t * execute full barriers to prevent this from happening.",
                "\t\t *",
                "\t\t * Thus, switch_mm needs a full barrier between the",
                "\t\t * store to mm_cpumask and any operation that could load",
                "\t\t * from next->pgd.  This barrier synchronizes with",
                "\t\t * remote TLB flushers.  Fortunately, load_cr3 is",
                "\t\t * serializing and thus acts as a full barrier.",
                "\t\t *",
                "\t\t */",
                "",
                "",
                "\t\t\t *",
                "\t\t\t * As above, this is a barrier that forces",
                "\t\t\t * TLB repopulation to be ordered after the",
                "\t\t\t * store to mm_cpumask."
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2069",
        "func_name": "torvalds/linux/flush_tlb_mm_range",
        "description": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU.",
        "git_url": "https://github.com/torvalds/linux/commit/71b3c126e61177eb693423f2e18a1914205b165e",
        "commit_title": "x86/mm: Add barriers and document switch_mm()-vs-flush synchronization",
        "commit_text": " When switch_mm() activates a new PGD, it also sets a bit that tells other CPUs that the PGD is in use so that TLB flush IPIs will be sent.  In order for that to work correctly, the bit needs to be visible prior to loading the PGD and therefore starting to fill the local TLB.  Document all the barriers that make this work correctly and add a couple that were missing.  Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Andy Lutomirski <luto@amacapital.net> Cc: Borislav Petkov <bp@alien8.de> Cc: Brian Gerst <brgerst@gmail.com> Cc: Dave Hansen <dave.hansen@linux.intel.com> Cc: Denys Vlasenko <dvlasenk@redhat.com> Cc: H. Peter Anvin <hpa@zytor.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Rik van Riel <riel@redhat.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: linux-mm@kvack.org Cc: stable@vger.kernel.org",
        "func_before": "void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned long vmflag)\n{\n\tunsigned long addr;\n\t/* do a global flush by default */\n\tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n\n\tpreempt_disable();\n\tif (current->active_mm != mm)\n\t\tgoto out;\n\n\tif (!current->mm) {\n\t\tleave_mm(smp_processor_id());\n\t\tgoto out;\n\t}\n\n\tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n\t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n\n\tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n\t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\t\tlocal_flush_tlb();\n\t} else {\n\t\t/* flush range by one by one 'invlpg' */\n\t\tfor (addr = start; addr < end;\taddr += PAGE_SIZE) {\n\t\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);\n\t\t\t__flush_tlb_single(addr);\n\t\t}\n\t}\n\ttrace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);\nout:\n\tif (base_pages_to_flush == TLB_FLUSH_ALL) {\n\t\tstart = 0UL;\n\t\tend = TLB_FLUSH_ALL;\n\t}\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, end);\n\tpreempt_enable();\n}",
        "func": "void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned long vmflag)\n{\n\tunsigned long addr;\n\t/* do a global flush by default */\n\tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n\n\tpreempt_disable();\n\tif (current->active_mm != mm) {\n\t\t/* Synchronize with switch_mm. */\n\t\tsmp_mb();\n\n\t\tgoto out;\n\t}\n\n\tif (!current->mm) {\n\t\tleave_mm(smp_processor_id());\n\n\t\t/* Synchronize with switch_mm. */\n\t\tsmp_mb();\n\n\t\tgoto out;\n\t}\n\n\tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n\t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n\n\t/*\n\t * Both branches below are implicit full barriers (MOV to CR or\n\t * INVLPG) that synchronize with switch_mm.\n\t */\n\tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n\t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\t\tlocal_flush_tlb();\n\t} else {\n\t\t/* flush range by one by one 'invlpg' */\n\t\tfor (addr = start; addr < end;\taddr += PAGE_SIZE) {\n\t\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);\n\t\t\t__flush_tlb_single(addr);\n\t\t}\n\t}\n\ttrace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);\nout:\n\tif (base_pages_to_flush == TLB_FLUSH_ALL) {\n\t\tstart = 0UL;\n\t\tend = TLB_FLUSH_ALL;\n\t}\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, end);\n\tpreempt_enable();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,17 +6,29 @@\n \tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n \n \tpreempt_disable();\n-\tif (current->active_mm != mm)\n+\tif (current->active_mm != mm) {\n+\t\t/* Synchronize with switch_mm. */\n+\t\tsmp_mb();\n+\n \t\tgoto out;\n+\t}\n \n \tif (!current->mm) {\n \t\tleave_mm(smp_processor_id());\n+\n+\t\t/* Synchronize with switch_mm. */\n+\t\tsmp_mb();\n+\n \t\tgoto out;\n \t}\n \n \tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n \t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n \n+\t/*\n+\t * Both branches below are implicit full barriers (MOV to CR or\n+\t * INVLPG) that synchronize with switch_mm.\n+\t */\n \tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n \t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n \t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (current->active_mm != mm)"
            ],
            "added_lines": [
                "\tif (current->active_mm != mm) {",
                "\t\t/* Synchronize with switch_mm. */",
                "\t\tsmp_mb();",
                "",
                "\t}",
                "",
                "\t\t/* Synchronize with switch_mm. */",
                "\t\tsmp_mb();",
                "",
                "\t/*",
                "\t * Both branches below are implicit full barriers (MOV to CR or",
                "\t * INVLPG) that synchronize with switch_mm.",
                "\t */"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2069",
        "func_name": "torvalds/linux/flush_tlb_current_task",
        "description": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU.",
        "git_url": "https://github.com/torvalds/linux/commit/71b3c126e61177eb693423f2e18a1914205b165e",
        "commit_title": "x86/mm: Add barriers and document switch_mm()-vs-flush synchronization",
        "commit_text": " When switch_mm() activates a new PGD, it also sets a bit that tells other CPUs that the PGD is in use so that TLB flush IPIs will be sent.  In order for that to work correctly, the bit needs to be visible prior to loading the PGD and therefore starting to fill the local TLB.  Document all the barriers that make this work correctly and add a couple that were missing.  Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Andy Lutomirski <luto@amacapital.net> Cc: Borislav Petkov <bp@alien8.de> Cc: Brian Gerst <brgerst@gmail.com> Cc: Dave Hansen <dave.hansen@linux.intel.com> Cc: Denys Vlasenko <dvlasenk@redhat.com> Cc: H. Peter Anvin <hpa@zytor.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Rik van Riel <riel@redhat.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: linux-mm@kvack.org Cc: stable@vger.kernel.org",
        "func_before": "void flush_tlb_current_task(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tpreempt_disable();\n\n\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\tlocal_flush_tlb();\n\ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);\n\tpreempt_enable();\n}",
        "func": "void flush_tlb_current_task(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tpreempt_disable();\n\n\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\n\t/* This is an implicit full barrier that synchronizes with switch_mm. */\n\tlocal_flush_tlb();\n\n\ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);\n\tpreempt_enable();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,10 @@\n \tpreempt_disable();\n \n \tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n+\n+\t/* This is an implicit full barrier that synchronizes with switch_mm. */\n \tlocal_flush_tlb();\n+\n \ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n \tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n \t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t/* This is an implicit full barrier that synchronizes with switch_mm. */",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2069",
        "func_name": "torvalds/linux/flush_tlb_page",
        "description": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU.",
        "git_url": "https://github.com/torvalds/linux/commit/71b3c126e61177eb693423f2e18a1914205b165e",
        "commit_title": "x86/mm: Add barriers and document switch_mm()-vs-flush synchronization",
        "commit_text": " When switch_mm() activates a new PGD, it also sets a bit that tells other CPUs that the PGD is in use so that TLB flush IPIs will be sent.  In order for that to work correctly, the bit needs to be visible prior to loading the PGD and therefore starting to fill the local TLB.  Document all the barriers that make this work correctly and add a couple that were missing.  Cc: Andrew Morton <akpm@linux-foundation.org> Cc: Andy Lutomirski <luto@amacapital.net> Cc: Borislav Petkov <bp@alien8.de> Cc: Brian Gerst <brgerst@gmail.com> Cc: Dave Hansen <dave.hansen@linux.intel.com> Cc: Denys Vlasenko <dvlasenk@redhat.com> Cc: H. Peter Anvin <hpa@zytor.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Rik van Riel <riel@redhat.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: linux-mm@kvack.org Cc: stable@vger.kernel.org",
        "func_before": "void flush_tlb_page(struct vm_area_struct *vma, unsigned long start)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpreempt_disable();\n\n\tif (current->active_mm == mm) {\n\t\tif (current->mm)\n\t\t\t__flush_tlb_one(start);\n\t\telse\n\t\t\tleave_mm(smp_processor_id());\n\t}\n\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, 0UL);\n\n\tpreempt_enable();\n}",
        "func": "void flush_tlb_page(struct vm_area_struct *vma, unsigned long start)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpreempt_disable();\n\n\tif (current->active_mm == mm) {\n\t\tif (current->mm) {\n\t\t\t/*\n\t\t\t * Implicit full barrier (INVLPG) that synchronizes\n\t\t\t * with switch_mm.\n\t\t\t */\n\t\t\t__flush_tlb_one(start);\n\t\t} else {\n\t\t\tleave_mm(smp_processor_id());\n\n\t\t\t/* Synchronize with switch_mm. */\n\t\t\tsmp_mb();\n\t\t}\n\t}\n\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, 0UL);\n\n\tpreempt_enable();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,10 +5,18 @@\n \tpreempt_disable();\n \n \tif (current->active_mm == mm) {\n-\t\tif (current->mm)\n+\t\tif (current->mm) {\n+\t\t\t/*\n+\t\t\t * Implicit full barrier (INVLPG) that synchronizes\n+\t\t\t * with switch_mm.\n+\t\t\t */\n \t\t\t__flush_tlb_one(start);\n-\t\telse\n+\t\t} else {\n \t\t\tleave_mm(smp_processor_id());\n+\n+\t\t\t/* Synchronize with switch_mm. */\n+\t\t\tsmp_mb();\n+\t\t}\n \t}\n \n \tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (current->mm)",
                "\t\telse"
            ],
            "added_lines": [
                "\t\tif (current->mm) {",
                "\t\t\t/*",
                "\t\t\t * Implicit full barrier (INVLPG) that synchronizes",
                "\t\t\t * with switch_mm.",
                "\t\t\t */",
                "\t\t} else {",
                "",
                "\t\t\t/* Synchronize with switch_mm. */",
                "\t\t\tsmp_mb();",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2544",
        "func_name": "torvalds/linux/queue_delete",
        "description": "Race condition in the queue_delete function in sound/core/seq/seq_queue.c in the Linux kernel before 4.4.1 allows local users to cause a denial of service (use-after-free and system crash) by making an ioctl call at a certain time.",
        "git_url": "https://github.com/torvalds/linux/commit/3567eb6af614dac436c4b16a8d426f9faed639b3",
        "commit_title": "ALSA: seq: Fix race at timer setup and close",
        "commit_text": " ALSA sequencer code has an open race between the timer setup ioctl and the close of the client.  This was triggered by syzkaller fuzzer, and a use-after-free was caught there as a result.  This patch papers over it by adding a proper queue->timer_mutex lock around the timer-related calls in the relevant code path.  Cc: <stable@vger.kernel.org>",
        "func_before": "static void queue_delete(struct snd_seq_queue *q)\n{\n\t/* stop and release the timer */\n\tsnd_seq_timer_stop(q->timer);\n\tsnd_seq_timer_close(q);\n\t/* wait until access free */\n\tsnd_use_lock_sync(&q->use_lock);\n\t/* release resources... */\n\tsnd_seq_prioq_delete(&q->tickq);\n\tsnd_seq_prioq_delete(&q->timeq);\n\tsnd_seq_timer_delete(&q->timer);\n\n\tkfree(q);\n}",
        "func": "static void queue_delete(struct snd_seq_queue *q)\n{\n\t/* stop and release the timer */\n\tmutex_lock(&q->timer_mutex);\n\tsnd_seq_timer_stop(q->timer);\n\tsnd_seq_timer_close(q);\n\tmutex_unlock(&q->timer_mutex);\n\t/* wait until access free */\n\tsnd_use_lock_sync(&q->use_lock);\n\t/* release resources... */\n\tsnd_seq_prioq_delete(&q->tickq);\n\tsnd_seq_prioq_delete(&q->timeq);\n\tsnd_seq_timer_delete(&q->timer);\n\n\tkfree(q);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,10 @@\n static void queue_delete(struct snd_seq_queue *q)\n {\n \t/* stop and release the timer */\n+\tmutex_lock(&q->timer_mutex);\n \tsnd_seq_timer_stop(q->timer);\n \tsnd_seq_timer_close(q);\n+\tmutex_unlock(&q->timer_mutex);\n \t/* wait until access free */\n \tsnd_use_lock_sync(&q->use_lock);\n \t/* release resources... */",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_lock(&q->timer_mutex);",
                "\tmutex_unlock(&q->timer_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2545",
        "func_name": "torvalds/linux/snd_timer_interrupt",
        "description": "The snd_timer_interrupt function in sound/core/timer.c in the Linux kernel before 4.4.1 does not properly maintain a certain linked list, which allows local users to cause a denial of service (race condition and system crash) via a crafted ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/ee8413b01045c74340aa13ad5bdf905de32be736",
        "commit_title": "ALSA: timer: Fix double unlink of active_list",
        "commit_text": " ALSA timer instance object has a couple of linked lists and they are unlinked unconditionally at snd_timer_stop().  Meanwhile snd_timer_interrupt() unlinks it, but it calls list_del() which leaves the element list itself unchanged.  This ends up with unlinking twice, and it was caught by syzkaller fuzzer.  The fix is to use list_del_init() variant properly there, too.  Cc: <stable@vger.kernel.org>",
        "func_before": "void snd_timer_interrupt(struct snd_timer * timer, unsigned long ticks_left)\n{\n\tstruct snd_timer_instance *ti, *ts, *tmp;\n\tunsigned long resolution, ticks;\n\tstruct list_head *p, *ack_list_head;\n\tunsigned long flags;\n\tint use_tasklet = 0;\n\n\tif (timer == NULL)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\n\t/* remember the current resolution */\n\tif (timer->hw.c_resolution)\n\t\tresolution = timer->hw.c_resolution(timer);\n\telse\n\t\tresolution = timer->hw.resolution;\n\n\t/* loop for all active instances\n\t * Here we cannot use list_for_each_entry because the active_list of a\n\t * processed instance is relinked to done_list_head before the callback\n\t * is called.\n\t */\n\tlist_for_each_entry_safe(ti, tmp, &timer->active_list_head,\n\t\t\t\t active_list) {\n\t\tif (!(ti->flags & SNDRV_TIMER_IFLG_RUNNING))\n\t\t\tcontinue;\n\t\tti->pticks += ticks_left;\n\t\tti->resolution = resolution;\n\t\tif (ti->cticks < ticks_left)\n\t\t\tti->cticks = 0;\n\t\telse\n\t\t\tti->cticks -= ticks_left;\n\t\tif (ti->cticks) /* not expired */\n\t\t\tcontinue;\n\t\tif (ti->flags & SNDRV_TIMER_IFLG_AUTO) {\n\t\t\tti->cticks = ti->ticks;\n\t\t} else {\n\t\t\tti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tif (--timer->running)\n\t\t\t\tlist_del(&ti->active_list);\n\t\t}\n\t\tif ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||\n\t\t    (ti->flags & SNDRV_TIMER_IFLG_FAST))\n\t\t\tack_list_head = &timer->ack_list_head;\n\t\telse\n\t\t\tack_list_head = &timer->sack_list_head;\n\t\tif (list_empty(&ti->ack_list))\n\t\t\tlist_add_tail(&ti->ack_list, ack_list_head);\n\t\tlist_for_each_entry(ts, &ti->slave_active_head, active_list) {\n\t\t\tts->pticks = ti->pticks;\n\t\t\tts->resolution = resolution;\n\t\t\tif (list_empty(&ts->ack_list))\n\t\t\t\tlist_add_tail(&ts->ack_list, ack_list_head);\n\t\t}\n\t}\n\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED)\n\t\tsnd_timer_reschedule(timer, timer->sticks);\n\tif (timer->running) {\n\t\tif (timer->hw.flags & SNDRV_TIMER_HW_STOP) {\n\t\t\ttimer->hw.stop(timer);\n\t\t\ttimer->flags |= SNDRV_TIMER_FLG_CHANGE;\n\t\t}\n\t\tif (!(timer->hw.flags & SNDRV_TIMER_HW_AUTO) ||\n\t\t    (timer->flags & SNDRV_TIMER_FLG_CHANGE)) {\n\t\t\t/* restart timer */\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\ttimer->hw.start(timer);\n\t\t}\n\t} else {\n\t\ttimer->hw.stop(timer);\n\t}\n\n\t/* now process all fast callbacks */\n\twhile (!list_empty(&timer->ack_list_head)) {\n\t\tp = timer->ack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\n\t/* do we have any slow callbacks? */\n\tuse_tasklet = !list_empty(&timer->sack_list_head);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n\n\tif (use_tasklet)\n\t\ttasklet_schedule(&timer->task_queue);\n}",
        "func": "void snd_timer_interrupt(struct snd_timer * timer, unsigned long ticks_left)\n{\n\tstruct snd_timer_instance *ti, *ts, *tmp;\n\tunsigned long resolution, ticks;\n\tstruct list_head *p, *ack_list_head;\n\tunsigned long flags;\n\tint use_tasklet = 0;\n\n\tif (timer == NULL)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\n\t/* remember the current resolution */\n\tif (timer->hw.c_resolution)\n\t\tresolution = timer->hw.c_resolution(timer);\n\telse\n\t\tresolution = timer->hw.resolution;\n\n\t/* loop for all active instances\n\t * Here we cannot use list_for_each_entry because the active_list of a\n\t * processed instance is relinked to done_list_head before the callback\n\t * is called.\n\t */\n\tlist_for_each_entry_safe(ti, tmp, &timer->active_list_head,\n\t\t\t\t active_list) {\n\t\tif (!(ti->flags & SNDRV_TIMER_IFLG_RUNNING))\n\t\t\tcontinue;\n\t\tti->pticks += ticks_left;\n\t\tti->resolution = resolution;\n\t\tif (ti->cticks < ticks_left)\n\t\t\tti->cticks = 0;\n\t\telse\n\t\t\tti->cticks -= ticks_left;\n\t\tif (ti->cticks) /* not expired */\n\t\t\tcontinue;\n\t\tif (ti->flags & SNDRV_TIMER_IFLG_AUTO) {\n\t\t\tti->cticks = ti->ticks;\n\t\t} else {\n\t\t\tti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tif (--timer->running)\n\t\t\t\tlist_del_init(&ti->active_list);\n\t\t}\n\t\tif ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||\n\t\t    (ti->flags & SNDRV_TIMER_IFLG_FAST))\n\t\t\tack_list_head = &timer->ack_list_head;\n\t\telse\n\t\t\tack_list_head = &timer->sack_list_head;\n\t\tif (list_empty(&ti->ack_list))\n\t\t\tlist_add_tail(&ti->ack_list, ack_list_head);\n\t\tlist_for_each_entry(ts, &ti->slave_active_head, active_list) {\n\t\t\tts->pticks = ti->pticks;\n\t\t\tts->resolution = resolution;\n\t\t\tif (list_empty(&ts->ack_list))\n\t\t\t\tlist_add_tail(&ts->ack_list, ack_list_head);\n\t\t}\n\t}\n\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED)\n\t\tsnd_timer_reschedule(timer, timer->sticks);\n\tif (timer->running) {\n\t\tif (timer->hw.flags & SNDRV_TIMER_HW_STOP) {\n\t\t\ttimer->hw.stop(timer);\n\t\t\ttimer->flags |= SNDRV_TIMER_FLG_CHANGE;\n\t\t}\n\t\tif (!(timer->hw.flags & SNDRV_TIMER_HW_AUTO) ||\n\t\t    (timer->flags & SNDRV_TIMER_FLG_CHANGE)) {\n\t\t\t/* restart timer */\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\ttimer->hw.start(timer);\n\t\t}\n\t} else {\n\t\ttimer->hw.stop(timer);\n\t}\n\n\t/* now process all fast callbacks */\n\twhile (!list_empty(&timer->ack_list_head)) {\n\t\tp = timer->ack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\n\t/* do we have any slow callbacks? */\n\tuse_tasklet = !list_empty(&timer->sack_list_head);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n\n\tif (use_tasklet)\n\t\ttasklet_schedule(&timer->task_queue);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -39,7 +39,7 @@\n \t\t} else {\n \t\t\tti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n \t\t\tif (--timer->running)\n-\t\t\t\tlist_del(&ti->active_list);\n+\t\t\t\tlist_del_init(&ti->active_list);\n \t\t}\n \t\tif ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||\n \t\t    (ti->flags & SNDRV_TIMER_IFLG_FAST))",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\tlist_del(&ti->active_list);"
            ],
            "added_lines": [
                "\t\t\t\tlist_del_init(&ti->active_list);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2546",
        "func_name": "torvalds/linux/snd_timer_user_ioctl",
        "description": "sound/core/timer.c in the Linux kernel before 4.4.1 uses an incorrect type of mutex, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/af368027a49a751d6ff4ee9e3f9961f35bb4fede",
        "commit_title": "ALSA: timer: Fix race among timer ioctls",
        "commit_text": " ALSA timer ioctls have an open race and this may lead to a use-after-free of timer instance object.  A simplistic fix is to make each ioctl exclusive.  We have already tread_sem for controlling the tread, and extend this as a global mutex to be applied to each ioctl.  The downside is, of course, the worse concurrency.  But these ioctls aren't to be parallel accessible, in anyway, so it should be fine to serialize there.  Cc: <stable@vger.kernel.org>",
        "func_before": "static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct snd_timer_user *tu;\n\tvoid __user *argp = (void __user *)arg;\n\tint __user *p = argp;\n\n\ttu = file->private_data;\n\tswitch (cmd) {\n\tcase SNDRV_TIMER_IOCTL_PVERSION:\n\t\treturn put_user(SNDRV_TIMER_VERSION, p) ? -EFAULT : 0;\n\tcase SNDRV_TIMER_IOCTL_NEXT_DEVICE:\n\t\treturn snd_timer_user_next_device(argp);\n\tcase SNDRV_TIMER_IOCTL_TREAD:\n\t{\n\t\tint xarg;\n\n\t\tmutex_lock(&tu->tread_sem);\n\t\tif (tu->timeri)\t{\t/* too late */\n\t\t\tmutex_unlock(&tu->tread_sem);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tif (get_user(xarg, p)) {\n\t\t\tmutex_unlock(&tu->tread_sem);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\ttu->tread = xarg ? 1 : 0;\n\t\tmutex_unlock(&tu->tread_sem);\n\t\treturn 0;\n\t}\n\tcase SNDRV_TIMER_IOCTL_GINFO:\n\t\treturn snd_timer_user_ginfo(file, argp);\n\tcase SNDRV_TIMER_IOCTL_GPARAMS:\n\t\treturn snd_timer_user_gparams(file, argp);\n\tcase SNDRV_TIMER_IOCTL_GSTATUS:\n\t\treturn snd_timer_user_gstatus(file, argp);\n\tcase SNDRV_TIMER_IOCTL_SELECT:\n\t\treturn snd_timer_user_tselect(file, argp);\n\tcase SNDRV_TIMER_IOCTL_INFO:\n\t\treturn snd_timer_user_info(file, argp);\n\tcase SNDRV_TIMER_IOCTL_PARAMS:\n\t\treturn snd_timer_user_params(file, argp);\n\tcase SNDRV_TIMER_IOCTL_STATUS:\n\t\treturn snd_timer_user_status(file, argp);\n\tcase SNDRV_TIMER_IOCTL_START:\n\tcase SNDRV_TIMER_IOCTL_START_OLD:\n\t\treturn snd_timer_user_start(file);\n\tcase SNDRV_TIMER_IOCTL_STOP:\n\tcase SNDRV_TIMER_IOCTL_STOP_OLD:\n\t\treturn snd_timer_user_stop(file);\n\tcase SNDRV_TIMER_IOCTL_CONTINUE:\n\tcase SNDRV_TIMER_IOCTL_CONTINUE_OLD:\n\t\treturn snd_timer_user_continue(file);\n\tcase SNDRV_TIMER_IOCTL_PAUSE:\n\tcase SNDRV_TIMER_IOCTL_PAUSE_OLD:\n\t\treturn snd_timer_user_pause(file);\n\t}\n\treturn -ENOTTY;\n}",
        "func": "static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct snd_timer_user *tu = file->private_data;\n\tlong ret;\n\n\tmutex_lock(&tu->ioctl_lock);\n\tret = __snd_timer_user_ioctl(file, cmd, arg);\n\tmutex_unlock(&tu->ioctl_lock);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,59 +1,11 @@\n static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,\n \t\t\t\t unsigned long arg)\n {\n-\tstruct snd_timer_user *tu;\n-\tvoid __user *argp = (void __user *)arg;\n-\tint __user *p = argp;\n+\tstruct snd_timer_user *tu = file->private_data;\n+\tlong ret;\n \n-\ttu = file->private_data;\n-\tswitch (cmd) {\n-\tcase SNDRV_TIMER_IOCTL_PVERSION:\n-\t\treturn put_user(SNDRV_TIMER_VERSION, p) ? -EFAULT : 0;\n-\tcase SNDRV_TIMER_IOCTL_NEXT_DEVICE:\n-\t\treturn snd_timer_user_next_device(argp);\n-\tcase SNDRV_TIMER_IOCTL_TREAD:\n-\t{\n-\t\tint xarg;\n-\n-\t\tmutex_lock(&tu->tread_sem);\n-\t\tif (tu->timeri)\t{\t/* too late */\n-\t\t\tmutex_unlock(&tu->tread_sem);\n-\t\t\treturn -EBUSY;\n-\t\t}\n-\t\tif (get_user(xarg, p)) {\n-\t\t\tmutex_unlock(&tu->tread_sem);\n-\t\t\treturn -EFAULT;\n-\t\t}\n-\t\ttu->tread = xarg ? 1 : 0;\n-\t\tmutex_unlock(&tu->tread_sem);\n-\t\treturn 0;\n-\t}\n-\tcase SNDRV_TIMER_IOCTL_GINFO:\n-\t\treturn snd_timer_user_ginfo(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_GPARAMS:\n-\t\treturn snd_timer_user_gparams(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_GSTATUS:\n-\t\treturn snd_timer_user_gstatus(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_SELECT:\n-\t\treturn snd_timer_user_tselect(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_INFO:\n-\t\treturn snd_timer_user_info(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_PARAMS:\n-\t\treturn snd_timer_user_params(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_STATUS:\n-\t\treturn snd_timer_user_status(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_START:\n-\tcase SNDRV_TIMER_IOCTL_START_OLD:\n-\t\treturn snd_timer_user_start(file);\n-\tcase SNDRV_TIMER_IOCTL_STOP:\n-\tcase SNDRV_TIMER_IOCTL_STOP_OLD:\n-\t\treturn snd_timer_user_stop(file);\n-\tcase SNDRV_TIMER_IOCTL_CONTINUE:\n-\tcase SNDRV_TIMER_IOCTL_CONTINUE_OLD:\n-\t\treturn snd_timer_user_continue(file);\n-\tcase SNDRV_TIMER_IOCTL_PAUSE:\n-\tcase SNDRV_TIMER_IOCTL_PAUSE_OLD:\n-\t\treturn snd_timer_user_pause(file);\n-\t}\n-\treturn -ENOTTY;\n+\tmutex_lock(&tu->ioctl_lock);\n+\tret = __snd_timer_user_ioctl(file, cmd, arg);\n+\tmutex_unlock(&tu->ioctl_lock);\n+\treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct snd_timer_user *tu;",
                "\tvoid __user *argp = (void __user *)arg;",
                "\tint __user *p = argp;",
                "\ttu = file->private_data;",
                "\tswitch (cmd) {",
                "\tcase SNDRV_TIMER_IOCTL_PVERSION:",
                "\t\treturn put_user(SNDRV_TIMER_VERSION, p) ? -EFAULT : 0;",
                "\tcase SNDRV_TIMER_IOCTL_NEXT_DEVICE:",
                "\t\treturn snd_timer_user_next_device(argp);",
                "\tcase SNDRV_TIMER_IOCTL_TREAD:",
                "\t{",
                "\t\tint xarg;",
                "",
                "\t\tmutex_lock(&tu->tread_sem);",
                "\t\tif (tu->timeri)\t{\t/* too late */",
                "\t\t\tmutex_unlock(&tu->tread_sem);",
                "\t\t\treturn -EBUSY;",
                "\t\t}",
                "\t\tif (get_user(xarg, p)) {",
                "\t\t\tmutex_unlock(&tu->tread_sem);",
                "\t\t\treturn -EFAULT;",
                "\t\t}",
                "\t\ttu->tread = xarg ? 1 : 0;",
                "\t\tmutex_unlock(&tu->tread_sem);",
                "\t\treturn 0;",
                "\t}",
                "\tcase SNDRV_TIMER_IOCTL_GINFO:",
                "\t\treturn snd_timer_user_ginfo(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_GPARAMS:",
                "\t\treturn snd_timer_user_gparams(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_GSTATUS:",
                "\t\treturn snd_timer_user_gstatus(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_SELECT:",
                "\t\treturn snd_timer_user_tselect(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_INFO:",
                "\t\treturn snd_timer_user_info(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_PARAMS:",
                "\t\treturn snd_timer_user_params(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_STATUS:",
                "\t\treturn snd_timer_user_status(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_START:",
                "\tcase SNDRV_TIMER_IOCTL_START_OLD:",
                "\t\treturn snd_timer_user_start(file);",
                "\tcase SNDRV_TIMER_IOCTL_STOP:",
                "\tcase SNDRV_TIMER_IOCTL_STOP_OLD:",
                "\t\treturn snd_timer_user_stop(file);",
                "\tcase SNDRV_TIMER_IOCTL_CONTINUE:",
                "\tcase SNDRV_TIMER_IOCTL_CONTINUE_OLD:",
                "\t\treturn snd_timer_user_continue(file);",
                "\tcase SNDRV_TIMER_IOCTL_PAUSE:",
                "\tcase SNDRV_TIMER_IOCTL_PAUSE_OLD:",
                "\t\treturn snd_timer_user_pause(file);",
                "\t}",
                "\treturn -ENOTTY;"
            ],
            "added_lines": [
                "\tstruct snd_timer_user *tu = file->private_data;",
                "\tlong ret;",
                "\tmutex_lock(&tu->ioctl_lock);",
                "\tret = __snd_timer_user_ioctl(file, cmd, arg);",
                "\tmutex_unlock(&tu->ioctl_lock);",
                "\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2546",
        "func_name": "torvalds/linux/snd_timer_user_open",
        "description": "sound/core/timer.c in the Linux kernel before 4.4.1 uses an incorrect type of mutex, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/af368027a49a751d6ff4ee9e3f9961f35bb4fede",
        "commit_title": "ALSA: timer: Fix race among timer ioctls",
        "commit_text": " ALSA timer ioctls have an open race and this may lead to a use-after-free of timer instance object.  A simplistic fix is to make each ioctl exclusive.  We have already tread_sem for controlling the tread, and extend this as a global mutex to be applied to each ioctl.  The downside is, of course, the worse concurrency.  But these ioctls aren't to be parallel accessible, in anyway, so it should be fine to serialize there.  Cc: <stable@vger.kernel.org>",
        "func_before": "static int snd_timer_user_open(struct inode *inode, struct file *file)\n{\n\tstruct snd_timer_user *tu;\n\tint err;\n\n\terr = nonseekable_open(inode, file);\n\tif (err < 0)\n\t\treturn err;\n\n\ttu = kzalloc(sizeof(*tu), GFP_KERNEL);\n\tif (tu == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&tu->qlock);\n\tinit_waitqueue_head(&tu->qchange_sleep);\n\tmutex_init(&tu->tread_sem);\n\ttu->ticks = 1;\n\ttu->queue_size = 128;\n\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t    GFP_KERNEL);\n\tif (tu->queue == NULL) {\n\t\tkfree(tu);\n\t\treturn -ENOMEM;\n\t}\n\tfile->private_data = tu;\n\treturn 0;\n}",
        "func": "static int snd_timer_user_open(struct inode *inode, struct file *file)\n{\n\tstruct snd_timer_user *tu;\n\tint err;\n\n\terr = nonseekable_open(inode, file);\n\tif (err < 0)\n\t\treturn err;\n\n\ttu = kzalloc(sizeof(*tu), GFP_KERNEL);\n\tif (tu == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&tu->qlock);\n\tinit_waitqueue_head(&tu->qchange_sleep);\n\tmutex_init(&tu->ioctl_lock);\n\ttu->ticks = 1;\n\ttu->queue_size = 128;\n\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t    GFP_KERNEL);\n\tif (tu->queue == NULL) {\n\t\tkfree(tu);\n\t\treturn -ENOMEM;\n\t}\n\tfile->private_data = tu;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,7 @@\n \t\treturn -ENOMEM;\n \tspin_lock_init(&tu->qlock);\n \tinit_waitqueue_head(&tu->qchange_sleep);\n-\tmutex_init(&tu->tread_sem);\n+\tmutex_init(&tu->ioctl_lock);\n \ttu->ticks = 1;\n \ttu->queue_size = 128;\n \ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),",
        "diff_line_info": {
            "deleted_lines": [
                "\tmutex_init(&tu->tread_sem);"
            ],
            "added_lines": [
                "\tmutex_init(&tu->ioctl_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2546",
        "func_name": "torvalds/linux/snd_timer_user_release",
        "description": "sound/core/timer.c in the Linux kernel before 4.4.1 uses an incorrect type of mutex, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/af368027a49a751d6ff4ee9e3f9961f35bb4fede",
        "commit_title": "ALSA: timer: Fix race among timer ioctls",
        "commit_text": " ALSA timer ioctls have an open race and this may lead to a use-after-free of timer instance object.  A simplistic fix is to make each ioctl exclusive.  We have already tread_sem for controlling the tread, and extend this as a global mutex to be applied to each ioctl.  The downside is, of course, the worse concurrency.  But these ioctls aren't to be parallel accessible, in anyway, so it should be fine to serialize there.  Cc: <stable@vger.kernel.org>",
        "func_before": "static int snd_timer_user_release(struct inode *inode, struct file *file)\n{\n\tstruct snd_timer_user *tu;\n\n\tif (file->private_data) {\n\t\ttu = file->private_data;\n\t\tfile->private_data = NULL;\n\t\tif (tu->timeri)\n\t\t\tsnd_timer_close(tu->timeri);\n\t\tkfree(tu->queue);\n\t\tkfree(tu->tqueue);\n\t\tkfree(tu);\n\t}\n\treturn 0;\n}",
        "func": "static int snd_timer_user_release(struct inode *inode, struct file *file)\n{\n\tstruct snd_timer_user *tu;\n\n\tif (file->private_data) {\n\t\ttu = file->private_data;\n\t\tfile->private_data = NULL;\n\t\tmutex_lock(&tu->ioctl_lock);\n\t\tif (tu->timeri)\n\t\t\tsnd_timer_close(tu->timeri);\n\t\tmutex_unlock(&tu->ioctl_lock);\n\t\tkfree(tu->queue);\n\t\tkfree(tu->tqueue);\n\t\tkfree(tu);\n\t}\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,8 +5,10 @@\n \tif (file->private_data) {\n \t\ttu = file->private_data;\n \t\tfile->private_data = NULL;\n+\t\tmutex_lock(&tu->ioctl_lock);\n \t\tif (tu->timeri)\n \t\t\tsnd_timer_close(tu->timeri);\n+\t\tmutex_unlock(&tu->ioctl_lock);\n \t\tkfree(tu->queue);\n \t\tkfree(tu->tqueue);\n \t\tkfree(tu);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tmutex_lock(&tu->ioctl_lock);",
                "\t\tmutex_unlock(&tu->ioctl_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2546",
        "func_name": "torvalds/linux/snd_timer_user_tselect",
        "description": "sound/core/timer.c in the Linux kernel before 4.4.1 uses an incorrect type of mutex, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/af368027a49a751d6ff4ee9e3f9961f35bb4fede",
        "commit_title": "ALSA: timer: Fix race among timer ioctls",
        "commit_text": " ALSA timer ioctls have an open race and this may lead to a use-after-free of timer instance object.  A simplistic fix is to make each ioctl exclusive.  We have already tread_sem for controlling the tread, and extend this as a global mutex to be applied to each ioctl.  The downside is, of course, the worse concurrency.  But these ioctls aren't to be parallel accessible, in anyway, so it should be fine to serialize there.  Cc: <stable@vger.kernel.org>",
        "func_before": "static int snd_timer_user_tselect(struct file *file,\n\t\t\t\t  struct snd_timer_select __user *_tselect)\n{\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_select tselect;\n\tchar str[32];\n\tint err = 0;\n\n\ttu = file->private_data;\n\tmutex_lock(&tu->tread_sem);\n\tif (tu->timeri) {\n\t\tsnd_timer_close(tu->timeri);\n\t\ttu->timeri = NULL;\n\t}\n\tif (copy_from_user(&tselect, _tselect, sizeof(tselect))) {\n\t\terr = -EFAULT;\n\t\tgoto __err;\n\t}\n\tsprintf(str, \"application %i\", current->pid);\n\tif (tselect.id.dev_class != SNDRV_TIMER_CLASS_SLAVE)\n\t\ttselect.id.dev_sclass = SNDRV_TIMER_SCLASS_APPLICATION;\n\terr = snd_timer_open(&tu->timeri, str, &tselect.id, current->pid);\n\tif (err < 0)\n\t\tgoto __err;\n\n\tkfree(tu->queue);\n\ttu->queue = NULL;\n\tkfree(tu->tqueue);\n\ttu->tqueue = NULL;\n\tif (tu->tread) {\n\t\ttu->tqueue = kmalloc(tu->queue_size * sizeof(struct snd_timer_tread),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (tu->tqueue == NULL)\n\t\t\terr = -ENOMEM;\n\t} else {\n\t\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (tu->queue == NULL)\n\t\t\terr = -ENOMEM;\n\t}\n\n      \tif (err < 0) {\n\t\tsnd_timer_close(tu->timeri);\n      \t\ttu->timeri = NULL;\n      \t} else {\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_FAST;\n\t\ttu->timeri->callback = tu->tread\n\t\t\t? snd_timer_user_tinterrupt : snd_timer_user_interrupt;\n\t\ttu->timeri->ccallback = snd_timer_user_ccallback;\n\t\ttu->timeri->callback_data = (void *)tu;\n\t}\n\n      __err:\n      \tmutex_unlock(&tu->tread_sem);\n\treturn err;\n}",
        "func": "static int snd_timer_user_tselect(struct file *file,\n\t\t\t\t  struct snd_timer_select __user *_tselect)\n{\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_select tselect;\n\tchar str[32];\n\tint err = 0;\n\n\ttu = file->private_data;\n\tif (tu->timeri) {\n\t\tsnd_timer_close(tu->timeri);\n\t\ttu->timeri = NULL;\n\t}\n\tif (copy_from_user(&tselect, _tselect, sizeof(tselect))) {\n\t\terr = -EFAULT;\n\t\tgoto __err;\n\t}\n\tsprintf(str, \"application %i\", current->pid);\n\tif (tselect.id.dev_class != SNDRV_TIMER_CLASS_SLAVE)\n\t\ttselect.id.dev_sclass = SNDRV_TIMER_SCLASS_APPLICATION;\n\terr = snd_timer_open(&tu->timeri, str, &tselect.id, current->pid);\n\tif (err < 0)\n\t\tgoto __err;\n\n\tkfree(tu->queue);\n\ttu->queue = NULL;\n\tkfree(tu->tqueue);\n\ttu->tqueue = NULL;\n\tif (tu->tread) {\n\t\ttu->tqueue = kmalloc(tu->queue_size * sizeof(struct snd_timer_tread),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (tu->tqueue == NULL)\n\t\t\terr = -ENOMEM;\n\t} else {\n\t\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (tu->queue == NULL)\n\t\t\terr = -ENOMEM;\n\t}\n\n      \tif (err < 0) {\n\t\tsnd_timer_close(tu->timeri);\n      \t\ttu->timeri = NULL;\n      \t} else {\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_FAST;\n\t\ttu->timeri->callback = tu->tread\n\t\t\t? snd_timer_user_tinterrupt : snd_timer_user_interrupt;\n\t\ttu->timeri->ccallback = snd_timer_user_ccallback;\n\t\ttu->timeri->callback_data = (void *)tu;\n\t}\n\n      __err:\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,6 @@\n \tint err = 0;\n \n \ttu = file->private_data;\n-\tmutex_lock(&tu->tread_sem);\n \tif (tu->timeri) {\n \t\tsnd_timer_close(tu->timeri);\n \t\ttu->timeri = NULL;\n@@ -51,6 +50,5 @@\n \t}\n \n       __err:\n-      \tmutex_unlock(&tu->tread_sem);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tmutex_lock(&tu->tread_sem);",
                "      \tmutex_unlock(&tu->tread_sem);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2016-2547",
        "func_name": "torvalds/linux/_snd_timer_stop",
        "description": "sound/core/timer.c in the Linux kernel before 4.4.1 employs a locking approach that does not consider slave timer instances, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/b5a663aa426f4884c71cd8580adae73f33570f0d",
        "commit_title": "ALSA: timer: Harden slave timer list handling",
        "commit_text": " A slave timer instance might be still accessible in a racy way while operating the master instance as it lacks of locking.  Since the master operation is mostly protected with timer->lock, we should cope with it while changing the slave instance, too.  Also, some linked lists (active_list and ack_list) of slave instances aren't unlinked immediately at stopping or closing, and this may lead to unexpected accesses.  This patch tries to address these issues.  It adds spin lock of timer->lock (either from master or slave, which is equivalent) in a few places.  For avoiding a deadlock, we ensure that the global slave_active_lock is always locked at first before each timer lock.  Also, ack and active_list of slave instances are properly unlinked at snd_timer_stop() and snd_timer_close().  Last but not least, remove the superfluous call of _snd_timer_stop() at removing slave links.  This is a noop, and calling it may confuse readers wrt locking.  Further cleanup will follow in a later patch.  Actually we've got reports of use-after-free by syzkaller fuzzer, and this hopefully fixes these issues.  Cc: <stable@vger.kernel.org>",
        "func_before": "static int _snd_timer_stop(struct snd_timer_instance * timeri,\n\t\t\t   int keep_flag, int event)\n{\n\tstruct snd_timer *timer;\n\tunsigned long flags;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\tif (!keep_flag) {\n\t\t\tspin_lock_irqsave(&slave_active_lock, flags);\n\t\t\ttimeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\t\t}\n\t\tgoto __end;\n\t}\n\ttimer = timeri->timer;\n\tif (!timer)\n\t\treturn -EINVAL;\n\tspin_lock_irqsave(&timer->lock, flags);\n\tlist_del_init(&timeri->ack_list);\n\tlist_del_init(&timeri->active_list);\n\tif ((timeri->flags & SNDRV_TIMER_IFLG_RUNNING) &&\n\t    !(--timer->running)) {\n\t\ttimer->hw.stop(timer);\n\t\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED) {\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_RESCHED;\n\t\t\tsnd_timer_reschedule(timer, 0);\n\t\t\tif (timer->flags & SNDRV_TIMER_FLG_CHANGE) {\n\t\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\t\ttimer->hw.start(timer);\n\t\t\t}\n\t\t}\n\t}\n\tif (!keep_flag)\n\t\ttimeri->flags &=\n\t\t\t~(SNDRV_TIMER_IFLG_RUNNING | SNDRV_TIMER_IFLG_START);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n      __end:\n\tif (event != SNDRV_TIMER_EVENT_RESOLUTION)\n\t\tsnd_timer_notify1(timeri, event);\n\treturn 0;\n}",
        "func": "static int _snd_timer_stop(struct snd_timer_instance * timeri,\n\t\t\t   int keep_flag, int event)\n{\n\tstruct snd_timer *timer;\n\tunsigned long flags;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\tif (!keep_flag) {\n\t\t\tspin_lock_irqsave(&slave_active_lock, flags);\n\t\t\ttimeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tlist_del_init(&timeri->ack_list);\n\t\t\tlist_del_init(&timeri->active_list);\n\t\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\t\t}\n\t\tgoto __end;\n\t}\n\ttimer = timeri->timer;\n\tif (!timer)\n\t\treturn -EINVAL;\n\tspin_lock_irqsave(&timer->lock, flags);\n\tlist_del_init(&timeri->ack_list);\n\tlist_del_init(&timeri->active_list);\n\tif ((timeri->flags & SNDRV_TIMER_IFLG_RUNNING) &&\n\t    !(--timer->running)) {\n\t\ttimer->hw.stop(timer);\n\t\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED) {\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_RESCHED;\n\t\t\tsnd_timer_reschedule(timer, 0);\n\t\t\tif (timer->flags & SNDRV_TIMER_FLG_CHANGE) {\n\t\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\t\ttimer->hw.start(timer);\n\t\t\t}\n\t\t}\n\t}\n\tif (!keep_flag)\n\t\ttimeri->flags &=\n\t\t\t~(SNDRV_TIMER_IFLG_RUNNING | SNDRV_TIMER_IFLG_START);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n      __end:\n\tif (event != SNDRV_TIMER_EVENT_RESOLUTION)\n\t\tsnd_timer_notify1(timeri, event);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,6 +11,8 @@\n \t\tif (!keep_flag) {\n \t\t\tspin_lock_irqsave(&slave_active_lock, flags);\n \t\t\ttimeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n+\t\t\tlist_del_init(&timeri->ack_list);\n+\t\t\tlist_del_init(&timeri->active_list);\n \t\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n \t\t}\n \t\tgoto __end;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tlist_del_init(&timeri->ack_list);",
                "\t\t\tlist_del_init(&timeri->active_list);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2547",
        "func_name": "torvalds/linux/snd_timer_check_master",
        "description": "sound/core/timer.c in the Linux kernel before 4.4.1 employs a locking approach that does not consider slave timer instances, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/b5a663aa426f4884c71cd8580adae73f33570f0d",
        "commit_title": "ALSA: timer: Harden slave timer list handling",
        "commit_text": " A slave timer instance might be still accessible in a racy way while operating the master instance as it lacks of locking.  Since the master operation is mostly protected with timer->lock, we should cope with it while changing the slave instance, too.  Also, some linked lists (active_list and ack_list) of slave instances aren't unlinked immediately at stopping or closing, and this may lead to unexpected accesses.  This patch tries to address these issues.  It adds spin lock of timer->lock (either from master or slave, which is equivalent) in a few places.  For avoiding a deadlock, we ensure that the global slave_active_lock is always locked at first before each timer lock.  Also, ack and active_list of slave instances are properly unlinked at snd_timer_stop() and snd_timer_close().  Last but not least, remove the superfluous call of _snd_timer_stop() at removing slave links.  This is a noop, and calling it may confuse readers wrt locking.  Further cleanup will follow in a later patch.  Actually we've got reports of use-after-free by syzkaller fuzzer, and this hopefully fixes these issues.  Cc: <stable@vger.kernel.org>",
        "func_before": "static void snd_timer_check_master(struct snd_timer_instance *master)\n{\n\tstruct snd_timer_instance *slave, *tmp;\n\n\t/* check all pending slaves */\n\tlist_for_each_entry_safe(slave, tmp, &snd_timer_slave_list, open_list) {\n\t\tif (slave->slave_class == master->slave_class &&\n\t\t    slave->slave_id == master->slave_id) {\n\t\t\tlist_move_tail(&slave->open_list, &master->slave_list_head);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\tslave->master = master;\n\t\t\tslave->timer = master->timer;\n\t\t\tif (slave->flags & SNDRV_TIMER_IFLG_RUNNING)\n\t\t\t\tlist_add_tail(&slave->active_list,\n\t\t\t\t\t      &master->slave_active_head);\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t}\n}",
        "func": "static void snd_timer_check_master(struct snd_timer_instance *master)\n{\n\tstruct snd_timer_instance *slave, *tmp;\n\n\t/* check all pending slaves */\n\tlist_for_each_entry_safe(slave, tmp, &snd_timer_slave_list, open_list) {\n\t\tif (slave->slave_class == master->slave_class &&\n\t\t    slave->slave_id == master->slave_id) {\n\t\t\tlist_move_tail(&slave->open_list, &master->slave_list_head);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\tspin_lock(&master->timer->lock);\n\t\t\tslave->master = master;\n\t\t\tslave->timer = master->timer;\n\t\t\tif (slave->flags & SNDRV_TIMER_IFLG_RUNNING)\n\t\t\t\tlist_add_tail(&slave->active_list,\n\t\t\t\t\t      &master->slave_active_head);\n\t\t\tspin_unlock(&master->timer->lock);\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,11 +8,13 @@\n \t\t    slave->slave_id == master->slave_id) {\n \t\t\tlist_move_tail(&slave->open_list, &master->slave_list_head);\n \t\t\tspin_lock_irq(&slave_active_lock);\n+\t\t\tspin_lock(&master->timer->lock);\n \t\t\tslave->master = master;\n \t\t\tslave->timer = master->timer;\n \t\t\tif (slave->flags & SNDRV_TIMER_IFLG_RUNNING)\n \t\t\t\tlist_add_tail(&slave->active_list,\n \t\t\t\t\t      &master->slave_active_head);\n+\t\t\tspin_unlock(&master->timer->lock);\n \t\t\tspin_unlock_irq(&slave_active_lock);\n \t\t}\n \t}",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tspin_lock(&master->timer->lock);",
                "\t\t\tspin_unlock(&master->timer->lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2547",
        "func_name": "torvalds/linux/snd_timer_close",
        "description": "sound/core/timer.c in the Linux kernel before 4.4.1 employs a locking approach that does not consider slave timer instances, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/b5a663aa426f4884c71cd8580adae73f33570f0d",
        "commit_title": "ALSA: timer: Harden slave timer list handling",
        "commit_text": " A slave timer instance might be still accessible in a racy way while operating the master instance as it lacks of locking.  Since the master operation is mostly protected with timer->lock, we should cope with it while changing the slave instance, too.  Also, some linked lists (active_list and ack_list) of slave instances aren't unlinked immediately at stopping or closing, and this may lead to unexpected accesses.  This patch tries to address these issues.  It adds spin lock of timer->lock (either from master or slave, which is equivalent) in a few places.  For avoiding a deadlock, we ensure that the global slave_active_lock is always locked at first before each timer lock.  Also, ack and active_list of slave instances are properly unlinked at snd_timer_stop() and snd_timer_close().  Last but not least, remove the superfluous call of _snd_timer_stop() at removing slave links.  This is a noop, and calling it may confuse readers wrt locking.  Further cleanup will follow in a later patch.  Actually we've got reports of use-after-free by syzkaller fuzzer, and this hopefully fixes these issues.  Cc: <stable@vger.kernel.org>",
        "func_before": "int snd_timer_close(struct snd_timer_instance *timeri)\n{\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t}\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tmutex_unlock(&register_mutex);\n\t} else {\n\t\ttimer = timeri->timer;\n\t\tif (snd_BUG_ON(!timer))\n\t\t\tgoto out;\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tif (timer && list_empty(&timer->open_list_head) &&\n\t\t    timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* remove slave links */\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t\tmutex_unlock(&register_mutex);\n\t}\n out:\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\tif (timer)\n\t\tmodule_put(timer->module);\n\treturn 0;\n}",
        "func": "int snd_timer_close(struct snd_timer_instance *timeri)\n{\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t}\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tmutex_unlock(&register_mutex);\n\t} else {\n\t\ttimer = timeri->timer;\n\t\tif (snd_BUG_ON(!timer))\n\t\t\tgoto out;\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tif (timer && list_empty(&timer->open_list_head) &&\n\t\t    timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* remove slave links */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\tspin_lock(&timer->lock);\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tlist_del_init(&slave->ack_list);\n\t\t\tlist_del_init(&slave->active_list);\n\t\t}\n\t\tspin_unlock(&timer->lock);\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_unlock(&register_mutex);\n\t}\n out:\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\tif (timer)\n\t\tmodule_put(timer->module);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -39,15 +39,18 @@\n \t\t    timer->hw.close)\n \t\t\ttimer->hw.close(timer);\n \t\t/* remove slave links */\n+\t\tspin_lock_irq(&slave_active_lock);\n+\t\tspin_lock(&timer->lock);\n \t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n \t\t\t\t\t open_list) {\n-\t\t\tspin_lock_irq(&slave_active_lock);\n-\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);\n \t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n \t\t\tslave->master = NULL;\n \t\t\tslave->timer = NULL;\n-\t\t\tspin_unlock_irq(&slave_active_lock);\n+\t\t\tlist_del_init(&slave->ack_list);\n+\t\t\tlist_del_init(&slave->active_list);\n \t\t}\n+\t\tspin_unlock(&timer->lock);\n+\t\tspin_unlock_irq(&slave_active_lock);\n \t\tmutex_unlock(&register_mutex);\n \t}\n  out:",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tspin_lock_irq(&slave_active_lock);",
                "\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);",
                "\t\t\tspin_unlock_irq(&slave_active_lock);"
            ],
            "added_lines": [
                "\t\tspin_lock_irq(&slave_active_lock);",
                "\t\tspin_lock(&timer->lock);",
                "\t\t\tlist_del_init(&slave->ack_list);",
                "\t\t\tlist_del_init(&slave->active_list);",
                "\t\tspin_unlock(&timer->lock);",
                "\t\tspin_unlock_irq(&slave_active_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-2547",
        "func_name": "torvalds/linux/snd_timer_start_slave",
        "description": "sound/core/timer.c in the Linux kernel before 4.4.1 employs a locking approach that does not consider slave timer instances, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/b5a663aa426f4884c71cd8580adae73f33570f0d",
        "commit_title": "ALSA: timer: Harden slave timer list handling",
        "commit_text": " A slave timer instance might be still accessible in a racy way while operating the master instance as it lacks of locking.  Since the master operation is mostly protected with timer->lock, we should cope with it while changing the slave instance, too.  Also, some linked lists (active_list and ack_list) of slave instances aren't unlinked immediately at stopping or closing, and this may lead to unexpected accesses.  This patch tries to address these issues.  It adds spin lock of timer->lock (either from master or slave, which is equivalent) in a few places.  For avoiding a deadlock, we ensure that the global slave_active_lock is always locked at first before each timer lock.  Also, ack and active_list of slave instances are properly unlinked at snd_timer_stop() and snd_timer_close().  Last but not least, remove the superfluous call of _snd_timer_stop() at removing slave links.  This is a noop, and calling it may confuse readers wrt locking.  Further cleanup will follow in a later patch.  Actually we've got reports of use-after-free by syzkaller fuzzer, and this hopefully fixes these issues.  Cc: <stable@vger.kernel.org>",
        "func_before": "static int snd_timer_start_slave(struct snd_timer_instance *timeri)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master)\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
        "func": "static int snd_timer_start_slave(struct snd_timer_instance *timeri)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master && timeri->timer) {\n\t\tspin_lock(&timeri->timer->lock);\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\t\tspin_unlock(&timeri->timer->lock);\n\t}\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,9 +4,12 @@\n \n \tspin_lock_irqsave(&slave_active_lock, flags);\n \ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n-\tif (timeri->master)\n+\tif (timeri->master && timeri->timer) {\n+\t\tspin_lock(&timeri->timer->lock);\n \t\tlist_add_tail(&timeri->active_list,\n \t\t\t      &timeri->master->slave_active_head);\n+\t\tspin_unlock(&timeri->timer->lock);\n+\t}\n \tspin_unlock_irqrestore(&slave_active_lock, flags);\n \treturn 1; /* delayed start */\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (timeri->master)"
            ],
            "added_lines": [
                "\tif (timeri->master && timeri->timer) {",
                "\t\tspin_lock(&timeri->timer->lock);",
                "\t\tspin_unlock(&timeri->timer->lock);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-4170",
        "func_name": "torvalds/linux/ldsem_cmpxchg",
        "description": "Race condition in the ldsem_cmpxchg function in drivers/tty/tty_ldsem.c in the Linux kernel before 3.13-rc4-next-20131218 allows local users to cause a denial of service (ldsem_down_read and ldsem_down_write deadlock) by establishing a new tty thread during shutdown of a previous tty thread.",
        "git_url": "https://github.com/torvalds/linux/commit/cf872776fc84128bb779ce2b83a37c884c3203ae",
        "commit_title": "tty: Fix hang at ldsem_down_read()",
        "commit_text": " When a controlling tty is being hung up and the hang up is waiting for a just-signalled tty reader or writer to exit, and a new tty reader/writer tries to acquire an ldisc reference concurrently with the ldisc reference release from the signalled reader/writer, the hangup can hang. The new reader/writer is sleeping in ldsem_down_read() and the hangup is sleeping in ldsem_down_write() [1].  The new reader/writer fails to wakeup the waiting hangup because the wrong lock count value is checked (the old lock count rather than the new lock count) to see if the lock is unowned.  Change helper function to return the new lock count if the cmpxchg was successful; document this behavior.  [1] edited dmesg log from reporter  SysRq : Show Blocked State   task                        PC stack   pid father systemd         D ffff88040c4f0000     0     1      0 0x00000000  ffff88040c49fbe0 0000000000000046 ffff88040c4a0000 ffff88040c49ffd8  00000000001d3980 00000000001d3980 ffff88040c4a0000 ffff88040593d840  ffff88040c49fb40 ffffffff810a4cc0 0000000000000006 0000000000000023 Call Trace:  [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4  [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4  [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4  [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4  [<ffffffff817a6649>] schedule+0x24/0x5e  [<ffffffff817a588b>] schedule_timeout+0x15b/0x1ec  [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4  [<ffffffff817aa691>] ? _raw_spin_unlock_irq+0x24/0x26  [<ffffffff817aa10c>] down_read_failed+0xe3/0x1b9  [<ffffffff817aa26d>] ldsem_down_read+0x8b/0xa5  [<ffffffff8142b5ca>] ? tty_ldisc_ref_wait+0x1b/0x44  [<ffffffff8142b5ca>] tty_ldisc_ref_wait+0x1b/0x44  [<ffffffff81423f5b>] tty_write+0x7d/0x28a  [<ffffffff814241f5>] redirected_tty_write+0x8d/0x98  [<ffffffff81424168>] ? tty_write+0x28a/0x28a  [<ffffffff8115d03f>] do_loop_readv_writev+0x56/0x79  [<ffffffff8115e604>] do_readv_writev+0x1b0/0x1ff  [<ffffffff8116ea0b>] ? do_vfs_ioctl+0x32a/0x489  [<ffffffff81167d9d>] ? final_putname+0x1d/0x3a  [<ffffffff8115e6c7>] vfs_writev+0x2e/0x49  [<ffffffff8115e7d3>] SyS_writev+0x47/0xaa  [<ffffffff817ab822>] system_call_fastpath+0x16/0x1b bash            D ffffffff81c104c0     0  5469   5302 0x00000082  ffff8800cf817ac0 0000000000000046 ffff8804086b22a0 ffff8800cf817fd8  00000000001d3980 00000000001d3980 ffff8804086b22a0 ffff8800cf817a48  000000000000b9a0 ffff8800cf817a78 ffffffff81004675 ffff8800cf817a44 Call Trace:  [<ffffffff81004675>] ? dump_trace+0x165/0x29c  [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4  [<ffffffff8100edda>] ? save_stack_trace+0x26/0x41  [<ffffffff817a6649>] schedule+0x24/0x5e  [<ffffffff817a588b>] schedule_timeout+0x15b/0x1ec  [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4  [<ffffffff817a9f03>] ? down_write_failed+0xa3/0x1c9  [<ffffffff817aa691>] ? _raw_spin_unlock_irq+0x24/0x26  [<ffffffff817a9f0b>] down_write_failed+0xab/0x1c9  [<ffffffff817aa300>] ldsem_down_write+0x79/0xb1  [<ffffffff817aada3>] ? tty_ldisc_lock_pair_timeout+0xa5/0xd9  [<ffffffff817aada3>] tty_ldisc_lock_pair_timeout+0xa5/0xd9  [<ffffffff8142bf33>] tty_ldisc_hangup+0xc4/0x218  [<ffffffff81423ab3>] __tty_hangup+0x2e2/0x3ed  [<ffffffff81424a76>] disassociate_ctty+0x63/0x226  [<ffffffff81078aa7>] do_exit+0x79f/0xa11  [<ffffffff81086bdb>] ? get_signal_to_deliver+0x206/0x62f  [<ffffffff810b4bfb>] ? lock_release_holdtime.part.8+0xf/0x16e  [<ffffffff81079b05>] do_group_exit+0x47/0xb5  [<ffffffff81086c16>] get_signal_to_deliver+0x241/0x62f  [<ffffffff810020a7>] do_signal+0x43/0x59d  [<ffffffff810f2af7>] ? __audit_syscall_exit+0x21a/0x2a8  [<ffffffff810b4bfb>] ? lock_release_holdtime.part.8+0xf/0x16e  [<ffffffff81002655>] do_notify_resume+0x54/0x6c  [<ffffffff817abaf8>] int_signal+0x12/0x17  Cc: <stable@vger.kernel.org> # 3.12.x",
        "func_before": "static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n{\n\tlong tmp = *old;\n\t*old = atomic_long_cmpxchg(&sem->count, *old, new);\n\treturn *old == tmp;\n}",
        "func": "static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n{\n\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);\n\tif (tmp == *old) {\n\t\t*old = new;\n\t\treturn 1;\n\t} else {\n\t\t*old = tmp;\n\t\treturn 0;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,11 @@\n static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n {\n-\tlong tmp = *old;\n-\t*old = atomic_long_cmpxchg(&sem->count, *old, new);\n-\treturn *old == tmp;\n+\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);\n+\tif (tmp == *old) {\n+\t\t*old = new;\n+\t\treturn 1;\n+\t} else {\n+\t\t*old = tmp;\n+\t\treturn 0;\n+\t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tlong tmp = *old;",
                "\t*old = atomic_long_cmpxchg(&sem->count, *old, new);",
                "\treturn *old == tmp;"
            ],
            "added_lines": [
                "\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);",
                "\tif (tmp == *old) {",
                "\t\t*old = new;",
                "\t\treturn 1;",
                "\t} else {",
                "\t\t*old = tmp;",
                "\t\treturn 0;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-1670",
        "func_name": "chromium/ResourceDispatcherHostImpl::BeginRequest",
        "description": "Race condition in the ResourceDispatcherHostImpl::BeginRequest function in content/browser/loader/resource_dispatcher_host_impl.cc in Google Chrome before 50.0.2661.102 allows remote attackers to make arbitrary HTTP requests by leveraging access to a renderer process and reusing a request ID.",
        "git_url": "https://chromium.googlesource.com/chromium/src/+/1af4fada49c4f3890f16daac31d38379a9d782b2",
        "commit_title": "Block a compromised renderer from reusing request ids.",
        "commit_text": "   ",
        "func_before": "void ResourceDispatcherHostImpl::BeginRequest(\n    int request_id,\n    const ResourceHostMsg_Request& request_data,\n    IPC::Message* sync_result,  // only valid for sync\n    int route_id) {\n  int process_type = filter_->process_type();\n  int child_id = filter_->child_id();\n\n  // PlzNavigate: reject invalid renderer main resource request.\n  if (IsBrowserSideNavigationEnabled() &&\n      IsResourceTypeFrame(request_data.resource_type) &&\n      !request_data.url.SchemeIs(url::kBlobScheme)) {\n    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);\n    return;\n  }\n\n  // Reject invalid priority.\n  if (request_data.priority < net::MINIMUM_PRIORITY ||\n      request_data.priority > net::MAXIMUM_PRIORITY) {\n    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);\n    return;\n  }\n\n  // If we crash here, figure out what URL the renderer was requesting.\n  // http://crbug.com/91398\n  char url_buf[128];\n  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));\n  base::debug::Alias(url_buf);\n\n  // If the request that's coming in is being transferred from another process,\n  // we want to reuse and resume the old loader rather than start a new one.\n  LoaderMap::iterator it = pending_loaders_.find(\n      GlobalRequestID(request_data.transferred_request_child_id,\n                      request_data.transferred_request_request_id));\n  if (it != pending_loaders_.end()) {\n    // If the request is transferring to a new process, we can update our\n    // state and let it resume with its existing ResourceHandlers.\n    if (it->second->is_transferring()) {\n      ResourceLoader* deferred_loader = it->second.get();\n      UpdateRequestForTransfer(child_id, route_id, request_id,\n                               request_data, it);\n      deferred_loader->CompleteTransfer();\n    } else {\n      bad_message::ReceivedBadMessage(\n          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);\n    }\n    return;\n  }\n\n  ResourceContext* resource_context = NULL;\n  net::URLRequestContext* request_context = NULL;\n  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,\n                       &resource_context, &request_context);\n  // http://crbug.com/90971\n  CHECK(ContainsKey(active_resource_contexts_, resource_context));\n\n  // Parse the headers before calling ShouldServiceRequest, so that they are\n  // available to be validated.\n  net::HttpRequestHeaders headers;\n  headers.AddHeadersFromString(request_data.headers);\n\n  if (is_shutdown_ ||\n      !ShouldServiceRequest(process_type, child_id, request_data, headers,\n                            filter_, resource_context)) {\n    AbortRequestBeforeItStarts(filter_, sync_result, request_id);\n    return;\n  }\n\n  // Allow the observer to block/handle the request.\n  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,\n                                                  request_data.url,\n                                                  request_data.resource_type,\n                                                  resource_context)) {\n    AbortRequestBeforeItStarts(filter_, sync_result, request_id);\n    return;\n  }\n\n  // Construct the request.\n  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(\n      request_data.url, request_data.priority, NULL);\n\n  new_request->set_method(request_data.method);\n  new_request->set_first_party_for_cookies(\n      request_data.first_party_for_cookies);\n  new_request->set_initiator(request_data.request_initiator);\n\n  // If the request is a MAIN_FRAME request, the first-party URL gets updated on\n  // redirects.\n  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {\n    new_request->set_first_party_url_policy(\n        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);\n  }\n\n  const Referrer referrer(request_data.referrer, request_data.referrer_policy);\n  SetReferrerForRequest(new_request.get(), referrer);\n\n  new_request->SetExtraRequestHeaders(headers);\n\n  storage::BlobStorageContext* blob_context =\n      GetBlobStorageContext(filter_->blob_storage_context());\n  // Resolve elements from request_body and prepare upload data.\n  if (request_data.request_body.get()) {\n    // |blob_context| could be null when the request is from the plugins because\n    // ResourceMessageFilters created in PluginProcessHost don't have the blob\n    // context.\n    if (blob_context) {\n      // Attaches the BlobDataHandles to request_body not to free the blobs and\n      // any attached shareable files until upload completion. These data will\n      // be used in UploadDataStream and ServiceWorkerURLRequestJob.\n      AttachRequestBodyBlobDataHandles(\n          request_data.request_body.get(),\n          blob_context);\n    }\n    new_request->set_upload(UploadDataStreamBuilder::Build(\n        request_data.request_body.get(),\n        blob_context,\n        filter_->file_system_context(),\n        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)\n            .get()));\n  }\n\n  bool allow_download = request_data.allow_download &&\n      IsResourceTypeFrame(request_data.resource_type);\n  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;\n  bool is_sync_load = sync_result != NULL;\n\n  // Raw headers are sensitive, as they include Cookie/Set-Cookie, so only\n  // allow requesting them if requester has ReadRawCookies permission.\n  ChildProcessSecurityPolicyImpl* policy =\n      ChildProcessSecurityPolicyImpl::GetInstance();\n  bool report_raw_headers = request_data.report_raw_headers;\n  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {\n    // TODO: crbug.com/523063 can we call bad_message::ReceivedBadMessage here?\n    VLOG(1) << \"Denied unauthorized request for raw headers\";\n    report_raw_headers = false;\n  }\n  int load_flags =\n      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);\n  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||\n      request_data.resource_type == RESOURCE_TYPE_FAVICON) {\n    do_not_prompt_for_login = true;\n  }\n  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&\n      HTTP_AUTH_RELATION_BLOCKED_CROSS ==\n          HttpAuthRelationTypeOf(request_data.url,\n                                 request_data.first_party_for_cookies)) {\n    // Prevent third-party image content from prompting for login, as this\n    // is often a scam to extract credentials for another domain from the user.\n    // Only block image loads, as the attack applies largely to the \"src\"\n    // property of the <img> tag. It is common for web properties to allow\n    // untrusted values for <img src>; this is considered a fair thing for an\n    // HTML sanitizer to do. Conversely, any HTML sanitizer that didn't\n    // filter sources for <script>, <link>, <embed>, <object>, <iframe> tags\n    // would be considered vulnerable in and of itself.\n    do_not_prompt_for_login = true;\n    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;\n  }\n\n  bool support_async_revalidation =\n      !is_sync_load && async_revalidation_manager_ &&\n      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);\n\n  if (support_async_revalidation)\n    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;\n\n  // Sync loads should have maximum priority and should be the only\n  // requets that have the ignore limits flag set.\n  if (is_sync_load) {\n    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);\n    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);\n  } else {\n    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);\n  }\n  new_request->SetLoadFlags(load_flags);\n\n  // Make extra info and read footer (contains request ID).\n  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(\n      process_type, child_id, route_id,\n      -1,  // frame_tree_node_id\n      request_data.origin_pid, request_id, request_data.render_frame_id,\n      request_data.is_main_frame, request_data.parent_is_main_frame,\n      request_data.resource_type, request_data.transition_type,\n      request_data.should_replace_current_entry,\n      false,  // is download\n      false,  // is stream\n      allow_download, request_data.has_user_gesture,\n      request_data.enable_load_timing, request_data.enable_upload_progress,\n      do_not_prompt_for_login, request_data.referrer_policy,\n      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),\n      report_raw_headers, !is_sync_load,\n      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,\n                  resource_context,\n                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),\n      support_async_revalidation ? request_data.headers : std::string());\n  // Request takes ownership.\n  extra_info->AssociateWithRequest(new_request.get());\n\n  if (new_request->url().SchemeIs(url::kBlobScheme)) {\n    // Hang on to a reference to ensure the blob is not released prior\n    // to the job being started.\n    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(\n        new_request.get(),\n        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(\n            new_request->url()));\n  }\n\n  // Initialize the service worker handler for the request. We don't use\n  // ServiceWorker for synchronous loads to avoid renderer deadlocks.\n  const bool should_skip_service_worker =\n      request_data.skip_service_worker || is_sync_load;\n  ServiceWorkerRequestHandler::InitializeHandler(\n      new_request.get(), filter_->service_worker_context(), blob_context,\n      child_id, request_data.service_worker_provider_id,\n      should_skip_service_worker,\n      request_data.fetch_request_mode, request_data.fetch_credentials_mode,\n      request_data.fetch_redirect_mode, request_data.resource_type,\n      request_data.fetch_request_context_type, request_data.fetch_frame_type,\n      request_data.request_body);\n\n  if (base::CommandLine::ForCurrentProcess()->HasSwitch(\n          switches::kEnableExperimentalWebPlatformFeatures)) {\n    ForeignFetchRequestHandler::InitializeHandler(\n        new_request.get(), filter_->service_worker_context(), blob_context,\n        child_id, request_data.service_worker_provider_id,\n        should_skip_service_worker,\n        request_data.fetch_request_mode, request_data.fetch_credentials_mode,\n        request_data.fetch_redirect_mode, request_data.resource_type,\n        request_data.fetch_request_context_type, request_data.fetch_frame_type,\n        request_data.request_body);\n  }\n\n  // Have the appcache associate its extra info with the request.\n  AppCacheInterceptor::SetExtraRequestInfo(\n      new_request.get(), filter_->appcache_service(), child_id,\n      request_data.appcache_host_id, request_data.resource_type,\n      request_data.should_reset_appcache);\n\n  scoped_ptr<ResourceHandler> handler(\n       CreateResourceHandler(\n           new_request.get(),\n           request_data, sync_result, route_id, process_type, child_id,\n           resource_context));\n\n  if (handler)\n    BeginRequestInternal(std::move(new_request), std::move(handler));\n}",
        "func": "void ResourceDispatcherHostImpl::BeginRequest(\n    int request_id,\n    const ResourceHostMsg_Request& request_data,\n    IPC::Message* sync_result,  // only valid for sync\n    int route_id) {\n  int process_type = filter_->process_type();\n  int child_id = filter_->child_id();\n\n  // Reject request id that's currently in use.\n  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {\n    bad_message::ReceivedBadMessage(filter_,\n                                    bad_message::RDH_INVALID_REQUEST_ID);\n    return;\n  }\n\n  // PlzNavigate: reject invalid renderer main resource request.\n  if (IsBrowserSideNavigationEnabled() &&\n      IsResourceTypeFrame(request_data.resource_type) &&\n      !request_data.url.SchemeIs(url::kBlobScheme)) {\n    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);\n    return;\n  }\n\n  // Reject invalid priority.\n  if (request_data.priority < net::MINIMUM_PRIORITY ||\n      request_data.priority > net::MAXIMUM_PRIORITY) {\n    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);\n    return;\n  }\n\n  // If we crash here, figure out what URL the renderer was requesting.\n  // http://crbug.com/91398\n  char url_buf[128];\n  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));\n  base::debug::Alias(url_buf);\n\n  // If the request that's coming in is being transferred from another process,\n  // we want to reuse and resume the old loader rather than start a new one.\n  LoaderMap::iterator it = pending_loaders_.find(\n      GlobalRequestID(request_data.transferred_request_child_id,\n                      request_data.transferred_request_request_id));\n  if (it != pending_loaders_.end()) {\n    // If the request is transferring to a new process, we can update our\n    // state and let it resume with its existing ResourceHandlers.\n    if (it->second->is_transferring()) {\n      ResourceLoader* deferred_loader = it->second.get();\n      UpdateRequestForTransfer(child_id, route_id, request_id,\n                               request_data, it);\n      deferred_loader->CompleteTransfer();\n    } else {\n      bad_message::ReceivedBadMessage(\n          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);\n    }\n    return;\n  }\n\n  ResourceContext* resource_context = NULL;\n  net::URLRequestContext* request_context = NULL;\n  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,\n                       &resource_context, &request_context);\n  // http://crbug.com/90971\n  CHECK(ContainsKey(active_resource_contexts_, resource_context));\n\n  // Parse the headers before calling ShouldServiceRequest, so that they are\n  // available to be validated.\n  net::HttpRequestHeaders headers;\n  headers.AddHeadersFromString(request_data.headers);\n\n  if (is_shutdown_ ||\n      !ShouldServiceRequest(process_type, child_id, request_data, headers,\n                            filter_, resource_context)) {\n    AbortRequestBeforeItStarts(filter_, sync_result, request_id);\n    return;\n  }\n\n  // Allow the observer to block/handle the request.\n  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,\n                                                  request_data.url,\n                                                  request_data.resource_type,\n                                                  resource_context)) {\n    AbortRequestBeforeItStarts(filter_, sync_result, request_id);\n    return;\n  }\n\n  // Construct the request.\n  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(\n      request_data.url, request_data.priority, NULL);\n\n  new_request->set_method(request_data.method);\n  new_request->set_first_party_for_cookies(\n      request_data.first_party_for_cookies);\n  new_request->set_initiator(request_data.request_initiator);\n\n  // If the request is a MAIN_FRAME request, the first-party URL gets updated on\n  // redirects.\n  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {\n    new_request->set_first_party_url_policy(\n        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);\n  }\n\n  const Referrer referrer(request_data.referrer, request_data.referrer_policy);\n  SetReferrerForRequest(new_request.get(), referrer);\n\n  new_request->SetExtraRequestHeaders(headers);\n\n  storage::BlobStorageContext* blob_context =\n      GetBlobStorageContext(filter_->blob_storage_context());\n  // Resolve elements from request_body and prepare upload data.\n  if (request_data.request_body.get()) {\n    // |blob_context| could be null when the request is from the plugins because\n    // ResourceMessageFilters created in PluginProcessHost don't have the blob\n    // context.\n    if (blob_context) {\n      // Attaches the BlobDataHandles to request_body not to free the blobs and\n      // any attached shareable files until upload completion. These data will\n      // be used in UploadDataStream and ServiceWorkerURLRequestJob.\n      AttachRequestBodyBlobDataHandles(\n          request_data.request_body.get(),\n          blob_context);\n    }\n    new_request->set_upload(UploadDataStreamBuilder::Build(\n        request_data.request_body.get(),\n        blob_context,\n        filter_->file_system_context(),\n        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)\n            .get()));\n  }\n\n  bool allow_download = request_data.allow_download &&\n      IsResourceTypeFrame(request_data.resource_type);\n  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;\n  bool is_sync_load = sync_result != NULL;\n\n  // Raw headers are sensitive, as they include Cookie/Set-Cookie, so only\n  // allow requesting them if requester has ReadRawCookies permission.\n  ChildProcessSecurityPolicyImpl* policy =\n      ChildProcessSecurityPolicyImpl::GetInstance();\n  bool report_raw_headers = request_data.report_raw_headers;\n  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {\n    // TODO: crbug.com/523063 can we call bad_message::ReceivedBadMessage here?\n    VLOG(1) << \"Denied unauthorized request for raw headers\";\n    report_raw_headers = false;\n  }\n  int load_flags =\n      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);\n  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||\n      request_data.resource_type == RESOURCE_TYPE_FAVICON) {\n    do_not_prompt_for_login = true;\n  }\n  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&\n      HTTP_AUTH_RELATION_BLOCKED_CROSS ==\n          HttpAuthRelationTypeOf(request_data.url,\n                                 request_data.first_party_for_cookies)) {\n    // Prevent third-party image content from prompting for login, as this\n    // is often a scam to extract credentials for another domain from the user.\n    // Only block image loads, as the attack applies largely to the \"src\"\n    // property of the <img> tag. It is common for web properties to allow\n    // untrusted values for <img src>; this is considered a fair thing for an\n    // HTML sanitizer to do. Conversely, any HTML sanitizer that didn't\n    // filter sources for <script>, <link>, <embed>, <object>, <iframe> tags\n    // would be considered vulnerable in and of itself.\n    do_not_prompt_for_login = true;\n    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;\n  }\n\n  bool support_async_revalidation =\n      !is_sync_load && async_revalidation_manager_ &&\n      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);\n\n  if (support_async_revalidation)\n    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;\n\n  // Sync loads should have maximum priority and should be the only\n  // requets that have the ignore limits flag set.\n  if (is_sync_load) {\n    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);\n    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);\n  } else {\n    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);\n  }\n  new_request->SetLoadFlags(load_flags);\n\n  // Make extra info and read footer (contains request ID).\n  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(\n      process_type, child_id, route_id,\n      -1,  // frame_tree_node_id\n      request_data.origin_pid, request_id, request_data.render_frame_id,\n      request_data.is_main_frame, request_data.parent_is_main_frame,\n      request_data.resource_type, request_data.transition_type,\n      request_data.should_replace_current_entry,\n      false,  // is download\n      false,  // is stream\n      allow_download, request_data.has_user_gesture,\n      request_data.enable_load_timing, request_data.enable_upload_progress,\n      do_not_prompt_for_login, request_data.referrer_policy,\n      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),\n      report_raw_headers, !is_sync_load,\n      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,\n                  resource_context,\n                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),\n      support_async_revalidation ? request_data.headers : std::string());\n  // Request takes ownership.\n  extra_info->AssociateWithRequest(new_request.get());\n\n  if (new_request->url().SchemeIs(url::kBlobScheme)) {\n    // Hang on to a reference to ensure the blob is not released prior\n    // to the job being started.\n    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(\n        new_request.get(),\n        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(\n            new_request->url()));\n  }\n\n  // Initialize the service worker handler for the request. We don't use\n  // ServiceWorker for synchronous loads to avoid renderer deadlocks.\n  const bool should_skip_service_worker =\n      request_data.skip_service_worker || is_sync_load;\n  ServiceWorkerRequestHandler::InitializeHandler(\n      new_request.get(), filter_->service_worker_context(), blob_context,\n      child_id, request_data.service_worker_provider_id,\n      should_skip_service_worker,\n      request_data.fetch_request_mode, request_data.fetch_credentials_mode,\n      request_data.fetch_redirect_mode, request_data.resource_type,\n      request_data.fetch_request_context_type, request_data.fetch_frame_type,\n      request_data.request_body);\n\n  if (base::CommandLine::ForCurrentProcess()->HasSwitch(\n          switches::kEnableExperimentalWebPlatformFeatures)) {\n    ForeignFetchRequestHandler::InitializeHandler(\n        new_request.get(), filter_->service_worker_context(), blob_context,\n        child_id, request_data.service_worker_provider_id,\n        should_skip_service_worker,\n        request_data.fetch_request_mode, request_data.fetch_credentials_mode,\n        request_data.fetch_redirect_mode, request_data.resource_type,\n        request_data.fetch_request_context_type, request_data.fetch_frame_type,\n        request_data.request_body);\n  }\n\n  // Have the appcache associate its extra info with the request.\n  AppCacheInterceptor::SetExtraRequestInfo(\n      new_request.get(), filter_->appcache_service(), child_id,\n      request_data.appcache_host_id, request_data.resource_type,\n      request_data.should_reset_appcache);\n\n  scoped_ptr<ResourceHandler> handler(\n       CreateResourceHandler(\n           new_request.get(),\n           request_data, sync_result, route_id, process_type, child_id,\n           resource_context));\n\n  if (handler)\n    BeginRequestInternal(std::move(new_request), std::move(handler));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,13 @@\n     int route_id) {\n   int process_type = filter_->process_type();\n   int child_id = filter_->child_id();\n+\n+  // Reject request id that's currently in use.\n+  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {\n+    bad_message::ReceivedBadMessage(filter_,\n+                                    bad_message::RDH_INVALID_REQUEST_ID);\n+    return;\n+  }\n \n   // PlzNavigate: reject invalid renderer main resource request.\n   if (IsBrowserSideNavigationEnabled() &&",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  // Reject request id that's currently in use.",
                "  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {",
                "    bad_message::ReceivedBadMessage(filter_,",
                "                                    bad_message::RDH_INVALID_REQUEST_ID);",
                "    return;",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-3509",
        "func_name": "openssl/ssl_scan_serverhello_tlsext",
        "description": "Race condition in the ssl_parse_serverhello_tlsext function in t1_lib.c in OpenSSL 1.0.0 before 1.0.0n and 1.0.1 before 1.0.1i, when multithreading and session resumption are used, allows remote SSL servers to cause a denial of service (memory overwrite and client application crash) or possibly have unspecified other impact by sending Elliptic Curve (EC) Supported Point Formats Extension data.",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=fb0bc2b273bcc2d5401dd883fe869af4fc74bb21",
        "commit_title": "",
        "commit_text": "Fix race condition in ssl_parse_serverhello_tlsext  CVE-2014-3509 ",
        "func_before": "static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)\n\t{\n\tunsigned short length;\n\tunsigned short type;\n\tunsigned short size;\n\tunsigned char *data = *p;\n\tint tlsext_servername = 0;\n\tint renegotiate_seen = 0;\n\n#ifndef OPENSSL_NO_NEXTPROTONEG\n\ts->s3->next_proto_neg_seen = 0;\n#endif\n\n\tif (s->s3->alpn_selected)\n\t\t{\n\t\tOPENSSL_free(s->s3->alpn_selected);\n\t\ts->s3->alpn_selected = NULL;\n\t\t}\n\n#ifndef OPENSSL_NO_HEARTBEATS\n\ts->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |\n\t                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);\n#endif\n\n#ifdef TLSEXT_TYPE_encrypt_then_mac\n\ts->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;\n#endif\n\n\tif (data >= (d+n-2))\n\t\tgoto ri_check;\n\n\tn2s(data,length);\n\tif (data+length != d+n)\n\t\t{\n\t\t*al = SSL_AD_DECODE_ERROR;\n\t\treturn 0;\n\t\t}\n\n\twhile(data <= (d+n-4))\n\t\t{\n\t\tn2s(data,type);\n\t\tn2s(data,size);\n\n\t\tif (data+size > (d+n))\n\t   \t\tgoto ri_check;\n\n\t\tif (s->tlsext_debug_cb)\n\t\t\ts->tlsext_debug_cb(s, 1, type, data, size,\n\t\t\t\t\t\ts->tlsext_debug_arg);\n\n\t\tif (type == TLSEXT_TYPE_server_name)\n\t\t\t{\n\t\t\tif (s->tlsext_hostname == NULL || size > 0)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNRECOGNIZED_NAME;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\ttlsext_servername = 1;   \n\t\t\t}\n\n#ifndef OPENSSL_NO_EC\n\t\telse if (type == TLSEXT_TYPE_ec_point_formats)\n\t\t\t{\n\t\t\tunsigned char *sdata = data;\n\t\t\tint ecpointformatlist_length = *(sdata++);\n\n\t\t\tif (ecpointformatlist_length != size - 1)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\ts->session->tlsext_ecpointformatlist_length = 0;\n\t\t\tif (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);\n\t\t\tif ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\ts->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;\n\t\t\tmemcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);\n#if 0\n\t\t\tfprintf(stderr,\"ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist \");\n\t\t\tsdata = s->session->tlsext_ecpointformatlist;\n\t\t\tfor (i = 0; i < s->session->tlsext_ecpointformatlist_length; i++)\n\t\t\t\tfprintf(stderr,\"%i \",*(sdata++));\n\t\t\tfprintf(stderr,\"\\n\");\n#endif\n\t\t\t}\n#endif /* OPENSSL_NO_EC */\n\n\t\telse if (type == TLSEXT_TYPE_session_ticket)\n\t\t\t{\n\t\t\tif (s->tls_session_ticket_ext_cb &&\n\t\t\t    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tif (!tls_use_ticket(s) || (size > 0))\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNSUPPORTED_EXTENSION;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\ts->tlsext_ticket_expected = 1;\n\t\t\t}\n#ifdef TLSEXT_TYPE_opaque_prf_input\n\t\telse if (type == TLSEXT_TYPE_opaque_prf_input)\n\t\t\t{\n\t\t\tunsigned char *sdata = data;\n\n\t\t\tif (size < 2)\n\t\t\t\t{\n\t\t\t\t*al = SSL_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tn2s(sdata, s->s3->server_opaque_prf_input_len);\n\t\t\tif (s->s3->server_opaque_prf_input_len != size - 2)\n\t\t\t\t{\n\t\t\t\t*al = SSL_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\n\t\t\tif (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */\n\t\t\t\tOPENSSL_free(s->s3->server_opaque_prf_input);\n\t\t\tif (s->s3->server_opaque_prf_input_len == 0)\n\t\t\t\ts->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */\n\t\t\telse\n\t\t\t\ts->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);\n\n\t\t\tif (s->s3->server_opaque_prf_input == NULL)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\telse if (type == TLSEXT_TYPE_status_request)\n\t\t\t{\n\t\t\t/* MUST be empty and only sent if we've requested\n\t\t\t * a status request message.\n\t\t\t */ \n\t\t\tif ((s->tlsext_status_type == -1) || (size > 0))\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNSUPPORTED_EXTENSION;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t/* Set flag to expect CertificateStatus message */\n\t\t\ts->tlsext_status_expected = 1;\n\t\t\t}\n#ifndef OPENSSL_NO_NEXTPROTONEG\n\t\telse if (type == TLSEXT_TYPE_next_proto_neg &&\n\t\t\t s->s3->tmp.finish_md_len == 0)\n\t\t\t{\n\t\t\tunsigned char *selected;\n\t\t\tunsigned char selected_len;\n\n\t\t\t/* We must have requested it. */\n\t\t\tif (s->ctx->next_proto_select_cb == NULL)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNSUPPORTED_EXTENSION;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t/* The data must be valid */\n\t\t\tif (!ssl_next_proto_validate(data, size))\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tif (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\ts->next_proto_negotiated = OPENSSL_malloc(selected_len);\n\t\t\tif (!s->next_proto_negotiated)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tmemcpy(s->next_proto_negotiated, selected, selected_len);\n\t\t\ts->next_proto_negotiated_len = selected_len;\n\t\t\ts->s3->next_proto_neg_seen = 1;\n\t\t\t}\n#endif\n\n\t\telse if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)\n\t\t\t{\n\t\t\tunsigned len;\n\n\t\t\t/* We must have requested it. */\n\t\t\tif (s->alpn_client_proto_list == NULL)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNSUPPORTED_EXTENSION;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tif (size < 4)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t/* The extension data consists of:\n\t\t\t *   uint16 list_length\n\t\t\t *   uint8 proto_length;\n\t\t\t *   uint8 proto[proto_length]; */\n\t\t\tlen = data[0];\n\t\t\tlen <<= 8;\n\t\t\tlen |= data[1];\n\t\t\tif (len != (unsigned) size - 2)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tlen = data[2];\n\t\t\tif (len != (unsigned) size - 3)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tif (s->s3->alpn_selected)\n\t\t\t\tOPENSSL_free(s->s3->alpn_selected);\n\t\t\ts->s3->alpn_selected = OPENSSL_malloc(len);\n\t\t\tif (!s->s3->alpn_selected)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tmemcpy(s->s3->alpn_selected, data + 3, len);\n\t\t\ts->s3->alpn_selected_len = len;\n\t\t\t}\n\n\t\telse if (type == TLSEXT_TYPE_renegotiate)\n\t\t\t{\n\t\t\tif(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))\n\t\t\t\treturn 0;\n\t\t\trenegotiate_seen = 1;\n\t\t\t}\n#ifndef OPENSSL_NO_HEARTBEATS\n\t\telse if (type == TLSEXT_TYPE_heartbeat)\n\t\t\t{\n\t\t\tswitch(data[0])\n\t\t\t\t{\n\t\t\t\tcase 0x01:\t/* Server allows us to send HB requests */\n\t\t\t\t\t\t\ts->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\tcase 0x02:\t/* Server doesn't accept HB requests */\n\t\t\t\t\t\t\ts->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;\n\t\t\t\t\t\t\ts->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\tdefault:\t*al = SSL_AD_ILLEGAL_PARAMETER;\n\t\t\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\telse if (type == TLSEXT_TYPE_use_srtp)\n                        {\n                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,\n\t\t\t\t\t\t\t      al))\n                                return 0;\n                        }\n\t\t/* If this extension type was not otherwise handled, but \n\t\t * matches a custom_cli_ext_record, then send it to the c\n\t\t * callback */\n\t\telse if (s->ctx->custom_cli_ext_records_count)\n\t\t\t{\n\t\t\tsize_t i;\n\t\t\tcustom_cli_ext_record* record;\n\n\t\t\tfor (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)\n\t\t\t\t{\n\t\t\t\trecord = &s->ctx->custom_cli_ext_records[i];\n\t\t\t\tif (record->ext_type == type)\n\t\t\t\t\t{\n\t\t\t\t\tif (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))\n\t\t\t\t\t\treturn 0;\n\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\t\t\t\n\t\t\t}\n#ifdef TLSEXT_TYPE_encrypt_then_mac\n\t\telse if (type == TLSEXT_TYPE_encrypt_then_mac)\n\t\t\t{\n\t\t\t/* Ignore if inappropriate ciphersuite */\n\t\t\tif (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)\n\t\t\t\ts->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;\n\t\t\t}\n#endif\n \n\t\tdata += size;\n\t\t}\n\n\tif (data != d+n)\n\t\t{\n\t\t*al = SSL_AD_DECODE_ERROR;\n\t\treturn 0;\n\t\t}\n\n\tif (!s->hit && tlsext_servername == 1)\n\t\t{\n \t\tif (s->tlsext_hostname)\n\t\t\t{\n\t\t\tif (s->session->tlsext_hostname == NULL)\n\t\t\t\t{\n\t\t\t\ts->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);\t\n\t\t\t\tif (!s->session->tlsext_hostname)\n\t\t\t\t\t{\n\t\t\t\t\t*al = SSL_AD_UNRECOGNIZED_NAME;\n\t\t\t\t\treturn 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\telse \n\t\t\t\t{\n\t\t\t\t*al = SSL_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t*p = data;\n\n\tri_check:\n\n\t/* Determine if we need to see RI. Strictly speaking if we want to\n\t * avoid an attack we should *always* see RI even on initial server\n\t * hello because the client doesn't see any renegotiation during an\n\t * attack. However this would mean we could not connect to any server\n\t * which doesn't support RI so for the immediate future tolerate RI\n\t * absence on initial connect only.\n\t */\n\tif (!renegotiate_seen\n\t\t&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)\n\t\t&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))\n\t\t{\n\t\t*al = SSL_AD_HANDSHAKE_FAILURE;\n\t\tSSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,\n\t\t\t\tSSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);\n\t\treturn 0;\n\t\t}\n\n\treturn 1;\n\t}",
        "func": "static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)\n\t{\n\tunsigned short length;\n\tunsigned short type;\n\tunsigned short size;\n\tunsigned char *data = *p;\n\tint tlsext_servername = 0;\n\tint renegotiate_seen = 0;\n\n#ifndef OPENSSL_NO_NEXTPROTONEG\n\ts->s3->next_proto_neg_seen = 0;\n#endif\n\n\tif (s->s3->alpn_selected)\n\t\t{\n\t\tOPENSSL_free(s->s3->alpn_selected);\n\t\ts->s3->alpn_selected = NULL;\n\t\t}\n\n#ifndef OPENSSL_NO_HEARTBEATS\n\ts->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |\n\t                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);\n#endif\n\n#ifdef TLSEXT_TYPE_encrypt_then_mac\n\ts->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;\n#endif\n\n\tif (data >= (d+n-2))\n\t\tgoto ri_check;\n\n\tn2s(data,length);\n\tif (data+length != d+n)\n\t\t{\n\t\t*al = SSL_AD_DECODE_ERROR;\n\t\treturn 0;\n\t\t}\n\n\twhile(data <= (d+n-4))\n\t\t{\n\t\tn2s(data,type);\n\t\tn2s(data,size);\n\n\t\tif (data+size > (d+n))\n\t   \t\tgoto ri_check;\n\n\t\tif (s->tlsext_debug_cb)\n\t\t\ts->tlsext_debug_cb(s, 1, type, data, size,\n\t\t\t\t\t\ts->tlsext_debug_arg);\n\n\t\tif (type == TLSEXT_TYPE_server_name)\n\t\t\t{\n\t\t\tif (s->tlsext_hostname == NULL || size > 0)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNRECOGNIZED_NAME;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\ttlsext_servername = 1;   \n\t\t\t}\n\n#ifndef OPENSSL_NO_EC\n\t\telse if (type == TLSEXT_TYPE_ec_point_formats)\n\t\t\t{\n\t\t\tunsigned char *sdata = data;\n\t\t\tint ecpointformatlist_length = *(sdata++);\n\n\t\t\tif (ecpointformatlist_length != size - 1)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tif (!s->hit)\n\t\t\t\t{\n\t\t\t\ts->session->tlsext_ecpointformatlist_length = 0;\n\t\t\t\tif (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);\n\t\t\t\tif ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)\n\t\t\t\t\t{\n\t\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\t\treturn 0;\n\t\t\t\t\t}\n\t\t\t\ts->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;\n\t\t\t\tmemcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);\n\t\t\t\t}\n#if 0\n\t\t\tfprintf(stderr,\"ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist \");\n\t\t\tsdata = s->session->tlsext_ecpointformatlist;\n\t\t\tfor (i = 0; i < s->session->tlsext_ecpointformatlist_length; i++)\n\t\t\t\tfprintf(stderr,\"%i \",*(sdata++));\n\t\t\tfprintf(stderr,\"\\n\");\n#endif\n\t\t\t}\n#endif /* OPENSSL_NO_EC */\n\n\t\telse if (type == TLSEXT_TYPE_session_ticket)\n\t\t\t{\n\t\t\tif (s->tls_session_ticket_ext_cb &&\n\t\t\t    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tif (!tls_use_ticket(s) || (size > 0))\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNSUPPORTED_EXTENSION;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\ts->tlsext_ticket_expected = 1;\n\t\t\t}\n#ifdef TLSEXT_TYPE_opaque_prf_input\n\t\telse if (type == TLSEXT_TYPE_opaque_prf_input)\n\t\t\t{\n\t\t\tunsigned char *sdata = data;\n\n\t\t\tif (size < 2)\n\t\t\t\t{\n\t\t\t\t*al = SSL_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tn2s(sdata, s->s3->server_opaque_prf_input_len);\n\t\t\tif (s->s3->server_opaque_prf_input_len != size - 2)\n\t\t\t\t{\n\t\t\t\t*al = SSL_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\n\t\t\tif (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */\n\t\t\t\tOPENSSL_free(s->s3->server_opaque_prf_input);\n\t\t\tif (s->s3->server_opaque_prf_input_len == 0)\n\t\t\t\ts->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */\n\t\t\telse\n\t\t\t\ts->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);\n\n\t\t\tif (s->s3->server_opaque_prf_input == NULL)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\telse if (type == TLSEXT_TYPE_status_request)\n\t\t\t{\n\t\t\t/* MUST be empty and only sent if we've requested\n\t\t\t * a status request message.\n\t\t\t */ \n\t\t\tif ((s->tlsext_status_type == -1) || (size > 0))\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNSUPPORTED_EXTENSION;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t/* Set flag to expect CertificateStatus message */\n\t\t\ts->tlsext_status_expected = 1;\n\t\t\t}\n#ifndef OPENSSL_NO_NEXTPROTONEG\n\t\telse if (type == TLSEXT_TYPE_next_proto_neg &&\n\t\t\t s->s3->tmp.finish_md_len == 0)\n\t\t\t{\n\t\t\tunsigned char *selected;\n\t\t\tunsigned char selected_len;\n\n\t\t\t/* We must have requested it. */\n\t\t\tif (s->ctx->next_proto_select_cb == NULL)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNSUPPORTED_EXTENSION;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t/* The data must be valid */\n\t\t\tif (!ssl_next_proto_validate(data, size))\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tif (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\ts->next_proto_negotiated = OPENSSL_malloc(selected_len);\n\t\t\tif (!s->next_proto_negotiated)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tmemcpy(s->next_proto_negotiated, selected, selected_len);\n\t\t\ts->next_proto_negotiated_len = selected_len;\n\t\t\ts->s3->next_proto_neg_seen = 1;\n\t\t\t}\n#endif\n\n\t\telse if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)\n\t\t\t{\n\t\t\tunsigned len;\n\n\t\t\t/* We must have requested it. */\n\t\t\tif (s->alpn_client_proto_list == NULL)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_UNSUPPORTED_EXTENSION;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tif (size < 4)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t/* The extension data consists of:\n\t\t\t *   uint16 list_length\n\t\t\t *   uint8 proto_length;\n\t\t\t *   uint8 proto[proto_length]; */\n\t\t\tlen = data[0];\n\t\t\tlen <<= 8;\n\t\t\tlen |= data[1];\n\t\t\tif (len != (unsigned) size - 2)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tlen = data[2];\n\t\t\tif (len != (unsigned) size - 3)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tif (s->s3->alpn_selected)\n\t\t\t\tOPENSSL_free(s->s3->alpn_selected);\n\t\t\ts->s3->alpn_selected = OPENSSL_malloc(len);\n\t\t\tif (!s->s3->alpn_selected)\n\t\t\t\t{\n\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\tmemcpy(s->s3->alpn_selected, data + 3, len);\n\t\t\ts->s3->alpn_selected_len = len;\n\t\t\t}\n\n\t\telse if (type == TLSEXT_TYPE_renegotiate)\n\t\t\t{\n\t\t\tif(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))\n\t\t\t\treturn 0;\n\t\t\trenegotiate_seen = 1;\n\t\t\t}\n#ifndef OPENSSL_NO_HEARTBEATS\n\t\telse if (type == TLSEXT_TYPE_heartbeat)\n\t\t\t{\n\t\t\tswitch(data[0])\n\t\t\t\t{\n\t\t\t\tcase 0x01:\t/* Server allows us to send HB requests */\n\t\t\t\t\t\t\ts->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\tcase 0x02:\t/* Server doesn't accept HB requests */\n\t\t\t\t\t\t\ts->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;\n\t\t\t\t\t\t\ts->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\tdefault:\t*al = SSL_AD_ILLEGAL_PARAMETER;\n\t\t\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\telse if (type == TLSEXT_TYPE_use_srtp)\n                        {\n                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,\n\t\t\t\t\t\t\t      al))\n                                return 0;\n                        }\n\t\t/* If this extension type was not otherwise handled, but \n\t\t * matches a custom_cli_ext_record, then send it to the c\n\t\t * callback */\n\t\telse if (s->ctx->custom_cli_ext_records_count)\n\t\t\t{\n\t\t\tsize_t i;\n\t\t\tcustom_cli_ext_record* record;\n\n\t\t\tfor (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)\n\t\t\t\t{\n\t\t\t\trecord = &s->ctx->custom_cli_ext_records[i];\n\t\t\t\tif (record->ext_type == type)\n\t\t\t\t\t{\n\t\t\t\t\tif (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))\n\t\t\t\t\t\treturn 0;\n\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\t\t\t\n\t\t\t}\n#ifdef TLSEXT_TYPE_encrypt_then_mac\n\t\telse if (type == TLSEXT_TYPE_encrypt_then_mac)\n\t\t\t{\n\t\t\t/* Ignore if inappropriate ciphersuite */\n\t\t\tif (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)\n\t\t\t\ts->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;\n\t\t\t}\n#endif\n \n\t\tdata += size;\n\t\t}\n\n\tif (data != d+n)\n\t\t{\n\t\t*al = SSL_AD_DECODE_ERROR;\n\t\treturn 0;\n\t\t}\n\n\tif (!s->hit && tlsext_servername == 1)\n\t\t{\n \t\tif (s->tlsext_hostname)\n\t\t\t{\n\t\t\tif (s->session->tlsext_hostname == NULL)\n\t\t\t\t{\n\t\t\t\ts->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);\t\n\t\t\t\tif (!s->session->tlsext_hostname)\n\t\t\t\t\t{\n\t\t\t\t\t*al = SSL_AD_UNRECOGNIZED_NAME;\n\t\t\t\t\treturn 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\telse \n\t\t\t\t{\n\t\t\t\t*al = SSL_AD_DECODE_ERROR;\n\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t*p = data;\n\n\tri_check:\n\n\t/* Determine if we need to see RI. Strictly speaking if we want to\n\t * avoid an attack we should *always* see RI even on initial server\n\t * hello because the client doesn't see any renegotiation during an\n\t * attack. However this would mean we could not connect to any server\n\t * which doesn't support RI so for the immediate future tolerate RI\n\t * absence on initial connect only.\n\t */\n\tif (!renegotiate_seen\n\t\t&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)\n\t\t&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))\n\t\t{\n\t\t*al = SSL_AD_HANDSHAKE_FAILURE;\n\t\tSSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,\n\t\t\t\tSSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);\n\t\treturn 0;\n\t\t}\n\n\treturn 1;\n\t}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -69,15 +69,18 @@\n \t\t\t\t*al = TLS1_AD_DECODE_ERROR;\n \t\t\t\treturn 0;\n \t\t\t\t}\n-\t\t\ts->session->tlsext_ecpointformatlist_length = 0;\n-\t\t\tif (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);\n-\t\t\tif ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)\n-\t\t\t\t{\n-\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n-\t\t\t\treturn 0;\n-\t\t\t\t}\n-\t\t\ts->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;\n-\t\t\tmemcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);\n+\t\t\tif (!s->hit)\n+\t\t\t\t{\n+\t\t\t\ts->session->tlsext_ecpointformatlist_length = 0;\n+\t\t\t\tif (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);\n+\t\t\t\tif ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)\n+\t\t\t\t\t{\n+\t\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;\n+\t\t\t\t\treturn 0;\n+\t\t\t\t\t}\n+\t\t\t\ts->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;\n+\t\t\t\tmemcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);\n+\t\t\t\t}\n #if 0\n \t\t\tfprintf(stderr,\"ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist \");\n \t\t\tsdata = s->session->tlsext_ecpointformatlist;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\ts->session->tlsext_ecpointformatlist_length = 0;",
                "\t\t\tif (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);",
                "\t\t\tif ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)",
                "\t\t\t\t{",
                "\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;",
                "\t\t\t\treturn 0;",
                "\t\t\t\t}",
                "\t\t\ts->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;",
                "\t\t\tmemcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);"
            ],
            "added_lines": [
                "\t\t\tif (!s->hit)",
                "\t\t\t\t{",
                "\t\t\t\ts->session->tlsext_ecpointformatlist_length = 0;",
                "\t\t\t\tif (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);",
                "\t\t\t\tif ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)",
                "\t\t\t\t\t{",
                "\t\t\t\t\t*al = TLS1_AD_INTERNAL_ERROR;",
                "\t\t\t\t\treturn 0;",
                "\t\t\t\t\t}",
                "\t\t\t\ts->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;",
                "\t\t\t\tmemcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);",
                "\t\t\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-7154",
        "func_name": "xen-project/xen/shadow_track_dirty_vram",
        "description": "Race condition in HVMOP_track_dirty_vram in Xen 4.0.0 through 4.4.x does not ensure possession of the guarding lock for dirty video RAM tracking, which allows certain local guest domains to cause a denial of service via unspecified vectors.",
        "git_url": "https://github.com/xen-project/xen/commit/46a49b91f1026f64430b84dd83e845a33f06415e",
        "commit_title": "x86/shadow: fix race condition sampling the dirty vram state",
        "commit_text": " d->arch.hvm_domain.dirty_vram must be read with the domain's paging lock held.  If not, two concurrent hypercalls could both end up attempting to free dirty_vram (the second of which will free a wild pointer), or both end up allocating a new dirty_vram structure (the first of which will be leaked).  This is XSA-104. ",
        "func_before": "int shadow_track_dirty_vram(struct domain *d,\n                            unsigned long begin_pfn,\n                            unsigned long nr,\n                            XEN_GUEST_HANDLE_64(uint8) dirty_bitmap)\n{\n    int rc;\n    unsigned long end_pfn = begin_pfn + nr;\n    unsigned long dirty_size = (nr + 7) / 8;\n    int flush_tlb = 0;\n    unsigned long i;\n    p2m_type_t t;\n    struct sh_dirty_vram *dirty_vram = d->arch.hvm_domain.dirty_vram;\n    struct p2m_domain *p2m = p2m_get_hostp2m(d);\n\n    if ( end_pfn < begin_pfn || end_pfn > p2m->max_mapped_pfn + 1 )\n        return -EINVAL;\n\n    /* We perform p2m lookups, so lock the p2m upfront to avoid deadlock */\n    p2m_lock(p2m_get_hostp2m(d));\n    paging_lock(d);\n\n    if ( dirty_vram && (!nr ||\n             ( begin_pfn != dirty_vram->begin_pfn\n            || end_pfn   != dirty_vram->end_pfn )) )\n    {\n        /* Different tracking, tear the previous down. */\n        gdprintk(XENLOG_INFO, \"stopping tracking VRAM %lx - %lx\\n\", dirty_vram->begin_pfn, dirty_vram->end_pfn);\n        xfree(dirty_vram->sl1ma);\n        xfree(dirty_vram->dirty_bitmap);\n        xfree(dirty_vram);\n        dirty_vram = d->arch.hvm_domain.dirty_vram = NULL;\n    }\n\n    if ( !nr )\n    {\n        rc = 0;\n        goto out;\n    }\n\n    /* This should happen seldomly (Video mode change),\n     * no need to be careful. */\n    if ( !dirty_vram )\n    {\n        /* Throw away all the shadows rather than walking through them \n         * up to nr times getting rid of mappings of each pfn */\n        shadow_blow_tables(d);\n\n        gdprintk(XENLOG_INFO, \"tracking VRAM %lx - %lx\\n\", begin_pfn, end_pfn);\n\n        rc = -ENOMEM;\n        if ( (dirty_vram = xmalloc(struct sh_dirty_vram)) == NULL )\n            goto out;\n        dirty_vram->begin_pfn = begin_pfn;\n        dirty_vram->end_pfn = end_pfn;\n        d->arch.hvm_domain.dirty_vram = dirty_vram;\n\n        if ( (dirty_vram->sl1ma = xmalloc_array(paddr_t, nr)) == NULL )\n            goto out_dirty_vram;\n        memset(dirty_vram->sl1ma, ~0, sizeof(paddr_t) * nr);\n\n        if ( (dirty_vram->dirty_bitmap = xzalloc_array(uint8_t, dirty_size)) == NULL )\n            goto out_sl1ma;\n\n        dirty_vram->last_dirty = NOW();\n\n        /* Tell the caller that this time we could not track dirty bits. */\n        rc = -ENODATA;\n    }\n    else if (dirty_vram->last_dirty == -1)\n    {\n        /* still completely clean, just copy our empty bitmap */\n        rc = -EFAULT;\n        if ( copy_to_guest(dirty_bitmap, dirty_vram->dirty_bitmap, dirty_size) == 0 )\n            rc = 0;\n    }\n    else\n    {\n        unsigned long map_mfn = INVALID_MFN;\n        void *map_sl1p = NULL;\n\n        /* Iterate over VRAM to track dirty bits. */\n        for ( i = 0; i < nr; i++ ) {\n            mfn_t mfn = get_gfn_query_unlocked(d, begin_pfn + i, &t);\n            struct page_info *page;\n            int dirty = 0;\n            paddr_t sl1ma = dirty_vram->sl1ma[i];\n\n            if (mfn_x(mfn) == INVALID_MFN)\n            {\n                dirty = 1;\n            }\n            else\n            {\n                page = mfn_to_page(mfn);\n                switch (page->u.inuse.type_info & PGT_count_mask)\n                {\n                case 0:\n                    /* No guest reference, nothing to track. */\n                    break;\n                case 1:\n                    /* One guest reference. */\n                    if ( sl1ma == INVALID_PADDR )\n                    {\n                        /* We don't know which sl1e points to this, too bad. */\n                        dirty = 1;\n                        /* TODO: Heuristics for finding the single mapping of\n                         * this gmfn */\n                        flush_tlb |= sh_remove_all_mappings(d->vcpu[0], mfn);\n                    }\n                    else\n                    {\n                        /* Hopefully the most common case: only one mapping,\n                         * whose dirty bit we can use. */\n                        l1_pgentry_t *sl1e;\n                        unsigned long sl1mfn = paddr_to_pfn(sl1ma);\n\n                        if ( sl1mfn != map_mfn )\n                        {\n                            if ( map_sl1p )\n                                sh_unmap_domain_page(map_sl1p);\n                            map_sl1p = sh_map_domain_page(_mfn(sl1mfn));\n                            map_mfn = sl1mfn;\n                        }\n                        sl1e = map_sl1p + (sl1ma & ~PAGE_MASK);\n\n                        if ( l1e_get_flags(*sl1e) & _PAGE_DIRTY )\n                        {\n                            dirty = 1;\n                            /* Note: this is atomic, so we may clear a\n                             * _PAGE_ACCESSED set by another processor. */\n                            l1e_remove_flags(*sl1e, _PAGE_DIRTY);\n                            flush_tlb = 1;\n                        }\n                    }\n                    break;\n                default:\n                    /* More than one guest reference,\n                     * we don't afford tracking that. */\n                    dirty = 1;\n                    break;\n                }\n            }\n\n            if ( dirty )\n            {\n                dirty_vram->dirty_bitmap[i / 8] |= 1 << (i % 8);\n                dirty_vram->last_dirty = NOW();\n            }\n        }\n\n        if ( map_sl1p )\n            sh_unmap_domain_page(map_sl1p);\n\n        rc = -EFAULT;\n        if ( copy_to_guest(dirty_bitmap, dirty_vram->dirty_bitmap, dirty_size) == 0 ) {\n            memset(dirty_vram->dirty_bitmap, 0, dirty_size);\n            if (dirty_vram->last_dirty + SECONDS(2) < NOW())\n            {\n                /* was clean for more than two seconds, try to disable guest\n                 * write access */\n                for ( i = begin_pfn; i < end_pfn; i++ ) {\n                    mfn_t mfn = get_gfn_query_unlocked(d, i, &t);\n                    if (mfn_x(mfn) != INVALID_MFN)\n                        flush_tlb |= sh_remove_write_access(d->vcpu[0], mfn, 1, 0);\n                }\n                dirty_vram->last_dirty = -1;\n            }\n            rc = 0;\n        }\n    }\n    if ( flush_tlb )\n        flush_tlb_mask(d->domain_dirty_cpumask);\n    goto out;\n\nout_sl1ma:\n    xfree(dirty_vram->sl1ma);\nout_dirty_vram:\n    xfree(dirty_vram);\n    dirty_vram = d->arch.hvm_domain.dirty_vram = NULL;\n\nout:\n    paging_unlock(d);\n    p2m_unlock(p2m_get_hostp2m(d));\n    return rc;\n}",
        "func": "int shadow_track_dirty_vram(struct domain *d,\n                            unsigned long begin_pfn,\n                            unsigned long nr,\n                            XEN_GUEST_HANDLE_64(uint8) dirty_bitmap)\n{\n    int rc;\n    unsigned long end_pfn = begin_pfn + nr;\n    unsigned long dirty_size = (nr + 7) / 8;\n    int flush_tlb = 0;\n    unsigned long i;\n    p2m_type_t t;\n    struct sh_dirty_vram *dirty_vram;\n    struct p2m_domain *p2m = p2m_get_hostp2m(d);\n\n    if ( end_pfn < begin_pfn || end_pfn > p2m->max_mapped_pfn + 1 )\n        return -EINVAL;\n\n    /* We perform p2m lookups, so lock the p2m upfront to avoid deadlock */\n    p2m_lock(p2m_get_hostp2m(d));\n    paging_lock(d);\n\n    dirty_vram = d->arch.hvm_domain.dirty_vram;\n\n    if ( dirty_vram && (!nr ||\n             ( begin_pfn != dirty_vram->begin_pfn\n            || end_pfn   != dirty_vram->end_pfn )) )\n    {\n        /* Different tracking, tear the previous down. */\n        gdprintk(XENLOG_INFO, \"stopping tracking VRAM %lx - %lx\\n\", dirty_vram->begin_pfn, dirty_vram->end_pfn);\n        xfree(dirty_vram->sl1ma);\n        xfree(dirty_vram->dirty_bitmap);\n        xfree(dirty_vram);\n        dirty_vram = d->arch.hvm_domain.dirty_vram = NULL;\n    }\n\n    if ( !nr )\n    {\n        rc = 0;\n        goto out;\n    }\n\n    /* This should happen seldomly (Video mode change),\n     * no need to be careful. */\n    if ( !dirty_vram )\n    {\n        /* Throw away all the shadows rather than walking through them \n         * up to nr times getting rid of mappings of each pfn */\n        shadow_blow_tables(d);\n\n        gdprintk(XENLOG_INFO, \"tracking VRAM %lx - %lx\\n\", begin_pfn, end_pfn);\n\n        rc = -ENOMEM;\n        if ( (dirty_vram = xmalloc(struct sh_dirty_vram)) == NULL )\n            goto out;\n        dirty_vram->begin_pfn = begin_pfn;\n        dirty_vram->end_pfn = end_pfn;\n        d->arch.hvm_domain.dirty_vram = dirty_vram;\n\n        if ( (dirty_vram->sl1ma = xmalloc_array(paddr_t, nr)) == NULL )\n            goto out_dirty_vram;\n        memset(dirty_vram->sl1ma, ~0, sizeof(paddr_t) * nr);\n\n        if ( (dirty_vram->dirty_bitmap = xzalloc_array(uint8_t, dirty_size)) == NULL )\n            goto out_sl1ma;\n\n        dirty_vram->last_dirty = NOW();\n\n        /* Tell the caller that this time we could not track dirty bits. */\n        rc = -ENODATA;\n    }\n    else if (dirty_vram->last_dirty == -1)\n    {\n        /* still completely clean, just copy our empty bitmap */\n        rc = -EFAULT;\n        if ( copy_to_guest(dirty_bitmap, dirty_vram->dirty_bitmap, dirty_size) == 0 )\n            rc = 0;\n    }\n    else\n    {\n        unsigned long map_mfn = INVALID_MFN;\n        void *map_sl1p = NULL;\n\n        /* Iterate over VRAM to track dirty bits. */\n        for ( i = 0; i < nr; i++ ) {\n            mfn_t mfn = get_gfn_query_unlocked(d, begin_pfn + i, &t);\n            struct page_info *page;\n            int dirty = 0;\n            paddr_t sl1ma = dirty_vram->sl1ma[i];\n\n            if (mfn_x(mfn) == INVALID_MFN)\n            {\n                dirty = 1;\n            }\n            else\n            {\n                page = mfn_to_page(mfn);\n                switch (page->u.inuse.type_info & PGT_count_mask)\n                {\n                case 0:\n                    /* No guest reference, nothing to track. */\n                    break;\n                case 1:\n                    /* One guest reference. */\n                    if ( sl1ma == INVALID_PADDR )\n                    {\n                        /* We don't know which sl1e points to this, too bad. */\n                        dirty = 1;\n                        /* TODO: Heuristics for finding the single mapping of\n                         * this gmfn */\n                        flush_tlb |= sh_remove_all_mappings(d->vcpu[0], mfn);\n                    }\n                    else\n                    {\n                        /* Hopefully the most common case: only one mapping,\n                         * whose dirty bit we can use. */\n                        l1_pgentry_t *sl1e;\n                        unsigned long sl1mfn = paddr_to_pfn(sl1ma);\n\n                        if ( sl1mfn != map_mfn )\n                        {\n                            if ( map_sl1p )\n                                sh_unmap_domain_page(map_sl1p);\n                            map_sl1p = sh_map_domain_page(_mfn(sl1mfn));\n                            map_mfn = sl1mfn;\n                        }\n                        sl1e = map_sl1p + (sl1ma & ~PAGE_MASK);\n\n                        if ( l1e_get_flags(*sl1e) & _PAGE_DIRTY )\n                        {\n                            dirty = 1;\n                            /* Note: this is atomic, so we may clear a\n                             * _PAGE_ACCESSED set by another processor. */\n                            l1e_remove_flags(*sl1e, _PAGE_DIRTY);\n                            flush_tlb = 1;\n                        }\n                    }\n                    break;\n                default:\n                    /* More than one guest reference,\n                     * we don't afford tracking that. */\n                    dirty = 1;\n                    break;\n                }\n            }\n\n            if ( dirty )\n            {\n                dirty_vram->dirty_bitmap[i / 8] |= 1 << (i % 8);\n                dirty_vram->last_dirty = NOW();\n            }\n        }\n\n        if ( map_sl1p )\n            sh_unmap_domain_page(map_sl1p);\n\n        rc = -EFAULT;\n        if ( copy_to_guest(dirty_bitmap, dirty_vram->dirty_bitmap, dirty_size) == 0 ) {\n            memset(dirty_vram->dirty_bitmap, 0, dirty_size);\n            if (dirty_vram->last_dirty + SECONDS(2) < NOW())\n            {\n                /* was clean for more than two seconds, try to disable guest\n                 * write access */\n                for ( i = begin_pfn; i < end_pfn; i++ ) {\n                    mfn_t mfn = get_gfn_query_unlocked(d, i, &t);\n                    if (mfn_x(mfn) != INVALID_MFN)\n                        flush_tlb |= sh_remove_write_access(d->vcpu[0], mfn, 1, 0);\n                }\n                dirty_vram->last_dirty = -1;\n            }\n            rc = 0;\n        }\n    }\n    if ( flush_tlb )\n        flush_tlb_mask(d->domain_dirty_cpumask);\n    goto out;\n\nout_sl1ma:\n    xfree(dirty_vram->sl1ma);\nout_dirty_vram:\n    xfree(dirty_vram);\n    dirty_vram = d->arch.hvm_domain.dirty_vram = NULL;\n\nout:\n    paging_unlock(d);\n    p2m_unlock(p2m_get_hostp2m(d));\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n     int flush_tlb = 0;\n     unsigned long i;\n     p2m_type_t t;\n-    struct sh_dirty_vram *dirty_vram = d->arch.hvm_domain.dirty_vram;\n+    struct sh_dirty_vram *dirty_vram;\n     struct p2m_domain *p2m = p2m_get_hostp2m(d);\n \n     if ( end_pfn < begin_pfn || end_pfn > p2m->max_mapped_pfn + 1 )\n@@ -18,6 +18,8 @@\n     /* We perform p2m lookups, so lock the p2m upfront to avoid deadlock */\n     p2m_lock(p2m_get_hostp2m(d));\n     paging_lock(d);\n+\n+    dirty_vram = d->arch.hvm_domain.dirty_vram;\n \n     if ( dirty_vram && (!nr ||\n              ( begin_pfn != dirty_vram->begin_pfn",
        "diff_line_info": {
            "deleted_lines": [
                "    struct sh_dirty_vram *dirty_vram = d->arch.hvm_domain.dirty_vram;"
            ],
            "added_lines": [
                "    struct sh_dirty_vram *dirty_vram;",
                "",
                "    dirty_vram = d->arch.hvm_domain.dirty_vram;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-6133",
        "func_name": "polkit/temporary_authorization_store_has_authorization",
        "description": "In PolicyKit (aka polkit) 0.115, the \"start time\" protection mechanism can be bypassed because fork() is not atomic, and therefore authorization decisions are improperly cached. This is related to lack of uid checking in polkitbackend/polkitbackendinteractiveauthority.c.",
        "git_url": "https://cgit.freedesktop.org/polkit/commit/?id=6cc6aafee135ba44ea748250d7d29b562ca190e3",
        "commit_title": "It turns out that the combination of `(pid, start time)` is not",
        "commit_text": "enough to be unique.  For temporary authorizations, we can avoid separate users racing on pid reuse by simply comparing the uid.  https://bugs.chromium.org/p/project-zero/issues/detail?id=1692  And the above original email report is included in full in a new comment.   Closes: https://gitlab.freedesktop.org/polkit/polkit/issues/75 ",
        "func_before": "static gboolean\ntemporary_authorization_store_has_authorization (TemporaryAuthorizationStore *store,\n                                                 PolkitSubject               *subject,\n                                                 const gchar                 *action_id,\n                                                 const gchar                **out_tmp_authz_id)\n{\n  GList *l;\n  gboolean ret;\n  PolkitSubject *subject_to_use;\n\n  g_return_val_if_fail (store != NULL, FALSE);\n  g_return_val_if_fail (POLKIT_IS_SUBJECT (subject), FALSE);\n  g_return_val_if_fail (action_id != NULL, FALSE);\n\n  /* XXX: for now, prefer to store the process */\n  if (POLKIT_IS_SYSTEM_BUS_NAME (subject))\n    {\n      GError *error;\n      error = NULL;\n      subject_to_use = polkit_system_bus_name_get_process_sync (POLKIT_SYSTEM_BUS_NAME (subject),\n                                                                NULL,\n                                                                &error);\n      if (subject_to_use == NULL)\n        {\n          g_printerr (\"Error getting process for system bus name `%s': %s\\n\",\n                      polkit_system_bus_name_get_name (POLKIT_SYSTEM_BUS_NAME (subject)),\n                      error->message);\n          g_error_free (error);\n          subject_to_use = g_object_ref (subject);\n        }\n    }\n  else\n    {\n      subject_to_use = g_object_ref (subject);\n    }\n\n  ret = FALSE;\n\n  for (l = store->authorizations; l != NULL; l = l->next) {\n    TemporaryAuthorization *authorization = l->data;\n\n    if (strcmp (action_id, authorization->action_id) == 0 &&\n        polkit_subject_equal (subject_to_use, authorization->subject))\n      {\n        ret = TRUE;\n        if (out_tmp_authz_id != NULL)\n          *out_tmp_authz_id = authorization->id;\n        goto out;\n      }\n  }\n\n out:\n  g_object_unref (subject_to_use);\n  return ret;\n}",
        "func": "static gboolean\ntemporary_authorization_store_has_authorization (TemporaryAuthorizationStore *store,\n                                                 PolkitSubject               *subject,\n                                                 const gchar                 *action_id,\n                                                 const gchar                **out_tmp_authz_id)\n{\n  GList *l;\n  gboolean ret;\n  PolkitSubject *subject_to_use;\n\n  g_return_val_if_fail (store != NULL, FALSE);\n  g_return_val_if_fail (POLKIT_IS_SUBJECT (subject), FALSE);\n  g_return_val_if_fail (action_id != NULL, FALSE);\n\n  /* XXX: for now, prefer to store the process */\n  if (POLKIT_IS_SYSTEM_BUS_NAME (subject))\n    {\n      GError *error;\n      error = NULL;\n      subject_to_use = polkit_system_bus_name_get_process_sync (POLKIT_SYSTEM_BUS_NAME (subject),\n                                                                NULL,\n                                                                &error);\n      if (subject_to_use == NULL)\n        {\n          g_printerr (\"Error getting process for system bus name `%s': %s\\n\",\n                      polkit_system_bus_name_get_name (POLKIT_SYSTEM_BUS_NAME (subject)),\n                      error->message);\n          g_error_free (error);\n          subject_to_use = g_object_ref (subject);\n        }\n    }\n  else\n    {\n      subject_to_use = g_object_ref (subject);\n    }\n\n  ret = FALSE;\n\n  for (l = store->authorizations; l != NULL; l = l->next) {\n    TemporaryAuthorization *authorization = l->data;\n\n    if (strcmp (action_id, authorization->action_id) == 0 &&\n        subject_equal_for_authz (subject_to_use, authorization->subject))\n      {\n        ret = TRUE;\n        if (out_tmp_authz_id != NULL)\n          *out_tmp_authz_id = authorization->id;\n        goto out;\n      }\n  }\n\n out:\n  g_object_unref (subject_to_use);\n  return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,7 +40,7 @@\n     TemporaryAuthorization *authorization = l->data;\n \n     if (strcmp (action_id, authorization->action_id) == 0 &&\n-        polkit_subject_equal (subject_to_use, authorization->subject))\n+        subject_equal_for_authz (subject_to_use, authorization->subject))\n       {\n         ret = TRUE;\n         if (out_tmp_authz_id != NULL)",
        "diff_line_info": {
            "deleted_lines": [
                "        polkit_subject_equal (subject_to_use, authorization->subject))"
            ],
            "added_lines": [
                "        subject_equal_for_authz (subject_to_use, authorization->subject))"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-6133",
        "func_name": "polkit/temporary_authorization_store_has_authorization",
        "description": "In PolicyKit (aka polkit) 0.115, the \"start time\" protection mechanism can be bypassed because fork() is not atomic, and therefore authorization decisions are improperly cached. This is related to lack of uid checking in polkitbackend/polkitbackendinteractiveauthority.c.",
        "git_url": "https://cgit.freedesktop.org/polkit/commit/?id=c898fdf4b1aafaa04f8ada9d73d77c8bb76e2f81",
        "commit_title": "backend: Compare PolkitUnixProcess uids for temporary authorizations",
        "commit_text": " Closes #75  See merge request polkit/polkit!19",
        "func_before": "static gboolean\ntemporary_authorization_store_has_authorization (TemporaryAuthorizationStore *store,\n                                                 PolkitSubject               *subject,\n                                                 const gchar                 *action_id,\n                                                 const gchar                **out_tmp_authz_id)\n{\n  GList *l;\n  gboolean ret;\n  PolkitSubject *subject_to_use;\n\n  g_return_val_if_fail (store != NULL, FALSE);\n  g_return_val_if_fail (POLKIT_IS_SUBJECT (subject), FALSE);\n  g_return_val_if_fail (action_id != NULL, FALSE);\n\n  /* XXX: for now, prefer to store the process */\n  if (POLKIT_IS_SYSTEM_BUS_NAME (subject))\n    {\n      GError *error;\n      error = NULL;\n      subject_to_use = polkit_system_bus_name_get_process_sync (POLKIT_SYSTEM_BUS_NAME (subject),\n                                                                NULL,\n                                                                &error);\n      if (subject_to_use == NULL)\n        {\n          g_printerr (\"Error getting process for system bus name `%s': %s\\n\",\n                      polkit_system_bus_name_get_name (POLKIT_SYSTEM_BUS_NAME (subject)),\n                      error->message);\n          g_error_free (error);\n          subject_to_use = g_object_ref (subject);\n        }\n    }\n  else\n    {\n      subject_to_use = g_object_ref (subject);\n    }\n\n  ret = FALSE;\n\n  for (l = store->authorizations; l != NULL; l = l->next) {\n    TemporaryAuthorization *authorization = l->data;\n\n    if (strcmp (action_id, authorization->action_id) == 0 &&\n        polkit_subject_equal (subject_to_use, authorization->subject))\n      {\n        ret = TRUE;\n        if (out_tmp_authz_id != NULL)\n          *out_tmp_authz_id = authorization->id;\n        goto out;\n      }\n  }\n\n out:\n  g_object_unref (subject_to_use);\n  return ret;\n}",
        "func": "static gboolean\ntemporary_authorization_store_has_authorization (TemporaryAuthorizationStore *store,\n                                                 PolkitSubject               *subject,\n                                                 const gchar                 *action_id,\n                                                 const gchar                **out_tmp_authz_id)\n{\n  GList *l;\n  gboolean ret;\n  PolkitSubject *subject_to_use;\n\n  g_return_val_if_fail (store != NULL, FALSE);\n  g_return_val_if_fail (POLKIT_IS_SUBJECT (subject), FALSE);\n  g_return_val_if_fail (action_id != NULL, FALSE);\n\n  /* XXX: for now, prefer to store the process */\n  if (POLKIT_IS_SYSTEM_BUS_NAME (subject))\n    {\n      GError *error;\n      error = NULL;\n      subject_to_use = polkit_system_bus_name_get_process_sync (POLKIT_SYSTEM_BUS_NAME (subject),\n                                                                NULL,\n                                                                &error);\n      if (subject_to_use == NULL)\n        {\n          g_printerr (\"Error getting process for system bus name `%s': %s\\n\",\n                      polkit_system_bus_name_get_name (POLKIT_SYSTEM_BUS_NAME (subject)),\n                      error->message);\n          g_error_free (error);\n          subject_to_use = g_object_ref (subject);\n        }\n    }\n  else\n    {\n      subject_to_use = g_object_ref (subject);\n    }\n\n  ret = FALSE;\n\n  for (l = store->authorizations; l != NULL; l = l->next) {\n    TemporaryAuthorization *authorization = l->data;\n\n    if (strcmp (action_id, authorization->action_id) == 0 &&\n        subject_equal_for_authz (subject_to_use, authorization->subject))\n      {\n        ret = TRUE;\n        if (out_tmp_authz_id != NULL)\n          *out_tmp_authz_id = authorization->id;\n        goto out;\n      }\n  }\n\n out:\n  g_object_unref (subject_to_use);\n  return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,7 +40,7 @@\n     TemporaryAuthorization *authorization = l->data;\n \n     if (strcmp (action_id, authorization->action_id) == 0 &&\n-        polkit_subject_equal (subject_to_use, authorization->subject))\n+        subject_equal_for_authz (subject_to_use, authorization->subject))\n       {\n         ret = TRUE;\n         if (out_tmp_authz_id != NULL)",
        "diff_line_info": {
            "deleted_lines": [
                "        polkit_subject_equal (subject_to_use, authorization->subject))"
            ],
            "added_lines": [
                "        subject_equal_for_authz (subject_to_use, authorization->subject))"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-6133",
        "func_name": "torvalds/linux/copy_process",
        "description": "In PolicyKit (aka polkit) 0.115, the \"start time\" protection mechanism can be bypassed because fork() is not atomic, and therefore authorization decisions are improperly cached. This is related to lack of uid checking in polkitbackend/polkitbackendinteractiveauthority.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=7b55851367136b1efd84d98fea81ba57a98304cf",
        "commit_title": "This changes the fork(2) syscall to record the process start_time after",
        "commit_text": "initializing the basic task structure but still before making the new process visible to user-space.  Technically, we could record the start_time anytime during fork(2).  But this might lead to scenarios where a start_time is recorded long before a process becomes visible to user-space.  For instance, with userfaultfd(2) and TLS, user-space can delay the execution of fork(2) for an indefinite amount of time (and will, if this causes network access, or similar).  By recording the start_time late, it much closer reflects the point in time where the process becomes live and can be observed by other processes.  Lastly, this makes it much harder for user-space to predict and control the start_time they get assigned.  Previously, user-space could fork a process and stall it in copy_thread_tls() before its pid is allocated, but after its start_time is recorded.  This can be misused to later-on cycle through PIDs and resume the stalled fork(2) yielding a process that has the same pid and start_time as a process that existed before. This can be used to circumvent security systems that identify processes by their pid+start_time combination.  Even though user-space was always aware that start_time recording is flaky (but several projects are known to still rely on start_time-based identification), changing the start_time to be recorded late will help mitigate existing attacks and make it much harder for user-space to control the start_time a process gets assigned.  ",
        "func_before": "task_struct *copy_process(\n\t\t\t\t\tunsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tunsigned long tls,\n\t\t\t\t\tint node)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) !=\n\t\t\t\tcurrent->nsproxy->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->start_time = ktime_get_ns();\n\tp->real_start_time = ktime_get_boot_ns();\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\tcgroup_threadgroup_change_begin(current);\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p);\n\tif (retval)\n\t\tgoto bad_fork_free_pid;\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\tatomic_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tcgroup_threadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p);\nbad_fork_free_pid:\n\tcgroup_threadgroup_change_end(current);\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tfree_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "func": "task_struct *copy_process(\n\t\t\t\t\tunsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tunsigned long tls,\n\t\t\t\t\tint node)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) !=\n\t\t\t\tcurrent->nsproxy->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\tcgroup_threadgroup_change_begin(current);\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p);\n\tif (retval)\n\t\tgoto bad_fork_free_pid;\n\n\t/*\n\t * From this point on we must avoid any synchronous user-space\n\t * communication until we take the tasklist-lock. In particular, we do\n\t * not want user-space to be able to predict the process start-time by\n\t * stalling fork(2) after we recorded the start_time but before it is\n\t * visible to the system.\n\t */\n\n\tp->start_time = ktime_get_ns();\n\tp->real_start_time = ktime_get_boot_ns();\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\tatomic_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tcgroup_threadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p);\nbad_fork_free_pid:\n\tcgroup_threadgroup_change_end(current);\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tfree_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -161,8 +161,6 @@\n \n \tposix_cpu_timers_init(p);\n \n-\tp->start_time = ktime_get_ns();\n-\tp->real_start_time = ktime_get_boot_ns();\n \tp->io_context = NULL;\n \taudit_set_context(p, NULL);\n \tcgroup_fork(p);\n@@ -327,6 +325,17 @@\n \tretval = cgroup_can_fork(p);\n \tif (retval)\n \t\tgoto bad_fork_free_pid;\n+\n+\t/*\n+\t * From this point on we must avoid any synchronous user-space\n+\t * communication until we take the tasklist-lock. In particular, we do\n+\t * not want user-space to be able to predict the process start-time by\n+\t * stalling fork(2) after we recorded the start_time but before it is\n+\t * visible to the system.\n+\t */\n+\n+\tp->start_time = ktime_get_ns();\n+\tp->real_start_time = ktime_get_boot_ns();\n \n \t/*\n \t * Make it visible to the rest of the system, but dont wake it up yet.",
        "diff_line_info": {
            "deleted_lines": [
                "\tp->start_time = ktime_get_ns();",
                "\tp->real_start_time = ktime_get_boot_ns();"
            ],
            "added_lines": [
                "",
                "\t/*",
                "\t * From this point on we must avoid any synchronous user-space",
                "\t * communication until we take the tasklist-lock. In particular, we do",
                "\t * not want user-space to be able to predict the process start-time by",
                "\t * stalling fork(2) after we recorded the start_time but before it is",
                "\t * visible to the system.",
                "\t */",
                "",
                "\tp->start_time = ktime_get_ns();",
                "\tp->real_start_time = ktime_get_boot_ns();"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-10741",
        "func_name": "torvalds/linux/__xfs_get_blocks",
        "description": "In the Linux kernel before 4.9.3, fs/xfs/xfs_aops.c allows local users to cause a denial of service (system crash) because there is a race condition between direct and memory-mapped I/O (associated with a hole) that is handled with BUG_ON instead of an I/O failure.",
        "git_url": "https://github.com/torvalds/linux/commit/04197b341f23b908193308b8d63d17ff23232598",
        "commit_title": "xfs: don't BUG() on mixed direct and mapped I/O",
        "commit_text": " We've had reports of generic/095 causing XFS to BUG() in __xfs_get_blocks() due to the existence of delalloc blocks on a direct I/O read. generic/095 issues a mix of various types of I/O, including direct and memory mapped I/O to a single file. This is clearly not supported behavior and is known to lead to such problems. E.g., the lack of exclusion between the direct I/O and write fault paths means that a write fault can allocate delalloc blocks in a region of a file that was previously a hole after the direct read has attempted to flush/inval the file range, but before it actually reads the block mapping. In turn, the direct read discovers a delalloc extent and cannot proceed.  While the appropriate solution here is to not mix direct and memory mapped I/O to the same regions of the same file, the current BUG_ON() behavior is probably overkill as it can crash the entire system.  Instead, localize the failure to the I/O in question by returning an error for a direct I/O that cannot be handled safely due to delalloc blocks. Be careful to allow the case of a direct write to post-eof delalloc blocks. This can occur due to speculative preallocation and is safe as post-eof blocks are not accompanied by dirty pages in pagecache (conversely, preallocation within eof must have been zeroed, and thus dirtied, before the inode size could have been increased beyond said blocks).  Finally, provide an additional warning if a direct I/O write occurs while the file is memory mapped. This may not catch all problematic scenarios, but provides a hint that some known-to-be-problematic I/O methods are in use. ",
        "func_before": "STATIC int\n__xfs_get_blocks(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate,\n\tbool\t\t\tdirect,\n\tbool\t\t\tdax_fault)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\txfs_fileoff_t\t\toffset_fsb, end_fsb;\n\tint\t\t\terror = 0;\n\tint\t\t\tlockmode = 0;\n\tstruct xfs_bmbt_irec\timap;\n\tint\t\t\tnimaps = 1;\n\txfs_off_t\t\toffset;\n\tssize_t\t\t\tsize;\n\tint\t\t\tnew = 0;\n\tbool\t\t\tis_cow = false;\n\tbool\t\t\tneed_alloc = false;\n\n\tBUG_ON(create && !direct);\n\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn -EIO;\n\n\toffset = (xfs_off_t)iblock << inode->i_blkbits;\n\tASSERT(bh_result->b_size >= (1 << inode->i_blkbits));\n\tsize = bh_result->b_size;\n\n\tif (!create && offset >= i_size_read(inode))\n\t\treturn 0;\n\n\t/*\n\t * Direct I/O is usually done on preallocated files, so try getting\n\t * a block mapping without an exclusive lock first.\n\t */\n\tlockmode = xfs_ilock_data_map_shared(ip);\n\n\tASSERT(offset <= mp->m_super->s_maxbytes);\n\tif (offset + size > mp->m_super->s_maxbytes)\n\t\tsize = mp->m_super->s_maxbytes - offset;\n\tend_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);\n\toffset_fsb = XFS_B_TO_FSBT(mp, offset);\n\n\tif (create && direct && xfs_is_reflink_inode(ip))\n\t\tis_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,\n\t\t\t\t\t&need_alloc);\n\tif (!is_cow) {\n\t\terror = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,\n\t\t\t\t\t&imap, &nimaps, XFS_BMAPI_ENTIRE);\n\t\t/*\n\t\t * Truncate an overwrite extent if there's a pending CoW\n\t\t * reservation before the end of this extent.  This\n\t\t * forces us to come back to get_blocks to take care of\n\t\t * the CoW.\n\t\t */\n\t\tif (create && direct && nimaps &&\n\t\t    imap.br_startblock != HOLESTARTBLOCK &&\n\t\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t\t    !ISUNWRITTEN(&imap))\n\t\t\txfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t}\n\tASSERT(!need_alloc);\n\tif (error)\n\t\tgoto out_unlock;\n\n\t/* for DAX, we convert unwritten extents directly */\n\tif (create &&\n\t    (!nimaps ||\n\t     (imap.br_startblock == HOLESTARTBLOCK ||\n\t      imap.br_startblock == DELAYSTARTBLOCK) ||\n\t     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {\n\t\t/*\n\t\t * xfs_iomap_write_direct() expects the shared lock. It\n\t\t * is unlocked on return.\n\t\t */\n\t\tif (lockmode == XFS_ILOCK_EXCL)\n\t\t\txfs_ilock_demote(ip, lockmode);\n\n\t\terror = xfs_iomap_write_direct(ip, offset, size,\n\t\t\t\t\t       &imap, nimaps);\n\t\tif (error)\n\t\t\treturn error;\n\t\tnew = 1;\n\n\t\ttrace_xfs_get_blocks_alloc(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_DELALLOC, &imap);\n\t} else if (nimaps) {\n\t\ttrace_xfs_get_blocks_found(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_OVERWRITE, &imap);\n\t\txfs_iunlock(ip, lockmode);\n\t} else {\n\t\ttrace_xfs_get_blocks_notfound(ip, offset, size);\n\t\tgoto out_unlock;\n\t}\n\n\tif (IS_DAX(inode) && create) {\n\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t/* zeroing is not needed at a higher layer */\n\t\tnew = 0;\n\t}\n\n\t/* trim mapping down to size requested */\n\txfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);\n\n\t/*\n\t * For unwritten extents do not report a disk address in the buffered\n\t * read case (treat as if we're reading into a hole).\n\t */\n\tif (imap.br_startblock != HOLESTARTBLOCK &&\n\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t    (create || !ISUNWRITTEN(&imap))) {\n\t\tif (create && direct && !is_cow) {\n\t\t\terror = xfs_bounce_unaligned_dio_write(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\n\t\txfs_map_buffer(inode, bh_result, &imap, offset);\n\t\tif (ISUNWRITTEN(&imap))\n\t\t\tset_buffer_unwritten(bh_result);\n\t\t/* direct IO needs special help */\n\t\tif (create) {\n\t\t\tif (dax_fault)\n\t\t\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t\telse\n\t\t\t\txfs_map_direct(inode, bh_result, &imap, offset,\n\t\t\t\t\t\tis_cow);\n\t\t}\n\t}\n\n\t/*\n\t * If this is a realtime file, data may be on a different device.\n\t * to that pointed to from the buffer_head b_bdev currently.\n\t */\n\tbh_result->b_bdev = xfs_find_bdev_for_inode(inode);\n\n\t/*\n\t * If we previously allocated a block out beyond eof and we are now\n\t * coming back to use it then we will need to flag it as new even if it\n\t * has a disk address.\n\t *\n\t * With sub-block writes into unwritten extents we also need to mark\n\t * the buffer as new so that the unwritten parts of the buffer gets\n\t * correctly zeroed.\n\t */\n\tif (create &&\n\t    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||\n\t     (offset >= i_size_read(inode)) ||\n\t     (new || ISUNWRITTEN(&imap))))\n\t\tset_buffer_new(bh_result);\n\n\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);\n\n\treturn 0;\n\nout_unlock:\n\txfs_iunlock(ip, lockmode);\n\treturn error;\n}",
        "func": "STATIC int\n__xfs_get_blocks(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate,\n\tbool\t\t\tdirect,\n\tbool\t\t\tdax_fault)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\txfs_fileoff_t\t\toffset_fsb, end_fsb;\n\tint\t\t\terror = 0;\n\tint\t\t\tlockmode = 0;\n\tstruct xfs_bmbt_irec\timap;\n\tint\t\t\tnimaps = 1;\n\txfs_off_t\t\toffset;\n\tssize_t\t\t\tsize;\n\tint\t\t\tnew = 0;\n\tbool\t\t\tis_cow = false;\n\tbool\t\t\tneed_alloc = false;\n\n\tBUG_ON(create && !direct);\n\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn -EIO;\n\n\toffset = (xfs_off_t)iblock << inode->i_blkbits;\n\tASSERT(bh_result->b_size >= (1 << inode->i_blkbits));\n\tsize = bh_result->b_size;\n\n\tif (!create && offset >= i_size_read(inode))\n\t\treturn 0;\n\n\t/*\n\t * Direct I/O is usually done on preallocated files, so try getting\n\t * a block mapping without an exclusive lock first.\n\t */\n\tlockmode = xfs_ilock_data_map_shared(ip);\n\n\tASSERT(offset <= mp->m_super->s_maxbytes);\n\tif (offset + size > mp->m_super->s_maxbytes)\n\t\tsize = mp->m_super->s_maxbytes - offset;\n\tend_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);\n\toffset_fsb = XFS_B_TO_FSBT(mp, offset);\n\n\tif (create && direct && xfs_is_reflink_inode(ip))\n\t\tis_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,\n\t\t\t\t\t&need_alloc);\n\tif (!is_cow) {\n\t\terror = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,\n\t\t\t\t\t&imap, &nimaps, XFS_BMAPI_ENTIRE);\n\t\t/*\n\t\t * Truncate an overwrite extent if there's a pending CoW\n\t\t * reservation before the end of this extent.  This\n\t\t * forces us to come back to get_blocks to take care of\n\t\t * the CoW.\n\t\t */\n\t\tif (create && direct && nimaps &&\n\t\t    imap.br_startblock != HOLESTARTBLOCK &&\n\t\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t\t    !ISUNWRITTEN(&imap))\n\t\t\txfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t}\n\tASSERT(!need_alloc);\n\tif (error)\n\t\tgoto out_unlock;\n\n\t/*\n\t * The only time we can ever safely find delalloc blocks on direct I/O\n\t * is a dio write to post-eof speculative preallocation. All other\n\t * scenarios are indicative of a problem or misuse (such as mixing\n\t * direct and mapped I/O).\n\t *\n\t * The file may be unmapped by the time we get here so we cannot\n\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this\n\t * is a read or a write within eof. Otherwise, carry on but warn as a\n\t * precuation if the file happens to be mapped.\n\t */\n\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {\n\t\tif (!create || offset < i_size_read(VFS_I(ip))) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\terror = -EIO;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));\n\t}\n\n\t/* for DAX, we convert unwritten extents directly */\n\tif (create &&\n\t    (!nimaps ||\n\t     (imap.br_startblock == HOLESTARTBLOCK ||\n\t      imap.br_startblock == DELAYSTARTBLOCK) ||\n\t     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {\n\t\t/*\n\t\t * xfs_iomap_write_direct() expects the shared lock. It\n\t\t * is unlocked on return.\n\t\t */\n\t\tif (lockmode == XFS_ILOCK_EXCL)\n\t\t\txfs_ilock_demote(ip, lockmode);\n\n\t\terror = xfs_iomap_write_direct(ip, offset, size,\n\t\t\t\t\t       &imap, nimaps);\n\t\tif (error)\n\t\t\treturn error;\n\t\tnew = 1;\n\n\t\ttrace_xfs_get_blocks_alloc(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_DELALLOC, &imap);\n\t} else if (nimaps) {\n\t\ttrace_xfs_get_blocks_found(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_OVERWRITE, &imap);\n\t\txfs_iunlock(ip, lockmode);\n\t} else {\n\t\ttrace_xfs_get_blocks_notfound(ip, offset, size);\n\t\tgoto out_unlock;\n\t}\n\n\tif (IS_DAX(inode) && create) {\n\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t/* zeroing is not needed at a higher layer */\n\t\tnew = 0;\n\t}\n\n\t/* trim mapping down to size requested */\n\txfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);\n\n\t/*\n\t * For unwritten extents do not report a disk address in the buffered\n\t * read case (treat as if we're reading into a hole).\n\t */\n\tif (imap.br_startblock != HOLESTARTBLOCK &&\n\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t    (create || !ISUNWRITTEN(&imap))) {\n\t\tif (create && direct && !is_cow) {\n\t\t\terror = xfs_bounce_unaligned_dio_write(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\n\t\txfs_map_buffer(inode, bh_result, &imap, offset);\n\t\tif (ISUNWRITTEN(&imap))\n\t\t\tset_buffer_unwritten(bh_result);\n\t\t/* direct IO needs special help */\n\t\tif (create) {\n\t\t\tif (dax_fault)\n\t\t\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t\telse\n\t\t\t\txfs_map_direct(inode, bh_result, &imap, offset,\n\t\t\t\t\t\tis_cow);\n\t\t}\n\t}\n\n\t/*\n\t * If this is a realtime file, data may be on a different device.\n\t * to that pointed to from the buffer_head b_bdev currently.\n\t */\n\tbh_result->b_bdev = xfs_find_bdev_for_inode(inode);\n\n\t/*\n\t * If we previously allocated a block out beyond eof and we are now\n\t * coming back to use it then we will need to flag it as new even if it\n\t * has a disk address.\n\t *\n\t * With sub-block writes into unwritten extents we also need to mark\n\t * the buffer as new so that the unwritten parts of the buffer gets\n\t * correctly zeroed.\n\t */\n\tif (create &&\n\t    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||\n\t     (offset >= i_size_read(inode)) ||\n\t     (new || ISUNWRITTEN(&imap))))\n\t\tset_buffer_new(bh_result);\n\n\treturn 0;\n\nout_unlock:\n\txfs_iunlock(ip, lockmode);\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -66,6 +66,26 @@\n \tASSERT(!need_alloc);\n \tif (error)\n \t\tgoto out_unlock;\n+\n+\t/*\n+\t * The only time we can ever safely find delalloc blocks on direct I/O\n+\t * is a dio write to post-eof speculative preallocation. All other\n+\t * scenarios are indicative of a problem or misuse (such as mixing\n+\t * direct and mapped I/O).\n+\t *\n+\t * The file may be unmapped by the time we get here so we cannot\n+\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this\n+\t * is a read or a write within eof. Otherwise, carry on but warn as a\n+\t * precuation if the file happens to be mapped.\n+\t */\n+\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {\n+\t\tif (!create || offset < i_size_read(VFS_I(ip))) {\n+\t\t\tWARN_ON_ONCE(1);\n+\t\t\terror = -EIO;\n+\t\t\tgoto out_unlock;\n+\t\t}\n+\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));\n+\t}\n \n \t/* for DAX, we convert unwritten extents directly */\n \tif (create &&\n@@ -156,8 +176,6 @@\n \t     (new || ISUNWRITTEN(&imap))))\n \t\tset_buffer_new(bh_result);\n \n-\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);\n-\n \treturn 0;\n \n out_unlock:",
        "diff_line_info": {
            "deleted_lines": [
                "\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);",
                ""
            ],
            "added_lines": [
                "",
                "\t/*",
                "\t * The only time we can ever safely find delalloc blocks on direct I/O",
                "\t * is a dio write to post-eof speculative preallocation. All other",
                "\t * scenarios are indicative of a problem or misuse (such as mixing",
                "\t * direct and mapped I/O).",
                "\t *",
                "\t * The file may be unmapped by the time we get here so we cannot",
                "\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this",
                "\t * is a read or a write within eof. Otherwise, carry on but warn as a",
                "\t * precuation if the file happens to be mapped.",
                "\t */",
                "\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {",
                "\t\tif (!create || offset < i_size_read(VFS_I(ip))) {",
                "\t\t\tWARN_ON_ONCE(1);",
                "\t\t\terror = -EIO;",
                "\t\t\tgoto out_unlock;",
                "\t\t}",
                "\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-6974",
        "func_name": "torvalds/linux/kvm_ioctl_create_device",
        "description": "In the Linux kernel before 4.20.8, kvm_ioctl_create_device in virt/kvm/kvm_main.c mishandles reference counting because of a race condition, leading to a use-after-free.",
        "git_url": "https://github.com/torvalds/linux/commit/cfa39381173d5f969daf43582c95ad679189cbc9",
        "commit_title": "kvm: fix kvm_ioctl_create_device() reference counting (CVE-2019-6974)",
        "commit_text": " kvm_ioctl_create_device() does the following:  1. creates a device that holds a reference to the VM object (with a borrowed    reference, the VM's refcount has not been bumped yet) 2. initializes the device 3. transfers the reference to the device to the caller's file descriptor table 4. calls kvm_get_kvm() to turn the borrowed reference to the VM into a real    reference  The ownership transfer in step 3 must not happen before the reference to the VM becomes a proper, non-borrowed reference, which only happens in step 4. After step 3, an attacker can close the file descriptor and drop the borrowed reference, which can cause the refcount of the kvm object to drop to zero.  This means that we need to grab a reference for the device before anon_inode_getfd(), otherwise the VM can disappear from under us.  Cc: stable@kernel.org",
        "func_before": "static int kvm_ioctl_create_device(struct kvm *kvm,\n\t\t\t\t   struct kvm_create_device *cd)\n{\n\tstruct kvm_device_ops *ops = NULL;\n\tstruct kvm_device *dev;\n\tbool test = cd->flags & KVM_CREATE_DEVICE_TEST;\n\tint ret;\n\n\tif (cd->type >= ARRAY_SIZE(kvm_device_ops_table))\n\t\treturn -ENODEV;\n\n\tops = kvm_device_ops_table[cd->type];\n\tif (ops == NULL)\n\t\treturn -ENODEV;\n\n\tif (test)\n\t\treturn 0;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->ops = ops;\n\tdev->kvm = kvm;\n\n\tmutex_lock(&kvm->lock);\n\tret = ops->create(dev, cd->type);\n\tif (ret < 0) {\n\t\tmutex_unlock(&kvm->lock);\n\t\tkfree(dev);\n\t\treturn ret;\n\t}\n\tlist_add(&dev->vm_node, &kvm->devices);\n\tmutex_unlock(&kvm->lock);\n\n\tif (ops->init)\n\t\tops->init(dev);\n\n\tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n\tif (ret < 0) {\n\t\tmutex_lock(&kvm->lock);\n\t\tlist_del(&dev->vm_node);\n\t\tmutex_unlock(&kvm->lock);\n\t\tops->destroy(dev);\n\t\treturn ret;\n\t}\n\n\tkvm_get_kvm(kvm);\n\tcd->fd = ret;\n\treturn 0;\n}",
        "func": "static int kvm_ioctl_create_device(struct kvm *kvm,\n\t\t\t\t   struct kvm_create_device *cd)\n{\n\tstruct kvm_device_ops *ops = NULL;\n\tstruct kvm_device *dev;\n\tbool test = cd->flags & KVM_CREATE_DEVICE_TEST;\n\tint ret;\n\n\tif (cd->type >= ARRAY_SIZE(kvm_device_ops_table))\n\t\treturn -ENODEV;\n\n\tops = kvm_device_ops_table[cd->type];\n\tif (ops == NULL)\n\t\treturn -ENODEV;\n\n\tif (test)\n\t\treturn 0;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->ops = ops;\n\tdev->kvm = kvm;\n\n\tmutex_lock(&kvm->lock);\n\tret = ops->create(dev, cd->type);\n\tif (ret < 0) {\n\t\tmutex_unlock(&kvm->lock);\n\t\tkfree(dev);\n\t\treturn ret;\n\t}\n\tlist_add(&dev->vm_node, &kvm->devices);\n\tmutex_unlock(&kvm->lock);\n\n\tif (ops->init)\n\t\tops->init(dev);\n\n\tkvm_get_kvm(kvm);\n\tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n\tif (ret < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\tmutex_lock(&kvm->lock);\n\t\tlist_del(&dev->vm_node);\n\t\tmutex_unlock(&kvm->lock);\n\t\tops->destroy(dev);\n\t\treturn ret;\n\t}\n\n\tcd->fd = ret;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,8 +36,10 @@\n \tif (ops->init)\n \t\tops->init(dev);\n \n+\tkvm_get_kvm(kvm);\n \tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n \tif (ret < 0) {\n+\t\tkvm_put_kvm(kvm);\n \t\tmutex_lock(&kvm->lock);\n \t\tlist_del(&dev->vm_node);\n \t\tmutex_unlock(&kvm->lock);\n@@ -45,7 +47,6 @@\n \t\treturn ret;\n \t}\n \n-\tkvm_get_kvm(kvm);\n \tcd->fd = ret;\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tkvm_get_kvm(kvm);"
            ],
            "added_lines": [
                "\tkvm_get_kvm(kvm);",
                "\t\tkvm_put_kvm(kvm);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-1234",
        "func_name": "chromium/GLES2DecoderImpl::HandleGetUniformfv",
        "description": "Race condition in gpu/command_buffer/service/gles2_cmd_decoder.cc in Google Chrome before 41.0.2272.118 allows remote attackers to cause a denial of service (buffer overflow) or possibly have unspecified other impact by manipulating OpenGL ES commands.",
        "git_url": "https://chromium.googlesource.com/chromium/src/+/181c7400b2bf50ba02ac77149749fb419b4d4797",
        "commit_title": "gpu: Use GetUniformSetup computed result size.",
        "commit_text": "   ",
        "func_before": "error::Error GLES2DecoderImpl::HandleGetUniformfv(uint32 immediate_data_size,\n                                                  const void* cmd_data) {\n  const gles2::cmds::GetUniformfv& c =\n      *static_cast<const gles2::cmds::GetUniformfv*>(cmd_data);\n  GLuint program = c.program;\n  GLint fake_location = c.location;\n  GLuint service_id;\n  GLint real_location = -1;\n  Error error;\n  typedef cmds::GetUniformfv::Result Result;\n  Result* result;\n  GLenum result_type;\n  if (GetUniformSetup(\n      program, fake_location, c.params_shm_id, c.params_shm_offset,\n      &error, &real_location, &service_id,\n      reinterpret_cast<void**>(&result), &result_type)) {\n    if (result_type == GL_BOOL || result_type == GL_BOOL_VEC2 ||\n        result_type == GL_BOOL_VEC3 || result_type == GL_BOOL_VEC4) {\n      GLsizei num_values = result->GetNumResults();\n      scoped_ptr<GLint[]> temp(new GLint[num_values]);\n      glGetUniformiv(service_id, real_location, temp.get());\n      GLfloat* dst = result->GetData();\n      for (GLsizei ii = 0; ii < num_values; ++ii) {\n        dst[ii] = (temp[ii] != 0);\n      }\n    } else {\n      glGetUniformfv(service_id, real_location, result->GetData());\n    }\n  }\n  return error;\n}",
        "func": "error::Error GLES2DecoderImpl::HandleGetUniformfv(uint32 immediate_data_size,\n                                                  const void* cmd_data) {\n  const gles2::cmds::GetUniformfv& c =\n      *static_cast<const gles2::cmds::GetUniformfv*>(cmd_data);\n  GLuint program = c.program;\n  GLint fake_location = c.location;\n  GLuint service_id;\n  GLint real_location = -1;\n  Error error;\n  typedef cmds::GetUniformfv::Result Result;\n  Result* result;\n  GLenum result_type;\n  GLsizei result_size;\n  if (GetUniformSetup(program, fake_location, c.params_shm_id,\n                      c.params_shm_offset, &error, &real_location, &service_id,\n                      reinterpret_cast<void**>(&result), &result_type,\n                      &result_size)) {\n    if (result_type == GL_BOOL || result_type == GL_BOOL_VEC2 ||\n        result_type == GL_BOOL_VEC3 || result_type == GL_BOOL_VEC4) {\n      GLsizei num_values = result_size / sizeof(Result::Type);\n      scoped_ptr<GLint[]> temp(new GLint[num_values]);\n      glGetUniformiv(service_id, real_location, temp.get());\n      GLfloat* dst = result->GetData();\n      for (GLsizei ii = 0; ii < num_values; ++ii) {\n        dst[ii] = (temp[ii] != 0);\n      }\n    } else {\n      glGetUniformfv(service_id, real_location, result->GetData());\n    }\n  }\n  return error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,13 +10,14 @@\n   typedef cmds::GetUniformfv::Result Result;\n   Result* result;\n   GLenum result_type;\n-  if (GetUniformSetup(\n-      program, fake_location, c.params_shm_id, c.params_shm_offset,\n-      &error, &real_location, &service_id,\n-      reinterpret_cast<void**>(&result), &result_type)) {\n+  GLsizei result_size;\n+  if (GetUniformSetup(program, fake_location, c.params_shm_id,\n+                      c.params_shm_offset, &error, &real_location, &service_id,\n+                      reinterpret_cast<void**>(&result), &result_type,\n+                      &result_size)) {\n     if (result_type == GL_BOOL || result_type == GL_BOOL_VEC2 ||\n         result_type == GL_BOOL_VEC3 || result_type == GL_BOOL_VEC4) {\n-      GLsizei num_values = result->GetNumResults();\n+      GLsizei num_values = result_size / sizeof(Result::Type);\n       scoped_ptr<GLint[]> temp(new GLint[num_values]);\n       glGetUniformiv(service_id, real_location, temp.get());\n       GLfloat* dst = result->GetData();",
        "diff_line_info": {
            "deleted_lines": [
                "  if (GetUniformSetup(",
                "      program, fake_location, c.params_shm_id, c.params_shm_offset,",
                "      &error, &real_location, &service_id,",
                "      reinterpret_cast<void**>(&result), &result_type)) {",
                "      GLsizei num_values = result->GetNumResults();"
            ],
            "added_lines": [
                "  GLsizei result_size;",
                "  if (GetUniformSetup(program, fake_location, c.params_shm_id,",
                "                      c.params_shm_offset, &error, &real_location, &service_id,",
                "                      reinterpret_cast<void**>(&result), &result_type,",
                "                      &result_size)) {",
                "      GLsizei num_values = result_size / sizeof(Result::Type);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-1234",
        "func_name": "chromium/GLES2DecoderImpl::HandleGetUniformiv",
        "description": "Race condition in gpu/command_buffer/service/gles2_cmd_decoder.cc in Google Chrome before 41.0.2272.118 allows remote attackers to cause a denial of service (buffer overflow) or possibly have unspecified other impact by manipulating OpenGL ES commands.",
        "git_url": "https://chromium.googlesource.com/chromium/src/+/181c7400b2bf50ba02ac77149749fb419b4d4797",
        "commit_title": "gpu: Use GetUniformSetup computed result size.",
        "commit_text": "   ",
        "func_before": "error::Error GLES2DecoderImpl::HandleGetUniformiv(uint32 immediate_data_size,\n                                                  const void* cmd_data) {\n  const gles2::cmds::GetUniformiv& c =\n      *static_cast<const gles2::cmds::GetUniformiv*>(cmd_data);\n  GLuint program = c.program;\n  GLint fake_location = c.location;\n  GLuint service_id;\n  GLenum result_type;\n  GLint real_location = -1;\n  Error error;\n  void* result;\n  if (GetUniformSetup(\n      program, fake_location, c.params_shm_id, c.params_shm_offset,\n      &error, &real_location, &service_id, &result, &result_type)) {\n    glGetUniformiv(\n        service_id, real_location,\n        static_cast<cmds::GetUniformiv::Result*>(result)->GetData());\n  }\n  return error;\n}",
        "func": "error::Error GLES2DecoderImpl::HandleGetUniformiv(uint32 immediate_data_size,\n                                                  const void* cmd_data) {\n  const gles2::cmds::GetUniformiv& c =\n      *static_cast<const gles2::cmds::GetUniformiv*>(cmd_data);\n  GLuint program = c.program;\n  GLint fake_location = c.location;\n  GLuint service_id;\n  GLenum result_type;\n  GLsizei result_size;\n  GLint real_location = -1;\n  Error error;\n  void* result;\n  if (GetUniformSetup(program, fake_location, c.params_shm_id,\n                      c.params_shm_offset, &error, &real_location, &service_id,\n                      &result, &result_type, &result_size)) {\n    glGetUniformiv(\n        service_id, real_location,\n        static_cast<cmds::GetUniformiv::Result*>(result)->GetData());\n  }\n  return error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,12 +6,13 @@\n   GLint fake_location = c.location;\n   GLuint service_id;\n   GLenum result_type;\n+  GLsizei result_size;\n   GLint real_location = -1;\n   Error error;\n   void* result;\n-  if (GetUniformSetup(\n-      program, fake_location, c.params_shm_id, c.params_shm_offset,\n-      &error, &real_location, &service_id, &result, &result_type)) {\n+  if (GetUniformSetup(program, fake_location, c.params_shm_id,\n+                      c.params_shm_offset, &error, &real_location, &service_id,\n+                      &result, &result_type, &result_size)) {\n     glGetUniformiv(\n         service_id, real_location,\n         static_cast<cmds::GetUniformiv::Result*>(result)->GetData());",
        "diff_line_info": {
            "deleted_lines": [
                "  if (GetUniformSetup(",
                "      program, fake_location, c.params_shm_id, c.params_shm_offset,",
                "      &error, &real_location, &service_id, &result, &result_type)) {"
            ],
            "added_lines": [
                "  GLsizei result_size;",
                "  if (GetUniformSetup(program, fake_location, c.params_shm_id,",
                "                      c.params_shm_offset, &error, &real_location, &service_id,",
                "                      &result, &result_type, &result_size)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-1234",
        "func_name": "chromium/GLES2DecoderImpl::GetUniformSetup",
        "description": "Race condition in gpu/command_buffer/service/gles2_cmd_decoder.cc in Google Chrome before 41.0.2272.118 allows remote attackers to cause a denial of service (buffer overflow) or possibly have unspecified other impact by manipulating OpenGL ES commands.",
        "git_url": "https://chromium.googlesource.com/chromium/src/+/181c7400b2bf50ba02ac77149749fb419b4d4797",
        "commit_title": "gpu: Use GetUniformSetup computed result size.",
        "commit_text": "   ",
        "func_before": "bool GLES2DecoderImpl::GetUniformSetup(\n    GLuint program_id, GLint fake_location,\n    uint32 shm_id, uint32 shm_offset,\n    error::Error* error, GLint* real_location,\n    GLuint* service_id, void** result_pointer, GLenum* result_type) {\n  DCHECK(error);\n  DCHECK(service_id);\n  DCHECK(result_pointer);\n  DCHECK(result_type);\n  DCHECK(real_location);\n  *error = error::kNoError;\n  // Make sure we have enough room for the result on failure.\n  SizedResult<GLint>* result;\n  result = GetSharedMemoryAs<SizedResult<GLint>*>(\n      shm_id, shm_offset, SizedResult<GLint>::ComputeSize(0));\n  if (!result) {\n    *error = error::kOutOfBounds;\n    return false;\n  }\n  *result_pointer = result;\n  // Set the result size to 0 so the client does not have to check for success.\n  result->SetNumResults(0);\n  Program* program = GetProgramInfoNotShader(program_id, \"glGetUniform\");\n  if (!program) {\n    return false;\n  }\n  if (!program->IsValid()) {\n    // Program was not linked successfully. (ie, glLinkProgram)\n    LOCAL_SET_GL_ERROR(\n        GL_INVALID_OPERATION, \"glGetUniform\", \"program not linked\");\n    return false;\n  }\n  *service_id = program->service_id();\n  GLint array_index = -1;\n  const Program::UniformInfo* uniform_info =\n      program->GetUniformInfoByFakeLocation(\n          fake_location, real_location, &array_index);\n  if (!uniform_info) {\n    // No such location.\n    LOCAL_SET_GL_ERROR(\n        GL_INVALID_OPERATION, \"glGetUniform\", \"unknown location\");\n    return false;\n  }\n  GLenum type = uniform_info->type;\n  GLsizei size = GLES2Util::GetGLDataTypeSizeForUniforms(type);\n  if (size == 0) {\n    LOCAL_SET_GL_ERROR(GL_INVALID_OPERATION, \"glGetUniform\", \"unknown type\");\n    return false;\n  }\n  result = GetSharedMemoryAs<SizedResult<GLint>*>(\n      shm_id, shm_offset, SizedResult<GLint>::ComputeSizeFromBytes(size));\n  if (!result) {\n    *error = error::kOutOfBounds;\n    return false;\n  }\n  result->size = size;\n  *result_type = type;\n  return true;\n}",
        "func": "bool GLES2DecoderImpl::GetUniformSetup(GLuint program_id,\n                                       GLint fake_location,\n                                       uint32 shm_id,\n                                       uint32 shm_offset,\n                                       error::Error* error,\n                                       GLint* real_location,\n                                       GLuint* service_id,\n                                       void** result_pointer,\n                                       GLenum* result_type,\n                                       GLsizei* result_size) {\n  DCHECK(error);\n  DCHECK(service_id);\n  DCHECK(result_pointer);\n  DCHECK(result_type);\n  DCHECK(real_location);\n  *error = error::kNoError;\n  // Make sure we have enough room for the result on failure.\n  SizedResult<GLint>* result;\n  result = GetSharedMemoryAs<SizedResult<GLint>*>(\n      shm_id, shm_offset, SizedResult<GLint>::ComputeSize(0));\n  if (!result) {\n    *error = error::kOutOfBounds;\n    return false;\n  }\n  *result_pointer = result;\n  // Set the result size to 0 so the client does not have to check for success.\n  result->SetNumResults(0);\n  Program* program = GetProgramInfoNotShader(program_id, \"glGetUniform\");\n  if (!program) {\n    return false;\n  }\n  if (!program->IsValid()) {\n    // Program was not linked successfully. (ie, glLinkProgram)\n    LOCAL_SET_GL_ERROR(\n        GL_INVALID_OPERATION, \"glGetUniform\", \"program not linked\");\n    return false;\n  }\n  *service_id = program->service_id();\n  GLint array_index = -1;\n  const Program::UniformInfo* uniform_info =\n      program->GetUniformInfoByFakeLocation(\n          fake_location, real_location, &array_index);\n  if (!uniform_info) {\n    // No such location.\n    LOCAL_SET_GL_ERROR(\n        GL_INVALID_OPERATION, \"glGetUniform\", \"unknown location\");\n    return false;\n  }\n  GLenum type = uniform_info->type;\n  GLsizei size = GLES2Util::GetGLDataTypeSizeForUniforms(type);\n  if (size == 0) {\n    LOCAL_SET_GL_ERROR(GL_INVALID_OPERATION, \"glGetUniform\", \"unknown type\");\n    return false;\n  }\n  result = GetSharedMemoryAs<SizedResult<GLint>*>(\n      shm_id, shm_offset, SizedResult<GLint>::ComputeSizeFromBytes(size));\n  if (!result) {\n    *error = error::kOutOfBounds;\n    return false;\n  }\n  result->size = size;\n  *result_size = size;\n  *result_type = type;\n  return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,13 @@\n-bool GLES2DecoderImpl::GetUniformSetup(\n-    GLuint program_id, GLint fake_location,\n-    uint32 shm_id, uint32 shm_offset,\n-    error::Error* error, GLint* real_location,\n-    GLuint* service_id, void** result_pointer, GLenum* result_type) {\n+bool GLES2DecoderImpl::GetUniformSetup(GLuint program_id,\n+                                       GLint fake_location,\n+                                       uint32 shm_id,\n+                                       uint32 shm_offset,\n+                                       error::Error* error,\n+                                       GLint* real_location,\n+                                       GLuint* service_id,\n+                                       void** result_pointer,\n+                                       GLenum* result_type,\n+                                       GLsizei* result_size) {\n   DCHECK(error);\n   DCHECK(service_id);\n   DCHECK(result_pointer);\n@@ -54,6 +59,7 @@\n     return false;\n   }\n   result->size = size;\n+  *result_size = size;\n   *result_type = type;\n   return true;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "bool GLES2DecoderImpl::GetUniformSetup(",
                "    GLuint program_id, GLint fake_location,",
                "    uint32 shm_id, uint32 shm_offset,",
                "    error::Error* error, GLint* real_location,",
                "    GLuint* service_id, void** result_pointer, GLenum* result_type) {"
            ],
            "added_lines": [
                "bool GLES2DecoderImpl::GetUniformSetup(GLuint program_id,",
                "                                       GLint fake_location,",
                "                                       uint32 shm_id,",
                "                                       uint32 shm_offset,",
                "                                       error::Error* error,",
                "                                       GLint* real_location,",
                "                                       GLuint* service_id,",
                "                                       void** result_pointer,",
                "                                       GLenum* result_type,",
                "                                       GLsizei* result_size) {",
                "  *result_size = size;"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-3339",
        "func_name": "torvalds/linux/prepare_binprm",
        "description": "Race condition in the prepare_binprm function in fs/exec.c in the Linux kernel before 3.19.6 allows local users to gain privileges by executing a setuid program at a time instant when a chown to root is in progress, and the ownership is changed but the setuid bit is not yet stripped.",
        "git_url": "https://github.com/torvalds/linux/commit/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543",
        "commit_title": "fs: take i_mutex during prepare_binprm for set[ug]id executables",
        "commit_text": " This prevents a race between chown() and execve(), where chowning a setuid-user binary to root would momentarily make the binary setuid root.  This patch was mostly written by Linus Torvalds. ",
        "func_before": "int prepare_binprm(struct linux_binprm *bprm)\n{\n\tstruct inode *inode = file_inode(bprm->file);\n\tumode_t mode = inode->i_mode;\n\tint retval;\n\n\n\t/* clear any previous set[ug]id data from a previous binary */\n\tbprm->cred->euid = current_euid();\n\tbprm->cred->egid = current_egid();\n\n\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&\n\t    !task_no_new_privs(current) &&\n\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&\n\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {\n\t\t/* Set-uid? */\n\t\tif (mode & S_ISUID) {\n\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\t\tbprm->cred->euid = inode->i_uid;\n\t\t}\n\n\t\t/* Set-gid? */\n\t\t/*\n\t\t * If setgid is set but no group execute bit then this\n\t\t * is a candidate for mandatory locking, not a setgid\n\t\t * executable.\n\t\t */\n\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\t\tbprm->cred->egid = inode->i_gid;\n\t\t}\n\t}\n\n\t/* fill in binprm security blob */\n\tretval = security_bprm_set_creds(bprm);\n\tif (retval)\n\t\treturn retval;\n\tbprm->cred_prepared = 1;\n\n\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\treturn kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);\n}",
        "func": "int prepare_binprm(struct linux_binprm *bprm)\n{\n\tint retval;\n\n\tbprm_fill_uid(bprm);\n\n\t/* fill in binprm security blob */\n\tretval = security_bprm_set_creds(bprm);\n\tif (retval)\n\t\treturn retval;\n\tbprm->cred_prepared = 1;\n\n\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\treturn kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,35 +1,8 @@\n int prepare_binprm(struct linux_binprm *bprm)\n {\n-\tstruct inode *inode = file_inode(bprm->file);\n-\tumode_t mode = inode->i_mode;\n \tint retval;\n \n-\n-\t/* clear any previous set[ug]id data from a previous binary */\n-\tbprm->cred->euid = current_euid();\n-\tbprm->cred->egid = current_egid();\n-\n-\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&\n-\t    !task_no_new_privs(current) &&\n-\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&\n-\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {\n-\t\t/* Set-uid? */\n-\t\tif (mode & S_ISUID) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->euid = inode->i_uid;\n-\t\t}\n-\n-\t\t/* Set-gid? */\n-\t\t/*\n-\t\t * If setgid is set but no group execute bit then this\n-\t\t * is a candidate for mandatory locking, not a setgid\n-\t\t * executable.\n-\t\t */\n-\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->egid = inode->i_gid;\n-\t\t}\n-\t}\n+\tbprm_fill_uid(bprm);\n \n \t/* fill in binprm security blob */\n \tretval = security_bprm_set_creds(bprm);",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct inode *inode = file_inode(bprm->file);",
                "\tumode_t mode = inode->i_mode;",
                "",
                "\t/* clear any previous set[ug]id data from a previous binary */",
                "\tbprm->cred->euid = current_euid();",
                "\tbprm->cred->egid = current_egid();",
                "",
                "\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&",
                "\t    !task_no_new_privs(current) &&",
                "\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&",
                "\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {",
                "\t\t/* Set-uid? */",
                "\t\tif (mode & S_ISUID) {",
                "\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;",
                "\t\t\tbprm->cred->euid = inode->i_uid;",
                "\t\t}",
                "",
                "\t\t/* Set-gid? */",
                "\t\t/*",
                "\t\t * If setgid is set but no group execute bit then this",
                "\t\t * is a candidate for mandatory locking, not a setgid",
                "\t\t * executable.",
                "\t\t */",
                "\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {",
                "\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;",
                "\t\t\tbprm->cred->egid = inode->i_gid;",
                "\t\t}",
                "\t}"
            ],
            "added_lines": [
                "\tbprm_fill_uid(bprm);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-1791",
        "func_name": "openssl/ssl3_get_new_session_ticket",
        "description": "Race condition in the ssl3_get_new_session_ticket function in ssl/s3_clnt.c in OpenSSL before 0.9.8zg, 1.0.0 before 1.0.0s, 1.0.1 before 1.0.1n, and 1.0.2 before 1.0.2b, when used for a multi-threaded client, allows remote attackers to cause a denial of service (double free and application crash) or possibly have unspecified other impact by providing a NewSessionTicket during an attempt to reuse a ticket that had been obtained earlier.",
        "git_url": "https://github.com/openssl/openssl/commit/98ece4eebfb6cd45cc8d550c6ac0022965071afc",
        "commit_title": "Fix race condition in NewSessionTicket",
        "commit_text": " If a NewSessionTicket is received by a multi-threaded client when attempting to reuse a previous ticket then a race condition can occur potentially leading to a double free of the ticket data.  CVE-2015-1791  This also fixes RT#3808 where a session ID is changed for a session already in the client session cache. Since the session ID is the key to the cache this breaks the cache access.  Parts of this patch were inspired by this Akamai change: https://github.com/akamai/openssl/commit/c0bf69a791239ceec64509f9f19fcafb2461b0d3 ",
        "func_before": "int ssl3_get_new_session_ticket(SSL *s)\n{\n    int ok, al, ret = 0, ticklen;\n    long n;\n    const unsigned char *p;\n    unsigned char *d;\n\n    n = s->method->ssl_get_message(s,\n                                   SSL3_ST_CR_SESSION_TICKET_A,\n                                   SSL3_ST_CR_SESSION_TICKET_B,\n                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);\n\n    if (!ok)\n        return ((int)n);\n\n    if (n < 6) {\n        /* need at least ticket_lifetime_hint + ticket length */\n        al = SSL_AD_DECODE_ERROR;\n        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);\n        goto f_err;\n    }\n\n    p = d = (unsigned char *)s->init_msg;\n    n2l(p, s->session->tlsext_tick_lifetime_hint);\n    n2s(p, ticklen);\n    /* ticket_lifetime_hint + ticket_length + ticket */\n    if (ticklen + 6 != n) {\n        al = SSL_AD_DECODE_ERROR;\n        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);\n        goto f_err;\n    }\n    OPENSSL_free(s->session->tlsext_tick);\n    s->session->tlsext_ticklen = 0;\n    s->session->tlsext_tick = OPENSSL_malloc(ticklen);\n    if (!s->session->tlsext_tick) {\n        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n    memcpy(s->session->tlsext_tick, p, ticklen);\n    s->session->tlsext_ticklen = ticklen;\n    /*\n     * There are two ways to detect a resumed ticket session. One is to set\n     * an appropriate session ID and then the server must return a match in\n     * ServerHello. This allows the normal client session ID matching to work\n     * and we know much earlier that the ticket has been accepted. The\n     * other way is to set zero length session ID when the ticket is\n     * presented and rely on the handshake to determine session resumption.\n     * We choose the former approach because this fits in with assumptions\n     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is\n     * SHA256 is disabled) hash of the ticket.\n     */\n    EVP_Digest(p, ticklen,\n               s->session->session_id, &s->session->session_id_length,\n               EVP_sha256(), NULL);\n    ret = 1;\n    return (ret);\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n err:\n    s->state = SSL_ST_ERR;\n    return (-1);\n}",
        "func": "int ssl3_get_new_session_ticket(SSL *s)\n{\n    int ok, al, ret = 0, ticklen;\n    long n;\n    const unsigned char *p;\n    unsigned char *d;\n\n    n = s->method->ssl_get_message(s,\n                                   SSL3_ST_CR_SESSION_TICKET_A,\n                                   SSL3_ST_CR_SESSION_TICKET_B,\n                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);\n\n    if (!ok)\n        return ((int)n);\n\n    if (n < 6) {\n        /* need at least ticket_lifetime_hint + ticket length */\n        al = SSL_AD_DECODE_ERROR;\n        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);\n        goto f_err;\n    }\n\n    p = d = (unsigned char *)s->init_msg;\n\n    if (s->session->session_id_length > 0) {\n        int i = s->session_ctx->session_cache_mode;\n        SSL_SESSION *new_sess;\n        /*\n         * We reused an existing session, so we need to replace it with a new\n         * one\n         */\n        if (i & SSL_SESS_CACHE_CLIENT) {\n            /*\n             * Remove the old session from the cache\n             */\n            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {\n                if (s->session_ctx->remove_session_cb != NULL)\n                    s->session_ctx->remove_session_cb(s->session_ctx,\n                                                      s->session);\n            } else {\n                /* We carry on if this fails */\n                SSL_CTX_remove_session(s->session_ctx, s->session);\n            }\n        }\n\n        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {\n            al = SSL_AD_INTERNAL_ERROR;\n            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);\n            goto f_err;\n        }\n\n        SSL_SESSION_free(s->session);\n        s->session = new_sess;\n    }\n\n    n2l(p, s->session->tlsext_tick_lifetime_hint);\n    n2s(p, ticklen);\n    /* ticket_lifetime_hint + ticket_length + ticket */\n    if (ticklen + 6 != n) {\n        al = SSL_AD_DECODE_ERROR;\n        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);\n        goto f_err;\n    }\n    OPENSSL_free(s->session->tlsext_tick);\n    s->session->tlsext_ticklen = 0;\n    s->session->tlsext_tick = OPENSSL_malloc(ticklen);\n    if (!s->session->tlsext_tick) {\n        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n    memcpy(s->session->tlsext_tick, p, ticklen);\n    s->session->tlsext_ticklen = ticklen;\n    /*\n     * There are two ways to detect a resumed ticket session. One is to set\n     * an appropriate session ID and then the server must return a match in\n     * ServerHello. This allows the normal client session ID matching to work\n     * and we know much earlier that the ticket has been accepted. The\n     * other way is to set zero length session ID when the ticket is\n     * presented and rely on the handshake to determine session resumption.\n     * We choose the former approach because this fits in with assumptions\n     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is\n     * SHA256 is disabled) hash of the ticket.\n     */\n    EVP_Digest(p, ticklen,\n               s->session->session_id, &s->session->session_id_length,\n               EVP_sha256(), NULL);\n    ret = 1;\n    return (ret);\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n err:\n    s->state = SSL_ST_ERR;\n    return (-1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,6 +21,38 @@\n     }\n \n     p = d = (unsigned char *)s->init_msg;\n+\n+    if (s->session->session_id_length > 0) {\n+        int i = s->session_ctx->session_cache_mode;\n+        SSL_SESSION *new_sess;\n+        /*\n+         * We reused an existing session, so we need to replace it with a new\n+         * one\n+         */\n+        if (i & SSL_SESS_CACHE_CLIENT) {\n+            /*\n+             * Remove the old session from the cache\n+             */\n+            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {\n+                if (s->session_ctx->remove_session_cb != NULL)\n+                    s->session_ctx->remove_session_cb(s->session_ctx,\n+                                                      s->session);\n+            } else {\n+                /* We carry on if this fails */\n+                SSL_CTX_remove_session(s->session_ctx, s->session);\n+            }\n+        }\n+\n+        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {\n+            al = SSL_AD_INTERNAL_ERROR;\n+            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);\n+            goto f_err;\n+        }\n+\n+        SSL_SESSION_free(s->session);\n+        s->session = new_sess;\n+    }\n+\n     n2l(p, s->session->tlsext_tick_lifetime_hint);\n     n2s(p, ticklen);\n     /* ticket_lifetime_hint + ticket_length + ticket */",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    if (s->session->session_id_length > 0) {",
                "        int i = s->session_ctx->session_cache_mode;",
                "        SSL_SESSION *new_sess;",
                "        /*",
                "         * We reused an existing session, so we need to replace it with a new",
                "         * one",
                "         */",
                "        if (i & SSL_SESS_CACHE_CLIENT) {",
                "            /*",
                "             * Remove the old session from the cache",
                "             */",
                "            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {",
                "                if (s->session_ctx->remove_session_cb != NULL)",
                "                    s->session_ctx->remove_session_cb(s->session_ctx,",
                "                                                      s->session);",
                "            } else {",
                "                /* We carry on if this fails */",
                "                SSL_CTX_remove_session(s->session_ctx, s->session);",
                "            }",
                "        }",
                "",
                "        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {",
                "            al = SSL_AD_INTERNAL_ERROR;",
                "            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);",
                "            goto f_err;",
                "        }",
                "",
                "        SSL_SESSION_free(s->session);",
                "        s->session = new_sess;",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2015-3212",
        "func_name": "torvalds/linux/sctp_close",
        "description": "Race condition in net/sctp/socket.c in the Linux kernel before 4.1.2 allows local users to cause a denial of service (list corruption and panic) via a rapid series of system calls related to sockets, as demonstrated by setsockopt calls.",
        "git_url": "https://github.com/torvalds/linux/commit/2d45a02d0166caf2627fe91897c6ffc3b19514c4",
        "commit_title": "sctp: fix ASCONF list handling",
        "commit_text": " ->auto_asconf_splist is per namespace and mangled by functions like sctp_setsockopt_auto_asconf() which doesn't guarantee any serialization.  Also, the call to inet_sk_copy_descendant() was backuping ->auto_asconf_list through the copy but was not honoring ->do_auto_asconf, which could lead to list corruption if it was different between both sockets.  This commit thus fixes the list handling by using ->addr_wq_lock spinlock to protect the list. A special handling is done upon socket creation and destruction for that. Error handlig on sctp_init_sock() will never return an error after having initialized asconf, so sctp_destroy_sock() can be called without addrq_wq_lock. The lock now will be take on sctp_close_sock(), before locking the socket, so we don't do it in inverse order compared to sctp_addr_wq_timeout_handler().  Instead of taking the lock on sctp_sock_migrate() for copying and restoring the list values, it's preferred to avoid rewritting it by implementing sctp_copy_descendant().  Issue was found with a test application that kept flipping sysctl default_auto_asconf on and off, but one could trigger it by issuing simultaneous setsockopt() calls on multiple sockets or by creating/destroying sockets fast enough. This is only triggerable locally.  Suggested-by: Neil Horman <nhorman@tuxdriver.com> Suggested-by: Hannes Frederic Sowa <hannes@stressinduktion.org>",
        "func_before": "static void sctp_close(struct sock *sk, long timeout)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *asoc;\n\tstruct list_head *pos, *temp;\n\tunsigned int data_was_unread;\n\n\tpr_debug(\"%s: sk:%p, timeout:%ld\\n\", __func__, sk, timeout);\n\n\tlock_sock(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tsk->sk_state = SCTP_SS_CLOSING;\n\n\tep = sctp_sk(sk)->ep;\n\n\t/* Clean up any skbs sitting on the receive queue.  */\n\tdata_was_unread = sctp_queue_purge_ulpevents(&sk->sk_receive_queue);\n\tdata_was_unread += sctp_queue_purge_ulpevents(&sctp_sk(sk)->pd_lobby);\n\n\t/* Walk all associations on an endpoint.  */\n\tlist_for_each_safe(pos, temp, &ep->asocs) {\n\t\tasoc = list_entry(pos, struct sctp_association, asocs);\n\n\t\tif (sctp_style(sk, TCP)) {\n\t\t\t/* A closed association can still be in the list if\n\t\t\t * it belongs to a TCP-style listening socket that is\n\t\t\t * not yet accepted. If so, free it. If not, send an\n\t\t\t * ABORT or SHUTDOWN based on the linger options.\n\t\t\t */\n\t\t\tif (sctp_state(asoc, CLOSED)) {\n\t\t\t\tsctp_unhash_established(asoc);\n\t\t\t\tsctp_association_free(asoc);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (data_was_unread || !skb_queue_empty(&asoc->ulpq.lobby) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm) ||\n\t\t    (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)) {\n\t\t\tstruct sctp_chunk *chunk;\n\n\t\t\tchunk = sctp_make_abort_user(asoc, NULL, 0);\n\t\t\tif (chunk)\n\t\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t} else\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t}\n\n\t/* On a TCP-style socket, block for at most linger_time if set. */\n\tif (sctp_style(sk, TCP) && timeout)\n\t\tsctp_wait_for_close(sk, timeout);\n\n\t/* This will run the backlog queue.  */\n\trelease_sock(sk);\n\n\t/* Supposedly, no process has access to the socket, but\n\t * the net layers still may.\n\t */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\n\t/* Hold the sock, since sk_common_release() will put sock_put()\n\t * and we have just a little more cleanup.\n\t */\n\tsock_hold(sk);\n\tsk_common_release(sk);\n\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\n\tsock_put(sk);\n\n\tSCTP_DBG_OBJCNT_DEC(sock);\n}",
        "func": "static void sctp_close(struct sock *sk, long timeout)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *asoc;\n\tstruct list_head *pos, *temp;\n\tunsigned int data_was_unread;\n\n\tpr_debug(\"%s: sk:%p, timeout:%ld\\n\", __func__, sk, timeout);\n\n\tlock_sock(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tsk->sk_state = SCTP_SS_CLOSING;\n\n\tep = sctp_sk(sk)->ep;\n\n\t/* Clean up any skbs sitting on the receive queue.  */\n\tdata_was_unread = sctp_queue_purge_ulpevents(&sk->sk_receive_queue);\n\tdata_was_unread += sctp_queue_purge_ulpevents(&sctp_sk(sk)->pd_lobby);\n\n\t/* Walk all associations on an endpoint.  */\n\tlist_for_each_safe(pos, temp, &ep->asocs) {\n\t\tasoc = list_entry(pos, struct sctp_association, asocs);\n\n\t\tif (sctp_style(sk, TCP)) {\n\t\t\t/* A closed association can still be in the list if\n\t\t\t * it belongs to a TCP-style listening socket that is\n\t\t\t * not yet accepted. If so, free it. If not, send an\n\t\t\t * ABORT or SHUTDOWN based on the linger options.\n\t\t\t */\n\t\t\tif (sctp_state(asoc, CLOSED)) {\n\t\t\t\tsctp_unhash_established(asoc);\n\t\t\t\tsctp_association_free(asoc);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (data_was_unread || !skb_queue_empty(&asoc->ulpq.lobby) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm) ||\n\t\t    (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)) {\n\t\t\tstruct sctp_chunk *chunk;\n\n\t\t\tchunk = sctp_make_abort_user(asoc, NULL, 0);\n\t\t\tif (chunk)\n\t\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t} else\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t}\n\n\t/* On a TCP-style socket, block for at most linger_time if set. */\n\tif (sctp_style(sk, TCP) && timeout)\n\t\tsctp_wait_for_close(sk, timeout);\n\n\t/* This will run the backlog queue.  */\n\trelease_sock(sk);\n\n\t/* Supposedly, no process has access to the socket, but\n\t * the net layers still may.\n\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock\n\t * held and that should be grabbed before socket lock.\n\t */\n\tspin_lock_bh(&net->sctp.addr_wq_lock);\n\tbh_lock_sock(sk);\n\n\t/* Hold the sock, since sk_common_release() will put sock_put()\n\t * and we have just a little more cleanup.\n\t */\n\tsock_hold(sk);\n\tsk_common_release(sk);\n\n\tbh_unlock_sock(sk);\n\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n\n\tsock_put(sk);\n\n\tSCTP_DBG_OBJCNT_DEC(sock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,8 +56,10 @@\n \n \t/* Supposedly, no process has access to the socket, but\n \t * the net layers still may.\n+\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock\n+\t * held and that should be grabbed before socket lock.\n \t */\n-\tlocal_bh_disable();\n+\tspin_lock_bh(&net->sctp.addr_wq_lock);\n \tbh_lock_sock(sk);\n \n \t/* Hold the sock, since sk_common_release() will put sock_put()\n@@ -67,7 +69,7 @@\n \tsk_common_release(sk);\n \n \tbh_unlock_sock(sk);\n-\tlocal_bh_enable();\n+\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n \n \tsock_put(sk);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tlocal_bh_disable();",
                "\tlocal_bh_enable();"
            ],
            "added_lines": [
                "\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock",
                "\t * held and that should be grabbed before socket lock.",
                "\tspin_lock_bh(&net->sctp.addr_wq_lock);",
                "\tspin_unlock_bh(&net->sctp.addr_wq_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-3212",
        "func_name": "torvalds/linux/sctp_setsockopt_auto_asconf",
        "description": "Race condition in net/sctp/socket.c in the Linux kernel before 4.1.2 allows local users to cause a denial of service (list corruption and panic) via a rapid series of system calls related to sockets, as demonstrated by setsockopt calls.",
        "git_url": "https://github.com/torvalds/linux/commit/2d45a02d0166caf2627fe91897c6ffc3b19514c4",
        "commit_title": "sctp: fix ASCONF list handling",
        "commit_text": " ->auto_asconf_splist is per namespace and mangled by functions like sctp_setsockopt_auto_asconf() which doesn't guarantee any serialization.  Also, the call to inet_sk_copy_descendant() was backuping ->auto_asconf_list through the copy but was not honoring ->do_auto_asconf, which could lead to list corruption if it was different between both sockets.  This commit thus fixes the list handling by using ->addr_wq_lock spinlock to protect the list. A special handling is done upon socket creation and destruction for that. Error handlig on sctp_init_sock() will never return an error after having initialized asconf, so sctp_destroy_sock() can be called without addrq_wq_lock. The lock now will be take on sctp_close_sock(), before locking the socket, so we don't do it in inverse order compared to sctp_addr_wq_timeout_handler().  Instead of taking the lock on sctp_sock_migrate() for copying and restoring the list values, it's preferred to avoid rewritting it by implementing sctp_copy_descendant().  Issue was found with a test application that kept flipping sysctl default_auto_asconf on and off, but one could trigger it by issuing simultaneous setsockopt() calls on multiple sockets or by creating/destroying sockets fast enough. This is only triggerable locally.  Suggested-by: Neil Horman <nhorman@tuxdriver.com> Suggested-by: Hannes Frederic Sowa <hannes@stressinduktion.org>",
        "func_before": "static int sctp_setsockopt_auto_asconf(struct sock *sk, char __user *optval,\n\t\t\t\t\tunsigned int optlen)\n{\n\tint val;\n\tstruct sctp_sock *sp = sctp_sk(sk);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\tif (!sctp_is_ep_boundall(sk) && val)\n\t\treturn -EINVAL;\n\tif ((val && sp->do_auto_asconf) || (!val && !sp->do_auto_asconf))\n\t\treturn 0;\n\n\tif (val == 0 && sp->do_auto_asconf) {\n\t\tlist_del(&sp->auto_asconf_list);\n\t\tsp->do_auto_asconf = 0;\n\t} else if (val && !sp->do_auto_asconf) {\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &sock_net(sk)->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t}\n\treturn 0;\n}",
        "func": "static int sctp_setsockopt_auto_asconf(struct sock *sk, char __user *optval,\n\t\t\t\t\tunsigned int optlen)\n{\n\tint val;\n\tstruct sctp_sock *sp = sctp_sk(sk);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\tif (!sctp_is_ep_boundall(sk) && val)\n\t\treturn -EINVAL;\n\tif ((val && sp->do_auto_asconf) || (!val && !sp->do_auto_asconf))\n\t\treturn 0;\n\n\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\tif (val == 0 && sp->do_auto_asconf) {\n\t\tlist_del(&sp->auto_asconf_list);\n\t\tsp->do_auto_asconf = 0;\n\t} else if (val && !sp->do_auto_asconf) {\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &sock_net(sk)->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t}\n\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,6 +13,7 @@\n \tif ((val && sp->do_auto_asconf) || (!val && !sp->do_auto_asconf))\n \t\treturn 0;\n \n+\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n \tif (val == 0 && sp->do_auto_asconf) {\n \t\tlist_del(&sp->auto_asconf_list);\n \t\tsp->do_auto_asconf = 0;\n@@ -21,5 +22,6 @@\n \t\t    &sock_net(sk)->sctp.auto_asconf_splist);\n \t\tsp->do_auto_asconf = 1;\n \t}\n+\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);",
                "\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-3212",
        "func_name": "torvalds/linux/sctp_sock_migrate",
        "description": "Race condition in net/sctp/socket.c in the Linux kernel before 4.1.2 allows local users to cause a denial of service (list corruption and panic) via a rapid series of system calls related to sockets, as demonstrated by setsockopt calls.",
        "git_url": "https://github.com/torvalds/linux/commit/2d45a02d0166caf2627fe91897c6ffc3b19514c4",
        "commit_title": "sctp: fix ASCONF list handling",
        "commit_text": " ->auto_asconf_splist is per namespace and mangled by functions like sctp_setsockopt_auto_asconf() which doesn't guarantee any serialization.  Also, the call to inet_sk_copy_descendant() was backuping ->auto_asconf_list through the copy but was not honoring ->do_auto_asconf, which could lead to list corruption if it was different between both sockets.  This commit thus fixes the list handling by using ->addr_wq_lock spinlock to protect the list. A special handling is done upon socket creation and destruction for that. Error handlig on sctp_init_sock() will never return an error after having initialized asconf, so sctp_destroy_sock() can be called without addrq_wq_lock. The lock now will be take on sctp_close_sock(), before locking the socket, so we don't do it in inverse order compared to sctp_addr_wq_timeout_handler().  Instead of taking the lock on sctp_sock_migrate() for copying and restoring the list values, it's preferred to avoid rewritting it by implementing sctp_copy_descendant().  Issue was found with a test application that kept flipping sysctl default_auto_asconf on and off, but one could trigger it by issuing simultaneous setsockopt() calls on multiple sockets or by creating/destroying sockets fast enough. This is only triggerable locally.  Suggested-by: Neil Horman <nhorman@tuxdriver.com> Suggested-by: Hannes Frederic Sowa <hannes@stressinduktion.org>",
        "func_before": "static void sctp_sock_migrate(struct sock *oldsk, struct sock *newsk,\n\t\t\t      struct sctp_association *assoc,\n\t\t\t      sctp_socket_type_t type)\n{\n\tstruct sctp_sock *oldsp = sctp_sk(oldsk);\n\tstruct sctp_sock *newsp = sctp_sk(newsk);\n\tstruct sctp_bind_bucket *pp; /* hash list port iterator */\n\tstruct sctp_endpoint *newep = newsp->ep;\n\tstruct sk_buff *skb, *tmp;\n\tstruct sctp_ulpevent *event;\n\tstruct sctp_bind_hashbucket *head;\n\tstruct list_head tmplist;\n\n\t/* Migrate socket buffer sizes and all the socket level options to the\n\t * new socket.\n\t */\n\tnewsk->sk_sndbuf = oldsk->sk_sndbuf;\n\tnewsk->sk_rcvbuf = oldsk->sk_rcvbuf;\n\t/* Brute force copy old sctp opt. */\n\tif (oldsp->do_auto_asconf) {\n\t\tmemcpy(&tmplist, &newsp->auto_asconf_list, sizeof(tmplist));\n\t\tinet_sk_copy_descendant(newsk, oldsk);\n\t\tmemcpy(&newsp->auto_asconf_list, &tmplist, sizeof(tmplist));\n\t} else\n\t\tinet_sk_copy_descendant(newsk, oldsk);\n\n\t/* Restore the ep value that was overwritten with the above structure\n\t * copy.\n\t */\n\tnewsp->ep = newep;\n\tnewsp->hmac = NULL;\n\n\t/* Hook this new socket in to the bind_hash list. */\n\thead = &sctp_port_hashtable[sctp_phashfn(sock_net(oldsk),\n\t\t\t\t\t\t inet_sk(oldsk)->inet_num)];\n\tlocal_bh_disable();\n\tspin_lock(&head->lock);\n\tpp = sctp_sk(oldsk)->bind_hash;\n\tsk_add_bind_node(newsk, &pp->owner);\n\tsctp_sk(newsk)->bind_hash = pp;\n\tinet_sk(newsk)->inet_num = inet_sk(oldsk)->inet_num;\n\tspin_unlock(&head->lock);\n\tlocal_bh_enable();\n\n\t/* Copy the bind_addr list from the original endpoint to the new\n\t * endpoint so that we can handle restarts properly\n\t */\n\tsctp_bind_addr_dup(&newsp->ep->base.bind_addr,\n\t\t\t\t&oldsp->ep->base.bind_addr, GFP_KERNEL);\n\n\t/* Move any messages in the old socket's receive queue that are for the\n\t * peeled off association to the new socket's receive queue.\n\t */\n\tsctp_skb_for_each(skb, &oldsk->sk_receive_queue, tmp) {\n\t\tevent = sctp_skb2event(skb);\n\t\tif (event->asoc == assoc) {\n\t\t\t__skb_unlink(skb, &oldsk->sk_receive_queue);\n\t\t\t__skb_queue_tail(&newsk->sk_receive_queue, skb);\n\t\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\t\t}\n\t}\n\n\t/* Clean up any messages pending delivery due to partial\n\t * delivery.   Three cases:\n\t * 1) No partial deliver;  no work.\n\t * 2) Peeling off partial delivery; keep pd_lobby in new pd_lobby.\n\t * 3) Peeling off non-partial delivery; move pd_lobby to receive_queue.\n\t */\n\tskb_queue_head_init(&newsp->pd_lobby);\n\tatomic_set(&sctp_sk(newsk)->pd_mode, assoc->ulpq.pd_mode);\n\n\tif (atomic_read(&sctp_sk(oldsk)->pd_mode)) {\n\t\tstruct sk_buff_head *queue;\n\n\t\t/* Decide which queue to move pd_lobby skbs to. */\n\t\tif (assoc->ulpq.pd_mode) {\n\t\t\tqueue = &newsp->pd_lobby;\n\t\t} else\n\t\t\tqueue = &newsk->sk_receive_queue;\n\n\t\t/* Walk through the pd_lobby, looking for skbs that\n\t\t * need moved to the new socket.\n\t\t */\n\t\tsctp_skb_for_each(skb, &oldsp->pd_lobby, tmp) {\n\t\t\tevent = sctp_skb2event(skb);\n\t\t\tif (event->asoc == assoc) {\n\t\t\t\t__skb_unlink(skb, &oldsp->pd_lobby);\n\t\t\t\t__skb_queue_tail(queue, skb);\n\t\t\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\t\t\t}\n\t\t}\n\n\t\t/* Clear up any skbs waiting for the partial\n\t\t * delivery to finish.\n\t\t */\n\t\tif (assoc->ulpq.pd_mode)\n\t\t\tsctp_clear_pd(oldsk, NULL);\n\n\t}\n\n\tsctp_skb_for_each(skb, &assoc->ulpq.reasm, tmp)\n\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\n\tsctp_skb_for_each(skb, &assoc->ulpq.lobby, tmp)\n\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\n\t/* Set the type of socket to indicate that it is peeled off from the\n\t * original UDP-style socket or created with the accept() call on a\n\t * TCP-style socket..\n\t */\n\tnewsp->type = type;\n\n\t/* Mark the new socket \"in-use\" by the user so that any packets\n\t * that may arrive on the association after we've moved it are\n\t * queued to the backlog.  This prevents a potential race between\n\t * backlog processing on the old socket and new-packet processing\n\t * on the new socket.\n\t *\n\t * The caller has just allocated newsk so we can guarantee that other\n\t * paths won't try to lock it and then oldsk.\n\t */\n\tlock_sock_nested(newsk, SINGLE_DEPTH_NESTING);\n\tsctp_assoc_migrate(assoc, newsk);\n\n\t/* If the association on the newsk is already closed before accept()\n\t * is called, set RCV_SHUTDOWN flag.\n\t */\n\tif (sctp_state(assoc, CLOSED) && sctp_style(newsk, TCP))\n\t\tnewsk->sk_shutdown |= RCV_SHUTDOWN;\n\n\tnewsk->sk_state = SCTP_SS_ESTABLISHED;\n\trelease_sock(newsk);\n}",
        "func": "static void sctp_sock_migrate(struct sock *oldsk, struct sock *newsk,\n\t\t\t      struct sctp_association *assoc,\n\t\t\t      sctp_socket_type_t type)\n{\n\tstruct sctp_sock *oldsp = sctp_sk(oldsk);\n\tstruct sctp_sock *newsp = sctp_sk(newsk);\n\tstruct sctp_bind_bucket *pp; /* hash list port iterator */\n\tstruct sctp_endpoint *newep = newsp->ep;\n\tstruct sk_buff *skb, *tmp;\n\tstruct sctp_ulpevent *event;\n\tstruct sctp_bind_hashbucket *head;\n\n\t/* Migrate socket buffer sizes and all the socket level options to the\n\t * new socket.\n\t */\n\tnewsk->sk_sndbuf = oldsk->sk_sndbuf;\n\tnewsk->sk_rcvbuf = oldsk->sk_rcvbuf;\n\t/* Brute force copy old sctp opt. */\n\tsctp_copy_descendant(newsk, oldsk);\n\n\t/* Restore the ep value that was overwritten with the above structure\n\t * copy.\n\t */\n\tnewsp->ep = newep;\n\tnewsp->hmac = NULL;\n\n\t/* Hook this new socket in to the bind_hash list. */\n\thead = &sctp_port_hashtable[sctp_phashfn(sock_net(oldsk),\n\t\t\t\t\t\t inet_sk(oldsk)->inet_num)];\n\tlocal_bh_disable();\n\tspin_lock(&head->lock);\n\tpp = sctp_sk(oldsk)->bind_hash;\n\tsk_add_bind_node(newsk, &pp->owner);\n\tsctp_sk(newsk)->bind_hash = pp;\n\tinet_sk(newsk)->inet_num = inet_sk(oldsk)->inet_num;\n\tspin_unlock(&head->lock);\n\tlocal_bh_enable();\n\n\t/* Copy the bind_addr list from the original endpoint to the new\n\t * endpoint so that we can handle restarts properly\n\t */\n\tsctp_bind_addr_dup(&newsp->ep->base.bind_addr,\n\t\t\t\t&oldsp->ep->base.bind_addr, GFP_KERNEL);\n\n\t/* Move any messages in the old socket's receive queue that are for the\n\t * peeled off association to the new socket's receive queue.\n\t */\n\tsctp_skb_for_each(skb, &oldsk->sk_receive_queue, tmp) {\n\t\tevent = sctp_skb2event(skb);\n\t\tif (event->asoc == assoc) {\n\t\t\t__skb_unlink(skb, &oldsk->sk_receive_queue);\n\t\t\t__skb_queue_tail(&newsk->sk_receive_queue, skb);\n\t\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\t\t}\n\t}\n\n\t/* Clean up any messages pending delivery due to partial\n\t * delivery.   Three cases:\n\t * 1) No partial deliver;  no work.\n\t * 2) Peeling off partial delivery; keep pd_lobby in new pd_lobby.\n\t * 3) Peeling off non-partial delivery; move pd_lobby to receive_queue.\n\t */\n\tskb_queue_head_init(&newsp->pd_lobby);\n\tatomic_set(&sctp_sk(newsk)->pd_mode, assoc->ulpq.pd_mode);\n\n\tif (atomic_read(&sctp_sk(oldsk)->pd_mode)) {\n\t\tstruct sk_buff_head *queue;\n\n\t\t/* Decide which queue to move pd_lobby skbs to. */\n\t\tif (assoc->ulpq.pd_mode) {\n\t\t\tqueue = &newsp->pd_lobby;\n\t\t} else\n\t\t\tqueue = &newsk->sk_receive_queue;\n\n\t\t/* Walk through the pd_lobby, looking for skbs that\n\t\t * need moved to the new socket.\n\t\t */\n\t\tsctp_skb_for_each(skb, &oldsp->pd_lobby, tmp) {\n\t\t\tevent = sctp_skb2event(skb);\n\t\t\tif (event->asoc == assoc) {\n\t\t\t\t__skb_unlink(skb, &oldsp->pd_lobby);\n\t\t\t\t__skb_queue_tail(queue, skb);\n\t\t\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\t\t\t}\n\t\t}\n\n\t\t/* Clear up any skbs waiting for the partial\n\t\t * delivery to finish.\n\t\t */\n\t\tif (assoc->ulpq.pd_mode)\n\t\t\tsctp_clear_pd(oldsk, NULL);\n\n\t}\n\n\tsctp_skb_for_each(skb, &assoc->ulpq.reasm, tmp)\n\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\n\tsctp_skb_for_each(skb, &assoc->ulpq.lobby, tmp)\n\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\n\t/* Set the type of socket to indicate that it is peeled off from the\n\t * original UDP-style socket or created with the accept() call on a\n\t * TCP-style socket..\n\t */\n\tnewsp->type = type;\n\n\t/* Mark the new socket \"in-use\" by the user so that any packets\n\t * that may arrive on the association after we've moved it are\n\t * queued to the backlog.  This prevents a potential race between\n\t * backlog processing on the old socket and new-packet processing\n\t * on the new socket.\n\t *\n\t * The caller has just allocated newsk so we can guarantee that other\n\t * paths won't try to lock it and then oldsk.\n\t */\n\tlock_sock_nested(newsk, SINGLE_DEPTH_NESTING);\n\tsctp_assoc_migrate(assoc, newsk);\n\n\t/* If the association on the newsk is already closed before accept()\n\t * is called, set RCV_SHUTDOWN flag.\n\t */\n\tif (sctp_state(assoc, CLOSED) && sctp_style(newsk, TCP))\n\t\tnewsk->sk_shutdown |= RCV_SHUTDOWN;\n\n\tnewsk->sk_state = SCTP_SS_ESTABLISHED;\n\trelease_sock(newsk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,6 @@\n \tstruct sk_buff *skb, *tmp;\n \tstruct sctp_ulpevent *event;\n \tstruct sctp_bind_hashbucket *head;\n-\tstruct list_head tmplist;\n \n \t/* Migrate socket buffer sizes and all the socket level options to the\n \t * new socket.\n@@ -17,12 +16,7 @@\n \tnewsk->sk_sndbuf = oldsk->sk_sndbuf;\n \tnewsk->sk_rcvbuf = oldsk->sk_rcvbuf;\n \t/* Brute force copy old sctp opt. */\n-\tif (oldsp->do_auto_asconf) {\n-\t\tmemcpy(&tmplist, &newsp->auto_asconf_list, sizeof(tmplist));\n-\t\tinet_sk_copy_descendant(newsk, oldsk);\n-\t\tmemcpy(&newsp->auto_asconf_list, &tmplist, sizeof(tmplist));\n-\t} else\n-\t\tinet_sk_copy_descendant(newsk, oldsk);\n+\tsctp_copy_descendant(newsk, oldsk);\n \n \t/* Restore the ep value that was overwritten with the above structure\n \t * copy.",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct list_head tmplist;",
                "\tif (oldsp->do_auto_asconf) {",
                "\t\tmemcpy(&tmplist, &newsp->auto_asconf_list, sizeof(tmplist));",
                "\t\tinet_sk_copy_descendant(newsk, oldsk);",
                "\t\tmemcpy(&newsp->auto_asconf_list, &tmplist, sizeof(tmplist));",
                "\t} else",
                "\t\tinet_sk_copy_descendant(newsk, oldsk);"
            ],
            "added_lines": [
                "\tsctp_copy_descendant(newsk, oldsk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-3212",
        "func_name": "torvalds/linux/sctp_init_sock",
        "description": "Race condition in net/sctp/socket.c in the Linux kernel before 4.1.2 allows local users to cause a denial of service (list corruption and panic) via a rapid series of system calls related to sockets, as demonstrated by setsockopt calls.",
        "git_url": "https://github.com/torvalds/linux/commit/2d45a02d0166caf2627fe91897c6ffc3b19514c4",
        "commit_title": "sctp: fix ASCONF list handling",
        "commit_text": " ->auto_asconf_splist is per namespace and mangled by functions like sctp_setsockopt_auto_asconf() which doesn't guarantee any serialization.  Also, the call to inet_sk_copy_descendant() was backuping ->auto_asconf_list through the copy but was not honoring ->do_auto_asconf, which could lead to list corruption if it was different between both sockets.  This commit thus fixes the list handling by using ->addr_wq_lock spinlock to protect the list. A special handling is done upon socket creation and destruction for that. Error handlig on sctp_init_sock() will never return an error after having initialized asconf, so sctp_destroy_sock() can be called without addrq_wq_lock. The lock now will be take on sctp_close_sock(), before locking the socket, so we don't do it in inverse order compared to sctp_addr_wq_timeout_handler().  Instead of taking the lock on sctp_sock_migrate() for copying and restoring the list values, it's preferred to avoid rewritting it by implementing sctp_copy_descendant().  Issue was found with a test application that kept flipping sysctl default_auto_asconf on and off, but one could trigger it by issuing simultaneous setsockopt() calls on multiple sockets or by creating/destroying sockets fast enough. This is only triggerable locally.  Suggested-by: Neil Horman <nhorman@tuxdriver.com> Suggested-by: Hannes Frederic Sowa <hannes@stressinduktion.org>",
        "func_before": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tmemset(&sp->subscribe, 0, sizeof(struct sctp_event_subscribe));\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&sctp_sockets_allocated);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\tif (net->sctp.default_auto_asconf) {\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t} else\n\t\tsp->do_auto_asconf = 0;\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "func": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tmemset(&sp->subscribe, 0, sizeof(struct sctp_event_subscribe));\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&sctp_sockets_allocated);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\n\t/* Nothing can fail after this block, otherwise\n\t * sctp_destroy_sock() will be called without addr_wq_lock held\n\t */\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n\t} else {\n\t\tsp->do_auto_asconf = 0;\n\t}\n\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -125,12 +125,20 @@\n \tlocal_bh_disable();\n \tpercpu_counter_inc(&sctp_sockets_allocated);\n \tsock_prot_inuse_add(net, sk->sk_prot, 1);\n+\n+\t/* Nothing can fail after this block, otherwise\n+\t * sctp_destroy_sock() will be called without addr_wq_lock held\n+\t */\n \tif (net->sctp.default_auto_asconf) {\n+\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n \t\tlist_add_tail(&sp->auto_asconf_list,\n \t\t    &net->sctp.auto_asconf_splist);\n \t\tsp->do_auto_asconf = 1;\n-\t} else\n+\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n+\t} else {\n \t\tsp->do_auto_asconf = 0;\n+\t}\n+\n \tlocal_bh_enable();\n \n \treturn 0;",
        "diff_line_info": {
            "deleted_lines": [
                "\t} else"
            ],
            "added_lines": [
                "",
                "\t/* Nothing can fail after this block, otherwise",
                "\t * sctp_destroy_sock() will be called without addr_wq_lock held",
                "\t */",
                "\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);",
                "\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);",
                "\t} else {",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-32686",
        "func_name": "pjsip/pjproject/on_accept_complete2",
        "description": "PJSIP is a free and open source multimedia communication library written in C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In PJSIP before version 2.11.1, there are a couple of issues found in the SSL socket. First, a race condition between callback and destroy, due to the accepted socket having no group lock. Second, the SSL socket parent/listener may get destroyed during handshake. Both issues were reported to happen intermittently in heavy load TLS connections. They cause a crash, resulting in a denial of service. These are fixed in version 2.11.1.",
        "git_url": "https://github.com/pjsip/pjproject/commit/d5f95aa066f878b0aef6a64e60b61e8626e664cd",
        "commit_title": "Merge pull request from GHSA-cv8x-p47p-99wr",
        "commit_text": " * - Avoid SSL socket parent/listener getting destroyed during handshake by increasing parent's reference count. - Add missing SSL socket close when the newly accepted SSL socket is discarded in SIP TLS transport.  * - Fix silly mistake: accepted active socket created without group lock in SSL socket. - Replace assertion with normal validation check of SSL socket instance in OpenSSL verification callback (verify_cb()) to avoid crash, e.g: if somehow race condition with SSL socket destroy happens or OpenSSL application data index somehow gets corrupted.",
        "func_before": "static pj_bool_t on_accept_complete2(pj_ssl_sock_t *ssock,\n\t\t\t\t     pj_ssl_sock_t *new_ssock,\n\t\t\t\t     const pj_sockaddr_t *src_addr,\n\t\t\t\t     int src_addr_len, \n\t\t\t\t     pj_status_t accept_status)\n{    \n    struct tls_listener *listener;\n    struct tls_transport *tls;\n    pj_ssl_sock_info ssl_info;\n    char addr[PJ_INET6_ADDRSTRLEN+10];\n    pjsip_tp_state_callback state_cb;\n    pj_sockaddr tmp_src_addr;\n    pj_bool_t is_shutdown;\n    pj_status_t status;\n    char addr_buf[PJ_INET6_ADDRSTRLEN+10];        \n\n    PJ_UNUSED_ARG(src_addr_len);\n\n    listener = (struct tls_listener*) pj_ssl_sock_get_user_data(ssock);\n\n    if (accept_status != PJ_SUCCESS) {\n\tif (listener && listener->tls_setting.on_accept_fail_cb) {\n\t    pjsip_tls_on_accept_fail_param param;\n\t    pj_ssl_sock_info ssi;\n\n\t    pj_bzero(&param, sizeof(param));\n\t    param.status = accept_status;\n\t    param.local_addr = &listener->factory.local_addr;\n\t    param.remote_addr = src_addr;\n\t    if (new_ssock &&\n\t\tpj_ssl_sock_get_info(new_ssock, &ssi) == PJ_SUCCESS)\n\t    {\n\t\tparam.last_native_err = ssi.last_native_err;\n\t    }\n\n\t    (*listener->tls_setting.on_accept_fail_cb) (&param);\n\t}\n\n\treturn PJ_FALSE;\n    }\n\n    PJ_ASSERT_RETURN(new_ssock, PJ_TRUE);\n\n    if (!listener->is_registered) {\n\tif (listener->tls_setting.on_accept_fail_cb) {\n\t    pjsip_tls_on_accept_fail_param param;\n\t    pj_bzero(&param, sizeof(param));\n\t    param.status = PJSIP_TLS_EACCEPT;\n\t    param.local_addr = &listener->factory.local_addr;\n\t    param.remote_addr = src_addr;\n\t    (*listener->tls_setting.on_accept_fail_cb) (&param);\n\t}\n\treturn PJ_FALSE;\n    }\t\n\n    PJ_LOG(4,(listener->factory.obj_name, \n\t      \"TLS listener %s: got incoming TLS connection \"\n\t      \"from %s, sock=%d\",\n\t      pj_addr_str_print(&listener->factory.addr_name.host, \n\t\t\t\tlistener->factory.addr_name.port, addr_buf, \n\t\t\t\tsizeof(addr_buf), 1),\n\t      pj_sockaddr_print(src_addr, addr, sizeof(addr), 3),\n\t      new_ssock));\n\n    /* Retrieve SSL socket info, close the socket if this is failed\n     * as the SSL socket info availability is rather critical here.\n     */\n    status = pj_ssl_sock_get_info(new_ssock, &ssl_info);\n    if (status != PJ_SUCCESS) {\n\tpj_ssl_sock_close(new_ssock);\n\n\tif (listener->tls_setting.on_accept_fail_cb) {\n\t    pjsip_tls_on_accept_fail_param param;\n\t    pj_bzero(&param, sizeof(param));\n\t    param.status = status;\n\t    param.local_addr = &listener->factory.local_addr;\n\t    param.remote_addr = src_addr;\n\t    (*listener->tls_setting.on_accept_fail_cb) (&param);\n\t}\n\treturn PJ_TRUE;\n    }\n\n    /* Copy to larger buffer, just in case */\n    pj_bzero(&tmp_src_addr, sizeof(tmp_src_addr));\n    pj_sockaddr_cp(&tmp_src_addr, src_addr);\n\n    /* \n     * Incoming connection!\n     * Create TLS transport for the new socket.\n     */\n    status = tls_create( listener, NULL, new_ssock, PJ_TRUE,\n\t\t\t &ssl_info.local_addr, &tmp_src_addr, NULL,\n\t\t\t ssl_info.grp_lock, &tls);\n    \n    if (status != PJ_SUCCESS) {\n\tif (listener->tls_setting.on_accept_fail_cb) {\n\t    pjsip_tls_on_accept_fail_param param;\n\t    pj_bzero(&param, sizeof(param));\n\t    param.status = status;\n\t    param.local_addr = &listener->factory.local_addr;\n\t    param.remote_addr = src_addr;\n\t    (*listener->tls_setting.on_accept_fail_cb) (&param);\n\t}\n\treturn PJ_TRUE;\n    }\n\n    /* Set the \"pending\" SSL socket user data */\n    pj_ssl_sock_set_user_data(new_ssock, tls);\n\n    /* Prevent immediate transport destroy as application may access it \n     * (getting info, etc) in transport state notification callback.\n     */\n    pjsip_transport_add_ref(&tls->base);\n\n    /* If there is verification error and verification is mandatory, shutdown\n     * and destroy the transport.\n     */\n    if (ssl_info.verify_status && listener->tls_setting.verify_client) {\n\tif (tls->close_reason == PJ_SUCCESS) \n\t    tls->close_reason = PJSIP_TLS_ECERTVERIF;\n\tpjsip_transport_shutdown(&tls->base);\n    }\n    /* Notify transport state to application */\n    state_cb = pjsip_tpmgr_get_state_cb(tls->base.tpmgr);\n    if (state_cb) {\n\tpjsip_transport_state_info state_info;\n\tpjsip_tls_state_info tls_info;\n\tpjsip_transport_state tp_state;\n\n\t/* Init transport state info */\n\tpj_bzero(&tls_info, sizeof(tls_info));\n\tpj_bzero(&state_info, sizeof(state_info));\n\ttls_info.ssl_sock_info = &ssl_info;\n\tstate_info.ext_info = &tls_info;\n\n\t/* Set transport state based on verification status */\n\tif (ssl_info.verify_status && listener->tls_setting.verify_client)\n\t{\n\t    tp_state = PJSIP_TP_STATE_DISCONNECTED;\n\t    state_info.status = PJSIP_TLS_ECERTVERIF;\n\t} else {\n\t    tp_state = PJSIP_TP_STATE_CONNECTED;\n\t    state_info.status = PJ_SUCCESS;\n\t}\n\n\t(*state_cb)(&tls->base, tp_state, &state_info);\n    }\n\n    /* Release transport reference. If transport is shutting down, it may\n     * get destroyed here.\n     */\n    is_shutdown = tls->base.is_shutdown;\n    pjsip_transport_dec_ref(&tls->base);\n    if (is_shutdown)\n\treturn PJ_TRUE;\n\n\n    status = tls_start_read(tls);\n    if (status != PJ_SUCCESS) {\n\tPJ_LOG(3,(tls->base.obj_name, \"New transport cancelled\"));\n\ttls_init_shutdown(tls, status);\n\ttls_destroy(&tls->base, status);\n    } else {\n\t/* Start keep-alive timer */\n\tif (pjsip_cfg()->tls.keep_alive_interval) {\n\t    pj_time_val delay = {0};\t    \n\t    delay.sec = pjsip_cfg()->tls.keep_alive_interval;\n\t    pjsip_endpt_schedule_timer(listener->endpt, \n\t\t\t\t       &tls->ka_timer, \n\t\t\t\t       &delay);\n\t    tls->ka_timer.id = PJ_TRUE;\n\t    pj_gettimeofday(&tls->last_activity);\n\t}\n    }\n\n    return PJ_TRUE;\n}",
        "func": "static pj_bool_t on_accept_complete2(pj_ssl_sock_t *ssock,\n\t\t\t\t     pj_ssl_sock_t *new_ssock,\n\t\t\t\t     const pj_sockaddr_t *src_addr,\n\t\t\t\t     int src_addr_len, \n\t\t\t\t     pj_status_t accept_status)\n{    \n    struct tls_listener *listener;\n    struct tls_transport *tls;\n    pj_ssl_sock_info ssl_info;\n    char addr[PJ_INET6_ADDRSTRLEN+10];\n    pjsip_tp_state_callback state_cb;\n    pj_sockaddr tmp_src_addr;\n    pj_bool_t is_shutdown;\n    pj_status_t status;\n    char addr_buf[PJ_INET6_ADDRSTRLEN+10];        \n\n    PJ_UNUSED_ARG(src_addr_len);\n\n    listener = (struct tls_listener*) pj_ssl_sock_get_user_data(ssock);\n    if (!listener) {\n\t/* Listener already destroyed, e.g: after TCP accept but before SSL\n\t * handshake is completed.\n\t */\n\tif (new_ssock && accept_status == PJ_SUCCESS) {\n\t    /* Close the SSL socket if the accept op is successful */\n\t    PJ_LOG(4,(THIS_FILE,\n\t\t      \"Incoming TLS connection from %s (sock=%d) is discarded \"\n\t\t      \"because listener is already destroyed\",\n\t\t      pj_sockaddr_print(src_addr, addr, sizeof(addr), 3),\n\t\t      new_ssock));\n\n\t    pj_ssl_sock_close(new_ssock);\n\t}\n\n\treturn PJ_FALSE;\n    }\n\n    if (accept_status != PJ_SUCCESS) {\n\tif (listener->tls_setting.on_accept_fail_cb) {\n\t    pjsip_tls_on_accept_fail_param param;\n\t    pj_ssl_sock_info ssi;\n\n\t    pj_bzero(&param, sizeof(param));\n\t    param.status = accept_status;\n\t    param.local_addr = &listener->factory.local_addr;\n\t    param.remote_addr = src_addr;\n\t    if (new_ssock &&\n\t\tpj_ssl_sock_get_info(new_ssock, &ssi) == PJ_SUCCESS)\n\t    {\n\t\tparam.last_native_err = ssi.last_native_err;\n\t    }\n\n\t    (*listener->tls_setting.on_accept_fail_cb) (&param);\n\t}\n\n\treturn PJ_FALSE;\n    }\n\n    PJ_ASSERT_RETURN(new_ssock, PJ_TRUE);\n\n    if (!listener->is_registered) {\n\tpj_ssl_sock_close(new_ssock);\n\n\tif (listener->tls_setting.on_accept_fail_cb) {\n\t    pjsip_tls_on_accept_fail_param param;\n\t    pj_bzero(&param, sizeof(param));\n\t    param.status = PJSIP_TLS_EACCEPT;\n\t    param.local_addr = &listener->factory.local_addr;\n\t    param.remote_addr = src_addr;\n\t    (*listener->tls_setting.on_accept_fail_cb) (&param);\n\t}\n\treturn PJ_FALSE;\n    }\t\n\n    PJ_LOG(4,(listener->factory.obj_name, \n\t      \"TLS listener %s: got incoming TLS connection \"\n\t      \"from %s, sock=%d\",\n\t      pj_addr_str_print(&listener->factory.addr_name.host, \n\t\t\t\tlistener->factory.addr_name.port, addr_buf, \n\t\t\t\tsizeof(addr_buf), 1),\n\t      pj_sockaddr_print(src_addr, addr, sizeof(addr), 3),\n\t      new_ssock));\n\n    /* Retrieve SSL socket info, close the socket if this is failed\n     * as the SSL socket info availability is rather critical here.\n     */\n    status = pj_ssl_sock_get_info(new_ssock, &ssl_info);\n    if (status != PJ_SUCCESS) {\n\tpj_ssl_sock_close(new_ssock);\n\n\tif (listener->tls_setting.on_accept_fail_cb) {\n\t    pjsip_tls_on_accept_fail_param param;\n\t    pj_bzero(&param, sizeof(param));\n\t    param.status = status;\n\t    param.local_addr = &listener->factory.local_addr;\n\t    param.remote_addr = src_addr;\n\t    (*listener->tls_setting.on_accept_fail_cb) (&param);\n\t}\n\treturn PJ_TRUE;\n    }\n\n    /* Copy to larger buffer, just in case */\n    pj_bzero(&tmp_src_addr, sizeof(tmp_src_addr));\n    pj_sockaddr_cp(&tmp_src_addr, src_addr);\n\n    /* \n     * Incoming connection!\n     * Create TLS transport for the new socket.\n     */\n    status = tls_create( listener, NULL, new_ssock, PJ_TRUE,\n\t\t\t &ssl_info.local_addr, &tmp_src_addr, NULL,\n\t\t\t ssl_info.grp_lock, &tls);\n    \n    if (status != PJ_SUCCESS) {\n\tpj_ssl_sock_close(new_ssock);\n\n\tif (listener->tls_setting.on_accept_fail_cb) {\n\t    pjsip_tls_on_accept_fail_param param;\n\t    pj_bzero(&param, sizeof(param));\n\t    param.status = status;\n\t    param.local_addr = &listener->factory.local_addr;\n\t    param.remote_addr = src_addr;\n\t    (*listener->tls_setting.on_accept_fail_cb) (&param);\n\t}\n\treturn PJ_TRUE;\n    }\n\n    /* Set the \"pending\" SSL socket user data */\n    pj_ssl_sock_set_user_data(new_ssock, tls);\n\n    /* Prevent immediate transport destroy as application may access it \n     * (getting info, etc) in transport state notification callback.\n     */\n    pjsip_transport_add_ref(&tls->base);\n\n    /* If there is verification error and verification is mandatory, shutdown\n     * and destroy the transport.\n     */\n    if (ssl_info.verify_status && listener->tls_setting.verify_client) {\n\tif (tls->close_reason == PJ_SUCCESS) \n\t    tls->close_reason = PJSIP_TLS_ECERTVERIF;\n\tpjsip_transport_shutdown(&tls->base);\n    }\n    /* Notify transport state to application */\n    state_cb = pjsip_tpmgr_get_state_cb(tls->base.tpmgr);\n    if (state_cb) {\n\tpjsip_transport_state_info state_info;\n\tpjsip_tls_state_info tls_info;\n\tpjsip_transport_state tp_state;\n\n\t/* Init transport state info */\n\tpj_bzero(&tls_info, sizeof(tls_info));\n\tpj_bzero(&state_info, sizeof(state_info));\n\ttls_info.ssl_sock_info = &ssl_info;\n\tstate_info.ext_info = &tls_info;\n\n\t/* Set transport state based on verification status */\n\tif (ssl_info.verify_status && listener->tls_setting.verify_client)\n\t{\n\t    tp_state = PJSIP_TP_STATE_DISCONNECTED;\n\t    state_info.status = PJSIP_TLS_ECERTVERIF;\n\t} else {\n\t    tp_state = PJSIP_TP_STATE_CONNECTED;\n\t    state_info.status = PJ_SUCCESS;\n\t}\n\n\t(*state_cb)(&tls->base, tp_state, &state_info);\n    }\n\n    /* Release transport reference. If transport is shutting down, it may\n     * get destroyed here.\n     */\n    is_shutdown = tls->base.is_shutdown;\n    pjsip_transport_dec_ref(&tls->base);\n    if (is_shutdown)\n\treturn PJ_TRUE;\n\n\n    status = tls_start_read(tls);\n    if (status != PJ_SUCCESS) {\n\tPJ_LOG(3,(tls->base.obj_name, \"New transport cancelled\"));\n\ttls_init_shutdown(tls, status);\n\ttls_destroy(&tls->base, status);\n    } else {\n\t/* Start keep-alive timer */\n\tif (pjsip_cfg()->tls.keep_alive_interval) {\n\t    pj_time_val delay = {0};\t    \n\t    delay.sec = pjsip_cfg()->tls.keep_alive_interval;\n\t    pjsip_endpt_schedule_timer(listener->endpt, \n\t\t\t\t       &tls->ka_timer, \n\t\t\t\t       &delay);\n\t    tls->ka_timer.id = PJ_TRUE;\n\t    pj_gettimeofday(&tls->last_activity);\n\t}\n    }\n\n    return PJ_TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,9 +17,26 @@\n     PJ_UNUSED_ARG(src_addr_len);\n \n     listener = (struct tls_listener*) pj_ssl_sock_get_user_data(ssock);\n+    if (!listener) {\n+\t/* Listener already destroyed, e.g: after TCP accept but before SSL\n+\t * handshake is completed.\n+\t */\n+\tif (new_ssock && accept_status == PJ_SUCCESS) {\n+\t    /* Close the SSL socket if the accept op is successful */\n+\t    PJ_LOG(4,(THIS_FILE,\n+\t\t      \"Incoming TLS connection from %s (sock=%d) is discarded \"\n+\t\t      \"because listener is already destroyed\",\n+\t\t      pj_sockaddr_print(src_addr, addr, sizeof(addr), 3),\n+\t\t      new_ssock));\n+\n+\t    pj_ssl_sock_close(new_ssock);\n+\t}\n+\n+\treturn PJ_FALSE;\n+    }\n \n     if (accept_status != PJ_SUCCESS) {\n-\tif (listener && listener->tls_setting.on_accept_fail_cb) {\n+\tif (listener->tls_setting.on_accept_fail_cb) {\n \t    pjsip_tls_on_accept_fail_param param;\n \t    pj_ssl_sock_info ssi;\n \n@@ -42,6 +59,8 @@\n     PJ_ASSERT_RETURN(new_ssock, PJ_TRUE);\n \n     if (!listener->is_registered) {\n+\tpj_ssl_sock_close(new_ssock);\n+\n \tif (listener->tls_setting.on_accept_fail_cb) {\n \t    pjsip_tls_on_accept_fail_param param;\n \t    pj_bzero(&param, sizeof(param));\n@@ -93,6 +112,8 @@\n \t\t\t ssl_info.grp_lock, &tls);\n     \n     if (status != PJ_SUCCESS) {\n+\tpj_ssl_sock_close(new_ssock);\n+\n \tif (listener->tls_setting.on_accept_fail_cb) {\n \t    pjsip_tls_on_accept_fail_param param;\n \t    pj_bzero(&param, sizeof(param));",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (listener && listener->tls_setting.on_accept_fail_cb) {"
            ],
            "added_lines": [
                "    if (!listener) {",
                "\t/* Listener already destroyed, e.g: after TCP accept but before SSL",
                "\t * handshake is completed.",
                "\t */",
                "\tif (new_ssock && accept_status == PJ_SUCCESS) {",
                "\t    /* Close the SSL socket if the accept op is successful */",
                "\t    PJ_LOG(4,(THIS_FILE,",
                "\t\t      \"Incoming TLS connection from %s (sock=%d) is discarded \"",
                "\t\t      \"because listener is already destroyed\",",
                "\t\t      pj_sockaddr_print(src_addr, addr, sizeof(addr), 3),",
                "\t\t      new_ssock));",
                "",
                "\t    pj_ssl_sock_close(new_ssock);",
                "\t}",
                "",
                "\treturn PJ_FALSE;",
                "    }",
                "\tif (listener->tls_setting.on_accept_fail_cb) {",
                "\tpj_ssl_sock_close(new_ssock);",
                "",
                "\tpj_ssl_sock_close(new_ssock);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-32686",
        "func_name": "pjsip/pjproject/STATUS_FROM_SSL_ERR",
        "description": "PJSIP is a free and open source multimedia communication library written in C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In PJSIP before version 2.11.1, there are a couple of issues found in the SSL socket. First, a race condition between callback and destroy, due to the accepted socket having no group lock. Second, the SSL socket parent/listener may get destroyed during handshake. Both issues were reported to happen intermittently in heavy load TLS connections. They cause a crash, resulting in a denial of service. These are fixed in version 2.11.1.",
        "git_url": "https://github.com/pjsip/pjproject/commit/d5f95aa066f878b0aef6a64e60b61e8626e664cd",
        "commit_title": "Merge pull request from GHSA-cv8x-p47p-99wr",
        "commit_text": " * - Avoid SSL socket parent/listener getting destroyed during handshake by increasing parent's reference count. - Add missing SSL socket close when the newly accepted SSL socket is discarded in SIP TLS transport.  * - Fix silly mistake: accepted active socket created without group lock in SSL socket. - Replace assertion with normal validation check of SSL socket instance in OpenSSL verification callback (verify_cb()) to avoid crash, e.g: if somehow race condition with SSL socket destroy happens or OpenSSL application data index somehow gets corrupted.",
        "func_before": "static pj_status_t STATUS_FROM_SSL_ERR(char *action, pj_ssl_sock_t *ssock,\n\t\t\t\t       unsigned long err)\n{\n    int level = 0;\n    int len = 0; //dummy\n\n    ERROR_LOG(\"STATUS_FROM_SSL_ERR\", err, ssock);\n    level++;\n\n    /* General SSL error, dig more from OpenSSL error queue */\n    if (err == SSL_ERROR_SSL) {\n\terr = ERR_get_error();\n\tERROR_LOG(\"STATUS_FROM_SSL_ERR\", err, ssock);\n    }\n\n    ssock->last_err = err;\n    return GET_STATUS_FROM_SSL_ERR(err);\n}",
        "func": "static pj_status_t STATUS_FROM_SSL_ERR(char *action, pj_ssl_sock_t *ssock,\n\t\t\t\t       unsigned long err)\n{\n    int level = 0;\n    int len = 0; //dummy\n\n    ERROR_LOG(\"STATUS_FROM_SSL_ERR\", err, ssock);\n    level++;\n\n    /* General SSL error, dig more from OpenSSL error queue */\n    if (err == SSL_ERROR_SSL) {\n\terr = ERR_get_error();\n\tERROR_LOG(\"STATUS_FROM_SSL_ERR\", err, ssock);\n    }\n\n    if (ssock)\n\tssock->last_err = err;\n    return GET_STATUS_FROM_SSL_ERR(err);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,6 +13,7 @@\n \tERROR_LOG(\"STATUS_FROM_SSL_ERR\", err, ssock);\n     }\n \n-    ssock->last_err = err;\n+    if (ssock)\n+\tssock->last_err = err;\n     return GET_STATUS_FROM_SSL_ERR(err);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    ssock->last_err = err;"
            ],
            "added_lines": [
                "    if (ssock)",
                "\tssock->last_err = err;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-32686",
        "func_name": "pjsip/pjproject/verify_cb",
        "description": "PJSIP is a free and open source multimedia communication library written in C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In PJSIP before version 2.11.1, there are a couple of issues found in the SSL socket. First, a race condition between callback and destroy, due to the accepted socket having no group lock. Second, the SSL socket parent/listener may get destroyed during handshake. Both issues were reported to happen intermittently in heavy load TLS connections. They cause a crash, resulting in a denial of service. These are fixed in version 2.11.1.",
        "git_url": "https://github.com/pjsip/pjproject/commit/d5f95aa066f878b0aef6a64e60b61e8626e664cd",
        "commit_title": "Merge pull request from GHSA-cv8x-p47p-99wr",
        "commit_text": " * - Avoid SSL socket parent/listener getting destroyed during handshake by increasing parent's reference count. - Add missing SSL socket close when the newly accepted SSL socket is discarded in SIP TLS transport.  * - Fix silly mistake: accepted active socket created without group lock in SSL socket. - Replace assertion with normal validation check of SSL socket instance in OpenSSL verification callback (verify_cb()) to avoid crash, e.g: if somehow race condition with SSL socket destroy happens or OpenSSL application data index somehow gets corrupted.",
        "func_before": "static int verify_cb(int preverify_ok, X509_STORE_CTX *x509_ctx)\n{\n    pj_ssl_sock_t *ssock;\n    SSL *ossl_ssl;\n    int err;\n\n    /* Get SSL instance */\n    ossl_ssl = X509_STORE_CTX_get_ex_data(x509_ctx, \n\t\t\t\t    SSL_get_ex_data_X509_STORE_CTX_idx());\n    pj_assert(ossl_ssl);\n\n    /* Get SSL socket instance */\n    ssock = SSL_get_ex_data(ossl_ssl, sslsock_idx);\n    pj_assert(ssock);\n\n    /* Store verification status */\n    err = X509_STORE_CTX_get_error(x509_ctx);\n    switch (err) {\n    case X509_V_OK:\n\tbreak;\n\n    case X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT:\n\tssock->verify_status |= PJ_SSL_CERT_EISSUER_NOT_FOUND;\n\tbreak;\n\n    case X509_V_ERR_ERROR_IN_CERT_NOT_BEFORE_FIELD:\n    case X509_V_ERR_ERROR_IN_CERT_NOT_AFTER_FIELD:\n    case X509_V_ERR_UNABLE_TO_DECRYPT_CERT_SIGNATURE:\n    case X509_V_ERR_UNABLE_TO_DECODE_ISSUER_PUBLIC_KEY:\n\tssock->verify_status |= PJ_SSL_CERT_EINVALID_FORMAT;\n\tbreak;\n\n    case X509_V_ERR_CERT_NOT_YET_VALID:\n    case X509_V_ERR_CERT_HAS_EXPIRED:\n\tssock->verify_status |= PJ_SSL_CERT_EVALIDITY_PERIOD;\n\tbreak;\n\n    case X509_V_ERR_UNABLE_TO_GET_CRL:\n    case X509_V_ERR_CRL_NOT_YET_VALID:\n    case X509_V_ERR_CRL_HAS_EXPIRED:\n    case X509_V_ERR_UNABLE_TO_DECRYPT_CRL_SIGNATURE:\n    case X509_V_ERR_CRL_SIGNATURE_FAILURE:\n    case X509_V_ERR_ERROR_IN_CRL_LAST_UPDATE_FIELD:\n    case X509_V_ERR_ERROR_IN_CRL_NEXT_UPDATE_FIELD:\n\tssock->verify_status |= PJ_SSL_CERT_ECRL_FAILURE;\n\tbreak;\t\n\n    case X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT:\n    case X509_V_ERR_CERT_UNTRUSTED:\n    case X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN:\n    case X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY:\n\tssock->verify_status |= PJ_SSL_CERT_EUNTRUSTED;\n\tbreak;\t\n\n    case X509_V_ERR_CERT_SIGNATURE_FAILURE:\n    case X509_V_ERR_UNABLE_TO_VERIFY_LEAF_SIGNATURE:\n    case X509_V_ERR_SUBJECT_ISSUER_MISMATCH:\n    case X509_V_ERR_AKID_SKID_MISMATCH:\n    case X509_V_ERR_AKID_ISSUER_SERIAL_MISMATCH:\n    case X509_V_ERR_KEYUSAGE_NO_CERTSIGN:\n\tssock->verify_status |= PJ_SSL_CERT_EISSUER_MISMATCH;\n\tbreak;\n\n    case X509_V_ERR_CERT_REVOKED:\n\tssock->verify_status |= PJ_SSL_CERT_EREVOKED;\n\tbreak;\t\n\n    case X509_V_ERR_INVALID_PURPOSE:\n    case X509_V_ERR_CERT_REJECTED:\n    case X509_V_ERR_INVALID_CA:\n\tssock->verify_status |= PJ_SSL_CERT_EINVALID_PURPOSE;\n\tbreak;\n\n    case X509_V_ERR_CERT_CHAIN_TOO_LONG: /* not really used */\n    case X509_V_ERR_PATH_LENGTH_EXCEEDED:\n\tssock->verify_status |= PJ_SSL_CERT_ECHAIN_TOO_LONG;\n\tbreak;\n\n    /* Unknown errors */\n    case X509_V_ERR_OUT_OF_MEM:\n    default:\n\tssock->verify_status |= PJ_SSL_CERT_EUNKNOWN;\n\tbreak;\n    }\n\n    /* When verification is not requested just return ok here, however\n     * application can still get the verification status.\n     */\n    if (PJ_FALSE == ssock->param.verify_peer)\n\tpreverify_ok = 1;\n\n    return preverify_ok;\n}",
        "func": "static int verify_cb(int preverify_ok, X509_STORE_CTX *x509_ctx)\n{\n    pj_ssl_sock_t *ssock = NULL;\n    SSL *ossl_ssl = NULL;\n    int err;\n\n    /* Get SSL instance */\n    ossl_ssl = X509_STORE_CTX_get_ex_data(x509_ctx, \n\t\t\t\t    SSL_get_ex_data_X509_STORE_CTX_idx());\n    if (!ossl_ssl) {\n\tPJ_LOG(1,(THIS_FILE,\n\t\t  \"SSL verification callback failed to get SSL instance\"));\n\tgoto on_return;\n    }\n\n    /* Get SSL socket instance */\n    ssock = SSL_get_ex_data(ossl_ssl, sslsock_idx);\n    if (!ssock) {\n\t/* SSL socket may have been destroyed */\n\tPJ_LOG(1,(THIS_FILE,\n\t\t  \"SSL verification callback failed to get SSL socket \"\n\t\t  \"instance (sslsock_idx=%d).\", sslsock_idx));\n\tgoto on_return;\n    }\n\n    /* Store verification status */\n    err = X509_STORE_CTX_get_error(x509_ctx);\n    switch (err) {\n    case X509_V_OK:\n\tbreak;\n\n    case X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT:\n\tssock->verify_status |= PJ_SSL_CERT_EISSUER_NOT_FOUND;\n\tbreak;\n\n    case X509_V_ERR_ERROR_IN_CERT_NOT_BEFORE_FIELD:\n    case X509_V_ERR_ERROR_IN_CERT_NOT_AFTER_FIELD:\n    case X509_V_ERR_UNABLE_TO_DECRYPT_CERT_SIGNATURE:\n    case X509_V_ERR_UNABLE_TO_DECODE_ISSUER_PUBLIC_KEY:\n\tssock->verify_status |= PJ_SSL_CERT_EINVALID_FORMAT;\n\tbreak;\n\n    case X509_V_ERR_CERT_NOT_YET_VALID:\n    case X509_V_ERR_CERT_HAS_EXPIRED:\n\tssock->verify_status |= PJ_SSL_CERT_EVALIDITY_PERIOD;\n\tbreak;\n\n    case X509_V_ERR_UNABLE_TO_GET_CRL:\n    case X509_V_ERR_CRL_NOT_YET_VALID:\n    case X509_V_ERR_CRL_HAS_EXPIRED:\n    case X509_V_ERR_UNABLE_TO_DECRYPT_CRL_SIGNATURE:\n    case X509_V_ERR_CRL_SIGNATURE_FAILURE:\n    case X509_V_ERR_ERROR_IN_CRL_LAST_UPDATE_FIELD:\n    case X509_V_ERR_ERROR_IN_CRL_NEXT_UPDATE_FIELD:\n\tssock->verify_status |= PJ_SSL_CERT_ECRL_FAILURE;\n\tbreak;\t\n\n    case X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT:\n    case X509_V_ERR_CERT_UNTRUSTED:\n    case X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN:\n    case X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY:\n\tssock->verify_status |= PJ_SSL_CERT_EUNTRUSTED;\n\tbreak;\t\n\n    case X509_V_ERR_CERT_SIGNATURE_FAILURE:\n    case X509_V_ERR_UNABLE_TO_VERIFY_LEAF_SIGNATURE:\n    case X509_V_ERR_SUBJECT_ISSUER_MISMATCH:\n    case X509_V_ERR_AKID_SKID_MISMATCH:\n    case X509_V_ERR_AKID_ISSUER_SERIAL_MISMATCH:\n    case X509_V_ERR_KEYUSAGE_NO_CERTSIGN:\n\tssock->verify_status |= PJ_SSL_CERT_EISSUER_MISMATCH;\n\tbreak;\n\n    case X509_V_ERR_CERT_REVOKED:\n\tssock->verify_status |= PJ_SSL_CERT_EREVOKED;\n\tbreak;\t\n\n    case X509_V_ERR_INVALID_PURPOSE:\n    case X509_V_ERR_CERT_REJECTED:\n    case X509_V_ERR_INVALID_CA:\n\tssock->verify_status |= PJ_SSL_CERT_EINVALID_PURPOSE;\n\tbreak;\n\n    case X509_V_ERR_CERT_CHAIN_TOO_LONG: /* not really used */\n    case X509_V_ERR_PATH_LENGTH_EXCEEDED:\n\tssock->verify_status |= PJ_SSL_CERT_ECHAIN_TOO_LONG;\n\tbreak;\n\n    /* Unknown errors */\n    case X509_V_ERR_OUT_OF_MEM:\n    default:\n\tssock->verify_status |= PJ_SSL_CERT_EUNKNOWN;\n\tbreak;\n    }\n\n    /* When verification is not requested just return ok here, however\n     * application can still get the verification status.\n     */\n    if (PJ_FALSE == ssock->param.verify_peer)\n\tpreverify_ok = 1;\n\non_return:\n    return preverify_ok;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,17 +1,27 @@\n static int verify_cb(int preverify_ok, X509_STORE_CTX *x509_ctx)\n {\n-    pj_ssl_sock_t *ssock;\n-    SSL *ossl_ssl;\n+    pj_ssl_sock_t *ssock = NULL;\n+    SSL *ossl_ssl = NULL;\n     int err;\n \n     /* Get SSL instance */\n     ossl_ssl = X509_STORE_CTX_get_ex_data(x509_ctx, \n \t\t\t\t    SSL_get_ex_data_X509_STORE_CTX_idx());\n-    pj_assert(ossl_ssl);\n+    if (!ossl_ssl) {\n+\tPJ_LOG(1,(THIS_FILE,\n+\t\t  \"SSL verification callback failed to get SSL instance\"));\n+\tgoto on_return;\n+    }\n \n     /* Get SSL socket instance */\n     ssock = SSL_get_ex_data(ossl_ssl, sslsock_idx);\n-    pj_assert(ssock);\n+    if (!ssock) {\n+\t/* SSL socket may have been destroyed */\n+\tPJ_LOG(1,(THIS_FILE,\n+\t\t  \"SSL verification callback failed to get SSL socket \"\n+\t\t  \"instance (sslsock_idx=%d).\", sslsock_idx));\n+\tgoto on_return;\n+    }\n \n     /* Store verification status */\n     err = X509_STORE_CTX_get_error(x509_ctx);\n@@ -89,5 +99,6 @@\n     if (PJ_FALSE == ssock->param.verify_peer)\n \tpreverify_ok = 1;\n \n+on_return:\n     return preverify_ok;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    pj_ssl_sock_t *ssock;",
                "    SSL *ossl_ssl;",
                "    pj_assert(ossl_ssl);",
                "    pj_assert(ssock);"
            ],
            "added_lines": [
                "    pj_ssl_sock_t *ssock = NULL;",
                "    SSL *ossl_ssl = NULL;",
                "    if (!ossl_ssl) {",
                "\tPJ_LOG(1,(THIS_FILE,",
                "\t\t  \"SSL verification callback failed to get SSL instance\"));",
                "\tgoto on_return;",
                "    }",
                "    if (!ssock) {",
                "\t/* SSL socket may have been destroyed */",
                "\tPJ_LOG(1,(THIS_FILE,",
                "\t\t  \"SSL verification callback failed to get SSL socket \"",
                "\t\t  \"instance (sslsock_idx=%d).\", sslsock_idx));",
                "\tgoto on_return;",
                "    }",
                "on_return:"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-32686",
        "func_name": "pjsip/pjproject/STATUS_FROM_SSL_ERR2",
        "description": "PJSIP is a free and open source multimedia communication library written in C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In PJSIP before version 2.11.1, there are a couple of issues found in the SSL socket. First, a race condition between callback and destroy, due to the accepted socket having no group lock. Second, the SSL socket parent/listener may get destroyed during handshake. Both issues were reported to happen intermittently in heavy load TLS connections. They cause a crash, resulting in a denial of service. These are fixed in version 2.11.1.",
        "git_url": "https://github.com/pjsip/pjproject/commit/d5f95aa066f878b0aef6a64e60b61e8626e664cd",
        "commit_title": "Merge pull request from GHSA-cv8x-p47p-99wr",
        "commit_text": " * - Avoid SSL socket parent/listener getting destroyed during handshake by increasing parent's reference count. - Add missing SSL socket close when the newly accepted SSL socket is discarded in SIP TLS transport.  * - Fix silly mistake: accepted active socket created without group lock in SSL socket. - Replace assertion with normal validation check of SSL socket instance in OpenSSL verification callback (verify_cb()) to avoid crash, e.g: if somehow race condition with SSL socket destroy happens or OpenSSL application data index somehow gets corrupted.",
        "func_before": "static pj_status_t STATUS_FROM_SSL_ERR2(char *action, pj_ssl_sock_t *ssock,\n\t\t\t\t\tint ret, int err, int len)\n{\n    unsigned long ssl_err = err;\n\n    if (err == SSL_ERROR_SSL) {\n\tssl_err = ERR_peek_error();\n    }\n\n    /* Dig for more from OpenSSL error queue */\n    SSLLogErrors(action, ret, err, len, ssock);\n\n    ssock->last_err = ssl_err;\n    return GET_STATUS_FROM_SSL_ERR(ssl_err);\n}",
        "func": "static pj_status_t STATUS_FROM_SSL_ERR2(char *action, pj_ssl_sock_t *ssock,\n\t\t\t\t\tint ret, int err, int len)\n{\n    unsigned long ssl_err = err;\n\n    if (err == SSL_ERROR_SSL) {\n\tssl_err = ERR_peek_error();\n    }\n\n    /* Dig for more from OpenSSL error queue */\n    SSLLogErrors(action, ret, err, len, ssock);\n\n    if (ssock)\n\tssock->last_err = ssl_err;\n    return GET_STATUS_FROM_SSL_ERR(ssl_err);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,7 @@\n     /* Dig for more from OpenSSL error queue */\n     SSLLogErrors(action, ret, err, len, ssock);\n \n-    ssock->last_err = ssl_err;\n+    if (ssock)\n+\tssock->last_err = ssl_err;\n     return GET_STATUS_FROM_SSL_ERR(ssl_err);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    ssock->last_err = ssl_err;"
            ],
            "added_lines": [
                "    if (ssock)",
                "\tssock->last_err = ssl_err;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-32686",
        "func_name": "pjsip/pjproject/init_openssl",
        "description": "PJSIP is a free and open source multimedia communication library written in C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In PJSIP before version 2.11.1, there are a couple of issues found in the SSL socket. First, a race condition between callback and destroy, due to the accepted socket having no group lock. Second, the SSL socket parent/listener may get destroyed during handshake. Both issues were reported to happen intermittently in heavy load TLS connections. They cause a crash, resulting in a denial of service. These are fixed in version 2.11.1.",
        "git_url": "https://github.com/pjsip/pjproject/commit/d5f95aa066f878b0aef6a64e60b61e8626e664cd",
        "commit_title": "Merge pull request from GHSA-cv8x-p47p-99wr",
        "commit_text": " * - Avoid SSL socket parent/listener getting destroyed during handshake by increasing parent's reference count. - Add missing SSL socket close when the newly accepted SSL socket is discarded in SIP TLS transport.  * - Fix silly mistake: accepted active socket created without group lock in SSL socket. - Replace assertion with normal validation check of SSL socket instance in OpenSSL verification callback (verify_cb()) to avoid crash, e.g: if somehow race condition with SSL socket destroy happens or OpenSSL application data index somehow gets corrupted.",
        "func_before": "static pj_status_t init_openssl(void)\n{\n    pj_status_t status;\n\n    if (openssl_init_count)\n\treturn PJ_SUCCESS;\n\n    openssl_init_count = 1;\n\n    /* Register error subsystem */\n    status = pj_register_strerror(PJ_SSL_ERRNO_START, \n\t\t\t\t  PJ_SSL_ERRNO_SPACE_SIZE, \n\t\t\t\t  &ssl_strerror);\n    pj_assert(status == PJ_SUCCESS);\n\n    /* Init OpenSSL lib */\n#if USING_LIBRESSL || OPENSSL_VERSION_NUMBER < 0x10100000L\n    SSL_library_init();\n    SSL_load_error_strings();\n#else\n    OPENSSL_init_ssl(0, NULL);\n#endif\n#if OPENSSL_VERSION_NUMBER < 0x009080ffL\n    /* This is now synonym of SSL_library_init() */\n    OpenSSL_add_all_algorithms();\n#endif\n\n    /* Init available ciphers */\n    if (ssl_cipher_num == 0 || ssl_curves_num == 0) {\n\tSSL_METHOD *meth = NULL;\n\tSSL_CTX *ctx;\n\tSSL *ssl;\n\tSTACK_OF(SSL_CIPHER) *sk_cipher;\n\tSSL_SESSION *ssl_sess;\n\tunsigned i, n;\n\tint nid;\n\tconst char *cname;\n\n#if (USING_LIBRESSL && LIBRESSL_VERSION_NUMBER < 0x2020100fL)\\\n    || OPENSSL_VERSION_NUMBER < 0x10100000L\n\n\tmeth = (SSL_METHOD*)SSLv23_server_method();\n\tif (!meth)\n\t    meth = (SSL_METHOD*)TLSv1_server_method();\n#ifndef OPENSSL_NO_SSL3_METHOD\n\tif (!meth)\n\t    meth = (SSL_METHOD*)SSLv3_server_method();\n#endif\n#ifndef OPENSSL_NO_SSL2\n\tif (!meth)\n\t    meth = (SSL_METHOD*)SSLv2_server_method();\n#endif\n\n#else\n\t/* Specific version methods are deprecated in 1.1.0 */\n\tmeth = (SSL_METHOD*)TLS_method();\n#endif\n\n\tpj_assert(meth);\n\n\tctx=SSL_CTX_new(meth);\n\tSSL_CTX_set_cipher_list(ctx, \"ALL:COMPLEMENTOFALL\");\n\n\tssl = SSL_new(ctx);\n\n\tsk_cipher = SSL_get_ciphers(ssl);\n\n\tn = sk_SSL_CIPHER_num(sk_cipher);\n\tif (n > PJ_ARRAY_SIZE(ssl_ciphers) - ADDITIONAL_CIPHER_COUNT)\n\t    n = PJ_ARRAY_SIZE(ssl_ciphers) - ADDITIONAL_CIPHER_COUNT;\n\n\tfor (i = 0; i < n; ++i) {\n\t    const SSL_CIPHER *c;\n\t    c = sk_SSL_CIPHER_value(sk_cipher,i);\n\t    ssl_ciphers[i].id = (pj_ssl_cipher)\n\t\t\t\t    (pj_uint32_t)SSL_CIPHER_get_id(c) &\n\t\t\t\t    0x00FFFFFF;\n\t    ssl_ciphers[i].name = SSL_CIPHER_get_name(c);\n\t}\n\n\t/* Add cipher aliases not returned from SSL_get_ciphers() */\n\tfor (i = 0; i < ADDITIONAL_CIPHER_COUNT; ++i) {\n\t    ssl_ciphers[n++] = ADDITIONAL_CIPHERS[i];\n\t}\n\tssl_cipher_num = n;\n\n\tssl_sess = SSL_SESSION_new();\n\tSSL_set_session(ssl, ssl_sess);\n\n#if !USING_LIBRESSL && !defined(OPENSSL_NO_EC) \\\n    && OPENSSL_VERSION_NUMBER >= 0x1000200fL\n#if OPENSSL_VERSION_NUMBER >= 0x1010100fL\n\tssl_curves_num = EC_get_builtin_curves(NULL, 0);\n#else\n\tssl_curves_num = SSL_get_shared_curve(ssl,-1);\n\n\tif (ssl_curves_num > PJ_ARRAY_SIZE(ssl_curves))\n\t    ssl_curves_num = PJ_ARRAY_SIZE(ssl_curves);\n#endif\n\n\tif( ssl_curves_num > 0 ) {\n#if OPENSSL_VERSION_NUMBER >= 0x1010100fL\n\t    EC_builtin_curve * curves = NULL;\n\n\t    curves = OPENSSL_malloc((int)sizeof(*curves) * ssl_curves_num);\n\t    if (!EC_get_builtin_curves(curves, ssl_curves_num)) {\n\t\tOPENSSL_free(curves);\n\t\tcurves = NULL;\n\t\tssl_curves_num = 0;\n\t    }\n\n\t    n = ssl_curves_num;\n\t    ssl_curves_num = 0;\n\n\t    for (i = 0; i < n; i++) {\n\t\tnid = curves[i].nid;\n\n\t\tif ( 0 != get_cid_from_nid(nid) ) {\n\t\t    cname = OBJ_nid2sn(nid);\n\n\t\t    if (!cname)\n\t\t\tcname = OBJ_nid2sn(nid);\n\n\t\t    if (cname) {\n\t\t\tssl_curves[ssl_curves_num].id = get_cid_from_nid(nid);\n\t\t\tssl_curves[ssl_curves_num].name = cname;\n\n\t\t\tssl_curves_num++;\n\n\t\t\tif (ssl_curves_num >= PJ_SSL_SOCK_MAX_CURVES )\n\t\t\t    break;\n\t\t    }\n\t\t}\n\t    }\n\n\t    if(curves)\n\t\tOPENSSL_free(curves);\n#else\n\tfor (i = 0; i < ssl_curves_num; i++) {\n\t    nid = SSL_get_shared_curve(ssl, i);\n\n\t    if (nid & TLSEXT_nid_unknown) {\n\t\tcname = \"curve unknown\";\n\t\tnid &= 0xFFFF;\n\t    } else {\n\t\tcname = EC_curve_nid2nist(nid);\n\t\tif (!cname)\n\t\t    cname = OBJ_nid2sn(nid);\n\t    }\n\n\t    ssl_curves[i].id   = get_cid_from_nid(nid);\n\t    ssl_curves[i].name = cname;\n\t}\n#endif\n\n\t}\n#else\n\tPJ_UNUSED_ARG(nid);\n\tPJ_UNUSED_ARG(cname);\n\tssl_curves_num = 0;\n#endif\n\n\tSSL_free(ssl);\n\n\t/* On OpenSSL 1.1.1, omitting SSL_SESSION_free() will cause \n\t * memory leak (e.g: as reported by Address Sanitizer). But using\n\t * SSL_SESSION_free() may cause crash (due to double free?) on 1.0.x.\n\t * As OpenSSL docs specifies to not calling SSL_SESSION_free() after\n\t * SSL_free(), perhaps it is safer to obey this, the leak amount seems\n\t * to be relatively small (<500 bytes) and should occur once only in\n\t * the library lifetime.\n#if OPENSSL_VERSION_NUMBER >= 0x10101000L\n\tSSL_SESSION_free(ssl_sess);\n#endif\n\t */\n\n\tSSL_CTX_free(ctx);\n    }\n\n    /* Create OpenSSL application data index for SSL socket */\n    sslsock_idx = SSL_get_ex_new_index(0, \"SSL socket\", NULL, NULL, NULL);\n\n#if defined(PJ_SSL_SOCK_OSSL_USE_THREAD_CB) && \\\n    PJ_SSL_SOCK_OSSL_USE_THREAD_CB != 0 && OPENSSL_VERSION_NUMBER < 0x10100000L\n\n    status = init_ossl_lock();\n    if (status != PJ_SUCCESS)\n        return status;\n#endif\n\n    return status;\n}",
        "func": "static pj_status_t init_openssl(void)\n{\n    pj_status_t status;\n\n    if (openssl_init_count)\n\treturn PJ_SUCCESS;\n\n    openssl_init_count = 1;\n\n    /* Register error subsystem */\n    status = pj_register_strerror(PJ_SSL_ERRNO_START, \n\t\t\t\t  PJ_SSL_ERRNO_SPACE_SIZE, \n\t\t\t\t  &ssl_strerror);\n    pj_assert(status == PJ_SUCCESS);\n\n    /* Init OpenSSL lib */\n#if USING_LIBRESSL || OPENSSL_VERSION_NUMBER < 0x10100000L\n    SSL_library_init();\n    SSL_load_error_strings();\n#else\n    OPENSSL_init_ssl(0, NULL);\n#endif\n#if OPENSSL_VERSION_NUMBER < 0x009080ffL\n    /* This is now synonym of SSL_library_init() */\n    OpenSSL_add_all_algorithms();\n#endif\n\n    /* Init available ciphers */\n    if (ssl_cipher_num == 0 || ssl_curves_num == 0) {\n\tSSL_METHOD *meth = NULL;\n\tSSL_CTX *ctx;\n\tSSL *ssl;\n\tSTACK_OF(SSL_CIPHER) *sk_cipher;\n\tSSL_SESSION *ssl_sess;\n\tunsigned i, n;\n\tint nid;\n\tconst char *cname;\n\n#if (USING_LIBRESSL && LIBRESSL_VERSION_NUMBER < 0x2020100fL)\\\n    || OPENSSL_VERSION_NUMBER < 0x10100000L\n\n\tmeth = (SSL_METHOD*)SSLv23_server_method();\n\tif (!meth)\n\t    meth = (SSL_METHOD*)TLSv1_server_method();\n#ifndef OPENSSL_NO_SSL3_METHOD\n\tif (!meth)\n\t    meth = (SSL_METHOD*)SSLv3_server_method();\n#endif\n#ifndef OPENSSL_NO_SSL2\n\tif (!meth)\n\t    meth = (SSL_METHOD*)SSLv2_server_method();\n#endif\n\n#else\n\t/* Specific version methods are deprecated in 1.1.0 */\n\tmeth = (SSL_METHOD*)TLS_method();\n#endif\n\n\tpj_assert(meth);\n\n\tctx=SSL_CTX_new(meth);\n\tSSL_CTX_set_cipher_list(ctx, \"ALL:COMPLEMENTOFALL\");\n\n\tssl = SSL_new(ctx);\n\n\tsk_cipher = SSL_get_ciphers(ssl);\n\n\tn = sk_SSL_CIPHER_num(sk_cipher);\n\tif (n > PJ_ARRAY_SIZE(ssl_ciphers) - ADDITIONAL_CIPHER_COUNT)\n\t    n = PJ_ARRAY_SIZE(ssl_ciphers) - ADDITIONAL_CIPHER_COUNT;\n\n\tfor (i = 0; i < n; ++i) {\n\t    const SSL_CIPHER *c;\n\t    c = sk_SSL_CIPHER_value(sk_cipher,i);\n\t    ssl_ciphers[i].id = (pj_ssl_cipher)\n\t\t\t\t    (pj_uint32_t)SSL_CIPHER_get_id(c) &\n\t\t\t\t    0x00FFFFFF;\n\t    ssl_ciphers[i].name = SSL_CIPHER_get_name(c);\n\t}\n\n\t/* Add cipher aliases not returned from SSL_get_ciphers() */\n\tfor (i = 0; i < ADDITIONAL_CIPHER_COUNT; ++i) {\n\t    ssl_ciphers[n++] = ADDITIONAL_CIPHERS[i];\n\t}\n\tssl_cipher_num = n;\n\n\tssl_sess = SSL_SESSION_new();\n\tSSL_set_session(ssl, ssl_sess);\n\n#if !USING_LIBRESSL && !defined(OPENSSL_NO_EC) \\\n    && OPENSSL_VERSION_NUMBER >= 0x1000200fL\n#if OPENSSL_VERSION_NUMBER >= 0x1010100fL\n\tssl_curves_num = EC_get_builtin_curves(NULL, 0);\n#else\n\tssl_curves_num = SSL_get_shared_curve(ssl,-1);\n\n\tif (ssl_curves_num > PJ_ARRAY_SIZE(ssl_curves))\n\t    ssl_curves_num = PJ_ARRAY_SIZE(ssl_curves);\n#endif\n\n\tif( ssl_curves_num > 0 ) {\n#if OPENSSL_VERSION_NUMBER >= 0x1010100fL\n\t    EC_builtin_curve * curves = NULL;\n\n\t    curves = OPENSSL_malloc((int)sizeof(*curves) * ssl_curves_num);\n\t    if (!EC_get_builtin_curves(curves, ssl_curves_num)) {\n\t\tOPENSSL_free(curves);\n\t\tcurves = NULL;\n\t\tssl_curves_num = 0;\n\t    }\n\n\t    n = ssl_curves_num;\n\t    ssl_curves_num = 0;\n\n\t    for (i = 0; i < n; i++) {\n\t\tnid = curves[i].nid;\n\n\t\tif ( 0 != get_cid_from_nid(nid) ) {\n\t\t    cname = OBJ_nid2sn(nid);\n\n\t\t    if (!cname)\n\t\t\tcname = OBJ_nid2sn(nid);\n\n\t\t    if (cname) {\n\t\t\tssl_curves[ssl_curves_num].id = get_cid_from_nid(nid);\n\t\t\tssl_curves[ssl_curves_num].name = cname;\n\n\t\t\tssl_curves_num++;\n\n\t\t\tif (ssl_curves_num >= PJ_SSL_SOCK_MAX_CURVES )\n\t\t\t    break;\n\t\t    }\n\t\t}\n\t    }\n\n\t    if(curves)\n\t\tOPENSSL_free(curves);\n#else\n\tfor (i = 0; i < ssl_curves_num; i++) {\n\t    nid = SSL_get_shared_curve(ssl, i);\n\n\t    if (nid & TLSEXT_nid_unknown) {\n\t\tcname = \"curve unknown\";\n\t\tnid &= 0xFFFF;\n\t    } else {\n\t\tcname = EC_curve_nid2nist(nid);\n\t\tif (!cname)\n\t\t    cname = OBJ_nid2sn(nid);\n\t    }\n\n\t    ssl_curves[i].id   = get_cid_from_nid(nid);\n\t    ssl_curves[i].name = cname;\n\t}\n#endif\n\n\t}\n#else\n\tPJ_UNUSED_ARG(nid);\n\tPJ_UNUSED_ARG(cname);\n\tssl_curves_num = 0;\n#endif\n\n\tSSL_free(ssl);\n\n\t/* On OpenSSL 1.1.1, omitting SSL_SESSION_free() will cause \n\t * memory leak (e.g: as reported by Address Sanitizer). But using\n\t * SSL_SESSION_free() may cause crash (due to double free?) on 1.0.x.\n\t * As OpenSSL docs specifies to not calling SSL_SESSION_free() after\n\t * SSL_free(), perhaps it is safer to obey this, the leak amount seems\n\t * to be relatively small (<500 bytes) and should occur once only in\n\t * the library lifetime.\n#if OPENSSL_VERSION_NUMBER >= 0x10101000L\n\tSSL_SESSION_free(ssl_sess);\n#endif\n\t */\n\n\tSSL_CTX_free(ctx);\n    }\n\n    /* Create OpenSSL application data index for SSL socket */\n    sslsock_idx = SSL_get_ex_new_index(0, \"SSL socket\", NULL, NULL, NULL);\n    if (sslsock_idx == -1) {\n\tstatus = STATUS_FROM_SSL_ERR2(\"Init\", NULL, -1, ERR_get_error(), 0);\n\tPJ_LOG(1,(THIS_FILE,\n\t       \"Fatal error: failed to get application data index for \"\n\t       \"SSL socket\"));\n\treturn status;\n    }\n\n#if defined(PJ_SSL_SOCK_OSSL_USE_THREAD_CB) && \\\n    PJ_SSL_SOCK_OSSL_USE_THREAD_CB != 0 && OPENSSL_VERSION_NUMBER < 0x10100000L\n\n    status = init_ossl_lock();\n    if (status != PJ_SUCCESS)\n        return status;\n#endif\n\n    return status;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -179,6 +179,13 @@\n \n     /* Create OpenSSL application data index for SSL socket */\n     sslsock_idx = SSL_get_ex_new_index(0, \"SSL socket\", NULL, NULL, NULL);\n+    if (sslsock_idx == -1) {\n+\tstatus = STATUS_FROM_SSL_ERR2(\"Init\", NULL, -1, ERR_get_error(), 0);\n+\tPJ_LOG(1,(THIS_FILE,\n+\t       \"Fatal error: failed to get application data index for \"\n+\t       \"SSL socket\"));\n+\treturn status;\n+    }\n \n #if defined(PJ_SSL_SOCK_OSSL_USE_THREAD_CB) && \\\n     PJ_SSL_SOCK_OSSL_USE_THREAD_CB != 0 && OPENSSL_VERSION_NUMBER < 0x10100000L",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (sslsock_idx == -1) {",
                "\tstatus = STATUS_FROM_SSL_ERR2(\"Init\", NULL, -1, ERR_get_error(), 0);",
                "\tPJ_LOG(1,(THIS_FILE,",
                "\t       \"Fatal error: failed to get application data index for \"",
                "\t       \"SSL socket\"));",
                "\treturn status;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-32686",
        "func_name": "pjsip/pjproject/ssl_reset_sock_state",
        "description": "PJSIP is a free and open source multimedia communication library written in C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In PJSIP before version 2.11.1, there are a couple of issues found in the SSL socket. First, a race condition between callback and destroy, due to the accepted socket having no group lock. Second, the SSL socket parent/listener may get destroyed during handshake. Both issues were reported to happen intermittently in heavy load TLS connections. They cause a crash, resulting in a denial of service. These are fixed in version 2.11.1.",
        "git_url": "https://github.com/pjsip/pjproject/commit/d5f95aa066f878b0aef6a64e60b61e8626e664cd",
        "commit_title": "Merge pull request from GHSA-cv8x-p47p-99wr",
        "commit_text": " * - Avoid SSL socket parent/listener getting destroyed during handshake by increasing parent's reference count. - Add missing SSL socket close when the newly accepted SSL socket is discarded in SIP TLS transport.  * - Fix silly mistake: accepted active socket created without group lock in SSL socket. - Replace assertion with normal validation check of SSL socket instance in OpenSSL verification callback (verify_cb()) to avoid crash, e.g: if somehow race condition with SSL socket destroy happens or OpenSSL application data index somehow gets corrupted.",
        "func_before": "static void ssl_reset_sock_state(pj_ssl_sock_t *ssock)\n{\n    ossl_sock_t *ossock = (ossl_sock_t *)ssock;\n    /**\n     * Avoid calling SSL_shutdown() if handshake wasn't completed.\n     * OpenSSL 1.0.2f complains if SSL_shutdown() is called during an\n     * SSL handshake, while previous versions always return 0.\n     */\n    if (ossock->ossl_ssl && SSL_in_init(ossock->ossl_ssl) == 0) {\n\tint ret = SSL_shutdown(ossock->ossl_ssl);\n\tif (ret == 0) {\n\t    /* Flush data to send close notify. */\n\t    flush_circ_buf_output(ssock, &ssock->shutdown_op_key, 0, 0);\n\t}\n    }\n\n    pj_lock_acquire(ssock->write_mutex);\n    ssock->ssl_state = SSL_STATE_NULL;\n    pj_lock_release(ssock->write_mutex);\n\n    ssl_close_sockets(ssock);\n\n    /* Upon error, OpenSSL may leave any error description in the thread \n     * error queue, which sometime may cause next call to SSL API returning\n     * false error alarm, e.g: in Linux, SSL_CTX_use_certificate_chain_file()\n     * returning false error after a handshake error (in different SSL_CTX!).\n     * For now, just clear thread error queue here.\n     */\n    ERR_clear_error();\n}",
        "func": "static void ssl_reset_sock_state(pj_ssl_sock_t *ssock)\n{\n    ossl_sock_t *ossock = (ossl_sock_t *)ssock;\n\n    /* Detach from SSL instance */\n    if (ossock->ossl_ssl) {\n\tSSL_set_ex_data(ossock->ossl_ssl, sslsock_idx, NULL);\n    }\n\n    /**\n     * Avoid calling SSL_shutdown() if handshake wasn't completed.\n     * OpenSSL 1.0.2f complains if SSL_shutdown() is called during an\n     * SSL handshake, while previous versions always return 0.\n     */\n    if (ossock->ossl_ssl && SSL_in_init(ossock->ossl_ssl) == 0) {\n\tint ret = SSL_shutdown(ossock->ossl_ssl);\n\tif (ret == 0) {\n\t    /* Flush data to send close notify. */\n\t    flush_circ_buf_output(ssock, &ssock->shutdown_op_key, 0, 0);\n\t}\n    }\n\n    pj_lock_acquire(ssock->write_mutex);\n    ssock->ssl_state = SSL_STATE_NULL;\n    pj_lock_release(ssock->write_mutex);\n\n    ssl_close_sockets(ssock);\n\n    /* Upon error, OpenSSL may leave any error description in the thread \n     * error queue, which sometime may cause next call to SSL API returning\n     * false error alarm, e.g: in Linux, SSL_CTX_use_certificate_chain_file()\n     * returning false error after a handshake error (in different SSL_CTX!).\n     * For now, just clear thread error queue here.\n     */\n    ERR_clear_error();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,12 @@\n static void ssl_reset_sock_state(pj_ssl_sock_t *ssock)\n {\n     ossl_sock_t *ossock = (ossl_sock_t *)ssock;\n+\n+    /* Detach from SSL instance */\n+    if (ossock->ossl_ssl) {\n+\tSSL_set_ex_data(ossock->ossl_ssl, sslsock_idx, NULL);\n+    }\n+\n     /**\n      * Avoid calling SSL_shutdown() if handshake wasn't completed.\n      * OpenSSL 1.0.2f complains if SSL_shutdown() is called during an",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    /* Detach from SSL instance */",
                "    if (ossock->ossl_ssl) {",
                "\tSSL_set_ex_data(ossock->ossl_ssl, sslsock_idx, NULL);",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-32686",
        "func_name": "pjsip/pjproject/on_handshake_complete",
        "description": "PJSIP is a free and open source multimedia communication library written in C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In PJSIP before version 2.11.1, there are a couple of issues found in the SSL socket. First, a race condition between callback and destroy, due to the accepted socket having no group lock. Second, the SSL socket parent/listener may get destroyed during handshake. Both issues were reported to happen intermittently in heavy load TLS connections. They cause a crash, resulting in a denial of service. These are fixed in version 2.11.1.",
        "git_url": "https://github.com/pjsip/pjproject/commit/d5f95aa066f878b0aef6a64e60b61e8626e664cd",
        "commit_title": "Merge pull request from GHSA-cv8x-p47p-99wr",
        "commit_text": " * - Avoid SSL socket parent/listener getting destroyed during handshake by increasing parent's reference count. - Add missing SSL socket close when the newly accepted SSL socket is discarded in SIP TLS transport.  * - Fix silly mistake: accepted active socket created without group lock in SSL socket. - Replace assertion with normal validation check of SSL socket instance in OpenSSL verification callback (verify_cb()) to avoid crash, e.g: if somehow race condition with SSL socket destroy happens or OpenSSL application data index somehow gets corrupted.",
        "func_before": "static pj_bool_t on_handshake_complete(pj_ssl_sock_t *ssock, \n\t\t\t\t       pj_status_t status)\n{\n    /* Cancel handshake timer */\n    if (ssock->timer.id == TIMER_HANDSHAKE_TIMEOUT) {\n\tpj_timer_heap_cancel(ssock->param.timer_heap, &ssock->timer);\n\tssock->timer.id = TIMER_NONE;\n    }\n\n    /* Update certificates info on successful handshake */\n    if (status == PJ_SUCCESS)\n\tssl_update_certs_info(ssock);\n\n    /* Accepting */\n    if (ssock->is_server) {\n\tif (status != PJ_SUCCESS) {\n\t    /* Handshake failed in accepting, destroy our self silently. */\n\n\t    char buf[PJ_INET6_ADDRSTRLEN+10];\n\n\t    PJ_PERROR(3,(ssock->pool->obj_name, status,\n\t\t\t \"Handshake failed in accepting %s\",\n\t\t\t pj_sockaddr_print(&ssock->rem_addr, buf,\n\t\t\t\t\t   sizeof(buf), 3)));\n\n\t    if (ssock->param.cb.on_accept_complete2) {\n\t\t(*ssock->param.cb.on_accept_complete2) \n\t\t      (ssock->parent, ssock, (pj_sockaddr_t*)&ssock->rem_addr, \n\t\t      pj_sockaddr_get_len((pj_sockaddr_t*)&ssock->rem_addr), \n\t\t      status);\n\t    }\n\n\t    /* Originally, this is a workaround for ticket #985. However,\n\t     * a race condition may occur in multiple worker threads\n\t     * environment when we are destroying SSL objects while other\n\t     * threads are still accessing them.\n\t     * Please see ticket #1930 for more info.\n\t     */\n#if 1 //(defined(PJ_WIN32) && PJ_WIN32!=0)||(defined(PJ_WIN64) && PJ_WIN64!=0)\n\t    if (ssock->param.timer_heap) {\n\t\tpj_time_val interval = {0, PJ_SSL_SOCK_DELAYED_CLOSE_TIMEOUT};\n\t\tpj_status_t status1;\n\n\t\tssock->ssl_state = SSL_STATE_NULL;\n\t\tssl_close_sockets(ssock);\n\n\t\tif (ssock->timer.id != TIMER_NONE) {\n\t\t    pj_timer_heap_cancel(ssock->param.timer_heap,\n\t\t\t\t\t &ssock->timer);\n\t\t}\n\t\tpj_time_val_normalize(&interval);\n\t\tstatus1 = pj_timer_heap_schedule_w_grp_lock(\n\t\t\t\t\t\t ssock->param.timer_heap, \n\t\t\t\t\t\t &ssock->timer,\n\t\t\t\t\t\t &interval,\n\t\t\t\t\t\t TIMER_CLOSE,\n\t\t\t\t\t\t ssock->param.grp_lock);\n\t\tif (status1 != PJ_SUCCESS) {\n\t    \t    PJ_PERROR(3,(ssock->pool->obj_name, status,\n\t\t\t\t \"Failed to schedule a delayed close. \"\n\t\t\t\t \"Race condition may occur.\"));\n\t\t    ssock->timer.id = TIMER_NONE;\n\t\t    pj_ssl_sock_close(ssock);\n\t\t}\n\t    } else {\n\t\tpj_ssl_sock_close(ssock);\n\t    }\n#else\n\t    {\n\t\tpj_ssl_sock_close(ssock);\n\t    }\n#endif\n\n\t    return PJ_FALSE;\n\t}\n\t/* Notify application the newly accepted SSL socket */\n\tif (ssock->param.cb.on_accept_complete2) {\n\t    pj_bool_t ret;\n\t    ret = (*ssock->param.cb.on_accept_complete2) \n\t\t    (ssock->parent, ssock, (pj_sockaddr_t*)&ssock->rem_addr, \n\t\t    pj_sockaddr_get_len((pj_sockaddr_t*)&ssock->rem_addr), \n\t\t    status);\n\t    if (ret == PJ_FALSE)\n\t\treturn PJ_FALSE;\t\n\t} else if (ssock->param.cb.on_accept_complete) {\n\t    pj_bool_t ret;\n\t    ret = (*ssock->param.cb.on_accept_complete)\n\t\t      (ssock->parent, ssock, (pj_sockaddr_t*)&ssock->rem_addr,\n\t\t       pj_sockaddr_get_len((pj_sockaddr_t*)&ssock->rem_addr));\n\t    if (ret == PJ_FALSE)\n\t\treturn PJ_FALSE;\n\t}\n    }\n\n    /* Connecting */\n    else {\n\t/* On failure, reset SSL socket state first, as app may try to \n\t * reconnect in the callback.\n\t */\n\tif (status != PJ_SUCCESS) {\n\t    /* Server disconnected us, possibly due to SSL nego failure */\n\t    ssl_reset_sock_state(ssock);\n\t}\n\tif (ssock->param.cb.on_connect_complete) {\n\t    pj_bool_t ret;\n\t    ret = (*ssock->param.cb.on_connect_complete)(ssock, status);\n\t    if (ret == PJ_FALSE)\n\t\treturn PJ_FALSE;\n\t}\n    }\n\n    return PJ_TRUE;\n}",
        "func": "static pj_bool_t on_handshake_complete(pj_ssl_sock_t *ssock, \n\t\t\t\t       pj_status_t status)\n{\n    /* Cancel handshake timer */\n    if (ssock->timer.id == TIMER_HANDSHAKE_TIMEOUT) {\n\tpj_timer_heap_cancel(ssock->param.timer_heap, &ssock->timer);\n\tssock->timer.id = TIMER_NONE;\n    }\n\n    /* Update certificates info on successful handshake */\n    if (status == PJ_SUCCESS)\n\tssl_update_certs_info(ssock);\n\n    /* Accepting */\n    if (ssock->is_server) {\n\tpj_bool_t ret = PJ_TRUE;\n\n\tif (status != PJ_SUCCESS) {\n\t    /* Handshake failed in accepting, destroy our self silently. */\n\n\t    char buf[PJ_INET6_ADDRSTRLEN+10];\n\n\t    PJ_PERROR(3,(ssock->pool->obj_name, status,\n\t\t\t \"Handshake failed in accepting %s\",\n\t\t\t pj_sockaddr_print(&ssock->rem_addr, buf,\n\t\t\t\t\t   sizeof(buf), 3)));\n\n\t    if (ssock->param.cb.on_accept_complete2) {\n\t\t(*ssock->param.cb.on_accept_complete2) \n\t\t      (ssock->parent, ssock, (pj_sockaddr_t*)&ssock->rem_addr, \n\t\t      pj_sockaddr_get_len((pj_sockaddr_t*)&ssock->rem_addr), \n\t\t      status);\n\t    }\n\n\t    /* Decrement ref count of parent */\n\t    if (ssock->parent->param.grp_lock) {\n\t\tpj_grp_lock_dec_ref(ssock->parent->param.grp_lock);\n\t\tssock->parent = NULL;\n\t    }\n\n\t    /* Originally, this is a workaround for ticket #985. However,\n\t     * a race condition may occur in multiple worker threads\n\t     * environment when we are destroying SSL objects while other\n\t     * threads are still accessing them.\n\t     * Please see ticket #1930 for more info.\n\t     */\n#if 1 //(defined(PJ_WIN32) && PJ_WIN32!=0)||(defined(PJ_WIN64) && PJ_WIN64!=0)\n\t    if (ssock->param.timer_heap) {\n\t\tpj_time_val interval = {0, PJ_SSL_SOCK_DELAYED_CLOSE_TIMEOUT};\n\t\tpj_status_t status1;\n\n\t\tssock->ssl_state = SSL_STATE_NULL;\n\t\tssl_close_sockets(ssock);\n\n\t\tif (ssock->timer.id != TIMER_NONE) {\n\t\t    pj_timer_heap_cancel(ssock->param.timer_heap,\n\t\t\t\t\t &ssock->timer);\n\t\t}\n\t\tpj_time_val_normalize(&interval);\n\t\tstatus1 = pj_timer_heap_schedule_w_grp_lock(\n\t\t\t\t\t\t ssock->param.timer_heap, \n\t\t\t\t\t\t &ssock->timer,\n\t\t\t\t\t\t &interval,\n\t\t\t\t\t\t TIMER_CLOSE,\n\t\t\t\t\t\t ssock->param.grp_lock);\n\t\tif (status1 != PJ_SUCCESS) {\n\t    \t    PJ_PERROR(3,(ssock->pool->obj_name, status,\n\t\t\t\t \"Failed to schedule a delayed close. \"\n\t\t\t\t \"Race condition may occur.\"));\n\t\t    ssock->timer.id = TIMER_NONE;\n\t\t    pj_ssl_sock_close(ssock);\n\t\t}\n\t    } else {\n\t\tpj_ssl_sock_close(ssock);\n\t    }\n#else\n\t    {\n\t\tpj_ssl_sock_close(ssock);\n\t    }\n#endif\n\n\t    return PJ_FALSE;\n\t}\n\n\t/* Notify application the newly accepted SSL socket */\n\tif (ssock->param.cb.on_accept_complete2) {\n\t    ret = (*ssock->param.cb.on_accept_complete2) \n\t\t    (ssock->parent, ssock, (pj_sockaddr_t*)&ssock->rem_addr, \n\t\t    pj_sockaddr_get_len((pj_sockaddr_t*)&ssock->rem_addr), \n\t\t    status);\n\t} else if (ssock->param.cb.on_accept_complete) {\n\t    ret = (*ssock->param.cb.on_accept_complete)\n\t\t      (ssock->parent, ssock, (pj_sockaddr_t*)&ssock->rem_addr,\n\t\t       pj_sockaddr_get_len((pj_sockaddr_t*)&ssock->rem_addr));\n\t}\n\n\t/* Decrement ref count of parent and reset parent (we don't need it\n\t * anymore, right?).\n\t */\n\tif (ssock->parent->param.grp_lock) {\n\t    pj_grp_lock_dec_ref(ssock->parent->param.grp_lock);\n\t    ssock->parent = NULL;\n\t}\n\n\tif (ret == PJ_FALSE)\n\t    return PJ_FALSE;\n    }\n\n    /* Connecting */\n    else {\n\t/* On failure, reset SSL socket state first, as app may try to \n\t * reconnect in the callback.\n\t */\n\tif (status != PJ_SUCCESS) {\n\t    /* Server disconnected us, possibly due to SSL nego failure */\n\t    ssl_reset_sock_state(ssock);\n\t}\n\tif (ssock->param.cb.on_connect_complete) {\n\t    pj_bool_t ret;\n\t    ret = (*ssock->param.cb.on_connect_complete)(ssock, status);\n\t    if (ret == PJ_FALSE)\n\t\treturn PJ_FALSE;\n\t}\n    }\n\n    return PJ_TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,6 +13,8 @@\n \n     /* Accepting */\n     if (ssock->is_server) {\n+\tpj_bool_t ret = PJ_TRUE;\n+\n \tif (status != PJ_SUCCESS) {\n \t    /* Handshake failed in accepting, destroy our self silently. */\n \n@@ -28,6 +30,12 @@\n \t\t      (ssock->parent, ssock, (pj_sockaddr_t*)&ssock->rem_addr, \n \t\t      pj_sockaddr_get_len((pj_sockaddr_t*)&ssock->rem_addr), \n \t\t      status);\n+\t    }\n+\n+\t    /* Decrement ref count of parent */\n+\t    if (ssock->parent->param.grp_lock) {\n+\t\tpj_grp_lock_dec_ref(ssock->parent->param.grp_lock);\n+\t\tssock->parent = NULL;\n \t    }\n \n \t    /* Originally, this is a workaround for ticket #985. However,\n@@ -73,23 +81,29 @@\n \n \t    return PJ_FALSE;\n \t}\n+\n \t/* Notify application the newly accepted SSL socket */\n \tif (ssock->param.cb.on_accept_complete2) {\n-\t    pj_bool_t ret;\n \t    ret = (*ssock->param.cb.on_accept_complete2) \n \t\t    (ssock->parent, ssock, (pj_sockaddr_t*)&ssock->rem_addr, \n \t\t    pj_sockaddr_get_len((pj_sockaddr_t*)&ssock->rem_addr), \n \t\t    status);\n-\t    if (ret == PJ_FALSE)\n-\t\treturn PJ_FALSE;\t\n \t} else if (ssock->param.cb.on_accept_complete) {\n-\t    pj_bool_t ret;\n \t    ret = (*ssock->param.cb.on_accept_complete)\n \t\t      (ssock->parent, ssock, (pj_sockaddr_t*)&ssock->rem_addr,\n \t\t       pj_sockaddr_get_len((pj_sockaddr_t*)&ssock->rem_addr));\n-\t    if (ret == PJ_FALSE)\n-\t\treturn PJ_FALSE;\n \t}\n+\n+\t/* Decrement ref count of parent and reset parent (we don't need it\n+\t * anymore, right?).\n+\t */\n+\tif (ssock->parent->param.grp_lock) {\n+\t    pj_grp_lock_dec_ref(ssock->parent->param.grp_lock);\n+\t    ssock->parent = NULL;\n+\t}\n+\n+\tif (ret == PJ_FALSE)\n+\t    return PJ_FALSE;\n     }\n \n     /* Connecting */",
        "diff_line_info": {
            "deleted_lines": [
                "\t    pj_bool_t ret;",
                "\t    if (ret == PJ_FALSE)",
                "\t\treturn PJ_FALSE;\t",
                "\t    pj_bool_t ret;",
                "\t    if (ret == PJ_FALSE)",
                "\t\treturn PJ_FALSE;"
            ],
            "added_lines": [
                "\tpj_bool_t ret = PJ_TRUE;",
                "",
                "\t    }",
                "",
                "\t    /* Decrement ref count of parent */",
                "\t    if (ssock->parent->param.grp_lock) {",
                "\t\tpj_grp_lock_dec_ref(ssock->parent->param.grp_lock);",
                "\t\tssock->parent = NULL;",
                "",
                "",
                "\t/* Decrement ref count of parent and reset parent (we don't need it",
                "\t * anymore, right?).",
                "\t */",
                "\tif (ssock->parent->param.grp_lock) {",
                "\t    pj_grp_lock_dec_ref(ssock->parent->param.grp_lock);",
                "\t    ssock->parent = NULL;",
                "\t}",
                "",
                "\tif (ret == PJ_FALSE)",
                "\t    return PJ_FALSE;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-32686",
        "func_name": "pjsip/pjproject/ssock_on_accept_complete",
        "description": "PJSIP is a free and open source multimedia communication library written in C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In PJSIP before version 2.11.1, there are a couple of issues found in the SSL socket. First, a race condition between callback and destroy, due to the accepted socket having no group lock. Second, the SSL socket parent/listener may get destroyed during handshake. Both issues were reported to happen intermittently in heavy load TLS connections. They cause a crash, resulting in a denial of service. These are fixed in version 2.11.1.",
        "git_url": "https://github.com/pjsip/pjproject/commit/d5f95aa066f878b0aef6a64e60b61e8626e664cd",
        "commit_title": "Merge pull request from GHSA-cv8x-p47p-99wr",
        "commit_text": " * - Avoid SSL socket parent/listener getting destroyed during handshake by increasing parent's reference count. - Add missing SSL socket close when the newly accepted SSL socket is discarded in SIP TLS transport.  * - Fix silly mistake: accepted active socket created without group lock in SSL socket. - Replace assertion with normal validation check of SSL socket instance in OpenSSL verification callback (verify_cb()) to avoid crash, e.g: if somehow race condition with SSL socket destroy happens or OpenSSL application data index somehow gets corrupted.",
        "func_before": "static pj_bool_t ssock_on_accept_complete (pj_ssl_sock_t *ssock_parent,\n\t\t\t\t\t   pj_sock_t newsock,\n\t\t\t\t\t   void *newconn,\n\t\t\t\t\t   const pj_sockaddr_t *src_addr,\n\t\t\t\t\t   int src_addr_len,\n\t\t\t\t\t   pj_status_t accept_status)\n{\n    pj_ssl_sock_t *ssock;\n#ifndef SSL_SOCK_IMP_USE_OWN_NETWORK\n    pj_activesock_cb asock_cb;\n#endif\n    pj_activesock_cfg asock_cfg;\n    unsigned i;\n    pj_status_t status;\n\n#ifndef SSL_SOCK_IMP_USE_OWN_NETWORK\n    PJ_UNUSED_ARG(newconn);\n#endif\n\n    if (accept_status != PJ_SUCCESS) {\n\tif (ssock_parent->param.cb.on_accept_complete2) {\n\t    (*ssock_parent->param.cb.on_accept_complete2)(ssock_parent, NULL,\n\t\t\t\t\t\t    \t  src_addr,\n\t\t\t\t\t\t    \t  src_addr_len,\n\t\t\t\t\t\t    \t  accept_status);\n\t}\n\treturn PJ_TRUE;\n    }\n\n    /* Create new SSL socket instance */\n    status = pj_ssl_sock_create(ssock_parent->pool,\n\t\t\t\t&ssock_parent->newsock_param, &ssock);\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n\n    /* Update new SSL socket attributes */\n    ssock->sock = newsock;\n    ssock->parent = ssock_parent;\n    ssock->is_server = PJ_TRUE;\n    if (ssock_parent->cert) {\n\tstatus = pj_ssl_sock_set_certificate(ssock, ssock->pool, \n\t\t\t\t\t     ssock_parent->cert);\n\tif (status != PJ_SUCCESS)\n\t    goto on_return;\n    }\n\n    /* Set local address */\n    ssock->addr_len = src_addr_len;\n    pj_sockaddr_cp(&ssock->local_addr, &ssock_parent->local_addr);\n\n    /* Set remote address */\n    pj_sockaddr_cp(&ssock->rem_addr, src_addr);\n\n    /* Create SSL context */\n    status = ssl_create(ssock);\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n\n    /* Prepare read buffer */\n    ssock->asock_rbuf = (void**)pj_pool_calloc(ssock->pool, \n\t\t\t\t\t       ssock->param.async_cnt,\n\t\t\t\t\t       sizeof(void*));\n    if (!ssock->asock_rbuf)\n        return PJ_ENOMEM;\n\n    for (i = 0; i<ssock->param.async_cnt; ++i) {\n\tssock->asock_rbuf[i] = (void*) pj_pool_alloc(\n\t\t\t\t\t    ssock->pool, \n\t\t\t\t\t    ssock->param.read_buffer_size + \n\t\t\t\t\t    sizeof(read_data_t*));\n        if (!ssock->asock_rbuf[i])\n            return PJ_ENOMEM;\n    }\n\n    /* If listener socket has group lock, automatically create group lock\n     * for the new socket.\n     */\n    if (ssock_parent->param.grp_lock) {\n\tpj_grp_lock_t *glock;\n\n\tstatus = pj_grp_lock_create(ssock->pool, NULL, &glock);\n\tif (status != PJ_SUCCESS)\n\t    goto on_return;\n\n\tpj_grp_lock_add_ref(glock);\n\tasock_cfg.grp_lock = ssock->param.grp_lock = glock;\n\tpj_grp_lock_add_handler(ssock->param.grp_lock, ssock->pool, ssock,\n\t\t\t\tssl_on_destroy);\n    }\n\n#ifdef SSL_SOCK_IMP_USE_OWN_NETWORK\n    status = network_setup_connection(ssock, newconn);\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n\n#else\n    /* Apply QoS, if specified */\n    status = pj_sock_apply_qos2(ssock->sock, ssock->param.qos_type,\n\t\t\t\t&ssock->param.qos_params, 1, \n\t\t\t\tssock->pool->obj_name, NULL);\n    if (status != PJ_SUCCESS && !ssock->param.qos_ignore_error)\n\tgoto on_return;\n\n    /* Apply socket options, if specified */\n    if (ssock->param.sockopt_params.cnt) {\n\tstatus = pj_sock_setsockopt_params(ssock->sock, \n\t\t\t\t\t   &ssock->param.sockopt_params);\n\tif (status != PJ_SUCCESS && !ssock->param.sockopt_ignore_error)\n\t    goto on_return;\n    }\n\n    /* Create active socket */\n    pj_activesock_cfg_default(&asock_cfg);\n    asock_cfg.async_cnt = ssock->param.async_cnt;\n    asock_cfg.concurrency = ssock->param.concurrency;\n    asock_cfg.whole_data = PJ_TRUE;\n\n    pj_bzero(&asock_cb, sizeof(asock_cb));\n    asock_cb.on_data_read = asock_on_data_read;\n    asock_cb.on_data_sent = asock_on_data_sent;\n\n    status = pj_activesock_create(ssock->pool,\n\t\t\t\t  ssock->sock, \n\t\t\t\t  ssock->param.sock_type,\n\t\t\t\t  &asock_cfg,\n\t\t\t\t  ssock->param.ioqueue, \n\t\t\t\t  &asock_cb,\n\t\t\t\t  ssock,\n\t\t\t\t  &ssock->asock);\n\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n\n    /* Start read */\n    status = pj_activesock_start_read2(ssock->asock, ssock->pool, \n\t\t\t\t       (unsigned)ssock->param.read_buffer_size,\n\t\t\t\t       ssock->asock_rbuf,\n\t\t\t\t       PJ_IOQUEUE_ALWAYS_ASYNC);\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n#endif\n\n    /* Update local address */\n    status = get_localaddr(ssock, &ssock->local_addr, &ssock->addr_len);\n    if (status != PJ_SUCCESS) {\n\t/* This fails on few envs, e.g: win IOCP, just tolerate this and\n\t * use parent local address instead.\n\t */\n\tpj_sockaddr_cp(&ssock->local_addr, &ssock_parent->local_addr);\n    }\n\n    /* Prepare write/send state */\n    pj_assert(ssock->send_buf.max_len == 0);\n    ssock->send_buf.buf = (char*)\n\t\t\t  pj_pool_alloc(ssock->pool, \n\t\t\t\t\tssock->param.send_buffer_size);\n    if (!ssock->send_buf.buf)\n        return PJ_ENOMEM;\n\n    ssock->send_buf.max_len = ssock->param.send_buffer_size;\n    ssock->send_buf.start = ssock->send_buf.buf;\n    ssock->send_buf.len = 0;\n\n    /* Start handshake timer */\n    if (ssock->param.timer_heap && (ssock->param.timeout.sec != 0 ||\n\tssock->param.timeout.msec != 0))\n    {\n\tpj_assert(ssock->timer.id == TIMER_NONE);\n\tstatus = pj_timer_heap_schedule_w_grp_lock(ssock->param.timer_heap, \n\t\t\t\t\t\t   &ssock->timer,\n\t\t\t\t\t\t   &ssock->param.timeout,\n\t\t\t\t\t\t   TIMER_HANDSHAKE_TIMEOUT,\n\t\t\t\t\t\t   ssock->param.grp_lock);\n\tif (status != PJ_SUCCESS) {\n\t    ssock->timer.id = TIMER_NONE;\n\t    status = PJ_SUCCESS;\n\t}\n    }\n\n    /* Start SSL handshake */\n    ssock->ssl_state = SSL_STATE_HANDSHAKING;\n    ssl_set_state(ssock, PJ_TRUE);\n    status = ssl_do_handshake(ssock);\n\non_return:\n    if (ssock && status != PJ_EPENDING) {\n\ton_handshake_complete(ssock, status);\n    }\n\n    /* Must return PJ_TRUE whatever happened, as we must continue listening */\n    return PJ_TRUE;\n}",
        "func": "static pj_bool_t ssock_on_accept_complete (pj_ssl_sock_t *ssock_parent,\n\t\t\t\t\t   pj_sock_t newsock,\n\t\t\t\t\t   void *newconn,\n\t\t\t\t\t   const pj_sockaddr_t *src_addr,\n\t\t\t\t\t   int src_addr_len,\n\t\t\t\t\t   pj_status_t accept_status)\n{\n    pj_ssl_sock_t *ssock;\n#ifndef SSL_SOCK_IMP_USE_OWN_NETWORK\n    pj_activesock_cb asock_cb;\n#endif\n    pj_activesock_cfg asock_cfg;\n    unsigned i;\n    pj_status_t status;\n\n#ifndef SSL_SOCK_IMP_USE_OWN_NETWORK\n    PJ_UNUSED_ARG(newconn);\n#endif\n\n    if (accept_status != PJ_SUCCESS) {\n\tif (ssock_parent->param.cb.on_accept_complete2) {\n\t    (*ssock_parent->param.cb.on_accept_complete2)(ssock_parent, NULL,\n\t\t\t\t\t\t    \t  src_addr,\n\t\t\t\t\t\t    \t  src_addr_len,\n\t\t\t\t\t\t    \t  accept_status);\n\t}\n\treturn PJ_TRUE;\n    }\n\n    /* Create new SSL socket instance */\n    status = pj_ssl_sock_create(ssock_parent->pool,\n\t\t\t\t&ssock_parent->newsock_param, &ssock);\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n\n    /* Set parent and add ref count (avoid parent destroy during handshake) */\n    ssock->parent = ssock_parent;\n    if (ssock->parent->param.grp_lock)\n\tpj_grp_lock_add_ref(ssock->parent->param.grp_lock);\n\n    /* Update new SSL socket attributes */\n    ssock->sock = newsock;\n    ssock->is_server = PJ_TRUE;\n    if (ssock_parent->cert) {\n\tstatus = pj_ssl_sock_set_certificate(ssock, ssock->pool, \n\t\t\t\t\t     ssock_parent->cert);\n\tif (status != PJ_SUCCESS)\n\t    goto on_return;\n    }\n\n    /* Set local address */\n    ssock->addr_len = src_addr_len;\n    pj_sockaddr_cp(&ssock->local_addr, &ssock_parent->local_addr);\n\n    /* Set remote address */\n    pj_sockaddr_cp(&ssock->rem_addr, src_addr);\n\n    /* Create SSL context */\n    status = ssl_create(ssock);\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n\n    /* Prepare read buffer */\n    ssock->asock_rbuf = (void**)pj_pool_calloc(ssock->pool, \n\t\t\t\t\t       ssock->param.async_cnt,\n\t\t\t\t\t       sizeof(void*));\n    if (!ssock->asock_rbuf) {\n\tstatus = PJ_ENOMEM;\n\tgoto on_return;\n    }\n\n    for (i = 0; i<ssock->param.async_cnt; ++i) {\n\tssock->asock_rbuf[i] = (void*) pj_pool_alloc(\n\t\t\t\t\t    ssock->pool, \n\t\t\t\t\t    ssock->param.read_buffer_size + \n\t\t\t\t\t    sizeof(read_data_t*));\n\tif (!ssock->asock_rbuf[i]) {\n\t    status = PJ_ENOMEM;\n\t    goto on_return;\n\t}\n    }\n\n    /* If listener socket has group lock, automatically create group lock\n     * for the new socket.\n     */\n    if (ssock_parent->param.grp_lock) {\n\tpj_grp_lock_t *glock;\n\n\tstatus = pj_grp_lock_create(ssock->pool, NULL, &glock);\n\tif (status != PJ_SUCCESS)\n\t    goto on_return;\n\n\tpj_grp_lock_add_ref(glock);\n\tssock->param.grp_lock = glock;\n\tpj_grp_lock_add_handler(ssock->param.grp_lock, ssock->pool, ssock,\n\t\t\t\tssl_on_destroy);\n    }\n\n#ifdef SSL_SOCK_IMP_USE_OWN_NETWORK\n    status = network_setup_connection(ssock, newconn);\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n\n#else\n    /* Apply QoS, if specified */\n    status = pj_sock_apply_qos2(ssock->sock, ssock->param.qos_type,\n\t\t\t\t&ssock->param.qos_params, 1, \n\t\t\t\tssock->pool->obj_name, NULL);\n    if (status != PJ_SUCCESS && !ssock->param.qos_ignore_error)\n\tgoto on_return;\n\n    /* Apply socket options, if specified */\n    if (ssock->param.sockopt_params.cnt) {\n\tstatus = pj_sock_setsockopt_params(ssock->sock, \n\t\t\t\t\t   &ssock->param.sockopt_params);\n\tif (status != PJ_SUCCESS && !ssock->param.sockopt_ignore_error)\n\t    goto on_return;\n    }\n\n    /* Create active socket */\n    pj_activesock_cfg_default(&asock_cfg);\n    asock_cfg.grp_lock = ssock->param.grp_lock;\n    asock_cfg.async_cnt = ssock->param.async_cnt;\n    asock_cfg.concurrency = ssock->param.concurrency;\n    asock_cfg.whole_data = PJ_TRUE;\n\n    pj_bzero(&asock_cb, sizeof(asock_cb));\n    asock_cb.on_data_read = asock_on_data_read;\n    asock_cb.on_data_sent = asock_on_data_sent;\n\n    status = pj_activesock_create(ssock->pool,\n\t\t\t\t  ssock->sock, \n\t\t\t\t  ssock->param.sock_type,\n\t\t\t\t  &asock_cfg,\n\t\t\t\t  ssock->param.ioqueue, \n\t\t\t\t  &asock_cb,\n\t\t\t\t  ssock,\n\t\t\t\t  &ssock->asock);\n\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n\n    /* Start read */\n    status = pj_activesock_start_read2(ssock->asock, ssock->pool, \n\t\t\t\t       (unsigned)ssock->param.read_buffer_size,\n\t\t\t\t       ssock->asock_rbuf,\n\t\t\t\t       PJ_IOQUEUE_ALWAYS_ASYNC);\n    if (status != PJ_SUCCESS)\n\tgoto on_return;\n#endif\n\n    /* Update local address */\n    status = get_localaddr(ssock, &ssock->local_addr, &ssock->addr_len);\n    if (status != PJ_SUCCESS) {\n\t/* This fails on few envs, e.g: win IOCP, just tolerate this and\n\t * use parent local address instead.\n\t */\n\tpj_sockaddr_cp(&ssock->local_addr, &ssock_parent->local_addr);\n    }\n\n    /* Prepare write/send state */\n    pj_assert(ssock->send_buf.max_len == 0);\n    ssock->send_buf.buf = (char*)\n\t\t\t  pj_pool_alloc(ssock->pool, \n\t\t\t\t\tssock->param.send_buffer_size);\n    if (!ssock->send_buf.buf)\n        return PJ_ENOMEM;\n\n    ssock->send_buf.max_len = ssock->param.send_buffer_size;\n    ssock->send_buf.start = ssock->send_buf.buf;\n    ssock->send_buf.len = 0;\n\n    /* Start handshake timer */\n    if (ssock->param.timer_heap && (ssock->param.timeout.sec != 0 ||\n\tssock->param.timeout.msec != 0))\n    {\n\tpj_assert(ssock->timer.id == TIMER_NONE);\n\tstatus = pj_timer_heap_schedule_w_grp_lock(ssock->param.timer_heap, \n\t\t\t\t\t\t   &ssock->timer,\n\t\t\t\t\t\t   &ssock->param.timeout,\n\t\t\t\t\t\t   TIMER_HANDSHAKE_TIMEOUT,\n\t\t\t\t\t\t   ssock->param.grp_lock);\n\tif (status != PJ_SUCCESS) {\n\t    ssock->timer.id = TIMER_NONE;\n\t    status = PJ_SUCCESS;\n\t}\n    }\n\n    /* Start SSL handshake */\n    ssock->ssl_state = SSL_STATE_HANDSHAKING;\n    ssl_set_state(ssock, PJ_TRUE);\n    status = ssl_do_handshake(ssock);\n\non_return:\n    if (ssock && status != PJ_EPENDING) {\n\ton_handshake_complete(ssock, status);\n    }\n\n    /* Must return PJ_TRUE whatever happened, as we must continue listening */\n    return PJ_TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -33,9 +33,13 @@\n     if (status != PJ_SUCCESS)\n \tgoto on_return;\n \n+    /* Set parent and add ref count (avoid parent destroy during handshake) */\n+    ssock->parent = ssock_parent;\n+    if (ssock->parent->param.grp_lock)\n+\tpj_grp_lock_add_ref(ssock->parent->param.grp_lock);\n+\n     /* Update new SSL socket attributes */\n     ssock->sock = newsock;\n-    ssock->parent = ssock_parent;\n     ssock->is_server = PJ_TRUE;\n     if (ssock_parent->cert) {\n \tstatus = pj_ssl_sock_set_certificate(ssock, ssock->pool, \n@@ -60,16 +64,20 @@\n     ssock->asock_rbuf = (void**)pj_pool_calloc(ssock->pool, \n \t\t\t\t\t       ssock->param.async_cnt,\n \t\t\t\t\t       sizeof(void*));\n-    if (!ssock->asock_rbuf)\n-        return PJ_ENOMEM;\n+    if (!ssock->asock_rbuf) {\n+\tstatus = PJ_ENOMEM;\n+\tgoto on_return;\n+    }\n \n     for (i = 0; i<ssock->param.async_cnt; ++i) {\n \tssock->asock_rbuf[i] = (void*) pj_pool_alloc(\n \t\t\t\t\t    ssock->pool, \n \t\t\t\t\t    ssock->param.read_buffer_size + \n \t\t\t\t\t    sizeof(read_data_t*));\n-        if (!ssock->asock_rbuf[i])\n-            return PJ_ENOMEM;\n+\tif (!ssock->asock_rbuf[i]) {\n+\t    status = PJ_ENOMEM;\n+\t    goto on_return;\n+\t}\n     }\n \n     /* If listener socket has group lock, automatically create group lock\n@@ -83,7 +91,7 @@\n \t    goto on_return;\n \n \tpj_grp_lock_add_ref(glock);\n-\tasock_cfg.grp_lock = ssock->param.grp_lock = glock;\n+\tssock->param.grp_lock = glock;\n \tpj_grp_lock_add_handler(ssock->param.grp_lock, ssock->pool, ssock,\n \t\t\t\tssl_on_destroy);\n     }\n@@ -111,6 +119,7 @@\n \n     /* Create active socket */\n     pj_activesock_cfg_default(&asock_cfg);\n+    asock_cfg.grp_lock = ssock->param.grp_lock;\n     asock_cfg.async_cnt = ssock->param.async_cnt;\n     asock_cfg.concurrency = ssock->param.concurrency;\n     asock_cfg.whole_data = PJ_TRUE;",
        "diff_line_info": {
            "deleted_lines": [
                "    ssock->parent = ssock_parent;",
                "    if (!ssock->asock_rbuf)",
                "        return PJ_ENOMEM;",
                "        if (!ssock->asock_rbuf[i])",
                "            return PJ_ENOMEM;",
                "\tasock_cfg.grp_lock = ssock->param.grp_lock = glock;"
            ],
            "added_lines": [
                "    /* Set parent and add ref count (avoid parent destroy during handshake) */",
                "    ssock->parent = ssock_parent;",
                "    if (ssock->parent->param.grp_lock)",
                "\tpj_grp_lock_add_ref(ssock->parent->param.grp_lock);",
                "",
                "    if (!ssock->asock_rbuf) {",
                "\tstatus = PJ_ENOMEM;",
                "\tgoto on_return;",
                "    }",
                "\tif (!ssock->asock_rbuf[i]) {",
                "\t    status = PJ_ENOMEM;",
                "\t    goto on_return;",
                "\t}",
                "\tssock->param.grp_lock = glock;",
                "    asock_cfg.grp_lock = ssock->param.grp_lock;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3573",
        "func_name": "kernel/git/bluetooth/bluetooth/hci_sock_dev_event",
        "description": "A use-after-free in function hci_sock_bound_ioctl() of the Linux kernel HCI subsystem was found in the way user calls ioct HCIUNBLOCKADDR or other way triggers race condition of the call hci_unregister_dev() together with one of the calls hci_sock_blacklist_add(), hci_sock_blacklist_del(), hci_get_conn_info(), hci_get_auth_info(). A privileged local user could use this flaw to crash the system or escalate their privileges on the system. This flaw affects the Linux kernel versions prior to 5.13-rc5.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bluetooth/bluetooth.git/commit/?h=e305509e678b3a4af2b3cfd410f409f7cdaabb52",
        "commit_title": "The hci_sock_dev_event() function will cleanup the hdev object for",
        "commit_text": "sockets even if this object may still be in used within the hci_sock_bound_ioctl() function, result in UAF vulnerability.  This patch replace the BH context lock to serialize these affairs and prevent the race condition.  ",
        "func_before": "void hci_sock_dev_event(struct hci_dev *hdev, int event)\n{\n\tBT_DBG(\"hdev %s event %d\", hdev->name, event);\n\n\tif (atomic_read(&monitor_promisc)) {\n\t\tstruct sk_buff *skb;\n\n\t\t/* Send event to monitor */\n\t\tskb = create_monitor_event(hdev, event);\n\t\tif (skb) {\n\t\t\thci_send_to_channel(HCI_CHANNEL_MONITOR, skb,\n\t\t\t\t\t    HCI_SOCK_TRUSTED, NULL);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tif (event <= HCI_DEV_DOWN) {\n\t\tstruct hci_ev_si_device ev;\n\n\t\t/* Send event to sockets */\n\t\tev.event  = event;\n\t\tev.dev_id = hdev->id;\n\t\thci_si_event(NULL, HCI_EV_SI_DEVICE, sizeof(ev), &ev);\n\t}\n\n\tif (event == HCI_DEV_UNREG) {\n\t\tstruct sock *sk;\n\n\t\t/* Detach sockets from device */\n\t\tread_lock(&hci_sk_list.lock);\n\t\tsk_for_each(sk, &hci_sk_list.head) {\n\t\t\tbh_lock_sock_nested(sk);\n\t\t\tif (hci_pi(sk)->hdev == hdev) {\n\t\t\t\thci_pi(sk)->hdev = NULL;\n\t\t\t\tsk->sk_err = EPIPE;\n\t\t\t\tsk->sk_state = BT_OPEN;\n\t\t\t\tsk->sk_state_change(sk);\n\n\t\t\t\thci_dev_put(hdev);\n\t\t\t}\n\t\t\tbh_unlock_sock(sk);\n\t\t}\n\t\tread_unlock(&hci_sk_list.lock);\n\t}\n}",
        "func": "void hci_sock_dev_event(struct hci_dev *hdev, int event)\n{\n\tBT_DBG(\"hdev %s event %d\", hdev->name, event);\n\n\tif (atomic_read(&monitor_promisc)) {\n\t\tstruct sk_buff *skb;\n\n\t\t/* Send event to monitor */\n\t\tskb = create_monitor_event(hdev, event);\n\t\tif (skb) {\n\t\t\thci_send_to_channel(HCI_CHANNEL_MONITOR, skb,\n\t\t\t\t\t    HCI_SOCK_TRUSTED, NULL);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tif (event <= HCI_DEV_DOWN) {\n\t\tstruct hci_ev_si_device ev;\n\n\t\t/* Send event to sockets */\n\t\tev.event  = event;\n\t\tev.dev_id = hdev->id;\n\t\thci_si_event(NULL, HCI_EV_SI_DEVICE, sizeof(ev), &ev);\n\t}\n\n\tif (event == HCI_DEV_UNREG) {\n\t\tstruct sock *sk;\n\n\t\t/* Detach sockets from device */\n\t\tread_lock(&hci_sk_list.lock);\n\t\tsk_for_each(sk, &hci_sk_list.head) {\n\t\t\tlock_sock(sk);\n\t\t\tif (hci_pi(sk)->hdev == hdev) {\n\t\t\t\thci_pi(sk)->hdev = NULL;\n\t\t\t\tsk->sk_err = EPIPE;\n\t\t\t\tsk->sk_state = BT_OPEN;\n\t\t\t\tsk->sk_state_change(sk);\n\n\t\t\t\thci_dev_put(hdev);\n\t\t\t}\n\t\t\trelease_sock(sk);\n\t\t}\n\t\tread_unlock(&hci_sk_list.lock);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -29,7 +29,7 @@\n \t\t/* Detach sockets from device */\n \t\tread_lock(&hci_sk_list.lock);\n \t\tsk_for_each(sk, &hci_sk_list.head) {\n-\t\t\tbh_lock_sock_nested(sk);\n+\t\t\tlock_sock(sk);\n \t\t\tif (hci_pi(sk)->hdev == hdev) {\n \t\t\t\thci_pi(sk)->hdev = NULL;\n \t\t\t\tsk->sk_err = EPIPE;\n@@ -38,7 +38,7 @@\n \n \t\t\t\thci_dev_put(hdev);\n \t\t\t}\n-\t\t\tbh_unlock_sock(sk);\n+\t\t\trelease_sock(sk);\n \t\t}\n \t\tread_unlock(&hci_sk_list.lock);\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tbh_lock_sock_nested(sk);",
                "\t\t\tbh_unlock_sock(sk);"
            ],
            "added_lines": [
                "\t\t\tlock_sock(sk);",
                "\t\t\trelease_sock(sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-40490",
        "func_name": "kernel/git/tytso/ext4/ext4_write_inline_data_end",
        "description": "A race condition was discovered in ext4_write_inline_data_end in fs/ext4/inline.c in the ext4 subsystem in the Linux kernel through 5.13.13.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4.git/commit/?h=9e445093e523f3277081314c864f708fd4bd34aa",
        "commit_title": "The location of the system.data extended attribute can change whenever",
        "commit_text": "xattr_sem is not taken.  So we need to recalculate the i_inline_off field since it mgiht have changed between ext4_write_begin() and ext4_write_end().  This means that caching i_inline_off is probably not helpful, so in the long run we should probably get rid of it and shrink the in-memory ext4 inode slightly, but let's fix the race the simple way for now.  Cc: stable@kernel.org ",
        "func_before": "int ext4_write_inline_data_end(struct inode *inode, loff_t pos, unsigned len,\n\t\t\t       unsigned copied, struct page *page)\n{\n\tint ret, no_expand;\n\tvoid *kaddr;\n\tstruct ext4_iloc iloc;\n\n\tif (unlikely(copied < len)) {\n\t\tif (!PageUptodate(page)) {\n\t\t\tcopied = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = ext4_get_inode_loc(inode, &iloc);\n\tif (ret) {\n\t\text4_std_error(inode->i_sb, ret);\n\t\tcopied = 0;\n\t\tgoto out;\n\t}\n\n\text4_write_lock_xattr(inode, &no_expand);\n\tBUG_ON(!ext4_has_inline_data(inode));\n\n\tkaddr = kmap_atomic(page);\n\text4_write_inline_data(inode, &iloc, kaddr, pos, len);\n\tkunmap_atomic(kaddr);\n\tSetPageUptodate(page);\n\t/* clear page dirty so that writepages wouldn't work for us. */\n\tClearPageDirty(page);\n\n\text4_write_unlock_xattr(inode, &no_expand);\n\tbrelse(iloc.bh);\n\tmark_inode_dirty(inode);\nout:\n\treturn copied;\n}",
        "func": "int ext4_write_inline_data_end(struct inode *inode, loff_t pos, unsigned len,\n\t\t\t       unsigned copied, struct page *page)\n{\n\tint ret, no_expand;\n\tvoid *kaddr;\n\tstruct ext4_iloc iloc;\n\n\tif (unlikely(copied < len)) {\n\t\tif (!PageUptodate(page)) {\n\t\t\tcopied = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = ext4_get_inode_loc(inode, &iloc);\n\tif (ret) {\n\t\text4_std_error(inode->i_sb, ret);\n\t\tcopied = 0;\n\t\tgoto out;\n\t}\n\n\text4_write_lock_xattr(inode, &no_expand);\n\tBUG_ON(!ext4_has_inline_data(inode));\n\n\t/*\n\t * ei->i_inline_off may have changed since ext4_write_begin()\n\t * called ext4_try_to_write_inline_data()\n\t */\n\t(void) ext4_find_inline_data_nolock(inode);\n\n\tkaddr = kmap_atomic(page);\n\text4_write_inline_data(inode, &iloc, kaddr, pos, len);\n\tkunmap_atomic(kaddr);\n\tSetPageUptodate(page);\n\t/* clear page dirty so that writepages wouldn't work for us. */\n\tClearPageDirty(page);\n\n\text4_write_unlock_xattr(inode, &no_expand);\n\tbrelse(iloc.bh);\n\tmark_inode_dirty(inode);\nout:\n\treturn copied;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,6 +22,12 @@\n \text4_write_lock_xattr(inode, &no_expand);\n \tBUG_ON(!ext4_has_inline_data(inode));\n \n+\t/*\n+\t * ei->i_inline_off may have changed since ext4_write_begin()\n+\t * called ext4_try_to_write_inline_data()\n+\t */\n+\t(void) ext4_find_inline_data_nolock(inode);\n+\n \tkaddr = kmap_atomic(page);\n \text4_write_inline_data(inode, &iloc, kaddr, pos, len);\n \tkunmap_atomic(kaddr);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t/*",
                "\t * ei->i_inline_off may have changed since ext4_write_begin()",
                "\t * called ext4_try_to_write_inline_data()",
                "\t */",
                "\t(void) ext4_find_inline_data_nolock(inode);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-28701",
        "func_name": "xen-project/xen/xenmem_add_to_physmap_one",
        "description": "Another race in XENMAPSPACE_grant_table handling Guests are permitted access to certain Xen-owned pages of memory. The majority of such pages remain allocated / associated with a guest for its entire lifetime. Grant table v2 status pages, however, are de-allocated when a guest switches (back) from v2 to v1. Freeing such pages requires that the hypervisor enforce that no parallel request can result in the addition of a mapping of such a page to a guest. That enforcement was missing, allowing guests to retain access to pages that were freed and perhaps re-used for other purposes. Unfortunately, when XSA-379 was being prepared, this similar issue was not noticed.",
        "git_url": "https://github.com/xen-project/xen/commit/eb6bbf7b30da5bae87932514d54d0e3c68b23757",
        "commit_title": "gnttab: deal with status frame mapping race",
        "commit_text": " Once gnttab_map_frame() drops the grant table lock, the MFN it reports back to its caller is free to other manipulation. In particular gnttab_unpopulate_status_frames() might free it, by a racing request on another CPU, thus resulting in a reference to a deallocated page getting added to a domain's P2M.  Obtain a page reference in gnttab_map_frame() to prevent freeing of the page until xenmem_add_to_physmap_one() has actually completed its acting on the page. Do so uniformly, even if only strictly required for v2 status pages, to avoid extra conditionals (which then would all need to be kept in sync going forward).  This is CVE-2021-28701 / XSA-384. ",
        "func_before": "int xenmem_add_to_physmap_one(\n    struct domain *d,\n    unsigned int space,\n    union add_to_physmap_extra extra,\n    unsigned long idx,\n    gfn_t gfn)\n{\n    mfn_t mfn = INVALID_MFN;\n    int rc;\n    p2m_type_t t;\n    struct page_info *page = NULL;\n\n    switch ( space )\n    {\n    case XENMAPSPACE_grant_table:\n        rc = gnttab_map_frame(d, idx, gfn, &mfn);\n        if ( rc )\n            return rc;\n\n        t = p2m_ram_rw;\n\n        break;\n    case XENMAPSPACE_shared_info:\n        if ( idx != 0 )\n            return -EINVAL;\n\n        mfn = virt_to_mfn(d->shared_info);\n        t = p2m_ram_rw;\n\n        break;\n    case XENMAPSPACE_gmfn_foreign:\n    {\n        struct domain *od;\n        p2m_type_t p2mt;\n\n        od = get_pg_owner(extra.foreign_domid);\n        if ( od == NULL )\n            return -ESRCH;\n\n        if ( od == d )\n        {\n            put_pg_owner(od);\n            return -EINVAL;\n        }\n\n        rc = xsm_map_gmfn_foreign(XSM_TARGET, d, od);\n        if ( rc )\n        {\n            put_pg_owner(od);\n            return rc;\n        }\n\n        /* Take reference to the foreign domain page.\n         * Reference will be released in XENMEM_remove_from_physmap */\n        page = get_page_from_gfn(od, idx, &p2mt, P2M_ALLOC);\n        if ( !page )\n        {\n            put_pg_owner(od);\n            return -EINVAL;\n        }\n\n        if ( p2m_is_ram(p2mt) )\n            t = (p2mt == p2m_ram_rw) ? p2m_map_foreign_rw : p2m_map_foreign_ro;\n        else\n        {\n            put_page(page);\n            put_pg_owner(od);\n            return -EINVAL;\n        }\n\n        mfn = page_to_mfn(page);\n\n        put_pg_owner(od);\n        break;\n    }\n    case XENMAPSPACE_dev_mmio:\n        rc = map_dev_mmio_region(d, gfn, 1, _mfn(idx));\n        return rc;\n\n    default:\n        return -ENOSYS;\n    }\n\n    /* Map at new location. */\n    rc = guest_physmap_add_entry(d, gfn, mfn, 0, t);\n\n    /* If we fail to add the mapping, we need to drop the reference we\n     * took earlier on foreign pages */\n    if ( rc && space == XENMAPSPACE_gmfn_foreign )\n    {\n        ASSERT(page != NULL);\n        put_page(page);\n    }\n\n    return rc;\n}",
        "func": "int xenmem_add_to_physmap_one(\n    struct domain *d,\n    unsigned int space,\n    union add_to_physmap_extra extra,\n    unsigned long idx,\n    gfn_t gfn)\n{\n    mfn_t mfn = INVALID_MFN;\n    int rc;\n    p2m_type_t t;\n    struct page_info *page = NULL;\n\n    switch ( space )\n    {\n    case XENMAPSPACE_grant_table:\n        rc = gnttab_map_frame(d, idx, gfn, &mfn);\n        if ( rc )\n            return rc;\n\n        /* Need to take care of the reference obtained in gnttab_map_frame(). */\n        page = mfn_to_page(mfn);\n        t = p2m_ram_rw;\n\n        break;\n    case XENMAPSPACE_shared_info:\n        if ( idx != 0 )\n            return -EINVAL;\n\n        mfn = virt_to_mfn(d->shared_info);\n        t = p2m_ram_rw;\n\n        break;\n    case XENMAPSPACE_gmfn_foreign:\n    {\n        struct domain *od;\n        p2m_type_t p2mt;\n\n        od = get_pg_owner(extra.foreign_domid);\n        if ( od == NULL )\n            return -ESRCH;\n\n        if ( od == d )\n        {\n            put_pg_owner(od);\n            return -EINVAL;\n        }\n\n        rc = xsm_map_gmfn_foreign(XSM_TARGET, d, od);\n        if ( rc )\n        {\n            put_pg_owner(od);\n            return rc;\n        }\n\n        /* Take reference to the foreign domain page.\n         * Reference will be released in XENMEM_remove_from_physmap */\n        page = get_page_from_gfn(od, idx, &p2mt, P2M_ALLOC);\n        if ( !page )\n        {\n            put_pg_owner(od);\n            return -EINVAL;\n        }\n\n        if ( p2m_is_ram(p2mt) )\n            t = (p2mt == p2m_ram_rw) ? p2m_map_foreign_rw : p2m_map_foreign_ro;\n        else\n        {\n            put_page(page);\n            put_pg_owner(od);\n            return -EINVAL;\n        }\n\n        mfn = page_to_mfn(page);\n\n        put_pg_owner(od);\n        break;\n    }\n    case XENMAPSPACE_dev_mmio:\n        rc = map_dev_mmio_region(d, gfn, 1, _mfn(idx));\n        return rc;\n\n    default:\n        return -ENOSYS;\n    }\n\n    /* Map at new location. */\n    rc = guest_physmap_add_entry(d, gfn, mfn, 0, t);\n\n    /*\n     * For XENMAPSPACE_gmfn_foreign if we failed to add the mapping, we need\n     * to drop the reference we took earlier. In all other cases we need to\n     * drop any reference we took earlier (perhaps indirectly).\n     */\n    if ( space == XENMAPSPACE_gmfn_foreign ? rc : page != NULL )\n    {\n        ASSERT(page != NULL);\n        put_page(page);\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,6 +17,8 @@\n         if ( rc )\n             return rc;\n \n+        /* Need to take care of the reference obtained in gnttab_map_frame(). */\n+        page = mfn_to_page(mfn);\n         t = p2m_ram_rw;\n \n         break;\n@@ -84,9 +86,12 @@\n     /* Map at new location. */\n     rc = guest_physmap_add_entry(d, gfn, mfn, 0, t);\n \n-    /* If we fail to add the mapping, we need to drop the reference we\n-     * took earlier on foreign pages */\n-    if ( rc && space == XENMAPSPACE_gmfn_foreign )\n+    /*\n+     * For XENMAPSPACE_gmfn_foreign if we failed to add the mapping, we need\n+     * to drop the reference we took earlier. In all other cases we need to\n+     * drop any reference we took earlier (perhaps indirectly).\n+     */\n+    if ( space == XENMAPSPACE_gmfn_foreign ? rc : page != NULL )\n     {\n         ASSERT(page != NULL);\n         put_page(page);",
        "diff_line_info": {
            "deleted_lines": [
                "    /* If we fail to add the mapping, we need to drop the reference we",
                "     * took earlier on foreign pages */",
                "    if ( rc && space == XENMAPSPACE_gmfn_foreign )"
            ],
            "added_lines": [
                "        /* Need to take care of the reference obtained in gnttab_map_frame(). */",
                "        page = mfn_to_page(mfn);",
                "    /*",
                "     * For XENMAPSPACE_gmfn_foreign if we failed to add the mapping, we need",
                "     * to drop the reference we took earlier. In all other cases we need to",
                "     * drop any reference we took earlier (perhaps indirectly).",
                "     */",
                "    if ( space == XENMAPSPACE_gmfn_foreign ? rc : page != NULL )"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-28701",
        "func_name": "xen-project/xen/gnttab_map_frame",
        "description": "Another race in XENMAPSPACE_grant_table handling Guests are permitted access to certain Xen-owned pages of memory. The majority of such pages remain allocated / associated with a guest for its entire lifetime. Grant table v2 status pages, however, are de-allocated when a guest switches (back) from v2 to v1. Freeing such pages requires that the hypervisor enforce that no parallel request can result in the addition of a mapping of such a page to a guest. That enforcement was missing, allowing guests to retain access to pages that were freed and perhaps re-used for other purposes. Unfortunately, when XSA-379 was being prepared, this similar issue was not noticed.",
        "git_url": "https://github.com/xen-project/xen/commit/eb6bbf7b30da5bae87932514d54d0e3c68b23757",
        "commit_title": "gnttab: deal with status frame mapping race",
        "commit_text": " Once gnttab_map_frame() drops the grant table lock, the MFN it reports back to its caller is free to other manipulation. In particular gnttab_unpopulate_status_frames() might free it, by a racing request on another CPU, thus resulting in a reference to a deallocated page getting added to a domain's P2M.  Obtain a page reference in gnttab_map_frame() to prevent freeing of the page until xenmem_add_to_physmap_one() has actually completed its acting on the page. Do so uniformly, even if only strictly required for v2 status pages, to avoid extra conditionals (which then would all need to be kept in sync going forward).  This is CVE-2021-28701 / XSA-384. ",
        "func_before": "int gnttab_map_frame(struct domain *d, unsigned long idx, gfn_t gfn, mfn_t *mfn)\n{\n    int rc = 0;\n    struct grant_table *gt = d->grant_table;\n    bool status = false;\n\n    grant_write_lock(gt);\n\n    if ( evaluate_nospec(gt->gt_version == 2) && (idx & XENMAPIDX_grant_table_status) )\n    {\n        idx &= ~XENMAPIDX_grant_table_status;\n        status = true;\n\n        rc = gnttab_get_status_frame_mfn(d, idx, mfn);\n    }\n    else\n        rc = gnttab_get_shared_frame_mfn(d, idx, mfn);\n\n    if ( !rc && paging_mode_translate(d) )\n    {\n        gfn_t gfn = gnttab_get_frame_gfn(gt, status, idx);\n\n        if ( !gfn_eq(gfn, INVALID_GFN) )\n            rc = guest_physmap_remove_page(d, gfn, *mfn, 0);\n    }\n\n    if ( !rc )\n        gnttab_set_frame_gfn(gt, status, idx, gfn);\n\n    grant_write_unlock(gt);\n\n    return rc;\n}",
        "func": "int gnttab_map_frame(struct domain *d, unsigned long idx, gfn_t gfn, mfn_t *mfn)\n{\n    int rc = 0;\n    struct grant_table *gt = d->grant_table;\n    bool status = false;\n\n    grant_write_lock(gt);\n\n    if ( evaluate_nospec(gt->gt_version == 2) && (idx & XENMAPIDX_grant_table_status) )\n    {\n        idx &= ~XENMAPIDX_grant_table_status;\n        status = true;\n\n        rc = gnttab_get_status_frame_mfn(d, idx, mfn);\n    }\n    else\n        rc = gnttab_get_shared_frame_mfn(d, idx, mfn);\n\n    if ( !rc && paging_mode_translate(d) )\n    {\n        gfn_t gfn = gnttab_get_frame_gfn(gt, status, idx);\n\n        if ( !gfn_eq(gfn, INVALID_GFN) )\n            rc = guest_physmap_remove_page(d, gfn, *mfn, 0);\n    }\n\n    if ( !rc )\n    {\n        /*\n         * Make sure gnttab_unpopulate_status_frames() won't (successfully)\n         * free the page until our caller has completed its operation.\n         */\n        if ( get_page(mfn_to_page(*mfn), d) )\n            gnttab_set_frame_gfn(gt, status, idx, gfn);\n        else\n            rc = -EBUSY;\n    }\n\n    grant_write_unlock(gt);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,7 +25,16 @@\n     }\n \n     if ( !rc )\n-        gnttab_set_frame_gfn(gt, status, idx, gfn);\n+    {\n+        /*\n+         * Make sure gnttab_unpopulate_status_frames() won't (successfully)\n+         * free the page until our caller has completed its operation.\n+         */\n+        if ( get_page(mfn_to_page(*mfn), d) )\n+            gnttab_set_frame_gfn(gt, status, idx, gfn);\n+        else\n+            rc = -EBUSY;\n+    }\n \n     grant_write_unlock(gt);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "        gnttab_set_frame_gfn(gt, status, idx, gfn);"
            ],
            "added_lines": [
                "    {",
                "        /*",
                "         * Make sure gnttab_unpopulate_status_frames() won't (successfully)",
                "         * free the page until our caller has completed its operation.",
                "         */",
                "        if ( get_page(mfn_to_page(*mfn), d) )",
                "            gnttab_set_frame_gfn(gt, status, idx, gfn);",
                "        else",
                "            rc = -EBUSY;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-28701",
        "func_name": "xen-project/xen/xenmem_add_to_physmap_one",
        "description": "Another race in XENMAPSPACE_grant_table handling Guests are permitted access to certain Xen-owned pages of memory. The majority of such pages remain allocated / associated with a guest for its entire lifetime. Grant table v2 status pages, however, are de-allocated when a guest switches (back) from v2 to v1. Freeing such pages requires that the hypervisor enforce that no parallel request can result in the addition of a mapping of such a page to a guest. That enforcement was missing, allowing guests to retain access to pages that were freed and perhaps re-used for other purposes. Unfortunately, when XSA-379 was being prepared, this similar issue was not noticed.",
        "git_url": "https://github.com/xen-project/xen/commit/eb6bbf7b30da5bae87932514d54d0e3c68b23757",
        "commit_title": "gnttab: deal with status frame mapping race",
        "commit_text": " Once gnttab_map_frame() drops the grant table lock, the MFN it reports back to its caller is free to other manipulation. In particular gnttab_unpopulate_status_frames() might free it, by a racing request on another CPU, thus resulting in a reference to a deallocated page getting added to a domain's P2M.  Obtain a page reference in gnttab_map_frame() to prevent freeing of the page until xenmem_add_to_physmap_one() has actually completed its acting on the page. Do so uniformly, even if only strictly required for v2 status pages, to avoid extra conditionals (which then would all need to be kept in sync going forward).  This is CVE-2021-28701 / XSA-384. ",
        "func_before": "int xenmem_add_to_physmap_one(\n    struct domain *d,\n    unsigned int space,\n    union add_to_physmap_extra extra,\n    unsigned long idx,\n    gfn_t gpfn)\n{\n    struct page_info *page = NULL;\n    unsigned long gfn = 0 /* gcc ... */, old_gpfn;\n    mfn_t prev_mfn;\n    int rc = 0;\n    mfn_t mfn = INVALID_MFN;\n    p2m_type_t p2mt;\n\n    switch ( space )\n    {\n    case XENMAPSPACE_shared_info:\n        if ( idx == 0 )\n            mfn = virt_to_mfn(d->shared_info);\n        break;\n\n    case XENMAPSPACE_grant_table:\n        rc = gnttab_map_frame(d, idx, gpfn, &mfn);\n        if ( rc )\n            return rc;\n        break;\n\n    case XENMAPSPACE_gmfn:\n    {\n        p2m_type_t p2mt;\n\n        gfn = idx;\n        mfn = get_gfn_unshare(d, gfn, &p2mt);\n        /* If the page is still shared, exit early */\n        if ( p2m_is_shared(p2mt) )\n        {\n            put_gfn(d, gfn);\n            return -ENOMEM;\n        }\n        page = get_page_from_mfn(mfn, d);\n        if ( unlikely(!page) )\n            mfn = INVALID_MFN;\n        break;\n    }\n\n    case XENMAPSPACE_gmfn_foreign:\n        return p2m_add_foreign(d, idx, gfn_x(gpfn), extra.foreign_domid);\n    }\n\n    if ( mfn_eq(mfn, INVALID_MFN) )\n    {\n        rc = -EINVAL;\n        goto put_both;\n    }\n\n    /*\n     * Note that we're (ab)using GFN locking (to really be locking of the\n     * entire P2M) here in (at least) two ways: Finer grained locking would\n     * expose lock order violations in the XENMAPSPACE_gmfn case (due to the\n     * earlier get_gfn_unshare() above). Plus at the very least for the grant\n     * table v2 status page case we need to guarantee that the same page can\n     * only appear at a single GFN. While this is a property we want in\n     * general, for pages which can subsequently be freed this imperative:\n     * Upon freeing we wouldn't be able to find other mappings in the P2M\n     * (unless we did a brute force search).\n     */\n    prev_mfn = get_gfn(d, gfn_x(gpfn), &p2mt);\n\n    /* XENMAPSPACE_gmfn: Check if the MFN is associated with another GFN. */\n    old_gpfn = get_gpfn_from_mfn(mfn_x(mfn));\n    ASSERT(!SHARED_M2P(old_gpfn));\n    if ( space == XENMAPSPACE_gmfn && old_gpfn != gfn )\n    {\n        rc = -EXDEV;\n        goto put_all;\n    }\n\n    /* Remove previously mapped page if it was present. */\n    if ( p2mt == p2m_mmio_direct )\n        rc = -EPERM;\n    else if ( mfn_valid(prev_mfn) )\n    {\n        if ( is_special_page(mfn_to_page(prev_mfn)) )\n            /* Special pages are simply unhooked from this phys slot. */\n            rc = guest_physmap_remove_page(d, gpfn, prev_mfn, PAGE_ORDER_4K);\n        else if ( !mfn_eq(mfn, prev_mfn) )\n            /* Normal domain memory is freed, to avoid leaking memory. */\n            rc = guest_remove_page(d, gfn_x(gpfn));\n    }\n\n    /* Unmap from old location, if any. */\n    if ( !rc && old_gpfn != INVALID_M2P_ENTRY )\n        rc = guest_physmap_remove_page(d, _gfn(old_gpfn), mfn, PAGE_ORDER_4K);\n\n    /* Map at new location. */\n    if ( !rc )\n        rc = guest_physmap_add_page(d, gpfn, mfn, PAGE_ORDER_4K);\n\n put_all:\n    put_gfn(d, gfn_x(gpfn));\n\n put_both:\n    /*\n     * In the XENMAPSPACE_gmfn case, we took a ref of the gfn at the top.\n     * We also may need to transfer ownership of the page reference to our\n     * caller.\n     */\n    if ( space == XENMAPSPACE_gmfn )\n    {\n        put_gfn(d, gfn);\n        if ( !rc && extra.ppage )\n        {\n            *extra.ppage = page;\n            page = NULL;\n        }\n    }\n\n    if ( page )\n        put_page(page);\n\n    return rc;\n}",
        "func": "int xenmem_add_to_physmap_one(\n    struct domain *d,\n    unsigned int space,\n    union add_to_physmap_extra extra,\n    unsigned long idx,\n    gfn_t gpfn)\n{\n    struct page_info *page = NULL;\n    unsigned long gfn = 0 /* gcc ... */, old_gpfn;\n    mfn_t prev_mfn;\n    int rc = 0;\n    mfn_t mfn = INVALID_MFN;\n    p2m_type_t p2mt;\n\n    switch ( space )\n    {\n    case XENMAPSPACE_shared_info:\n        if ( idx == 0 )\n            mfn = virt_to_mfn(d->shared_info);\n        break;\n\n    case XENMAPSPACE_grant_table:\n        rc = gnttab_map_frame(d, idx, gpfn, &mfn);\n        if ( rc )\n            return rc;\n        /* Need to take care of the reference obtained in gnttab_map_frame(). */\n        page = mfn_to_page(mfn);\n        break;\n\n    case XENMAPSPACE_gmfn:\n    {\n        p2m_type_t p2mt;\n\n        gfn = idx;\n        mfn = get_gfn_unshare(d, gfn, &p2mt);\n        /* If the page is still shared, exit early */\n        if ( p2m_is_shared(p2mt) )\n        {\n            put_gfn(d, gfn);\n            return -ENOMEM;\n        }\n        page = get_page_from_mfn(mfn, d);\n        if ( unlikely(!page) )\n            mfn = INVALID_MFN;\n        break;\n    }\n\n    case XENMAPSPACE_gmfn_foreign:\n        return p2m_add_foreign(d, idx, gfn_x(gpfn), extra.foreign_domid);\n    }\n\n    if ( mfn_eq(mfn, INVALID_MFN) )\n    {\n        rc = -EINVAL;\n        goto put_both;\n    }\n\n    /*\n     * Note that we're (ab)using GFN locking (to really be locking of the\n     * entire P2M) here in (at least) two ways: Finer grained locking would\n     * expose lock order violations in the XENMAPSPACE_gmfn case (due to the\n     * earlier get_gfn_unshare() above). Plus at the very least for the grant\n     * table v2 status page case we need to guarantee that the same page can\n     * only appear at a single GFN. While this is a property we want in\n     * general, for pages which can subsequently be freed this imperative:\n     * Upon freeing we wouldn't be able to find other mappings in the P2M\n     * (unless we did a brute force search).\n     */\n    prev_mfn = get_gfn(d, gfn_x(gpfn), &p2mt);\n\n    /* XENMAPSPACE_gmfn: Check if the MFN is associated with another GFN. */\n    old_gpfn = get_gpfn_from_mfn(mfn_x(mfn));\n    ASSERT(!SHARED_M2P(old_gpfn));\n    if ( space == XENMAPSPACE_gmfn && old_gpfn != gfn )\n    {\n        rc = -EXDEV;\n        goto put_all;\n    }\n\n    /* Remove previously mapped page if it was present. */\n    if ( p2mt == p2m_mmio_direct )\n        rc = -EPERM;\n    else if ( mfn_valid(prev_mfn) )\n    {\n        if ( is_special_page(mfn_to_page(prev_mfn)) )\n            /* Special pages are simply unhooked from this phys slot. */\n            rc = guest_physmap_remove_page(d, gpfn, prev_mfn, PAGE_ORDER_4K);\n        else if ( !mfn_eq(mfn, prev_mfn) )\n            /* Normal domain memory is freed, to avoid leaking memory. */\n            rc = guest_remove_page(d, gfn_x(gpfn));\n    }\n\n    /* Unmap from old location, if any. */\n    if ( !rc && old_gpfn != INVALID_M2P_ENTRY )\n        rc = guest_physmap_remove_page(d, _gfn(old_gpfn), mfn, PAGE_ORDER_4K);\n\n    /* Map at new location. */\n    if ( !rc )\n        rc = guest_physmap_add_page(d, gpfn, mfn, PAGE_ORDER_4K);\n\n put_all:\n    put_gfn(d, gfn_x(gpfn));\n\n put_both:\n    /*\n     * In the XENMAPSPACE_gmfn case, we took a ref of the gfn at the top.\n     * We also may need to transfer ownership of the page reference to our\n     * caller.\n     */\n    if ( space == XENMAPSPACE_gmfn )\n    {\n        put_gfn(d, gfn);\n        if ( !rc && extra.ppage )\n        {\n            *extra.ppage = page;\n            page = NULL;\n        }\n    }\n\n    if ( page )\n        put_page(page);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -23,6 +23,8 @@\n         rc = gnttab_map_frame(d, idx, gpfn, &mfn);\n         if ( rc )\n             return rc;\n+        /* Need to take care of the reference obtained in gnttab_map_frame(). */\n+        page = mfn_to_page(mfn);\n         break;\n \n     case XENMAPSPACE_gmfn:",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        /* Need to take care of the reference obtained in gnttab_map_frame(). */",
                "        page = mfn_to_page(mfn);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-17712",
        "func_name": "torvalds/linux/raw_sendmsg",
        "description": "The raw_sendmsg() function in net/ipv4/raw.c in the Linux kernel through 4.14.6 has a race condition in inet->hdrincl that leads to uninitialized stack pointer usage; this allows a local user to execute code and gain privileges.",
        "git_url": "https://github.com/torvalds/linux/commit/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483",
        "commit_title": "net: ipv4: fix for a race condition in raw_sendmsg",
        "commit_text": " inet->hdrincl is racy, and could lead to uninitialized stack pointer usage, so its value should be read only once. ",
        "func_before": "static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\tstruct ip_options_data opt_copy;\n\tstruct raw_frag_vec rfv;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tpr_info_once(\"%s: %s forgot to set AF_INET. Fix it!\\n\",\n\t\t\t\t     __func__, current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.sockc.tsflags = sk->sk_tsflags;\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.ttl = 0;\n\tipc.tos = -1;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sk, msg, &ipc, false);\n\t\tif (unlikely(err)) {\n\t\t\tkfree(ipc.opt);\n\t\t\tgoto out;\n\t\t}\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (inet->hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->opt.srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->opt.faddr;\n\t\t}\n\t}\n\ttos = get_rtconn_flags(&ipc, sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t} else if (!ipc.oif)\n\t\tipc.oif = inet->uc_index;\n\n\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t   inet_sk_flowi_flags(sk) |\n\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n\t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n\n\tif (!inet->hdrincl) {\n\t\trfv.msg = msg;\n\t\trfv.hlen = 0;\n\n\t\terr = raw_probe_proto_opt(&rfv, &fl4);\n\t\tif (err)\n\t\t\tgoto done;\n\t}\n\n\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto done;\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (inet->hdrincl)\n\t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n\t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n\n\t else {\n\t\tsock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);\n\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = fl4.daddr;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, &fl4, raw_getfrag,\n\t\t\t\t     &rfv, len, 0,\n\t\t\t\t     &ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk, &fl4);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(&rt->dst, &fl4.daddr);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "func": "static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\tstruct ip_options_data opt_copy;\n\tstruct raw_frag_vec rfv;\n\tint hdrincl;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n\t * but READ_ONCE() doesn't work with bit fields\n\t */\n\thdrincl = inet->hdrincl;\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tpr_info_once(\"%s: %s forgot to set AF_INET. Fix it!\\n\",\n\t\t\t\t     __func__, current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.sockc.tsflags = sk->sk_tsflags;\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.ttl = 0;\n\tipc.tos = -1;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sk, msg, &ipc, false);\n\t\tif (unlikely(err)) {\n\t\t\tkfree(ipc.opt);\n\t\t\tgoto out;\n\t\t}\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->opt.srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->opt.faddr;\n\t\t}\n\t}\n\ttos = get_rtconn_flags(&ipc, sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t} else if (!ipc.oif)\n\t\tipc.oif = inet->uc_index;\n\n\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t   inet_sk_flowi_flags(sk) |\n\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n\t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n\n\tif (!hdrincl) {\n\t\trfv.msg = msg;\n\t\trfv.hlen = 0;\n\n\t\terr = raw_probe_proto_opt(&rfv, &fl4);\n\t\tif (err)\n\t\t\tgoto done;\n\t}\n\n\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto done;\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (hdrincl)\n\t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n\t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n\n\t else {\n\t\tsock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);\n\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = fl4.daddr;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, &fl4, raw_getfrag,\n\t\t\t\t     &rfv, len, 0,\n\t\t\t\t     &ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk, &fl4);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(&rt->dst, &fl4.daddr);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,11 +12,16 @@\n \tint err;\n \tstruct ip_options_data opt_copy;\n \tstruct raw_frag_vec rfv;\n+\tint hdrincl;\n \n \terr = -EMSGSIZE;\n \tif (len > 0xFFFF)\n \t\tgoto out;\n \n+\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n+\t * but READ_ONCE() doesn't work with bit fields\n+\t */\n+\thdrincl = inet->hdrincl;\n \t/*\n \t *\tCheck the flags.\n \t */\n@@ -92,7 +97,7 @@\n \t\t/* Linux does not mangle headers on raw sockets,\n \t\t * so that IP options + IP_HDRINCL is non-sense.\n \t\t */\n-\t\tif (inet->hdrincl)\n+\t\tif (hdrincl)\n \t\t\tgoto done;\n \t\tif (ipc.opt->opt.srr) {\n \t\t\tif (!daddr)\n@@ -114,12 +119,12 @@\n \n \tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n \t\t\t   RT_SCOPE_UNIVERSE,\n-\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n+\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n \t\t\t   inet_sk_flowi_flags(sk) |\n-\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n+\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n \t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n \n-\tif (!inet->hdrincl) {\n+\tif (!hdrincl) {\n \t\trfv.msg = msg;\n \t\trfv.hlen = 0;\n \n@@ -144,7 +149,7 @@\n \t\tgoto do_confirm;\n back_from_confirm:\n \n-\tif (inet->hdrincl)\n+\tif (hdrincl)\n \t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n \t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (inet->hdrincl)",
                "\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,",
                "\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),",
                "\tif (!inet->hdrincl) {",
                "\tif (inet->hdrincl)"
            ],
            "added_lines": [
                "\tint hdrincl;",
                "\t/* hdrincl should be READ_ONCE(inet->hdrincl)",
                "\t * but READ_ONCE() doesn't work with bit fields",
                "\t */",
                "\thdrincl = inet->hdrincl;",
                "\t\tif (hdrincl)",
                "\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,",
                "\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),",
                "\tif (!hdrincl) {",
                "\tif (hdrincl)"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15129",
        "func_name": "torvalds/linux/get_net_ns_by_id",
        "description": "A use-after-free vulnerability was found in network namespaces code affecting the Linux kernel before 4.14.11. The function get_net_ns_by_id() in net/core/net_namespace.c does not check for the net::count value after it has found a peer network in netns_ids idr, which could lead to double free and memory corruption. This vulnerability could allow an unprivileged local user to induce kernel memory corruption on the system, leading to a crash. Due to the nature of the flaw, privilege escalation cannot be fully ruled out, although it is thought to be unlikely.",
        "git_url": "https://github.com/torvalds/linux/commit/21b5944350052d2583e82dd59b19a9ba94a007f0",
        "commit_title": "net: Fix double free and memory corruption in get_net_ns_by_id()",
        "commit_text": " (I can trivially verify that that idr_remove in cleanup_net happens  after the network namespace count has dropped to zero --EWB)  Function get_net_ns_by_id() does not check for net::count after it has found a peer in netns_ids idr.  It may dereference a peer, after its count has already been finaly decremented. This leads to double free and memory corruption:  put_net(peer)                                   rtnl_lock() atomic_dec_and_test(&peer->count) [count=0]     ... __put_net(peer)                                 get_net_ns_by_id(net, id)   spin_lock(&cleanup_list_lock)   list_add(&net->cleanup_list, &cleanup_list)   spin_unlock(&cleanup_list_lock) queue_work()                                      peer = idr_find(&net->netns_ids, id)   |                                               get_net(peer) [count=1]   |                                               ...   |                                               (use after final put)   v                                               ...   cleanup_net()                                   ...     spin_lock(&cleanup_list_lock)                 ...     list_replace_init(&cleanup_list, ..)          ...     spin_unlock(&cleanup_list_lock)               ...     ...                                           ...     ...                                           put_net(peer)     ...                                             atomic_dec_and_test(&peer->count) [count=0]     ...                                               spin_lock(&cleanup_list_lock)     ...                                               list_add(&net->cleanup_list, &cleanup_list)     ...                                               spin_unlock(&cleanup_list_lock)     ...                                             queue_work()     ...                                           rtnl_unlock()     rtnl_lock()                                   ...     for_each_net(tmp) {                           ...       id = __peernet2id(tmp, peer)                ...       spin_lock_irq(&tmp->nsid_lock)              ...       idr_remove(&tmp->netns_ids, id)             ...       ...                                         ...       net_drop_ns()                               ... \tnet_free(peer)                            ...     }                                             ...   |   v   cleanup_net()     ...     (Second free of peer)  Also, put_net() on the right cpu may reorder with left's cpu list_replace_init(&cleanup_list, ..), and then cleanup_list will be corrupted.  Since cleanup_net() is executed in worker thread, while put_net(peer) can happen everywhere, there should be enough time for concurrent get_net_ns_by_id() to pick the peer up, and the race does not seem to be unlikely. The patch fixes the problem in standard way.  (Also, there is possible problem in peernet2id_alloc(), which requires check for net::count under nsid_lock and maybe_get_net(peer), but in current stable kernel it's used under rtnl_lock() and it has to be safe. Openswitch begun to use peernet2id_alloc(), and possibly it should be fixed too. While this is not in stable kernel yet, so I'll send a separate message to netdev@ later).  Cc: Nicolas Dichtel <nicolas.dichtel@6wind.com>",
        "func_before": "struct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tspin_lock_bh(&net->nsid_lock);\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tget_net(peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\trcu_read_unlock();\n\n\treturn peer;\n}",
        "func": "struct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tspin_lock_bh(&net->nsid_lock);\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tpeer = maybe_get_net(peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\trcu_read_unlock();\n\n\treturn peer;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n \tspin_lock_bh(&net->nsid_lock);\n \tpeer = idr_find(&net->netns_ids, id);\n \tif (peer)\n-\t\tget_net(peer);\n+\t\tpeer = maybe_get_net(peer);\n \tspin_unlock_bh(&net->nsid_lock);\n \trcu_read_unlock();\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tget_net(peer);"
            ],
            "added_lines": [
                "\t\tpeer = maybe_get_net(peer);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5344",
        "func_name": "torvalds/linux/lo_release",
        "description": "In the Linux kernel through 4.14.13, drivers/block/loop.c mishandles lo_release serialization, which allows attackers to cause a denial of service (__lock_acquire use-after-free) or possibly have unspecified other impact.",
        "git_url": "https://github.com/torvalds/linux/commit/ae6650163c66a7eff1acd6eb8b0f752dcfa8eba5",
        "commit_title": "loop: fix concurrent lo_open/lo_release",
        "commit_text": " 范龙飞 reports that KASAN can report a use-after-free in __lock_acquire. The reason is due to insufficient serialization in lo_release(), which will continue to use the loop device even after it has decremented the lo_refcnt to zero.  In the meantime, another process can come in, open the loop device again as it is being shut down. Confusion ensues. ",
        "func_before": "static void lo_release(struct gendisk *disk, fmode_t mode)\n{\n\tstruct loop_device *lo = disk->private_data;\n\tint err;\n\n\tif (atomic_dec_return(&lo->lo_refcnt))\n\t\treturn;\n\n\tmutex_lock(&lo->lo_ctl_mutex);\n\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {\n\t\t/*\n\t\t * In autoclear mode, stop the loop thread\n\t\t * and remove configuration after last close.\n\t\t */\n\t\terr = loop_clr_fd(lo);\n\t\tif (!err)\n\t\t\treturn;\n\t} else if (lo->lo_state == Lo_bound) {\n\t\t/*\n\t\t * Otherwise keep thread (if running) and config,\n\t\t * but flush possible ongoing bios in thread.\n\t\t */\n\t\tblk_mq_freeze_queue(lo->lo_queue);\n\t\tblk_mq_unfreeze_queue(lo->lo_queue);\n\t}\n\n\tmutex_unlock(&lo->lo_ctl_mutex);\n}",
        "func": "static void lo_release(struct gendisk *disk, fmode_t mode)\n{\n\tmutex_lock(&loop_index_mutex);\n\t__lo_release(disk->private_data);\n\tmutex_unlock(&loop_index_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,28 +1,6 @@\n static void lo_release(struct gendisk *disk, fmode_t mode)\n {\n-\tstruct loop_device *lo = disk->private_data;\n-\tint err;\n-\n-\tif (atomic_dec_return(&lo->lo_refcnt))\n-\t\treturn;\n-\n-\tmutex_lock(&lo->lo_ctl_mutex);\n-\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {\n-\t\t/*\n-\t\t * In autoclear mode, stop the loop thread\n-\t\t * and remove configuration after last close.\n-\t\t */\n-\t\terr = loop_clr_fd(lo);\n-\t\tif (!err)\n-\t\t\treturn;\n-\t} else if (lo->lo_state == Lo_bound) {\n-\t\t/*\n-\t\t * Otherwise keep thread (if running) and config,\n-\t\t * but flush possible ongoing bios in thread.\n-\t\t */\n-\t\tblk_mq_freeze_queue(lo->lo_queue);\n-\t\tblk_mq_unfreeze_queue(lo->lo_queue);\n-\t}\n-\n-\tmutex_unlock(&lo->lo_ctl_mutex);\n+\tmutex_lock(&loop_index_mutex);\n+\t__lo_release(disk->private_data);\n+\tmutex_unlock(&loop_index_mutex);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct loop_device *lo = disk->private_data;",
                "\tint err;",
                "",
                "\tif (atomic_dec_return(&lo->lo_refcnt))",
                "\t\treturn;",
                "",
                "\tmutex_lock(&lo->lo_ctl_mutex);",
                "\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {",
                "\t\t/*",
                "\t\t * In autoclear mode, stop the loop thread",
                "\t\t * and remove configuration after last close.",
                "\t\t */",
                "\t\terr = loop_clr_fd(lo);",
                "\t\tif (!err)",
                "\t\t\treturn;",
                "\t} else if (lo->lo_state == Lo_bound) {",
                "\t\t/*",
                "\t\t * Otherwise keep thread (if running) and config,",
                "\t\t * but flush possible ongoing bios in thread.",
                "\t\t */",
                "\t\tblk_mq_freeze_queue(lo->lo_queue);",
                "\t\tblk_mq_unfreeze_queue(lo->lo_queue);",
                "\t}",
                "",
                "\tmutex_unlock(&lo->lo_ctl_mutex);"
            ],
            "added_lines": [
                "\tmutex_lock(&loop_index_mutex);",
                "\t__lo_release(disk->private_data);",
                "\tmutex_unlock(&loop_index_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-19824",
        "func_name": "mpv-player/mpv/fill_audio_out_buffers",
        "description": "An issue in MPV v.0.29.1 fixed in v0.30 allows attackers to execute arbitrary code and crash program via the ao_c parameter.",
        "git_url": "https://github.com/mpv-player/mpv/commit/5858e3cdbd6fbae3ed80366912dd5df0af4fa126",
        "commit_title": "audio: fix use-after-free with fuzzed file",
        "commit_text": " reinit_audio_filters_and_output() can fully shutdown the audio chain on failure. Specifically, it will deallocate mpctx->ao_chain. The value of that field was cached in ao_c. The code after the call did not account that the audio chain can be shutdown, and used the stale ao_c value. ",
        "func_before": "void fill_audio_out_buffers(struct MPContext *mpctx)\n{\n    struct MPOpts *opts = mpctx->opts;\n    bool was_eof = mpctx->audio_status == STATUS_EOF;\n\n    dump_audio_stats(mpctx);\n\n    if (mpctx->ao && ao_query_and_reset_events(mpctx->ao, AO_EVENT_RELOAD))\n        reload_audio_output(mpctx);\n\n    if (mpctx->ao && ao_query_and_reset_events(mpctx->ao,\n                                               AO_EVENT_INITIAL_UNBLOCK))\n        ao_unblock(mpctx->ao);\n\n    struct ao_chain *ao_c = mpctx->ao_chain;\n    if (!ao_c)\n        return;\n\n    if (ao_c->filter->failed_output_conversion) {\n        error_on_track(mpctx, ao_c->track);\n        return;\n    }\n\n    // (if AO is set due to gapless from previous file, then we can try to\n    // filter normally until the filter tells us to change the AO)\n    if (!mpctx->ao) {\n        // Probe the initial audio format.\n        mp_pin_out_request_data(ao_c->filter->f->pins[1]);\n        reinit_audio_filters_and_output(mpctx);\n        if (ao_c->filter->got_output_eof &&\n            mpctx->audio_status != STATUS_EOF)\n        {\n            mpctx->audio_status = STATUS_EOF;\n            MP_VERBOSE(mpctx, \"audio EOF without any data\\n\");\n            mp_filter_reset(ao_c->filter->f);\n            encode_lavc_stream_eof(mpctx->encode_lavc_ctx, STREAM_AUDIO);\n        }\n        return; // try again next iteration\n    }\n\n    if (ao_c->ao_resume_time > mp_time_sec()) {\n        double remaining = ao_c->ao_resume_time - mp_time_sec();\n        mp_set_timeout(mpctx, remaining);\n        return;\n    }\n\n    if (mpctx->vo_chain && ao_c->track && ao_c->track->dec &&\n        ao_c->track->dec->pts_reset)\n    {\n        MP_VERBOSE(mpctx, \"Reset playback due to audio timestamp reset.\\n\");\n        reset_playback_state(mpctx);\n        mp_wakeup_core(mpctx);\n        return;\n    }\n\n    int ao_rate;\n    int ao_format;\n    struct mp_chmap ao_channels;\n    ao_get_format(mpctx->ao, &ao_rate, &ao_format, &ao_channels);\n    double play_samplerate = ao_rate / mpctx->audio_speed;\n    int align = af_format_sample_alignment(ao_format);\n\n    // If audio is infinitely fast, somehow try keeping approximate A/V sync.\n    if (mpctx->audio_status == STATUS_PLAYING && ao_untimed(mpctx->ao) &&\n        mpctx->video_status != STATUS_EOF && mpctx->delay > 0)\n        return;\n\n    int playsize = ao_get_space(mpctx->ao);\n\n    int skip = 0;\n    bool sync_known = get_sync_samples(mpctx, &skip);\n    if (skip > 0) {\n        playsize = MPMIN(skip + 1, MPMAX(playsize, 2500)); // buffer extra data\n    } else if (skip < 0) {\n        playsize = MPMAX(1, playsize + skip); // silence will be prepended\n    }\n\n    int skip_duplicate = 0; // >0: skip, <0: duplicate\n    double drop_limit =\n        (opts->sync_max_audio_change + opts->sync_max_video_change) / 100;\n    if (mpctx->display_sync_active && opts->video_sync == VS_DISP_ADROP &&\n        fabs(mpctx->last_av_difference) >= opts->sync_audio_drop_size &&\n        mpctx->audio_drop_throttle < drop_limit &&\n        mpctx->audio_status == STATUS_PLAYING)\n    {\n        int samples = ceil(opts->sync_audio_drop_size * play_samplerate);\n        samples = (samples + align / 2) / align * align;\n\n        skip_duplicate = mpctx->last_av_difference >= 0 ? -samples : samples;\n\n        playsize = MPMAX(playsize, samples);\n\n        mpctx->audio_drop_throttle += 1 - drop_limit - samples / play_samplerate;\n    }\n\n    playsize = playsize / align * align;\n\n    int status = mpctx->audio_status >= STATUS_DRAINING ? AD_EOF : AD_OK;\n    bool working = false;\n    if (playsize > mp_audio_buffer_samples(ao_c->ao_buffer)) {\n        status = filter_audio(mpctx, ao_c->ao_buffer, playsize);\n        if (ao_c->filter->ao_needs_update) {\n            reinit_audio_filters_and_output(mpctx);\n            mp_wakeup_core(mpctx);\n            return; // retry on next iteration\n        }\n        if (status == AD_WAIT)\n            return;\n        working = true;\n    }\n\n    // If EOF was reached before, but now something can be decoded, try to\n    // restart audio properly. This helps with video files where audio starts\n    // later. Retrying is needed to get the correct sync PTS.\n    if (mpctx->audio_status >= STATUS_DRAINING &&\n        mp_audio_buffer_samples(ao_c->ao_buffer) > 0)\n    {\n        mpctx->audio_status = STATUS_SYNCING;\n        return; // retry on next iteration\n    }\n\n    bool end_sync = false;\n    if (skip >= 0) {\n        int max = mp_audio_buffer_samples(ao_c->ao_buffer);\n        mp_audio_buffer_skip(ao_c->ao_buffer, MPMIN(skip, max));\n        // If something is left, we definitely reached the target time.\n        end_sync |= sync_known && skip < max;\n        working |= skip > 0;\n    } else if (skip < 0) {\n        if (-skip > playsize) { // heuristic against making the buffer too large\n            ao_reset(mpctx->ao); // some AOs repeat data on underflow\n            mpctx->audio_status = STATUS_DRAINING;\n            mpctx->delay = 0;\n            return;\n        }\n        mp_audio_buffer_prepend_silence(ao_c->ao_buffer, -skip);\n        end_sync = true;\n    }\n\n    if (skip_duplicate) {\n        int max = mp_audio_buffer_samples(ao_c->ao_buffer);\n        if (abs(skip_duplicate) > max)\n            skip_duplicate = skip_duplicate >= 0 ? max : -max;\n        mpctx->last_av_difference += skip_duplicate / play_samplerate;\n        if (skip_duplicate >= 0) {\n            mp_audio_buffer_skip(ao_c->ao_buffer, skip_duplicate);\n            MP_STATS(mpctx, \"drop-audio\");\n        } else {\n            mp_audio_buffer_duplicate(ao_c->ao_buffer, -skip_duplicate);\n            MP_STATS(mpctx, \"duplicate-audio\");\n        }\n        MP_VERBOSE(mpctx, \"audio skip_duplicate=%d\\n\", skip_duplicate);\n    }\n\n    if (mpctx->audio_status == STATUS_SYNCING) {\n        if (end_sync)\n            mpctx->audio_status = STATUS_FILLING;\n        if (status != AD_OK && !mp_audio_buffer_samples(ao_c->ao_buffer))\n            mpctx->audio_status = STATUS_EOF;\n        if (working || end_sync)\n            mp_wakeup_core(mpctx);\n        return; // continue on next iteration\n    }\n\n    assert(mpctx->audio_status >= STATUS_FILLING);\n\n    // We already have as much data as the audio device wants, and can start\n    // writing it any time.\n    if (mpctx->audio_status == STATUS_FILLING)\n        mpctx->audio_status = STATUS_READY;\n\n    // Even if we're done decoding and syncing, let video start first - this is\n    // required, because sending audio to the AO already starts playback.\n    if (mpctx->audio_status == STATUS_READY) {\n        // Warning: relies on handle_playback_restart() being called afterwards.\n        return;\n    }\n\n    bool audio_eof = status == AD_EOF;\n    bool partial_fill = false;\n    int playflags = 0;\n\n    if (playsize > mp_audio_buffer_samples(ao_c->ao_buffer)) {\n        playsize = mp_audio_buffer_samples(ao_c->ao_buffer);\n        partial_fill = true;\n    }\n\n    audio_eof &= partial_fill;\n\n    // With gapless audio, delay this to ao_uninit. There must be only\n    // 1 final chunk, and that is handled when calling ao_uninit().\n    if (audio_eof && !opts->gapless_audio)\n        playflags |= AOPLAY_FINAL_CHUNK;\n\n    uint8_t **planes;\n    int samples;\n    mp_audio_buffer_peek(ao_c->ao_buffer, &planes, &samples);\n    if (audio_eof || samples >= align)\n        samples = samples / align * align;\n    samples = MPMIN(samples, mpctx->paused ? 0 : playsize);\n    int played = write_to_ao(mpctx, planes, samples, playflags);\n    assert(played >= 0 && played <= samples);\n    mp_audio_buffer_skip(ao_c->ao_buffer, played);\n\n    mpctx->audio_drop_throttle =\n        MPMAX(0, mpctx->audio_drop_throttle - played / play_samplerate);\n\n    dump_audio_stats(mpctx);\n\n    mpctx->audio_status = STATUS_PLAYING;\n    if (audio_eof && !playsize) {\n        mpctx->audio_status = STATUS_DRAINING;\n        // Wait until the AO has played all queued data. In the gapless case,\n        // we trigger EOF immediately, and let it play asynchronously.\n        if (ao_eof_reached(mpctx->ao) || opts->gapless_audio) {\n            mpctx->audio_status = STATUS_EOF;\n            if (!was_eof) {\n                MP_VERBOSE(mpctx, \"audio EOF reached\\n\");\n                mp_wakeup_core(mpctx);\n                encode_lavc_stream_eof(mpctx->encode_lavc_ctx, STREAM_AUDIO);\n            }\n        }\n    }\n}",
        "func": "void fill_audio_out_buffers(struct MPContext *mpctx)\n{\n    struct MPOpts *opts = mpctx->opts;\n    bool was_eof = mpctx->audio_status == STATUS_EOF;\n\n    dump_audio_stats(mpctx);\n\n    if (mpctx->ao && ao_query_and_reset_events(mpctx->ao, AO_EVENT_RELOAD))\n        reload_audio_output(mpctx);\n\n    if (mpctx->ao && ao_query_and_reset_events(mpctx->ao,\n                                               AO_EVENT_INITIAL_UNBLOCK))\n        ao_unblock(mpctx->ao);\n\n    struct ao_chain *ao_c = mpctx->ao_chain;\n    if (!ao_c)\n        return;\n\n    if (ao_c->filter->failed_output_conversion) {\n        error_on_track(mpctx, ao_c->track);\n        return;\n    }\n\n    // (if AO is set due to gapless from previous file, then we can try to\n    // filter normally until the filter tells us to change the AO)\n    if (!mpctx->ao) {\n        // Probe the initial audio format.\n        mp_pin_out_request_data(ao_c->filter->f->pins[1]);\n        reinit_audio_filters_and_output(mpctx);\n        if (!mpctx->ao_chain)\n            return;\n        if (ao_c->filter->got_output_eof &&\n            mpctx->audio_status != STATUS_EOF)\n        {\n            mpctx->audio_status = STATUS_EOF;\n            MP_VERBOSE(mpctx, \"audio EOF without any data\\n\");\n            mp_filter_reset(ao_c->filter->f);\n            encode_lavc_stream_eof(mpctx->encode_lavc_ctx, STREAM_AUDIO);\n        }\n        return; // try again next iteration\n    }\n\n    if (ao_c->ao_resume_time > mp_time_sec()) {\n        double remaining = ao_c->ao_resume_time - mp_time_sec();\n        mp_set_timeout(mpctx, remaining);\n        return;\n    }\n\n    if (mpctx->vo_chain && ao_c->track && ao_c->track->dec &&\n        ao_c->track->dec->pts_reset)\n    {\n        MP_VERBOSE(mpctx, \"Reset playback due to audio timestamp reset.\\n\");\n        reset_playback_state(mpctx);\n        mp_wakeup_core(mpctx);\n        return;\n    }\n\n    int ao_rate;\n    int ao_format;\n    struct mp_chmap ao_channels;\n    ao_get_format(mpctx->ao, &ao_rate, &ao_format, &ao_channels);\n    double play_samplerate = ao_rate / mpctx->audio_speed;\n    int align = af_format_sample_alignment(ao_format);\n\n    // If audio is infinitely fast, somehow try keeping approximate A/V sync.\n    if (mpctx->audio_status == STATUS_PLAYING && ao_untimed(mpctx->ao) &&\n        mpctx->video_status != STATUS_EOF && mpctx->delay > 0)\n        return;\n\n    int playsize = ao_get_space(mpctx->ao);\n\n    int skip = 0;\n    bool sync_known = get_sync_samples(mpctx, &skip);\n    if (skip > 0) {\n        playsize = MPMIN(skip + 1, MPMAX(playsize, 2500)); // buffer extra data\n    } else if (skip < 0) {\n        playsize = MPMAX(1, playsize + skip); // silence will be prepended\n    }\n\n    int skip_duplicate = 0; // >0: skip, <0: duplicate\n    double drop_limit =\n        (opts->sync_max_audio_change + opts->sync_max_video_change) / 100;\n    if (mpctx->display_sync_active && opts->video_sync == VS_DISP_ADROP &&\n        fabs(mpctx->last_av_difference) >= opts->sync_audio_drop_size &&\n        mpctx->audio_drop_throttle < drop_limit &&\n        mpctx->audio_status == STATUS_PLAYING)\n    {\n        int samples = ceil(opts->sync_audio_drop_size * play_samplerate);\n        samples = (samples + align / 2) / align * align;\n\n        skip_duplicate = mpctx->last_av_difference >= 0 ? -samples : samples;\n\n        playsize = MPMAX(playsize, samples);\n\n        mpctx->audio_drop_throttle += 1 - drop_limit - samples / play_samplerate;\n    }\n\n    playsize = playsize / align * align;\n\n    int status = mpctx->audio_status >= STATUS_DRAINING ? AD_EOF : AD_OK;\n    bool working = false;\n    if (playsize > mp_audio_buffer_samples(ao_c->ao_buffer)) {\n        status = filter_audio(mpctx, ao_c->ao_buffer, playsize);\n        if (ao_c->filter->ao_needs_update) {\n            reinit_audio_filters_and_output(mpctx);\n            mp_wakeup_core(mpctx);\n            return; // retry on next iteration\n        }\n        if (status == AD_WAIT)\n            return;\n        working = true;\n    }\n\n    // If EOF was reached before, but now something can be decoded, try to\n    // restart audio properly. This helps with video files where audio starts\n    // later. Retrying is needed to get the correct sync PTS.\n    if (mpctx->audio_status >= STATUS_DRAINING &&\n        mp_audio_buffer_samples(ao_c->ao_buffer) > 0)\n    {\n        mpctx->audio_status = STATUS_SYNCING;\n        return; // retry on next iteration\n    }\n\n    bool end_sync = false;\n    if (skip >= 0) {\n        int max = mp_audio_buffer_samples(ao_c->ao_buffer);\n        mp_audio_buffer_skip(ao_c->ao_buffer, MPMIN(skip, max));\n        // If something is left, we definitely reached the target time.\n        end_sync |= sync_known && skip < max;\n        working |= skip > 0;\n    } else if (skip < 0) {\n        if (-skip > playsize) { // heuristic against making the buffer too large\n            ao_reset(mpctx->ao); // some AOs repeat data on underflow\n            mpctx->audio_status = STATUS_DRAINING;\n            mpctx->delay = 0;\n            return;\n        }\n        mp_audio_buffer_prepend_silence(ao_c->ao_buffer, -skip);\n        end_sync = true;\n    }\n\n    if (skip_duplicate) {\n        int max = mp_audio_buffer_samples(ao_c->ao_buffer);\n        if (abs(skip_duplicate) > max)\n            skip_duplicate = skip_duplicate >= 0 ? max : -max;\n        mpctx->last_av_difference += skip_duplicate / play_samplerate;\n        if (skip_duplicate >= 0) {\n            mp_audio_buffer_skip(ao_c->ao_buffer, skip_duplicate);\n            MP_STATS(mpctx, \"drop-audio\");\n        } else {\n            mp_audio_buffer_duplicate(ao_c->ao_buffer, -skip_duplicate);\n            MP_STATS(mpctx, \"duplicate-audio\");\n        }\n        MP_VERBOSE(mpctx, \"audio skip_duplicate=%d\\n\", skip_duplicate);\n    }\n\n    if (mpctx->audio_status == STATUS_SYNCING) {\n        if (end_sync)\n            mpctx->audio_status = STATUS_FILLING;\n        if (status != AD_OK && !mp_audio_buffer_samples(ao_c->ao_buffer))\n            mpctx->audio_status = STATUS_EOF;\n        if (working || end_sync)\n            mp_wakeup_core(mpctx);\n        return; // continue on next iteration\n    }\n\n    assert(mpctx->audio_status >= STATUS_FILLING);\n\n    // We already have as much data as the audio device wants, and can start\n    // writing it any time.\n    if (mpctx->audio_status == STATUS_FILLING)\n        mpctx->audio_status = STATUS_READY;\n\n    // Even if we're done decoding and syncing, let video start first - this is\n    // required, because sending audio to the AO already starts playback.\n    if (mpctx->audio_status == STATUS_READY) {\n        // Warning: relies on handle_playback_restart() being called afterwards.\n        return;\n    }\n\n    bool audio_eof = status == AD_EOF;\n    bool partial_fill = false;\n    int playflags = 0;\n\n    if (playsize > mp_audio_buffer_samples(ao_c->ao_buffer)) {\n        playsize = mp_audio_buffer_samples(ao_c->ao_buffer);\n        partial_fill = true;\n    }\n\n    audio_eof &= partial_fill;\n\n    // With gapless audio, delay this to ao_uninit. There must be only\n    // 1 final chunk, and that is handled when calling ao_uninit().\n    if (audio_eof && !opts->gapless_audio)\n        playflags |= AOPLAY_FINAL_CHUNK;\n\n    uint8_t **planes;\n    int samples;\n    mp_audio_buffer_peek(ao_c->ao_buffer, &planes, &samples);\n    if (audio_eof || samples >= align)\n        samples = samples / align * align;\n    samples = MPMIN(samples, mpctx->paused ? 0 : playsize);\n    int played = write_to_ao(mpctx, planes, samples, playflags);\n    assert(played >= 0 && played <= samples);\n    mp_audio_buffer_skip(ao_c->ao_buffer, played);\n\n    mpctx->audio_drop_throttle =\n        MPMAX(0, mpctx->audio_drop_throttle - played / play_samplerate);\n\n    dump_audio_stats(mpctx);\n\n    mpctx->audio_status = STATUS_PLAYING;\n    if (audio_eof && !playsize) {\n        mpctx->audio_status = STATUS_DRAINING;\n        // Wait until the AO has played all queued data. In the gapless case,\n        // we trigger EOF immediately, and let it play asynchronously.\n        if (ao_eof_reached(mpctx->ao) || opts->gapless_audio) {\n            mpctx->audio_status = STATUS_EOF;\n            if (!was_eof) {\n                MP_VERBOSE(mpctx, \"audio EOF reached\\n\");\n                mp_wakeup_core(mpctx);\n                encode_lavc_stream_eof(mpctx->encode_lavc_ctx, STREAM_AUDIO);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,6 +27,8 @@\n         // Probe the initial audio format.\n         mp_pin_out_request_data(ao_c->filter->f->pins[1]);\n         reinit_audio_filters_and_output(mpctx);\n+        if (!mpctx->ao_chain)\n+            return;\n         if (ao_c->filter->got_output_eof &&\n             mpctx->audio_status != STATUS_EOF)\n         {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        if (!mpctx->ao_chain)",
                "            return;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3609",
        "func_name": "torvalds/linux/bcm_delete_rx_op",
        "description": ".A flaw was found in the CAN BCM networking protocol in the Linux kernel, where a local attacker can abuse a flaw in the CAN subsystem to corrupt memory, crash the system or escalate privileges. This race condition in net/can/bcm.c in the Linux kernel allows for local privilege escalation to root.",
        "git_url": "https://github.com/torvalds/linux/commit/d5f9023fa61ee8b94f37a93f08e94b136cf1e463",
        "commit_title": "can: bcm: delay release of struct bcm_op after synchronize_rcu()",
        "commit_text": " can_rx_register() callbacks may be called concurrently to the call to can_rx_unregister(). The callbacks and callback data, though, are protected by RCU and the struct sock reference count.  So the callback data is really attached to the life of sk, meaning that it should be released on sk_destruct. However, bcm_remove_op() calls tasklet_kill(), and RCU callbacks may be called under RCU softirq, so that cannot be used on kernels before the introduction of HRTIMER_MODE_SOFT.  However, bcm_rx_handler() is called under RCU protection, so after calling can_rx_unregister(), we may call synchronize_rcu() in order to wait for any RCU read-side critical sections to finish. That is, bcm_rx_handler() won't be called anymore for those ops. So, we only free them, after we do that synchronize_rcu().  Link: https://lore.kernel.org/r/20210619161813.2098382-1-cascardo@canonical.com Cc: linux-stable <stable@vger.kernel.org>",
        "func_before": "static int bcm_delete_rx_op(struct list_head *ops, struct bcm_msg_head *mh,\n\t\t\t    int ifindex)\n{\n\tstruct bcm_op *op, *n;\n\n\tlist_for_each_entry_safe(op, n, ops, list) {\n\t\tif ((op->can_id == mh->can_id) && (op->ifindex == ifindex) &&\n\t\t    (op->flags & CAN_FD_FRAME) == (mh->flags & CAN_FD_FRAME)) {\n\n\t\t\t/*\n\t\t\t * Don't care if we're bound or not (due to netdev\n\t\t\t * problems) can_rx_unregister() is always a save\n\t\t\t * thing to do here.\n\t\t\t */\n\t\t\tif (op->ifindex) {\n\t\t\t\t/*\n\t\t\t\t * Only remove subscriptions that had not\n\t\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t\t * in bcm_notifier()\n\t\t\t\t */\n\t\t\t\tif (op->rx_reg_dev) {\n\t\t\t\t\tstruct net_device *dev;\n\n\t\t\t\t\tdev = dev_get_by_index(sock_net(op->sk),\n\t\t\t\t\t\t\t       op->ifindex);\n\t\t\t\t\tif (dev) {\n\t\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\t\tdev_put(dev);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\tcan_rx_unregister(sock_net(op->sk), NULL,\n\t\t\t\t\t\t  op->can_id,\n\t\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t\t  bcm_rx_handler, op);\n\n\t\t\tlist_del(&op->list);\n\t\t\tbcm_remove_op(op);\n\t\t\treturn 1; /* done */\n\t\t}\n\t}\n\n\treturn 0; /* not found */\n}",
        "func": "static int bcm_delete_rx_op(struct list_head *ops, struct bcm_msg_head *mh,\n\t\t\t    int ifindex)\n{\n\tstruct bcm_op *op, *n;\n\n\tlist_for_each_entry_safe(op, n, ops, list) {\n\t\tif ((op->can_id == mh->can_id) && (op->ifindex == ifindex) &&\n\t\t    (op->flags & CAN_FD_FRAME) == (mh->flags & CAN_FD_FRAME)) {\n\n\t\t\t/*\n\t\t\t * Don't care if we're bound or not (due to netdev\n\t\t\t * problems) can_rx_unregister() is always a save\n\t\t\t * thing to do here.\n\t\t\t */\n\t\t\tif (op->ifindex) {\n\t\t\t\t/*\n\t\t\t\t * Only remove subscriptions that had not\n\t\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t\t * in bcm_notifier()\n\t\t\t\t */\n\t\t\t\tif (op->rx_reg_dev) {\n\t\t\t\t\tstruct net_device *dev;\n\n\t\t\t\t\tdev = dev_get_by_index(sock_net(op->sk),\n\t\t\t\t\t\t\t       op->ifindex);\n\t\t\t\t\tif (dev) {\n\t\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\t\tdev_put(dev);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\tcan_rx_unregister(sock_net(op->sk), NULL,\n\t\t\t\t\t\t  op->can_id,\n\t\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t\t  bcm_rx_handler, op);\n\n\t\t\tlist_del(&op->list);\n\t\t\tsynchronize_rcu();\n\t\t\tbcm_remove_op(op);\n\t\t\treturn 1; /* done */\n\t\t}\n\t}\n\n\treturn 0; /* not found */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -35,6 +35,7 @@\n \t\t\t\t\t\t  bcm_rx_handler, op);\n \n \t\t\tlist_del(&op->list);\n+\t\t\tsynchronize_rcu();\n \t\t\tbcm_remove_op(op);\n \t\t\treturn 1; /* done */\n \t\t}",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tsynchronize_rcu();"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3609",
        "func_name": "torvalds/linux/bcm_release",
        "description": ".A flaw was found in the CAN BCM networking protocol in the Linux kernel, where a local attacker can abuse a flaw in the CAN subsystem to corrupt memory, crash the system or escalate privileges. This race condition in net/can/bcm.c in the Linux kernel allows for local privilege escalation to root.",
        "git_url": "https://github.com/torvalds/linux/commit/d5f9023fa61ee8b94f37a93f08e94b136cf1e463",
        "commit_title": "can: bcm: delay release of struct bcm_op after synchronize_rcu()",
        "commit_text": " can_rx_register() callbacks may be called concurrently to the call to can_rx_unregister(). The callbacks and callback data, though, are protected by RCU and the struct sock reference count.  So the callback data is really attached to the life of sk, meaning that it should be released on sk_destruct. However, bcm_remove_op() calls tasklet_kill(), and RCU callbacks may be called under RCU softirq, so that cannot be used on kernels before the introduction of HRTIMER_MODE_SOFT.  However, bcm_rx_handler() is called under RCU protection, so after calling can_rx_unregister(), we may call synchronize_rcu() in order to wait for any RCU read-side critical sections to finish. That is, bcm_rx_handler() won't be called anymore for those ops. So, we only free them, after we do that synchronize_rcu().  Link: https://lore.kernel.org/r/20210619161813.2098382-1-cascardo@canonical.com Cc: linux-stable <stable@vger.kernel.org>",
        "func_before": "static int bcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net;\n\tstruct bcm_sock *bo;\n\tstruct bcm_op *op, *next;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tbo = bcm_sk(sk);\n\n\t/* remove bcm_ops, timer, rx_unregister(), etc. */\n\n\tspin_lock(&bcm_notifier_lock);\n\twhile (bcm_busy_notifier == bo) {\n\t\tspin_unlock(&bcm_notifier_lock);\n\t\tschedule_timeout_uninterruptible(1);\n\t\tspin_lock(&bcm_notifier_lock);\n\t}\n\tlist_del(&bo->notifier);\n\tspin_unlock(&bcm_notifier_lock);\n\n\tlock_sock(sk);\n\n\tlist_for_each_entry_safe(op, next, &bo->tx_ops, list)\n\t\tbcm_remove_op(op);\n\n\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list) {\n\t\t/*\n\t\t * Don't care if we're bound or not (due to netdev problems)\n\t\t * can_rx_unregister() is always a save thing to do here.\n\t\t */\n\t\tif (op->ifindex) {\n\t\t\t/*\n\t\t\t * Only remove subscriptions that had not\n\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t * in bcm_notifier()\n\t\t\t */\n\t\t\tif (op->rx_reg_dev) {\n\t\t\t\tstruct net_device *dev;\n\n\t\t\t\tdev = dev_get_by_index(net, op->ifindex);\n\t\t\t\tif (dev) {\n\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\tdev_put(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tcan_rx_unregister(net, NULL, op->can_id,\n\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t  bcm_rx_handler, op);\n\n\t\tbcm_remove_op(op);\n\t}\n\n#if IS_ENABLED(CONFIG_PROC_FS)\n\t/* remove procfs entry */\n\tif (net->can.bcmproc_dir && bo->bcm_proc_read)\n\t\tremove_proc_entry(bo->procname, net->can.bcmproc_dir);\n#endif /* CONFIG_PROC_FS */\n\n\t/* remove device reference */\n\tif (bo->bound) {\n\t\tbo->bound   = 0;\n\t\tbo->ifindex = 0;\n\t}\n\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\trelease_sock(sk);\n\tsock_put(sk);\n\n\treturn 0;\n}",
        "func": "static int bcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net;\n\tstruct bcm_sock *bo;\n\tstruct bcm_op *op, *next;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tbo = bcm_sk(sk);\n\n\t/* remove bcm_ops, timer, rx_unregister(), etc. */\n\n\tspin_lock(&bcm_notifier_lock);\n\twhile (bcm_busy_notifier == bo) {\n\t\tspin_unlock(&bcm_notifier_lock);\n\t\tschedule_timeout_uninterruptible(1);\n\t\tspin_lock(&bcm_notifier_lock);\n\t}\n\tlist_del(&bo->notifier);\n\tspin_unlock(&bcm_notifier_lock);\n\n\tlock_sock(sk);\n\n\tlist_for_each_entry_safe(op, next, &bo->tx_ops, list)\n\t\tbcm_remove_op(op);\n\n\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list) {\n\t\t/*\n\t\t * Don't care if we're bound or not (due to netdev problems)\n\t\t * can_rx_unregister() is always a save thing to do here.\n\t\t */\n\t\tif (op->ifindex) {\n\t\t\t/*\n\t\t\t * Only remove subscriptions that had not\n\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t * in bcm_notifier()\n\t\t\t */\n\t\t\tif (op->rx_reg_dev) {\n\t\t\t\tstruct net_device *dev;\n\n\t\t\t\tdev = dev_get_by_index(net, op->ifindex);\n\t\t\t\tif (dev) {\n\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\tdev_put(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tcan_rx_unregister(net, NULL, op->can_id,\n\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t  bcm_rx_handler, op);\n\n\t}\n\n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list)\n\t\tbcm_remove_op(op);\n\n#if IS_ENABLED(CONFIG_PROC_FS)\n\t/* remove procfs entry */\n\tif (net->can.bcmproc_dir && bo->bcm_proc_read)\n\t\tremove_proc_entry(bo->procname, net->can.bcmproc_dir);\n#endif /* CONFIG_PROC_FS */\n\n\t/* remove device reference */\n\tif (bo->bound) {\n\t\tbo->bound   = 0;\n\t\tbo->ifindex = 0;\n\t}\n\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\trelease_sock(sk);\n\tsock_put(sk);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -52,8 +52,12 @@\n \t\t\t\t\t  REGMASK(op->can_id),\n \t\t\t\t\t  bcm_rx_handler, op);\n \n+\t}\n+\n+\tsynchronize_rcu();\n+\n+\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list)\n \t\tbcm_remove_op(op);\n-\t}\n \n #if IS_ENABLED(CONFIG_PROC_FS)\n \t/* remove procfs entry */",
        "diff_line_info": {
            "deleted_lines": [
                "\t}"
            ],
            "added_lines": [
                "\t}",
                "",
                "\tsynchronize_rcu();",
                "",
                "\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list)"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3640",
        "func_name": "torvalds/linux/sco_sock_sendmsg",
        "description": "A flaw use-after-free in function sco_sock_sendmsg() of the Linux kernel HCI subsystem was found in the way user calls ioct UFFDIO_REGISTER or other way triggers race condition of the call sco_conn_del() together with the call sco_sock_sendmsg() with the expected controllable faulting memory page. A privileged local user could use this flaw to crash the system or escalate their privileges on the system.",
        "git_url": "https://github.com/torvalds/linux/commit/99c23da0eed4fd20cae8243f2b51e10e66aa0951",
        "commit_title": "Bluetooth: sco: Fix lock_sock() blockage by memcpy_from_msg()",
        "commit_text": " The sco_send_frame() also takes lock_sock() during memcpy_from_msg() call that may be endlessly blocked by a task with userfaultd technique, and this will result in a hung task watchdog trigger.  Just like the similar fix for hci_sock_sendmsg() in commit 92c685dc5de0 (\"Bluetooth: reorganize functions...\"), this patch moves the  memcpy_from_msg() out of lock_sock() for addressing the hang.  This should be the last piece for fixing CVE-2021-3640 after a few already queued fixes. ",
        "func_before": "static int sco_sock_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t    size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\terr = sock_error(sk);\n\tif (err)\n\t\treturn err;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == BT_CONNECTED)\n\t\terr = sco_send_frame(sk, msg, len);\n\telse\n\t\terr = -ENOTCONN;\n\n\trelease_sock(sk);\n\treturn err;\n}",
        "func": "static int sco_sock_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t    size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tvoid *buf;\n\tint err;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\terr = sock_error(sk);\n\tif (err)\n\t\treturn err;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tbuf = kmalloc(len, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tif (memcpy_from_msg(buf, msg, len)) {\n\t\tkfree(buf);\n\t\treturn -EFAULT;\n\t}\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == BT_CONNECTED)\n\t\terr = sco_send_frame(sk, buf, len, msg->msg_flags);\n\telse\n\t\terr = -ENOTCONN;\n\n\trelease_sock(sk);\n\tkfree(buf);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n \t\t\t    size_t len)\n {\n \tstruct sock *sk = sock->sk;\n+\tvoid *buf;\n \tint err;\n \n \tBT_DBG(\"sock %p, sk %p\", sock, sk);\n@@ -13,13 +14,23 @@\n \tif (msg->msg_flags & MSG_OOB)\n \t\treturn -EOPNOTSUPP;\n \n+\tbuf = kmalloc(len, GFP_KERNEL);\n+\tif (!buf)\n+\t\treturn -ENOMEM;\n+\n+\tif (memcpy_from_msg(buf, msg, len)) {\n+\t\tkfree(buf);\n+\t\treturn -EFAULT;\n+\t}\n+\n \tlock_sock(sk);\n \n \tif (sk->sk_state == BT_CONNECTED)\n-\t\terr = sco_send_frame(sk, msg, len);\n+\t\terr = sco_send_frame(sk, buf, len, msg->msg_flags);\n \telse\n \t\terr = -ENOTCONN;\n \n \trelease_sock(sk);\n+\tkfree(buf);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\terr = sco_send_frame(sk, msg, len);"
            ],
            "added_lines": [
                "\tvoid *buf;",
                "\tbuf = kmalloc(len, GFP_KERNEL);",
                "\tif (!buf)",
                "\t\treturn -ENOMEM;",
                "",
                "\tif (memcpy_from_msg(buf, msg, len)) {",
                "\t\tkfree(buf);",
                "\t\treturn -EFAULT;",
                "\t}",
                "",
                "\t\terr = sco_send_frame(sk, buf, len, msg->msg_flags);",
                "\tkfree(buf);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3640",
        "func_name": "torvalds/linux/sco_send_frame",
        "description": "A flaw use-after-free in function sco_sock_sendmsg() of the Linux kernel HCI subsystem was found in the way user calls ioct UFFDIO_REGISTER or other way triggers race condition of the call sco_conn_del() together with the call sco_sock_sendmsg() with the expected controllable faulting memory page. A privileged local user could use this flaw to crash the system or escalate their privileges on the system.",
        "git_url": "https://github.com/torvalds/linux/commit/99c23da0eed4fd20cae8243f2b51e10e66aa0951",
        "commit_title": "Bluetooth: sco: Fix lock_sock() blockage by memcpy_from_msg()",
        "commit_text": " The sco_send_frame() also takes lock_sock() during memcpy_from_msg() call that may be endlessly blocked by a task with userfaultd technique, and this will result in a hung task watchdog trigger.  Just like the similar fix for hci_sock_sendmsg() in commit 92c685dc5de0 (\"Bluetooth: reorganize functions...\"), this patch moves the  memcpy_from_msg() out of lock_sock() for addressing the hang.  This should be the last piece for fixing CVE-2021-3640 after a few already queued fixes. ",
        "func_before": "static int sco_send_frame(struct sock *sk, struct msghdr *msg, int len)\n{\n\tstruct sco_conn *conn = sco_pi(sk)->conn;\n\tstruct sk_buff *skb;\n\tint err;\n\n\t/* Check outgoing MTU */\n\tif (len > conn->mtu)\n\t\treturn -EINVAL;\n\n\tBT_DBG(\"sk %p len %d\", sk, len);\n\n\tskb = bt_skb_send_alloc(sk, len, msg->msg_flags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tif (memcpy_from_msg(skb_put(skb, len), msg, len)) {\n\t\tkfree_skb(skb);\n\t\treturn -EFAULT;\n\t}\n\n\thci_send_sco(conn->hcon, skb);\n\n\treturn len;\n}",
        "func": "static int sco_send_frame(struct sock *sk, void *buf, int len,\n\t\t\t  unsigned int msg_flags)\n{\n\tstruct sco_conn *conn = sco_pi(sk)->conn;\n\tstruct sk_buff *skb;\n\tint err;\n\n\t/* Check outgoing MTU */\n\tif (len > conn->mtu)\n\t\treturn -EINVAL;\n\n\tBT_DBG(\"sk %p len %d\", sk, len);\n\n\tskb = bt_skb_send_alloc(sk, len, msg_flags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tmemcpy(skb_put(skb, len), buf, len);\n\thci_send_sco(conn->hcon, skb);\n\n\treturn len;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n-static int sco_send_frame(struct sock *sk, struct msghdr *msg, int len)\n+static int sco_send_frame(struct sock *sk, void *buf, int len,\n+\t\t\t  unsigned int msg_flags)\n {\n \tstruct sco_conn *conn = sco_pi(sk)->conn;\n \tstruct sk_buff *skb;\n@@ -10,15 +11,11 @@\n \n \tBT_DBG(\"sk %p len %d\", sk, len);\n \n-\tskb = bt_skb_send_alloc(sk, len, msg->msg_flags & MSG_DONTWAIT, &err);\n+\tskb = bt_skb_send_alloc(sk, len, msg_flags & MSG_DONTWAIT, &err);\n \tif (!skb)\n \t\treturn err;\n \n-\tif (memcpy_from_msg(skb_put(skb, len), msg, len)) {\n-\t\tkfree_skb(skb);\n-\t\treturn -EFAULT;\n-\t}\n-\n+\tmemcpy(skb_put(skb, len), buf, len);\n \thci_send_sco(conn->hcon, skb);\n \n \treturn len;",
        "diff_line_info": {
            "deleted_lines": [
                "static int sco_send_frame(struct sock *sk, struct msghdr *msg, int len)",
                "\tskb = bt_skb_send_alloc(sk, len, msg->msg_flags & MSG_DONTWAIT, &err);",
                "\tif (memcpy_from_msg(skb_put(skb, len), msg, len)) {",
                "\t\tkfree_skb(skb);",
                "\t\treturn -EFAULT;",
                "\t}",
                ""
            ],
            "added_lines": [
                "static int sco_send_frame(struct sock *sk, void *buf, int len,",
                "\t\t\t  unsigned int msg_flags)",
                "\tskb = bt_skb_send_alloc(sk, len, msg_flags & MSG_DONTWAIT, &err);",
                "\tmemcpy(skb_put(skb, len), buf, len);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4202",
        "func_name": "torvalds/linux/nfc_register_device",
        "description": "A use-after-free flaw was found in nci_request in net/nfc/nci/core.c in NFC Controller Interface (NCI) in the Linux kernel. This flaw could allow a local attacker with user privileges to cause a data race problem while the device is getting removed, leading to a privilege escalation problem.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=3e3b5dfcd16a3e254aab61bd1e8c417dd4503102",
        "commit_title": "There is a potential UAF between the unregistration routine and the NFC",
        "commit_text": "netlink operations.  The race that cause that UAF can be shown as below:   (FREE)                      |  (USE) nfcmrvl_nci_unregister_dev   |  nfc_genl_dev_up   nci_close_device           |   nci_unregister_device      |    nfc_get_device     nfc_unregister_device    |    nfc_dev_up       rfkill_destory         |       device_del             |      rfkill_blocked   ...                        |    ...  The root cause for this race is concluded below: 1. The rfkill_blocked (USE) in nfc_dev_up is supposed to be placed after the device_is_registered check. 2. Since the netlink operations are possible just after the device_add in nfc_register_device, the nfc_dev_up() can happen anywhere during the rfkill creation process, which leads to data race.  This patch reorder these actions to permit 1. Once device_del is finished, the nfc_dev_up cannot dereference the rfkill object. 2. The rfkill_register need to be placed after the device_add of nfc_dev because the parent device need to be created first. So this patch keeps the order but inject device_lock to prevent the data race.  Link: https://lore.kernel.org/r/20211116152652.19217-1-linma@zju.edu.cn ",
        "func_before": "int nfc_register_device(struct nfc_dev *dev)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tmutex_lock(&nfc_devlist_mutex);\n\tnfc_devlist_generation++;\n\trc = device_add(&dev->dev);\n\tmutex_unlock(&nfc_devlist_mutex);\n\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = nfc_llcp_register_device(dev);\n\tif (rc)\n\t\tpr_err(\"Could not register llcp device\\n\");\n\n\trc = nfc_genl_device_added(dev);\n\tif (rc)\n\t\tpr_debug(\"The userspace won't be notified that the device %s was added\\n\",\n\t\t\t dev_name(&dev->dev));\n\n\tdev->rfkill = rfkill_alloc(dev_name(&dev->dev), &dev->dev,\n\t\t\t\t   RFKILL_TYPE_NFC, &nfc_rfkill_ops, dev);\n\tif (dev->rfkill) {\n\t\tif (rfkill_register(dev->rfkill) < 0) {\n\t\t\trfkill_destroy(dev->rfkill);\n\t\t\tdev->rfkill = NULL;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "func": "int nfc_register_device(struct nfc_dev *dev)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tmutex_lock(&nfc_devlist_mutex);\n\tnfc_devlist_generation++;\n\trc = device_add(&dev->dev);\n\tmutex_unlock(&nfc_devlist_mutex);\n\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = nfc_llcp_register_device(dev);\n\tif (rc)\n\t\tpr_err(\"Could not register llcp device\\n\");\n\n\tdevice_lock(&dev->dev);\n\tdev->rfkill = rfkill_alloc(dev_name(&dev->dev), &dev->dev,\n\t\t\t\t   RFKILL_TYPE_NFC, &nfc_rfkill_ops, dev);\n\tif (dev->rfkill) {\n\t\tif (rfkill_register(dev->rfkill) < 0) {\n\t\t\trfkill_destroy(dev->rfkill);\n\t\t\tdev->rfkill = NULL;\n\t\t}\n\t}\n\tdevice_unlock(&dev->dev);\n\n\trc = nfc_genl_device_added(dev);\n\tif (rc)\n\t\tpr_debug(\"The userspace won't be notified that the device %s was added\\n\",\n\t\t\t dev_name(&dev->dev));\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,11 +16,7 @@\n \tif (rc)\n \t\tpr_err(\"Could not register llcp device\\n\");\n \n-\trc = nfc_genl_device_added(dev);\n-\tif (rc)\n-\t\tpr_debug(\"The userspace won't be notified that the device %s was added\\n\",\n-\t\t\t dev_name(&dev->dev));\n-\n+\tdevice_lock(&dev->dev);\n \tdev->rfkill = rfkill_alloc(dev_name(&dev->dev), &dev->dev,\n \t\t\t\t   RFKILL_TYPE_NFC, &nfc_rfkill_ops, dev);\n \tif (dev->rfkill) {\n@@ -29,6 +25,12 @@\n \t\t\tdev->rfkill = NULL;\n \t\t}\n \t}\n+\tdevice_unlock(&dev->dev);\n+\n+\trc = nfc_genl_device_added(dev);\n+\tif (rc)\n+\t\tpr_debug(\"The userspace won't be notified that the device %s was added\\n\",\n+\t\t\t dev_name(&dev->dev));\n \n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\trc = nfc_genl_device_added(dev);",
                "\tif (rc)",
                "\t\tpr_debug(\"The userspace won't be notified that the device %s was added\\n\",",
                "\t\t\t dev_name(&dev->dev));",
                ""
            ],
            "added_lines": [
                "\tdevice_lock(&dev->dev);",
                "\tdevice_unlock(&dev->dev);",
                "",
                "\trc = nfc_genl_device_added(dev);",
                "\tif (rc)",
                "\t\tpr_debug(\"The userspace won't be notified that the device %s was added\\n\",",
                "\t\t\t dev_name(&dev->dev));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4202",
        "func_name": "torvalds/linux/nfc_unregister_device",
        "description": "A use-after-free flaw was found in nci_request in net/nfc/nci/core.c in NFC Controller Interface (NCI) in the Linux kernel. This flaw could allow a local attacker with user privileges to cause a data race problem while the device is getting removed, leading to a privilege escalation problem.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=3e3b5dfcd16a3e254aab61bd1e8c417dd4503102",
        "commit_title": "There is a potential UAF between the unregistration routine and the NFC",
        "commit_text": "netlink operations.  The race that cause that UAF can be shown as below:   (FREE)                      |  (USE) nfcmrvl_nci_unregister_dev   |  nfc_genl_dev_up   nci_close_device           |   nci_unregister_device      |    nfc_get_device     nfc_unregister_device    |    nfc_dev_up       rfkill_destory         |       device_del             |      rfkill_blocked   ...                        |    ...  The root cause for this race is concluded below: 1. The rfkill_blocked (USE) in nfc_dev_up is supposed to be placed after the device_is_registered check. 2. Since the netlink operations are possible just after the device_add in nfc_register_device, the nfc_dev_up() can happen anywhere during the rfkill creation process, which leads to data race.  This patch reorder these actions to permit 1. Once device_del is finished, the nfc_dev_up cannot dereference the rfkill object. 2. The rfkill_register need to be placed after the device_add of nfc_dev because the parent device need to be created first. So this patch keeps the order but inject device_lock to prevent the data race.  Link: https://lore.kernel.org/r/20211116152652.19217-1-linma@zju.edu.cn ",
        "func_before": "void nfc_unregister_device(struct nfc_dev *dev)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tif (dev->rfkill) {\n\t\trfkill_unregister(dev->rfkill);\n\t\trfkill_destroy(dev->rfkill);\n\t}\n\n\tif (dev->ops->check_presence) {\n\t\tdevice_lock(&dev->dev);\n\t\tdev->shutting_down = true;\n\t\tdevice_unlock(&dev->dev);\n\t\tdel_timer_sync(&dev->check_pres_timer);\n\t\tcancel_work_sync(&dev->check_pres_work);\n\t}\n\n\trc = nfc_genl_device_removed(dev);\n\tif (rc)\n\t\tpr_debug(\"The userspace won't be notified that the device %s \"\n\t\t\t \"was removed\\n\", dev_name(&dev->dev));\n\n\tnfc_llcp_unregister_device(dev);\n\n\tmutex_lock(&nfc_devlist_mutex);\n\tnfc_devlist_generation++;\n\tdevice_del(&dev->dev);\n\tmutex_unlock(&nfc_devlist_mutex);\n}",
        "func": "void nfc_unregister_device(struct nfc_dev *dev)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\trc = nfc_genl_device_removed(dev);\n\tif (rc)\n\t\tpr_debug(\"The userspace won't be notified that the device %s \"\n\t\t\t \"was removed\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\tif (dev->rfkill) {\n\t\trfkill_unregister(dev->rfkill);\n\t\trfkill_destroy(dev->rfkill);\n\t}\n\tdevice_unlock(&dev->dev);\n\n\tif (dev->ops->check_presence) {\n\t\tdevice_lock(&dev->dev);\n\t\tdev->shutting_down = true;\n\t\tdevice_unlock(&dev->dev);\n\t\tdel_timer_sync(&dev->check_pres_timer);\n\t\tcancel_work_sync(&dev->check_pres_work);\n\t}\n\n\tnfc_llcp_unregister_device(dev);\n\n\tmutex_lock(&nfc_devlist_mutex);\n\tnfc_devlist_generation++;\n\tdevice_del(&dev->dev);\n\tmutex_unlock(&nfc_devlist_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,10 +4,17 @@\n \n \tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n \n+\trc = nfc_genl_device_removed(dev);\n+\tif (rc)\n+\t\tpr_debug(\"The userspace won't be notified that the device %s \"\n+\t\t\t \"was removed\\n\", dev_name(&dev->dev));\n+\n+\tdevice_lock(&dev->dev);\n \tif (dev->rfkill) {\n \t\trfkill_unregister(dev->rfkill);\n \t\trfkill_destroy(dev->rfkill);\n \t}\n+\tdevice_unlock(&dev->dev);\n \n \tif (dev->ops->check_presence) {\n \t\tdevice_lock(&dev->dev);\n@@ -17,11 +24,6 @@\n \t\tcancel_work_sync(&dev->check_pres_work);\n \t}\n \n-\trc = nfc_genl_device_removed(dev);\n-\tif (rc)\n-\t\tpr_debug(\"The userspace won't be notified that the device %s \"\n-\t\t\t \"was removed\\n\", dev_name(&dev->dev));\n-\n \tnfc_llcp_unregister_device(dev);\n \n \tmutex_lock(&nfc_devlist_mutex);",
        "diff_line_info": {
            "deleted_lines": [
                "\trc = nfc_genl_device_removed(dev);",
                "\tif (rc)",
                "\t\tpr_debug(\"The userspace won't be notified that the device %s \"",
                "\t\t\t \"was removed\\n\", dev_name(&dev->dev));",
                ""
            ],
            "added_lines": [
                "\trc = nfc_genl_device_removed(dev);",
                "\tif (rc)",
                "\t\tpr_debug(\"The userspace won't be notified that the device %s \"",
                "\t\t\t \"was removed\\n\", dev_name(&dev->dev));",
                "",
                "\tdevice_lock(&dev->dev);",
                "\tdevice_unlock(&dev->dev);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4202",
        "func_name": "torvalds/linux/nfc_dev_up",
        "description": "A use-after-free flaw was found in nci_request in net/nfc/nci/core.c in NFC Controller Interface (NCI) in the Linux kernel. This flaw could allow a local attacker with user privileges to cause a data race problem while the device is getting removed, leading to a privilege escalation problem.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=3e3b5dfcd16a3e254aab61bd1e8c417dd4503102",
        "commit_title": "There is a potential UAF between the unregistration routine and the NFC",
        "commit_text": "netlink operations.  The race that cause that UAF can be shown as below:   (FREE)                      |  (USE) nfcmrvl_nci_unregister_dev   |  nfc_genl_dev_up   nci_close_device           |   nci_unregister_device      |    nfc_get_device     nfc_unregister_device    |    nfc_dev_up       rfkill_destory         |       device_del             |      rfkill_blocked   ...                        |    ...  The root cause for this race is concluded below: 1. The rfkill_blocked (USE) in nfc_dev_up is supposed to be placed after the device_is_registered check. 2. Since the netlink operations are possible just after the device_add in nfc_register_device, the nfc_dev_up() can happen anywhere during the rfkill creation process, which leads to data race.  This patch reorder these actions to permit 1. Once device_del is finished, the nfc_dev_up cannot dereference the rfkill object. 2. The rfkill_register need to be placed after the device_add of nfc_dev because the parent device need to be created first. So this patch keeps the order but inject device_lock to prevent the data race.  Link: https://lore.kernel.org/r/20211116152652.19217-1-linma@zju.edu.cn ",
        "func_before": "int nfc_dev_up(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->rfkill && rfkill_blocked(dev->rfkill)) {\n\t\trc = -ERFKILL;\n\t\tgoto error;\n\t}\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->fw_download_in_progress) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (dev->dev_up) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\tif (dev->ops->dev_up)\n\t\trc = dev->ops->dev_up(dev);\n\n\tif (!rc)\n\t\tdev->dev_up = true;\n\n\t/* We have to enable the device before discovering SEs */\n\tif (dev->ops->discover_se && dev->ops->discover_se(dev))\n\t\tpr_err(\"SE discovery failed\\n\");\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_dev_up(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->rfkill && rfkill_blocked(dev->rfkill)) {\n\t\trc = -ERFKILL;\n\t\tgoto error;\n\t}\n\n\tif (dev->fw_download_in_progress) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (dev->dev_up) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\tif (dev->ops->dev_up)\n\t\trc = dev->ops->dev_up(dev);\n\n\tif (!rc)\n\t\tdev->dev_up = true;\n\n\t/* We have to enable the device before discovering SEs */\n\tif (dev->ops->discover_se && dev->ops->discover_se(dev))\n\t\tpr_err(\"SE discovery failed\\n\");\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,13 +6,13 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (dev->rfkill && rfkill_blocked(dev->rfkill)) {\n-\t\trc = -ERFKILL;\n+\tif (!device_is_registered(&dev->dev)) {\n+\t\trc = -ENODEV;\n \t\tgoto error;\n \t}\n \n-\tif (!device_is_registered(&dev->dev)) {\n-\t\trc = -ENODEV;\n+\tif (dev->rfkill && rfkill_blocked(dev->rfkill)) {\n+\t\trc = -ERFKILL;\n \t\tgoto error;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (dev->rfkill && rfkill_blocked(dev->rfkill)) {",
                "\t\trc = -ERFKILL;",
                "\tif (!device_is_registered(&dev->dev)) {",
                "\t\trc = -ENODEV;"
            ],
            "added_lines": [
                "\tif (!device_is_registered(&dev->dev)) {",
                "\t\trc = -ENODEV;",
                "\tif (dev->rfkill && rfkill_blocked(dev->rfkill)) {",
                "\t\trc = -ERFKILL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4202",
        "func_name": "torvalds/linux/nci_open_device",
        "description": "A use-after-free flaw was found in nci_request in net/nfc/nci/core.c in NFC Controller Interface (NCI) in the Linux kernel. This flaw could allow a local attacker with user privileges to cause a data race problem while the device is getting removed, leading to a privilege escalation problem.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=48b71a9e66c2eab60564b1b1c85f4928ed04e406",
        "commit_title": "There are two sites that calls queue_work() after the",
        "commit_text": "destroy_workqueue() and lead to possible UAF.  The first site is nci_send_cmd(), which can happen after the nci_close_device as below  nfcmrvl_nci_unregister_dev   |  nfc_genl_dev_up   nci_close_device           |     flush_workqueue          |     del_timer_sync           |   nci_unregister_device      |    nfc_get_device     destroy_workqueue        |    nfc_dev_up     nfc_unregister_device    |      nci_dev_up       device_del             |        nci_open_device                              |          __nci_request                              |            nci_send_cmd                              |              queue_work !!!  Another site is nci_cmd_timer, awaked by the nci_cmd_work from the nci_send_cmd.    ...                        |  ...   nci_unregister_device      |  queue_work     destroy_workqueue        |     nfc_unregister_device    |  ...       device_del             |  nci_cmd_work                              |  mod_timer                              |  ...                              |  nci_cmd_timer                              |    queue_work !!!  For the above two UAF, the root cause is that the nfc_dev_up can race between the nci_unregister_device routine. Therefore, this patch introduce NCI_UNREG flag to easily eliminate the possible race. In addition, the mutex_lock in nci_close_device can act as a barrier.  Link: https://lore.kernel.org/r/20211116152732.19238-1-linma@zju.edu.cn ",
        "func_before": "static int nci_open_device(struct nci_dev *ndev)\n{\n\tint rc = 0;\n\n\tmutex_lock(&ndev->req_lock);\n\n\tif (test_bit(NCI_UP, &ndev->flags)) {\n\t\trc = -EALREADY;\n\t\tgoto done;\n\t}\n\n\tif (ndev->ops->open(ndev)) {\n\t\trc = -EIO;\n\t\tgoto done;\n\t}\n\n\tatomic_set(&ndev->cmd_cnt, 1);\n\n\tset_bit(NCI_INIT, &ndev->flags);\n\n\tif (ndev->ops->init)\n\t\trc = ndev->ops->init(ndev);\n\n\tif (!rc) {\n\t\trc = __nci_request(ndev, nci_reset_req, (void *)0,\n\t\t\t\t   msecs_to_jiffies(NCI_RESET_TIMEOUT));\n\t}\n\n\tif (!rc && ndev->ops->setup) {\n\t\trc = ndev->ops->setup(ndev);\n\t}\n\n\tif (!rc) {\n\t\tstruct nci_core_init_v2_cmd nci_init_v2_cmd = {\n\t\t\t.feature1 = NCI_FEATURE_DISABLE,\n\t\t\t.feature2 = NCI_FEATURE_DISABLE\n\t\t};\n\t\tconst void *opt = NULL;\n\n\t\tif (ndev->nci_ver & NCI_VER_2_MASK)\n\t\t\topt = &nci_init_v2_cmd;\n\n\t\trc = __nci_request(ndev, nci_init_req, opt,\n\t\t\t\t   msecs_to_jiffies(NCI_INIT_TIMEOUT));\n\t}\n\n\tif (!rc && ndev->ops->post_setup)\n\t\trc = ndev->ops->post_setup(ndev);\n\n\tif (!rc) {\n\t\trc = __nci_request(ndev, nci_init_complete_req, (void *)0,\n\t\t\t\t   msecs_to_jiffies(NCI_INIT_TIMEOUT));\n\t}\n\n\tclear_bit(NCI_INIT, &ndev->flags);\n\n\tif (!rc) {\n\t\tset_bit(NCI_UP, &ndev->flags);\n\t\tnci_clear_target_list(ndev);\n\t\tatomic_set(&ndev->state, NCI_IDLE);\n\t} else {\n\t\t/* Init failed, cleanup */\n\t\tskb_queue_purge(&ndev->cmd_q);\n\t\tskb_queue_purge(&ndev->rx_q);\n\t\tskb_queue_purge(&ndev->tx_q);\n\n\t\tndev->ops->close(ndev);\n\t\tndev->flags = 0;\n\t}\n\ndone:\n\tmutex_unlock(&ndev->req_lock);\n\treturn rc;\n}",
        "func": "static int nci_open_device(struct nci_dev *ndev)\n{\n\tint rc = 0;\n\n\tmutex_lock(&ndev->req_lock);\n\n\tif (test_bit(NCI_UNREG, &ndev->flags)) {\n\t\trc = -ENODEV;\n\t\tgoto done;\n\t}\n\n\tif (test_bit(NCI_UP, &ndev->flags)) {\n\t\trc = -EALREADY;\n\t\tgoto done;\n\t}\n\n\tif (ndev->ops->open(ndev)) {\n\t\trc = -EIO;\n\t\tgoto done;\n\t}\n\n\tatomic_set(&ndev->cmd_cnt, 1);\n\n\tset_bit(NCI_INIT, &ndev->flags);\n\n\tif (ndev->ops->init)\n\t\trc = ndev->ops->init(ndev);\n\n\tif (!rc) {\n\t\trc = __nci_request(ndev, nci_reset_req, (void *)0,\n\t\t\t\t   msecs_to_jiffies(NCI_RESET_TIMEOUT));\n\t}\n\n\tif (!rc && ndev->ops->setup) {\n\t\trc = ndev->ops->setup(ndev);\n\t}\n\n\tif (!rc) {\n\t\tstruct nci_core_init_v2_cmd nci_init_v2_cmd = {\n\t\t\t.feature1 = NCI_FEATURE_DISABLE,\n\t\t\t.feature2 = NCI_FEATURE_DISABLE\n\t\t};\n\t\tconst void *opt = NULL;\n\n\t\tif (ndev->nci_ver & NCI_VER_2_MASK)\n\t\t\topt = &nci_init_v2_cmd;\n\n\t\trc = __nci_request(ndev, nci_init_req, opt,\n\t\t\t\t   msecs_to_jiffies(NCI_INIT_TIMEOUT));\n\t}\n\n\tif (!rc && ndev->ops->post_setup)\n\t\trc = ndev->ops->post_setup(ndev);\n\n\tif (!rc) {\n\t\trc = __nci_request(ndev, nci_init_complete_req, (void *)0,\n\t\t\t\t   msecs_to_jiffies(NCI_INIT_TIMEOUT));\n\t}\n\n\tclear_bit(NCI_INIT, &ndev->flags);\n\n\tif (!rc) {\n\t\tset_bit(NCI_UP, &ndev->flags);\n\t\tnci_clear_target_list(ndev);\n\t\tatomic_set(&ndev->state, NCI_IDLE);\n\t} else {\n\t\t/* Init failed, cleanup */\n\t\tskb_queue_purge(&ndev->cmd_q);\n\t\tskb_queue_purge(&ndev->rx_q);\n\t\tskb_queue_purge(&ndev->tx_q);\n\n\t\tndev->ops->close(ndev);\n\t\tndev->flags = 0;\n\t}\n\ndone:\n\tmutex_unlock(&ndev->req_lock);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,11 @@\n \tint rc = 0;\n \n \tmutex_lock(&ndev->req_lock);\n+\n+\tif (test_bit(NCI_UNREG, &ndev->flags)) {\n+\t\trc = -ENODEV;\n+\t\tgoto done;\n+\t}\n \n \tif (test_bit(NCI_UP, &ndev->flags)) {\n \t\trc = -EALREADY;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (test_bit(NCI_UNREG, &ndev->flags)) {",
                "\t\trc = -ENODEV;",
                "\t\tgoto done;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4202",
        "func_name": "torvalds/linux/nci_close_device",
        "description": "A use-after-free flaw was found in nci_request in net/nfc/nci/core.c in NFC Controller Interface (NCI) in the Linux kernel. This flaw could allow a local attacker with user privileges to cause a data race problem while the device is getting removed, leading to a privilege escalation problem.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=48b71a9e66c2eab60564b1b1c85f4928ed04e406",
        "commit_title": "There are two sites that calls queue_work() after the",
        "commit_text": "destroy_workqueue() and lead to possible UAF.  The first site is nci_send_cmd(), which can happen after the nci_close_device as below  nfcmrvl_nci_unregister_dev   |  nfc_genl_dev_up   nci_close_device           |     flush_workqueue          |     del_timer_sync           |   nci_unregister_device      |    nfc_get_device     destroy_workqueue        |    nfc_dev_up     nfc_unregister_device    |      nci_dev_up       device_del             |        nci_open_device                              |          __nci_request                              |            nci_send_cmd                              |              queue_work !!!  Another site is nci_cmd_timer, awaked by the nci_cmd_work from the nci_send_cmd.    ...                        |  ...   nci_unregister_device      |  queue_work     destroy_workqueue        |     nfc_unregister_device    |  ...       device_del             |  nci_cmd_work                              |  mod_timer                              |  ...                              |  nci_cmd_timer                              |    queue_work !!!  For the above two UAF, the root cause is that the nfc_dev_up can race between the nci_unregister_device routine. Therefore, this patch introduce NCI_UNREG flag to easily eliminate the possible race. In addition, the mutex_lock in nci_close_device can act as a barrier.  Link: https://lore.kernel.org/r/20211116152732.19238-1-linma@zju.edu.cn ",
        "func_before": "static int nci_close_device(struct nci_dev *ndev)\n{\n\tnci_req_cancel(ndev, ENODEV);\n\tmutex_lock(&ndev->req_lock);\n\n\tif (!test_and_clear_bit(NCI_UP, &ndev->flags)) {\n\t\tdel_timer_sync(&ndev->cmd_timer);\n\t\tdel_timer_sync(&ndev->data_timer);\n\t\tmutex_unlock(&ndev->req_lock);\n\t\treturn 0;\n\t}\n\n\t/* Drop RX and TX queues */\n\tskb_queue_purge(&ndev->rx_q);\n\tskb_queue_purge(&ndev->tx_q);\n\n\t/* Flush RX and TX wq */\n\tflush_workqueue(ndev->rx_wq);\n\tflush_workqueue(ndev->tx_wq);\n\n\t/* Reset device */\n\tskb_queue_purge(&ndev->cmd_q);\n\tatomic_set(&ndev->cmd_cnt, 1);\n\n\tset_bit(NCI_INIT, &ndev->flags);\n\t__nci_request(ndev, nci_reset_req, (void *)0,\n\t\t      msecs_to_jiffies(NCI_RESET_TIMEOUT));\n\n\t/* After this point our queues are empty\n\t * and no works are scheduled.\n\t */\n\tndev->ops->close(ndev);\n\n\tclear_bit(NCI_INIT, &ndev->flags);\n\n\t/* Flush cmd wq */\n\tflush_workqueue(ndev->cmd_wq);\n\n\tdel_timer_sync(&ndev->cmd_timer);\n\n\t/* Clear flags */\n\tndev->flags = 0;\n\n\tmutex_unlock(&ndev->req_lock);\n\n\treturn 0;\n}",
        "func": "static int nci_close_device(struct nci_dev *ndev)\n{\n\tnci_req_cancel(ndev, ENODEV);\n\n\t/* This mutex needs to be held as a barrier for\n\t * caller nci_unregister_device\n\t */\n\tmutex_lock(&ndev->req_lock);\n\n\tif (!test_and_clear_bit(NCI_UP, &ndev->flags)) {\n\t\tdel_timer_sync(&ndev->cmd_timer);\n\t\tdel_timer_sync(&ndev->data_timer);\n\t\tmutex_unlock(&ndev->req_lock);\n\t\treturn 0;\n\t}\n\n\t/* Drop RX and TX queues */\n\tskb_queue_purge(&ndev->rx_q);\n\tskb_queue_purge(&ndev->tx_q);\n\n\t/* Flush RX and TX wq */\n\tflush_workqueue(ndev->rx_wq);\n\tflush_workqueue(ndev->tx_wq);\n\n\t/* Reset device */\n\tskb_queue_purge(&ndev->cmd_q);\n\tatomic_set(&ndev->cmd_cnt, 1);\n\n\tset_bit(NCI_INIT, &ndev->flags);\n\t__nci_request(ndev, nci_reset_req, (void *)0,\n\t\t      msecs_to_jiffies(NCI_RESET_TIMEOUT));\n\n\t/* After this point our queues are empty\n\t * and no works are scheduled.\n\t */\n\tndev->ops->close(ndev);\n\n\tclear_bit(NCI_INIT, &ndev->flags);\n\n\t/* Flush cmd wq */\n\tflush_workqueue(ndev->cmd_wq);\n\n\tdel_timer_sync(&ndev->cmd_timer);\n\n\t/* Clear flags except NCI_UNREG */\n\tndev->flags &= BIT(NCI_UNREG);\n\n\tmutex_unlock(&ndev->req_lock);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,10 @@\n static int nci_close_device(struct nci_dev *ndev)\n {\n \tnci_req_cancel(ndev, ENODEV);\n+\n+\t/* This mutex needs to be held as a barrier for\n+\t * caller nci_unregister_device\n+\t */\n \tmutex_lock(&ndev->req_lock);\n \n \tif (!test_and_clear_bit(NCI_UP, &ndev->flags)) {\n@@ -38,8 +42,8 @@\n \n \tdel_timer_sync(&ndev->cmd_timer);\n \n-\t/* Clear flags */\n-\tndev->flags = 0;\n+\t/* Clear flags except NCI_UNREG */\n+\tndev->flags &= BIT(NCI_UNREG);\n \n \tmutex_unlock(&ndev->req_lock);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* Clear flags */",
                "\tndev->flags = 0;"
            ],
            "added_lines": [
                "",
                "\t/* This mutex needs to be held as a barrier for",
                "\t * caller nci_unregister_device",
                "\t */",
                "\t/* Clear flags except NCI_UNREG */",
                "\tndev->flags &= BIT(NCI_UNREG);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4202",
        "func_name": "torvalds/linux/nci_unregister_device",
        "description": "A use-after-free flaw was found in nci_request in net/nfc/nci/core.c in NFC Controller Interface (NCI) in the Linux kernel. This flaw could allow a local attacker with user privileges to cause a data race problem while the device is getting removed, leading to a privilege escalation problem.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=48b71a9e66c2eab60564b1b1c85f4928ed04e406",
        "commit_title": "There are two sites that calls queue_work() after the",
        "commit_text": "destroy_workqueue() and lead to possible UAF.  The first site is nci_send_cmd(), which can happen after the nci_close_device as below  nfcmrvl_nci_unregister_dev   |  nfc_genl_dev_up   nci_close_device           |     flush_workqueue          |     del_timer_sync           |   nci_unregister_device      |    nfc_get_device     destroy_workqueue        |    nfc_dev_up     nfc_unregister_device    |      nci_dev_up       device_del             |        nci_open_device                              |          __nci_request                              |            nci_send_cmd                              |              queue_work !!!  Another site is nci_cmd_timer, awaked by the nci_cmd_work from the nci_send_cmd.    ...                        |  ...   nci_unregister_device      |  queue_work     destroy_workqueue        |     nfc_unregister_device    |  ...       device_del             |  nci_cmd_work                              |  mod_timer                              |  ...                              |  nci_cmd_timer                              |    queue_work !!!  For the above two UAF, the root cause is that the nfc_dev_up can race between the nci_unregister_device routine. Therefore, this patch introduce NCI_UNREG flag to easily eliminate the possible race. In addition, the mutex_lock in nci_close_device can act as a barrier.  Link: https://lore.kernel.org/r/20211116152732.19238-1-linma@zju.edu.cn ",
        "func_before": "void nci_unregister_device(struct nci_dev *ndev)\n{\n\tstruct nci_conn_info *conn_info, *n;\n\n\tnci_close_device(ndev);\n\n\tdestroy_workqueue(ndev->cmd_wq);\n\tdestroy_workqueue(ndev->rx_wq);\n\tdestroy_workqueue(ndev->tx_wq);\n\n\tlist_for_each_entry_safe(conn_info, n, &ndev->conn_info_list, list) {\n\t\tlist_del(&conn_info->list);\n\t\t/* conn_info is allocated with devm_kzalloc */\n\t}\n\n\tnfc_unregister_device(ndev->nfc_dev);\n}",
        "func": "void nci_unregister_device(struct nci_dev *ndev)\n{\n\tstruct nci_conn_info *conn_info, *n;\n\n\t/* This set_bit is not protected with specialized barrier,\n\t * However, it is fine because the mutex_lock(&ndev->req_lock);\n\t * in nci_close_device() will help to emit one.\n\t */\n\tset_bit(NCI_UNREG, &ndev->flags);\n\n\tnci_close_device(ndev);\n\n\tdestroy_workqueue(ndev->cmd_wq);\n\tdestroy_workqueue(ndev->rx_wq);\n\tdestroy_workqueue(ndev->tx_wq);\n\n\tlist_for_each_entry_safe(conn_info, n, &ndev->conn_info_list, list) {\n\t\tlist_del(&conn_info->list);\n\t\t/* conn_info is allocated with devm_kzalloc */\n\t}\n\n\tnfc_unregister_device(ndev->nfc_dev);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,12 @@\n void nci_unregister_device(struct nci_dev *ndev)\n {\n \tstruct nci_conn_info *conn_info, *n;\n+\n+\t/* This set_bit is not protected with specialized barrier,\n+\t * However, it is fine because the mutex_lock(&ndev->req_lock);\n+\t * in nci_close_device() will help to emit one.\n+\t */\n+\tset_bit(NCI_UNREG, &ndev->flags);\n \n \tnci_close_device(ndev);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t/* This set_bit is not protected with specialized barrier,",
                "\t * However, it is fine because the mutex_lock(&ndev->req_lock);",
                "\t * in nci_close_device() will help to emit one.",
                "\t */",
                "\tset_bit(NCI_UNREG, &ndev->flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4202",
        "func_name": "torvalds/linux/nci_request",
        "description": "A use-after-free flaw was found in nci_request in net/nfc/nci/core.c in NFC Controller Interface (NCI) in the Linux kernel. This flaw could allow a local attacker with user privileges to cause a data race problem while the device is getting removed, leading to a privilege escalation problem.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=86cdf8e38792545161dbe3350a7eced558ba4d15",
        "commit_title": "There is a possible data race as shown below:",
        "commit_text": " thread-A in nci_request()       | thread-B in nci_close_device()                                 | mutex_lock(&ndev->req_lock); test_bit(NCI_UP, &ndev->flags); | ...                             | test_and_clear_bit(NCI_UP, &ndev->flags) mutex_lock(&ndev->req_lock);    |                                 |  This race will allow __nci_request() to be awaked while the device is getting removed.  Similar to commit e2cb6b891ad2 (\"bluetooth: eliminate the potential race condition when removing the HCI controller\"). this patch alters the function sequence in nci_request() to prevent the data races between the nci_close_device().  Link: https://lore.kernel.org/r/20211115145600.8320-1-linma@zju.edu.cn ",
        "func_before": "inline int nci_request(struct nci_dev *ndev,\n\t\t       void (*req)(struct nci_dev *ndev,\n\t\t\t\t   const void *opt),\n\t\t       const void *opt, __u32 timeout)\n{\n\tint rc;\n\n\tif (!test_bit(NCI_UP, &ndev->flags))\n\t\treturn -ENETDOWN;\n\n\t/* Serialize all requests */\n\tmutex_lock(&ndev->req_lock);\n\trc = __nci_request(ndev, req, opt, timeout);\n\tmutex_unlock(&ndev->req_lock);\n\n\treturn rc;\n}",
        "func": "inline int nci_request(struct nci_dev *ndev,\n\t\t       void (*req)(struct nci_dev *ndev,\n\t\t\t\t   const void *opt),\n\t\t       const void *opt, __u32 timeout)\n{\n\tint rc;\n\n\t/* Serialize all requests */\n\tmutex_lock(&ndev->req_lock);\n\t/* check the state after obtaing the lock against any races\n\t * from nci_close_device when the device gets removed.\n\t */\n\tif (test_bit(NCI_UP, &ndev->flags))\n\t\trc = __nci_request(ndev, req, opt, timeout);\n\telse\n\t\trc = -ENETDOWN;\n\tmutex_unlock(&ndev->req_lock);\n\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,12 +5,15 @@\n {\n \tint rc;\n \n-\tif (!test_bit(NCI_UP, &ndev->flags))\n-\t\treturn -ENETDOWN;\n-\n \t/* Serialize all requests */\n \tmutex_lock(&ndev->req_lock);\n-\trc = __nci_request(ndev, req, opt, timeout);\n+\t/* check the state after obtaing the lock against any races\n+\t * from nci_close_device when the device gets removed.\n+\t */\n+\tif (test_bit(NCI_UP, &ndev->flags))\n+\t\trc = __nci_request(ndev, req, opt, timeout);\n+\telse\n+\t\trc = -ENETDOWN;\n \tmutex_unlock(&ndev->req_lock);\n \n \treturn rc;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!test_bit(NCI_UP, &ndev->flags))",
                "\t\treturn -ENETDOWN;",
                "",
                "\trc = __nci_request(ndev, req, opt, timeout);"
            ],
            "added_lines": [
                "\t/* check the state after obtaing the lock against any races",
                "\t * from nci_close_device when the device gets removed.",
                "\t */",
                "\tif (test_bit(NCI_UP, &ndev->flags))",
                "\t\trc = __nci_request(ndev, req, opt, timeout);",
                "\telse",
                "\t\trc = -ENETDOWN;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4203",
        "func_name": "kernel/git/netdev/net/sock_init_data",
        "description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net.git/commit/?h=35306eb23814",
        "commit_title": "Jann Horn reported that SO_PEERCRED and SO_PEERGROUPS implementations",
        "commit_text": "are racy, as af_unix can concurrently change sk_peer_pid and sk_peer_cred.  In order to fix this issue, this patch adds a new spinlock that needs to be used whenever these fields are read or written.  Jann also pointed out that l2cap_sock_get_peer_pid_cb() is currently reading sk->sk_peer_pid which makes no sense, as this field is only possibly set by AF_UNIX sockets. We will have to clean this in a separate patch. This could be done by reverting b48596d1dc25 \"Bluetooth: L2CAP: Add get_peer_pid callback\" or implementing what was truly expected.  Cc: Eric W. Biederman <ebiederm@xmission.com> Cc: Luiz Augusto von Dentz <luiz.von.dentz@intel.com> Cc: Marcel Holtmann <marcel@holtmann.org> ",
        "func_before": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tsk_init_common(sk);\n\tsk->sk_send_head\t=\tNULL;\n\n\ttimer_setup(&sk->sk_timer, NULL, 0);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tRCU_INIT_POINTER(sk->sk_wq, &sock->wq);\n\t\tsock->sk\t=\tsk;\n\t\tsk->sk_uid\t=\tSOCK_INODE(sock)->i_uid;\n\t} else {\n\t\tRCU_INIT_POINTER(sk->sk_wq, NULL);\n\t\tsk->sk_uid\t=\tmake_kuid(sock_net(sk)->user_ns, 0);\n\t}\n\n\trwlock_init(&sk->sk_callback_lock);\n\tif (sk->sk_kern_sock)\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_kern_callback_keys + sk->sk_family,\n\t\t\taf_family_kern_clock_key_strings[sk->sk_family]);\n\telse\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_frag.page\t=\tNULL;\n\tsk->sk_frag.offset\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = SK_DEFAULT_STAMP;\n#if BITS_PER_LONG==32\n\tseqlock_init(&sk->sk_stamp_seq);\n#endif\n\tatomic_set(&sk->sk_zckey, 0);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tsk->sk_napi_id\t\t=\t0;\n\tsk->sk_ll_usec\t\t=\tsysctl_net_busy_read;\n#endif\n\n\tsk->sk_max_pacing_rate = ~0UL;\n\tsk->sk_pacing_rate = ~0UL;\n\tWRITE_ONCE(sk->sk_pacing_shift, 10);\n\tsk->sk_incoming_cpu = -1;\n\n\tsk_rx_queue_clear(sk);\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.rst for details)\n\t */\n\tsmp_wmb();\n\trefcount_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
        "func": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tsk_init_common(sk);\n\tsk->sk_send_head\t=\tNULL;\n\n\ttimer_setup(&sk->sk_timer, NULL, 0);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tRCU_INIT_POINTER(sk->sk_wq, &sock->wq);\n\t\tsock->sk\t=\tsk;\n\t\tsk->sk_uid\t=\tSOCK_INODE(sock)->i_uid;\n\t} else {\n\t\tRCU_INIT_POINTER(sk->sk_wq, NULL);\n\t\tsk->sk_uid\t=\tmake_kuid(sock_net(sk)->user_ns, 0);\n\t}\n\n\trwlock_init(&sk->sk_callback_lock);\n\tif (sk->sk_kern_sock)\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_kern_callback_keys + sk->sk_family,\n\t\t\taf_family_kern_clock_key_strings[sk->sk_family]);\n\telse\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_frag.page\t=\tNULL;\n\tsk->sk_frag.offset\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tspin_lock_init(&sk->sk_peer_lock);\n\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = SK_DEFAULT_STAMP;\n#if BITS_PER_LONG==32\n\tseqlock_init(&sk->sk_stamp_seq);\n#endif\n\tatomic_set(&sk->sk_zckey, 0);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tsk->sk_napi_id\t\t=\t0;\n\tsk->sk_ll_usec\t\t=\tsysctl_net_busy_read;\n#endif\n\n\tsk->sk_max_pacing_rate = ~0UL;\n\tsk->sk_pacing_rate = ~0UL;\n\tWRITE_ONCE(sk->sk_pacing_shift, 10);\n\tsk->sk_incoming_cpu = -1;\n\n\tsk_rx_queue_clear(sk);\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.rst for details)\n\t */\n\tsmp_wmb();\n\trefcount_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -47,6 +47,8 @@\n \n \tsk->sk_peer_pid \t=\tNULL;\n \tsk->sk_peer_cred\t=\tNULL;\n+\tspin_lock_init(&sk->sk_peer_lock);\n+\n \tsk->sk_write_pending\t=\t0;\n \tsk->sk_rcvlowat\t\t=\t1;\n \tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tspin_lock_init(&sk->sk_peer_lock);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4203",
        "func_name": "kernel/git/netdev/net/__sk_destruct",
        "description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net.git/commit/?h=35306eb23814",
        "commit_title": "Jann Horn reported that SO_PEERCRED and SO_PEERGROUPS implementations",
        "commit_text": "are racy, as af_unix can concurrently change sk_peer_pid and sk_peer_cred.  In order to fix this issue, this patch adds a new spinlock that needs to be used whenever these fields are read or written.  Jann also pointed out that l2cap_sock_get_peer_pid_cb() is currently reading sk->sk_peer_pid which makes no sense, as this field is only possibly set by AF_UNIX sockets. We will have to clean this in a separate patch. This could be done by reverting b48596d1dc25 \"Bluetooth: L2CAP: Add get_peer_pid callback\" or implementing what was truly expected.  Cc: Eric W. Biederman <ebiederm@xmission.com> Cc: Luiz Augusto von Dentz <luiz.von.dentz@intel.com> Cc: Marcel Holtmann <marcel@holtmann.org> ",
        "func_before": "static void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       refcount_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_sk_storage_free(sk);\n#endif\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}",
        "func": "static void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       refcount_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_sk_storage_free(sk);\n#endif\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */\n\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,9 +28,10 @@\n \t\tsk->sk_frag.page = NULL;\n \t}\n \n-\tif (sk->sk_peer_cred)\n-\t\tput_cred(sk->sk_peer_cred);\n+\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */\n+\tput_cred(sk->sk_peer_cred);\n \tput_pid(sk->sk_peer_pid);\n+\n \tif (likely(sk->sk_net_refcnt))\n \t\tput_net(sock_net(sk));\n \tsk_prot_free(sk->sk_prot_creator, sk);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (sk->sk_peer_cred)",
                "\t\tput_cred(sk->sk_peer_cred);"
            ],
            "added_lines": [
                "\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */",
                "\tput_cred(sk->sk_peer_cred);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4203",
        "func_name": "kernel/git/netdev/net/sock_getsockopt",
        "description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net.git/commit/?h=35306eb23814",
        "commit_title": "Jann Horn reported that SO_PEERCRED and SO_PEERGROUPS implementations",
        "commit_text": "are racy, as af_unix can concurrently change sk_peer_pid and sk_peer_cred.  In order to fix this issue, this patch adds a new spinlock that needs to be used whenever these fields are read or written.  Jann also pointed out that l2cap_sock_get_peer_pid_cb() is currently reading sk->sk_peer_pid which makes no sense, as this field is only possibly set by AF_UNIX sockets. We will have to clean this in a separate patch. This could be done by reverting b48596d1dc25 \"Bluetooth: L2CAP: Add get_peer_pid callback\" or implementing what was truly expected.  Cc: Eric W. Biederman <ebiederm@xmission.com> Cc: Luiz Augusto von Dentz <luiz.von.dentz@intel.com> Cc: Marcel Holtmann <marcel@holtmann.org> ",
        "func_before": "int sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tu64 val64;\n\t\tunsigned long ulval;\n\t\tstruct linger ling;\n\t\tstruct old_timeval32 tm32;\n\t\tstruct __kernel_old_timeval tm;\n\t\tstruct  __kernel_sock_timeval stm;\n\t\tstruct sock_txtime txtime;\n\t\tstruct so_timestamping timestamping;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_REUSEPORT:\n\t\tv.val = sk->sk_reuseport;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check_tx;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_TSTAMP_NEW) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && !sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING_OLD:\n\t\tlv = sizeof(v.timestamping);\n\t\tv.timestamping.flags = sk->sk_tsflags;\n\t\tv.timestamping.bind_phc = sk->sk_bind_phc;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO_OLD:\n\tcase SO_RCVTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_rcvtimeo, &v, SO_RCVTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO_OLD:\n\tcase SO_SNDTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_sndtimeo, &v, SO_SNDTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERGROUPS:\n\t{\n\t\tint ret, n;\n\n\t\tif (!sk->sk_peer_cred)\n\t\t\treturn -ENODATA;\n\n\t\tn = sk->sk_peer_cred->group_info->ngroups;\n\t\tif (len < n * sizeof(gid_t)) {\n\t\t\tlen = n * sizeof(gid_t);\n\t\t\treturn put_user(len, optlen) ? -EFAULT : -ERANGE;\n\t\t}\n\t\tlen = n * sizeof(gid_t);\n\n\t\tret = groups_to_user((gid_t __user *)optval,\n\t\t\t\t     sk->sk_peer_cred->group_info);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tlv = sock->ops->getname(sock, (struct sockaddr *)address, 2);\n\t\tif (lv < 0)\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\n\tcase SO_BINDTODEVICE:\n\t\treturn sock_getbindtodevice(sk, optval, optlen, len);\n\n\tcase SO_GET_FILTER:\n\t\tlen = sk_get_filter(sk, (struct sock_filter __user *)optval, len);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tgoto lenout;\n\n\tcase SO_LOCK_FILTER:\n\t\tv.val = sock_flag(sk, SOCK_FILTER_LOCKED);\n\t\tbreak;\n\n\tcase SO_BPF_EXTENSIONS:\n\t\tv.val = bpf_tell_extensions();\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tv.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tv.val = sk->sk_ll_usec;\n\t\tbreak;\n\tcase SO_PREFER_BUSY_POLL:\n\t\tv.val = READ_ONCE(sk->sk_prefer_busy_poll);\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tif (sizeof(v.ulval) != sizeof(v.val) && len >= sizeof(v.ulval)) {\n\t\t\tlv = sizeof(v.ulval);\n\t\t\tv.ulval = sk->sk_max_pacing_rate;\n\t\t} else {\n\t\t\t/* 32bit version */\n\t\t\tv.val = min_t(unsigned long, sk->sk_max_pacing_rate, ~0U);\n\t\t}\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tv.val = READ_ONCE(sk->sk_incoming_cpu);\n\t\tbreak;\n\n\tcase SO_MEMINFO:\n\t{\n\t\tu32 meminfo[SK_MEMINFO_VARS];\n\n\t\tsk_get_meminfo(sk, meminfo);\n\n\t\tlen = min_t(unsigned int, len, sizeof(meminfo));\n\t\tif (copy_to_user(optval, &meminfo, len))\n\t\t\treturn -EFAULT;\n\n\t\tgoto lenout;\n\t}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_INCOMING_NAPI_ID:\n\t\tv.val = READ_ONCE(sk->sk_napi_id);\n\n\t\t/* aggregate non-NAPI IDs down to 0 */\n\t\tif (v.val < MIN_NAPI_ID)\n\t\t\tv.val = 0;\n\n\t\tbreak;\n#endif\n\n\tcase SO_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len < lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_gen_cookie(sk);\n\t\tbreak;\n\n\tcase SO_ZEROCOPY:\n\t\tv.val = sock_flag(sk, SOCK_ZEROCOPY);\n\t\tbreak;\n\n\tcase SO_TXTIME:\n\t\tlv = sizeof(v.txtime);\n\t\tv.txtime.clockid = sk->sk_clockid;\n\t\tv.txtime.flags |= sk->sk_txtime_deadline_mode ?\n\t\t\t\t  SOF_TXTIME_DEADLINE_MODE : 0;\n\t\tv.txtime.flags |= sk->sk_txtime_report_errors ?\n\t\t\t\t  SOF_TXTIME_REPORT_ERRORS : 0;\n\t\tbreak;\n\n\tcase SO_BINDTOIFINDEX:\n\t\tv.val = sk->sk_bound_dev_if;\n\t\tbreak;\n\n\tcase SO_NETNS_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len != lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_net(sk)->net_cookie;\n\t\tbreak;\n\n\tcase SO_BUF_LOCK:\n\t\tv.val = sk->sk_userlocks & SOCK_BUF_LOCK_MASK;\n\t\tbreak;\n\n\tdefault:\n\t\t/* We implement the SO_SNDLOWAT etc to not be settable\n\t\t * (1003.1g 7).\n\t\t */\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "func": "int sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tu64 val64;\n\t\tunsigned long ulval;\n\t\tstruct linger ling;\n\t\tstruct old_timeval32 tm32;\n\t\tstruct __kernel_old_timeval tm;\n\t\tstruct  __kernel_sock_timeval stm;\n\t\tstruct sock_txtime txtime;\n\t\tstruct so_timestamping timestamping;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_REUSEPORT:\n\t\tv.val = sk->sk_reuseport;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check_tx;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_TSTAMP_NEW) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && !sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING_OLD:\n\t\tlv = sizeof(v.timestamping);\n\t\tv.timestamping.flags = sk->sk_tsflags;\n\t\tv.timestamping.bind_phc = sk->sk_bind_phc;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO_OLD:\n\tcase SO_RCVTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_rcvtimeo, &v, SO_RCVTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO_OLD:\n\tcase SO_SNDTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_sndtimeo, &v, SO_SNDTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\n\t\tspin_lock(&sk->sk_peer_lock);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tspin_unlock(&sk->sk_peer_lock);\n\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERGROUPS:\n\t{\n\t\tconst struct cred *cred;\n\t\tint ret, n;\n\n\t\tcred = sk_get_peer_cred(sk);\n\t\tif (!cred)\n\t\t\treturn -ENODATA;\n\n\t\tn = cred->group_info->ngroups;\n\t\tif (len < n * sizeof(gid_t)) {\n\t\t\tlen = n * sizeof(gid_t);\n\t\t\tput_cred(cred);\n\t\t\treturn put_user(len, optlen) ? -EFAULT : -ERANGE;\n\t\t}\n\t\tlen = n * sizeof(gid_t);\n\n\t\tret = groups_to_user((gid_t __user *)optval, cred->group_info);\n\t\tput_cred(cred);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tlv = sock->ops->getname(sock, (struct sockaddr *)address, 2);\n\t\tif (lv < 0)\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\n\tcase SO_BINDTODEVICE:\n\t\treturn sock_getbindtodevice(sk, optval, optlen, len);\n\n\tcase SO_GET_FILTER:\n\t\tlen = sk_get_filter(sk, (struct sock_filter __user *)optval, len);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tgoto lenout;\n\n\tcase SO_LOCK_FILTER:\n\t\tv.val = sock_flag(sk, SOCK_FILTER_LOCKED);\n\t\tbreak;\n\n\tcase SO_BPF_EXTENSIONS:\n\t\tv.val = bpf_tell_extensions();\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tv.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tv.val = sk->sk_ll_usec;\n\t\tbreak;\n\tcase SO_PREFER_BUSY_POLL:\n\t\tv.val = READ_ONCE(sk->sk_prefer_busy_poll);\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tif (sizeof(v.ulval) != sizeof(v.val) && len >= sizeof(v.ulval)) {\n\t\t\tlv = sizeof(v.ulval);\n\t\t\tv.ulval = sk->sk_max_pacing_rate;\n\t\t} else {\n\t\t\t/* 32bit version */\n\t\t\tv.val = min_t(unsigned long, sk->sk_max_pacing_rate, ~0U);\n\t\t}\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tv.val = READ_ONCE(sk->sk_incoming_cpu);\n\t\tbreak;\n\n\tcase SO_MEMINFO:\n\t{\n\t\tu32 meminfo[SK_MEMINFO_VARS];\n\n\t\tsk_get_meminfo(sk, meminfo);\n\n\t\tlen = min_t(unsigned int, len, sizeof(meminfo));\n\t\tif (copy_to_user(optval, &meminfo, len))\n\t\t\treturn -EFAULT;\n\n\t\tgoto lenout;\n\t}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_INCOMING_NAPI_ID:\n\t\tv.val = READ_ONCE(sk->sk_napi_id);\n\n\t\t/* aggregate non-NAPI IDs down to 0 */\n\t\tif (v.val < MIN_NAPI_ID)\n\t\t\tv.val = 0;\n\n\t\tbreak;\n#endif\n\n\tcase SO_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len < lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_gen_cookie(sk);\n\t\tbreak;\n\n\tcase SO_ZEROCOPY:\n\t\tv.val = sock_flag(sk, SOCK_ZEROCOPY);\n\t\tbreak;\n\n\tcase SO_TXTIME:\n\t\tlv = sizeof(v.txtime);\n\t\tv.txtime.clockid = sk->sk_clockid;\n\t\tv.txtime.flags |= sk->sk_txtime_deadline_mode ?\n\t\t\t\t  SOF_TXTIME_DEADLINE_MODE : 0;\n\t\tv.txtime.flags |= sk->sk_txtime_report_errors ?\n\t\t\t\t  SOF_TXTIME_REPORT_ERRORS : 0;\n\t\tbreak;\n\n\tcase SO_BINDTOIFINDEX:\n\t\tv.val = sk->sk_bound_dev_if;\n\t\tbreak;\n\n\tcase SO_NETNS_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len != lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_net(sk)->net_cookie;\n\t\tbreak;\n\n\tcase SO_BUF_LOCK:\n\t\tv.val = sk->sk_userlocks & SOCK_BUF_LOCK_MASK;\n\t\tbreak;\n\n\tdefault:\n\t\t/* We implement the SO_SNDLOWAT etc to not be settable\n\t\t * (1003.1g 7).\n\t\t */\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -148,7 +148,11 @@\n \t\tstruct ucred peercred;\n \t\tif (len > sizeof(peercred))\n \t\t\tlen = sizeof(peercred);\n+\n+\t\tspin_lock(&sk->sk_peer_lock);\n \t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n+\t\tspin_unlock(&sk->sk_peer_lock);\n+\n \t\tif (copy_to_user(optval, &peercred, len))\n \t\t\treturn -EFAULT;\n \t\tgoto lenout;\n@@ -156,20 +160,23 @@\n \n \tcase SO_PEERGROUPS:\n \t{\n+\t\tconst struct cred *cred;\n \t\tint ret, n;\n \n-\t\tif (!sk->sk_peer_cred)\n+\t\tcred = sk_get_peer_cred(sk);\n+\t\tif (!cred)\n \t\t\treturn -ENODATA;\n \n-\t\tn = sk->sk_peer_cred->group_info->ngroups;\n+\t\tn = cred->group_info->ngroups;\n \t\tif (len < n * sizeof(gid_t)) {\n \t\t\tlen = n * sizeof(gid_t);\n+\t\t\tput_cred(cred);\n \t\t\treturn put_user(len, optlen) ? -EFAULT : -ERANGE;\n \t\t}\n \t\tlen = n * sizeof(gid_t);\n \n-\t\tret = groups_to_user((gid_t __user *)optval,\n-\t\t\t\t     sk->sk_peer_cred->group_info);\n+\t\tret = groups_to_user((gid_t __user *)optval, cred->group_info);\n+\t\tput_cred(cred);\n \t\tif (ret)\n \t\t\treturn ret;\n \t\tgoto lenout;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (!sk->sk_peer_cred)",
                "\t\tn = sk->sk_peer_cred->group_info->ngroups;",
                "\t\tret = groups_to_user((gid_t __user *)optval,",
                "\t\t\t\t     sk->sk_peer_cred->group_info);"
            ],
            "added_lines": [
                "",
                "\t\tspin_lock(&sk->sk_peer_lock);",
                "\t\tspin_unlock(&sk->sk_peer_lock);",
                "",
                "\t\tconst struct cred *cred;",
                "\t\tcred = sk_get_peer_cred(sk);",
                "\t\tif (!cred)",
                "\t\tn = cred->group_info->ngroups;",
                "\t\t\tput_cred(cred);",
                "\t\tret = groups_to_user((gid_t __user *)optval, cred->group_info);",
                "\t\tput_cred(cred);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4203",
        "func_name": "kernel/git/netdev/net/init_peercred",
        "description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net.git/commit/?h=35306eb23814",
        "commit_title": "Jann Horn reported that SO_PEERCRED and SO_PEERGROUPS implementations",
        "commit_text": "are racy, as af_unix can concurrently change sk_peer_pid and sk_peer_cred.  In order to fix this issue, this patch adds a new spinlock that needs to be used whenever these fields are read or written.  Jann also pointed out that l2cap_sock_get_peer_pid_cb() is currently reading sk->sk_peer_pid which makes no sense, as this field is only possibly set by AF_UNIX sockets. We will have to clean this in a separate patch. This could be done by reverting b48596d1dc25 \"Bluetooth: L2CAP: Add get_peer_pid callback\" or implementing what was truly expected.  Cc: Eric W. Biederman <ebiederm@xmission.com> Cc: Luiz Augusto von Dentz <luiz.von.dentz@intel.com> Cc: Marcel Holtmann <marcel@holtmann.org> ",
        "func_before": "static void init_peercred(struct sock *sk)\n{\n\tput_pid(sk->sk_peer_pid);\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tsk->sk_peer_pid  = get_pid(task_tgid(current));\n\tsk->sk_peer_cred = get_current_cred();\n}",
        "func": "static void init_peercred(struct sock *sk)\n{\n\tconst struct cred *old_cred;\n\tstruct pid *old_pid;\n\n\tspin_lock(&sk->sk_peer_lock);\n\told_pid = sk->sk_peer_pid;\n\told_cred = sk->sk_peer_cred;\n\tsk->sk_peer_pid  = get_pid(task_tgid(current));\n\tsk->sk_peer_cred = get_current_cred();\n\tspin_unlock(&sk->sk_peer_lock);\n\n\tput_pid(old_pid);\n\tput_cred(old_cred);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,15 @@\n static void init_peercred(struct sock *sk)\n {\n-\tput_pid(sk->sk_peer_pid);\n-\tif (sk->sk_peer_cred)\n-\t\tput_cred(sk->sk_peer_cred);\n+\tconst struct cred *old_cred;\n+\tstruct pid *old_pid;\n+\n+\tspin_lock(&sk->sk_peer_lock);\n+\told_pid = sk->sk_peer_pid;\n+\told_cred = sk->sk_peer_cred;\n \tsk->sk_peer_pid  = get_pid(task_tgid(current));\n \tsk->sk_peer_cred = get_current_cred();\n+\tspin_unlock(&sk->sk_peer_lock);\n+\n+\tput_pid(old_pid);\n+\tput_cred(old_cred);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tput_pid(sk->sk_peer_pid);",
                "\tif (sk->sk_peer_cred)",
                "\t\tput_cred(sk->sk_peer_cred);"
            ],
            "added_lines": [
                "\tconst struct cred *old_cred;",
                "\tstruct pid *old_pid;",
                "",
                "\tspin_lock(&sk->sk_peer_lock);",
                "\told_pid = sk->sk_peer_pid;",
                "\told_cred = sk->sk_peer_cred;",
                "\tspin_unlock(&sk->sk_peer_lock);",
                "",
                "\tput_pid(old_pid);",
                "\tput_cred(old_cred);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4203",
        "func_name": "kernel/git/netdev/net/copy_peercred",
        "description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net.git/commit/?h=35306eb23814",
        "commit_title": "Jann Horn reported that SO_PEERCRED and SO_PEERGROUPS implementations",
        "commit_text": "are racy, as af_unix can concurrently change sk_peer_pid and sk_peer_cred.  In order to fix this issue, this patch adds a new spinlock that needs to be used whenever these fields are read or written.  Jann also pointed out that l2cap_sock_get_peer_pid_cb() is currently reading sk->sk_peer_pid which makes no sense, as this field is only possibly set by AF_UNIX sockets. We will have to clean this in a separate patch. This could be done by reverting b48596d1dc25 \"Bluetooth: L2CAP: Add get_peer_pid callback\" or implementing what was truly expected.  Cc: Eric W. Biederman <ebiederm@xmission.com> Cc: Luiz Augusto von Dentz <luiz.von.dentz@intel.com> Cc: Marcel Holtmann <marcel@holtmann.org> ",
        "func_before": "static void copy_peercred(struct sock *sk, struct sock *peersk)\n{\n\tput_pid(sk->sk_peer_pid);\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n\tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n}",
        "func": "static void copy_peercred(struct sock *sk, struct sock *peersk)\n{\n\tconst struct cred *old_cred;\n\tstruct pid *old_pid;\n\n\tif (sk < peersk) {\n\t\tspin_lock(&sk->sk_peer_lock);\n\t\tspin_lock_nested(&peersk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n\t} else {\n\t\tspin_lock(&peersk->sk_peer_lock);\n\t\tspin_lock_nested(&sk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n\t}\n\told_pid = sk->sk_peer_pid;\n\told_cred = sk->sk_peer_cred;\n\tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n\tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n\n\tspin_unlock(&sk->sk_peer_lock);\n\tspin_unlock(&peersk->sk_peer_lock);\n\n\tput_pid(old_pid);\n\tput_cred(old_cred);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,23 @@\n static void copy_peercred(struct sock *sk, struct sock *peersk)\n {\n-\tput_pid(sk->sk_peer_pid);\n-\tif (sk->sk_peer_cred)\n-\t\tput_cred(sk->sk_peer_cred);\n+\tconst struct cred *old_cred;\n+\tstruct pid *old_pid;\n+\n+\tif (sk < peersk) {\n+\t\tspin_lock(&sk->sk_peer_lock);\n+\t\tspin_lock_nested(&peersk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n+\t} else {\n+\t\tspin_lock(&peersk->sk_peer_lock);\n+\t\tspin_lock_nested(&sk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n+\t}\n+\told_pid = sk->sk_peer_pid;\n+\told_cred = sk->sk_peer_cred;\n \tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n \tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n+\n+\tspin_unlock(&sk->sk_peer_lock);\n+\tspin_unlock(&peersk->sk_peer_lock);\n+\n+\tput_pid(old_pid);\n+\tput_cred(old_cred);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tput_pid(sk->sk_peer_pid);",
                "\tif (sk->sk_peer_cred)",
                "\t\tput_cred(sk->sk_peer_cred);"
            ],
            "added_lines": [
                "\tconst struct cred *old_cred;",
                "\tstruct pid *old_pid;",
                "",
                "\tif (sk < peersk) {",
                "\t\tspin_lock(&sk->sk_peer_lock);",
                "\t\tspin_lock_nested(&peersk->sk_peer_lock, SINGLE_DEPTH_NESTING);",
                "\t} else {",
                "\t\tspin_lock(&peersk->sk_peer_lock);",
                "\t\tspin_lock_nested(&sk->sk_peer_lock, SINGLE_DEPTH_NESTING);",
                "\t}",
                "\told_pid = sk->sk_peer_pid;",
                "\told_cred = sk->sk_peer_cred;",
                "",
                "\tspin_unlock(&sk->sk_peer_lock);",
                "\tspin_unlock(&peersk->sk_peer_lock);",
                "",
                "\tput_pid(old_pid);",
                "\tput_cred(old_cred);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-26357",
        "func_name": "xen-project/xen/cleanup_domid_map",
        "description": "race in VT-d domain ID cleanup Xen domain IDs are up to 15 bits wide. VT-d hardware may allow for only less than 15 bits to hold a domain ID associating a physical device with a particular domain. Therefore internally Xen domain IDs are mapped to the smaller value range. The cleaning up of the housekeeping structures has a race, allowing for VT-d domain IDs to be leaked and flushes to be bypassed.",
        "git_url": "https://github.com/xen-project/xen/commit/d9eca7bb6c6636eb87bb17b08ba7de270f47ecd0",
        "commit_title": "VT-d: correct ordering of operations in cleanup_domid_map()",
        "commit_text": " The function may be called without any locks held (leaving aside the domctl one, which we surely don't want to depend on here), so needs to play safe wrt other accesses to domid_map[] and domid_bitmap[]. This is to avoid context_set_domain_id()'s writing of domid_map[] to be reset to zero right away in the case of it racing the freeing of a DID.  For the interaction with context_set_domain_id() and did_to_domain_id() see the code comment.  {check_,}cleanup_domid_map() are called with pcidevs_lock held or during domain cleanup only (and pcidevs_lock is also held around context_set_domain_id()), i.e. racing calls with the same (dom, iommu) tuple cannot occur.  domain_iommu_domid(), besides its use by cleanup_domid_map(), has its result used only to control flushing, and hence a stale result would only lead to a stray extra flush.  This is CVE-2022-26357 / XSA-399. ",
        "func_before": "static void cleanup_domid_map(struct domain *domain, struct vtd_iommu *iommu)\n{\n    int iommu_domid;\n\n    if ( !domid_mapping(iommu) )\n        return;\n\n    iommu_domid = domain_iommu_domid(domain, iommu);\n\n    if ( iommu_domid >= 0 )\n    {\n        clear_bit(iommu_domid, iommu->domid_bitmap);\n        iommu->domid_map[iommu_domid] = 0;\n    }\n}",
        "func": "static void cleanup_domid_map(struct domain *domain, struct vtd_iommu *iommu)\n{\n    int iommu_domid;\n\n    if ( !domid_mapping(iommu) )\n        return;\n\n    iommu_domid = domain_iommu_domid(domain, iommu);\n\n    if ( iommu_domid >= 0 )\n    {\n        /*\n         * Update domid_map[] /before/ domid_bitmap[] to avoid a race with\n         * context_set_domain_id(), setting the slot to DOMID_INVALID for\n         * did_to_domain_id() to return a suitable value while the bit is\n         * still set.\n         */\n        iommu->domid_map[iommu_domid] = DOMID_INVALID;\n        clear_bit(iommu_domid, iommu->domid_bitmap);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,13 @@\n \n     if ( iommu_domid >= 0 )\n     {\n+        /*\n+         * Update domid_map[] /before/ domid_bitmap[] to avoid a race with\n+         * context_set_domain_id(), setting the slot to DOMID_INVALID for\n+         * did_to_domain_id() to return a suitable value while the bit is\n+         * still set.\n+         */\n+        iommu->domid_map[iommu_domid] = DOMID_INVALID;\n         clear_bit(iommu_domid, iommu->domid_bitmap);\n-        iommu->domid_map[iommu_domid] = 0;\n     }\n }",
        "diff_line_info": {
            "deleted_lines": [
                "        iommu->domid_map[iommu_domid] = 0;"
            ],
            "added_lines": [
                "        /*",
                "         * Update domid_map[] /before/ domid_bitmap[] to avoid a race with",
                "         * context_set_domain_id(), setting the slot to DOMID_INVALID for",
                "         * did_to_domain_id() to return a suitable value while the bit is",
                "         * still set.",
                "         */",
                "        iommu->domid_map[iommu_domid] = DOMID_INVALID;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-28796",
        "func_name": "torvalds/linux/jbd2_journal_lock_updates",
        "description": "jbd2_journal_wait_updates in fs/jbd2/transaction.c in the Linux kernel before 5.17.1 has a use-after-free caused by a transaction_t race condition.",
        "git_url": "https://github.com/torvalds/linux/commit/cc16eecae687912238ee6efbff71ad31e2bc414e",
        "commit_title": "jbd2: fix use-after-free of transaction_t race",
        "commit_text": " jbd2_journal_wait_updates() is called with j_state_lock held. But if there is a commit in progress, then this transaction might get committed and freed via jbd2_journal_commit_transaction() -> jbd2_journal_free_transaction(), when we release j_state_lock. So check for journal->j_running_transaction everytime we release and acquire j_state_lock to avoid use-after-free issue.  Link: https://lore.kernel.org/r/948c2fed518ae739db6a8f7f83f1d58b504f87d0.1644497105.git.ritesh.list@gmail.com Cc: stable@kernel.org Reported-and-tested-by: syzbot+afa2ca5171d93e44b348@syzkaller.appspotmail.com",
        "func_before": "void jbd2_journal_lock_updates(journal_t *journal)\n{\n\tDEFINE_WAIT(wait);\n\n\tjbd2_might_wait_for_commit(journal);\n\n\twrite_lock(&journal->j_state_lock);\n\t++journal->j_barrier_count;\n\n\t/* Wait until there are no reserved handles */\n\tif (atomic_read(&journal->j_reserved_credits)) {\n\t\twrite_unlock(&journal->j_state_lock);\n\t\twait_event(journal->j_wait_reserved,\n\t\t\t   atomic_read(&journal->j_reserved_credits) == 0);\n\t\twrite_lock(&journal->j_state_lock);\n\t}\n\n\t/* Wait until there are no running t_updates */\n\tjbd2_journal_wait_updates(journal);\n\n\twrite_unlock(&journal->j_state_lock);\n\n\t/*\n\t * We have now established a barrier against other normal updates, but\n\t * we also need to barrier against other jbd2_journal_lock_updates() calls\n\t * to make sure that we serialise special journal-locked operations\n\t * too.\n\t */\n\tmutex_lock(&journal->j_barrier);\n}",
        "func": "void jbd2_journal_lock_updates(journal_t *journal)\n{\n\tjbd2_might_wait_for_commit(journal);\n\n\twrite_lock(&journal->j_state_lock);\n\t++journal->j_barrier_count;\n\n\t/* Wait until there are no reserved handles */\n\tif (atomic_read(&journal->j_reserved_credits)) {\n\t\twrite_unlock(&journal->j_state_lock);\n\t\twait_event(journal->j_wait_reserved,\n\t\t\t   atomic_read(&journal->j_reserved_credits) == 0);\n\t\twrite_lock(&journal->j_state_lock);\n\t}\n\n\t/* Wait until there are no running t_updates */\n\tjbd2_journal_wait_updates(journal);\n\n\twrite_unlock(&journal->j_state_lock);\n\n\t/*\n\t * We have now established a barrier against other normal updates, but\n\t * we also need to barrier against other jbd2_journal_lock_updates() calls\n\t * to make sure that we serialise special journal-locked operations\n\t * too.\n\t */\n\tmutex_lock(&journal->j_barrier);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,5 @@\n void jbd2_journal_lock_updates(journal_t *journal)\n {\n-\tDEFINE_WAIT(wait);\n-\n \tjbd2_might_wait_for_commit(journal);\n \n \twrite_lock(&journal->j_state_lock);",
        "diff_line_info": {
            "deleted_lines": [
                "\tDEFINE_WAIT(wait);",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2022-28796",
        "func_name": "torvalds/linux/jbd2_journal_wait_updates",
        "description": "jbd2_journal_wait_updates in fs/jbd2/transaction.c in the Linux kernel before 5.17.1 has a use-after-free caused by a transaction_t race condition.",
        "git_url": "https://github.com/torvalds/linux/commit/cc16eecae687912238ee6efbff71ad31e2bc414e",
        "commit_title": "jbd2: fix use-after-free of transaction_t race",
        "commit_text": " jbd2_journal_wait_updates() is called with j_state_lock held. But if there is a commit in progress, then this transaction might get committed and freed via jbd2_journal_commit_transaction() -> jbd2_journal_free_transaction(), when we release j_state_lock. So check for journal->j_running_transaction everytime we release and acquire j_state_lock to avoid use-after-free issue.  Link: https://lore.kernel.org/r/948c2fed518ae739db6a8f7f83f1d58b504f87d0.1644497105.git.ritesh.list@gmail.com Cc: stable@kernel.org Reported-and-tested-by: syzbot+afa2ca5171d93e44b348@syzkaller.appspotmail.com",
        "func_before": "void jbd2_journal_wait_updates(journal_t *journal)\n{\n\ttransaction_t *commit_transaction = journal->j_running_transaction;\n\n\tif (!commit_transaction)\n\t\treturn;\n\n\tspin_lock(&commit_transaction->t_handle_lock);\n\twhile (atomic_read(&commit_transaction->t_updates)) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (atomic_read(&commit_transaction->t_updates)) {\n\t\t\tspin_unlock(&commit_transaction->t_handle_lock);\n\t\t\twrite_unlock(&journal->j_state_lock);\n\t\t\tschedule();\n\t\t\twrite_lock(&journal->j_state_lock);\n\t\t\tspin_lock(&commit_transaction->t_handle_lock);\n\t\t}\n\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t}\n\tspin_unlock(&commit_transaction->t_handle_lock);\n}",
        "func": "void jbd2_journal_wait_updates(journal_t *journal)\n{\n\tDEFINE_WAIT(wait);\n\n\twhile (1) {\n\t\t/*\n\t\t * Note that the running transaction can get freed under us if\n\t\t * this transaction is getting committed in\n\t\t * jbd2_journal_commit_transaction() ->\n\t\t * jbd2_journal_free_transaction(). This can only happen when we\n\t\t * release j_state_lock -> schedule() -> acquire j_state_lock.\n\t\t * Hence we should everytime retrieve new j_running_transaction\n\t\t * value (after j_state_lock release acquire cycle), else it may\n\t\t * lead to use-after-free of old freed transaction.\n\t\t */\n\t\ttransaction_t *transaction = journal->j_running_transaction;\n\n\t\tif (!transaction)\n\t\t\tbreak;\n\n\t\tspin_lock(&transaction->t_handle_lock);\n\t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (!atomic_read(&transaction->t_updates)) {\n\t\t\tspin_unlock(&transaction->t_handle_lock);\n\t\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&transaction->t_handle_lock);\n\t\twrite_unlock(&journal->j_state_lock);\n\t\tschedule();\n\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\twrite_lock(&journal->j_state_lock);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,24 +1,35 @@\n void jbd2_journal_wait_updates(journal_t *journal)\n {\n-\ttransaction_t *commit_transaction = journal->j_running_transaction;\n+\tDEFINE_WAIT(wait);\n \n-\tif (!commit_transaction)\n-\t\treturn;\n+\twhile (1) {\n+\t\t/*\n+\t\t * Note that the running transaction can get freed under us if\n+\t\t * this transaction is getting committed in\n+\t\t * jbd2_journal_commit_transaction() ->\n+\t\t * jbd2_journal_free_transaction(). This can only happen when we\n+\t\t * release j_state_lock -> schedule() -> acquire j_state_lock.\n+\t\t * Hence we should everytime retrieve new j_running_transaction\n+\t\t * value (after j_state_lock release acquire cycle), else it may\n+\t\t * lead to use-after-free of old freed transaction.\n+\t\t */\n+\t\ttransaction_t *transaction = journal->j_running_transaction;\n \n-\tspin_lock(&commit_transaction->t_handle_lock);\n-\twhile (atomic_read(&commit_transaction->t_updates)) {\n-\t\tDEFINE_WAIT(wait);\n+\t\tif (!transaction)\n+\t\t\tbreak;\n \n+\t\tspin_lock(&transaction->t_handle_lock);\n \t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n-\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n-\t\tif (atomic_read(&commit_transaction->t_updates)) {\n-\t\t\tspin_unlock(&commit_transaction->t_handle_lock);\n-\t\t\twrite_unlock(&journal->j_state_lock);\n-\t\t\tschedule();\n-\t\t\twrite_lock(&journal->j_state_lock);\n-\t\t\tspin_lock(&commit_transaction->t_handle_lock);\n+\t\t\t\tTASK_UNINTERRUPTIBLE);\n+\t\tif (!atomic_read(&transaction->t_updates)) {\n+\t\t\tspin_unlock(&transaction->t_handle_lock);\n+\t\t\tfinish_wait(&journal->j_wait_updates, &wait);\n+\t\t\tbreak;\n \t\t}\n+\t\tspin_unlock(&transaction->t_handle_lock);\n+\t\twrite_unlock(&journal->j_state_lock);\n+\t\tschedule();\n \t\tfinish_wait(&journal->j_wait_updates, &wait);\n+\t\twrite_lock(&journal->j_state_lock);\n \t}\n-\tspin_unlock(&commit_transaction->t_handle_lock);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\ttransaction_t *commit_transaction = journal->j_running_transaction;",
                "\tif (!commit_transaction)",
                "\t\treturn;",
                "\tspin_lock(&commit_transaction->t_handle_lock);",
                "\twhile (atomic_read(&commit_transaction->t_updates)) {",
                "\t\tDEFINE_WAIT(wait);",
                "\t\t\t\t\tTASK_UNINTERRUPTIBLE);",
                "\t\tif (atomic_read(&commit_transaction->t_updates)) {",
                "\t\t\tspin_unlock(&commit_transaction->t_handle_lock);",
                "\t\t\twrite_unlock(&journal->j_state_lock);",
                "\t\t\tschedule();",
                "\t\t\twrite_lock(&journal->j_state_lock);",
                "\t\t\tspin_lock(&commit_transaction->t_handle_lock);",
                "\tspin_unlock(&commit_transaction->t_handle_lock);"
            ],
            "added_lines": [
                "\tDEFINE_WAIT(wait);",
                "\twhile (1) {",
                "\t\t/*",
                "\t\t * Note that the running transaction can get freed under us if",
                "\t\t * this transaction is getting committed in",
                "\t\t * jbd2_journal_commit_transaction() ->",
                "\t\t * jbd2_journal_free_transaction(). This can only happen when we",
                "\t\t * release j_state_lock -> schedule() -> acquire j_state_lock.",
                "\t\t * Hence we should everytime retrieve new j_running_transaction",
                "\t\t * value (after j_state_lock release acquire cycle), else it may",
                "\t\t * lead to use-after-free of old freed transaction.",
                "\t\t */",
                "\t\ttransaction_t *transaction = journal->j_running_transaction;",
                "\t\tif (!transaction)",
                "\t\t\tbreak;",
                "\t\tspin_lock(&transaction->t_handle_lock);",
                "\t\t\t\tTASK_UNINTERRUPTIBLE);",
                "\t\tif (!atomic_read(&transaction->t_updates)) {",
                "\t\t\tspin_unlock(&transaction->t_handle_lock);",
                "\t\t\tfinish_wait(&journal->j_wait_updates, &wait);",
                "\t\t\tbreak;",
                "\t\tspin_unlock(&transaction->t_handle_lock);",
                "\t\twrite_unlock(&journal->j_state_lock);",
                "\t\tschedule();",
                "\t\twrite_lock(&journal->j_state_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-29582",
        "func_name": "torvalds/linux/io_timeout_prep",
        "description": "In the Linux kernel before 5.17.3, fs/io_uring.c has a use-after-free due to a race condition in io_uring timeouts. This can be triggered by a local user who has no access to any user namespace; however, the race condition perhaps can only be exploited infrequently.",
        "git_url": "https://github.com/torvalds/linux/commit/e677edbcabee849bfdd43f1602bccbecf736a646",
        "commit_title": "io_uring: fix race between timeout flush and removal",
        "commit_text": " io_flush_timeouts() assumes the timeout isn't in progress of triggering or being removed/canceled, so it unconditionally removes it from the timeout list and attempts to cancel it.  Leave it on the list and let the normal timeout cancelation take care of it.  Cc: stable@vger.kernel.org # 5.5+",
        "func_before": "static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1 ||\n\t    sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~(IORING_TIMEOUT_ABS | IORING_TIMEOUT_CLOCK_MASK |\n\t\t      IORING_TIMEOUT_ETIME_SUCCESS))\n\t\treturn -EINVAL;\n\t/* more than one clock specified is invalid, obviously */\n\tif (hweight32(flags & IORING_TIMEOUT_CLOCK_MASK) > 1)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&req->timeout.list);\n\treq->timeout.off = off;\n\tif (unlikely(off && !req->ctx->off_timeout_used))\n\t\treq->ctx->off_timeout_used = true;\n\n\tif (WARN_ON_ONCE(req_has_async_data(req)))\n\t\treturn -EFAULT;\n\tif (io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\tdata->flags = flags;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tif (data->ts.tv_sec < 0 || data->ts.tv_nsec < 0)\n\t\treturn -EINVAL;\n\n\tdata->mode = io_translate_timeout_mode(flags);\n\thrtimer_init(&data->timer, io_timeout_get_clock(data), data->mode);\n\n\tif (is_timeout_link) {\n\t\tstruct io_submit_link *link = &req->ctx->submit_state.link;\n\n\t\tif (!link->head)\n\t\t\treturn -EINVAL;\n\t\tif (link->last->opcode == IORING_OP_LINK_TIMEOUT)\n\t\t\treturn -EINVAL;\n\t\treq->timeout.head = link->last;\n\t\tlink->last->flags |= REQ_F_ARM_LTIMEOUT;\n\t}\n\treturn 0;\n}",
        "func": "static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1 ||\n\t    sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~(IORING_TIMEOUT_ABS | IORING_TIMEOUT_CLOCK_MASK |\n\t\t      IORING_TIMEOUT_ETIME_SUCCESS))\n\t\treturn -EINVAL;\n\t/* more than one clock specified is invalid, obviously */\n\tif (hweight32(flags & IORING_TIMEOUT_CLOCK_MASK) > 1)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&req->timeout.list);\n\treq->timeout.off = off;\n\tif (unlikely(off && !req->ctx->off_timeout_used))\n\t\treq->ctx->off_timeout_used = true;\n\n\tif (WARN_ON_ONCE(req_has_async_data(req)))\n\t\treturn -EFAULT;\n\tif (io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\tdata->flags = flags;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tif (data->ts.tv_sec < 0 || data->ts.tv_nsec < 0)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&req->timeout.list);\n\tdata->mode = io_translate_timeout_mode(flags);\n\thrtimer_init(&data->timer, io_timeout_get_clock(data), data->mode);\n\n\tif (is_timeout_link) {\n\t\tstruct io_submit_link *link = &req->ctx->submit_state.link;\n\n\t\tif (!link->head)\n\t\t\treturn -EINVAL;\n\t\tif (link->last->opcode == IORING_OP_LINK_TIMEOUT)\n\t\t\treturn -EINVAL;\n\t\treq->timeout.head = link->last;\n\t\tlink->last->flags |= REQ_F_ARM_LTIMEOUT;\n\t}\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,6 +40,7 @@\n \tif (data->ts.tv_sec < 0 || data->ts.tv_nsec < 0)\n \t\treturn -EINVAL;\n \n+\tINIT_LIST_HEAD(&req->timeout.list);\n \tdata->mode = io_translate_timeout_mode(flags);\n \thrtimer_init(&data->timer, io_timeout_get_clock(data), data->mode);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tINIT_LIST_HEAD(&req->timeout.list);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-29582",
        "func_name": "torvalds/linux/io_flush_timeouts",
        "description": "In the Linux kernel before 5.17.3, fs/io_uring.c has a use-after-free due to a race condition in io_uring timeouts. This can be triggered by a local user who has no access to any user namespace; however, the race condition perhaps can only be exploited infrequently.",
        "git_url": "https://github.com/torvalds/linux/commit/e677edbcabee849bfdd43f1602bccbecf736a646",
        "commit_title": "io_uring: fix race between timeout flush and removal",
        "commit_text": " io_flush_timeouts() assumes the timeout isn't in progress of triggering or being removed/canceled, so it unconditionally removes it from the timeout list and attempts to cancel it.  Leave it on the list and let the normal timeout cancelation take care of it.  Cc: stable@vger.kernel.org # 5.5+",
        "func_before": "static __cold void io_flush_timeouts(struct io_ring_ctx *ctx)\n\t__must_hold(&ctx->completion_lock)\n{\n\tu32 seq = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\n\tspin_lock_irq(&ctx->timeout_lock);\n\twhile (!list_empty(&ctx->timeout_list)) {\n\t\tu32 events_needed, events_got;\n\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,\n\t\t\t\t\t\tstruct io_kiocb, timeout.list);\n\n\t\tif (io_is_timeout_noseq(req))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Since seq can easily wrap around over time, subtract\n\t\t * the last seq at which timeouts were flushed before comparing.\n\t\t * Assuming not more than 2^31-1 events have happened since,\n\t\t * these subtractions won't have wrapped, so we can check if\n\t\t * target is in [last_seq, current_seq] by comparing the two.\n\t\t */\n\t\tevents_needed = req->timeout.target_seq - ctx->cq_last_tm_flush;\n\t\tevents_got = seq - ctx->cq_last_tm_flush;\n\t\tif (events_got < events_needed)\n\t\t\tbreak;\n\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_kill_timeout(req, 0);\n\t}\n\tctx->cq_last_tm_flush = seq;\n\tspin_unlock_irq(&ctx->timeout_lock);\n}",
        "func": "static __cold void io_flush_timeouts(struct io_ring_ctx *ctx)\n\t__must_hold(&ctx->completion_lock)\n{\n\tu32 seq = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\tstruct io_kiocb *req, *tmp;\n\n\tspin_lock_irq(&ctx->timeout_lock);\n\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {\n\t\tu32 events_needed, events_got;\n\n\t\tif (io_is_timeout_noseq(req))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Since seq can easily wrap around over time, subtract\n\t\t * the last seq at which timeouts were flushed before comparing.\n\t\t * Assuming not more than 2^31-1 events have happened since,\n\t\t * these subtractions won't have wrapped, so we can check if\n\t\t * target is in [last_seq, current_seq] by comparing the two.\n\t\t */\n\t\tevents_needed = req->timeout.target_seq - ctx->cq_last_tm_flush;\n\t\tevents_got = seq - ctx->cq_last_tm_flush;\n\t\tif (events_got < events_needed)\n\t\t\tbreak;\n\n\t\tio_kill_timeout(req, 0);\n\t}\n\tctx->cq_last_tm_flush = seq;\n\tspin_unlock_irq(&ctx->timeout_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,12 +2,11 @@\n \t__must_hold(&ctx->completion_lock)\n {\n \tu32 seq = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n+\tstruct io_kiocb *req, *tmp;\n \n \tspin_lock_irq(&ctx->timeout_lock);\n-\twhile (!list_empty(&ctx->timeout_list)) {\n+\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {\n \t\tu32 events_needed, events_got;\n-\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,\n-\t\t\t\t\t\tstruct io_kiocb, timeout.list);\n \n \t\tif (io_is_timeout_noseq(req))\n \t\t\tbreak;\n@@ -24,7 +23,6 @@\n \t\tif (events_got < events_needed)\n \t\t\tbreak;\n \n-\t\tlist_del_init(&req->timeout.list);\n \t\tio_kill_timeout(req, 0);\n \t}\n \tctx->cq_last_tm_flush = seq;",
        "diff_line_info": {
            "deleted_lines": [
                "\twhile (!list_empty(&ctx->timeout_list)) {",
                "\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,",
                "\t\t\t\t\t\tstruct io_kiocb, timeout.list);",
                "\t\tlist_del_init(&req->timeout.list);"
            ],
            "added_lines": [
                "\tstruct io_kiocb *req, *tmp;",
                "\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6130",
        "func_name": "torvalds/linux/sclp_ctl_ioctl_sccb",
        "description": "Race condition in the sclp_ctl_ioctl_sccb function in drivers/s390/char/sclp_ctl.c in the Linux kernel before 4.6 allows local users to obtain sensitive information from kernel memory by changing a certain length value, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/torvalds/linux/commit/532c34b5fbf1687df63b3fcd5b2846312ac943c6",
        "commit_title": "s390/sclp_ctl: fix potential information leak with /dev/sclp",
        "commit_text": " The sclp_ctl_ioctl_sccb function uses two copy_from_user calls to retrieve the sclp request from user space. The first copy_from_user fetches the length of the request which is stored in the first two bytes of the request. The second copy_from_user gets the complete sclp request, but this copies the length field a second time. A malicious user may have changed the length in the meantime. ",
        "func_before": "static int sclp_ctl_ioctl_sccb(void __user *user_area)\n{\n\tstruct sclp_ctl_sccb ctl_sccb;\n\tstruct sccb_header *sccb;\n\tint rc;\n\n\tif (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))\n\t\treturn -EFAULT;\n\tif (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))\n\t\treturn -EOPNOTSUPP;\n\tsccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);\n\tif (!sccb)\n\t\treturn -ENOMEM;\n\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {\n\t\trc = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (sccb->length > PAGE_SIZE || sccb->length < 8)\n\t\treturn -EINVAL;\n\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {\n\t\trc = -EFAULT;\n\t\tgoto out_free;\n\t}\n\trc = sclp_sync_request(ctl_sccb.cmdw, sccb);\n\tif (rc)\n\t\tgoto out_free;\n\tif (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))\n\t\trc = -EFAULT;\nout_free:\n\tfree_page((unsigned long) sccb);\n\treturn rc;\n}",
        "func": "static int sclp_ctl_ioctl_sccb(void __user *user_area)\n{\n\tstruct sclp_ctl_sccb ctl_sccb;\n\tstruct sccb_header *sccb;\n\tunsigned long copied;\n\tint rc;\n\n\tif (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))\n\t\treturn -EFAULT;\n\tif (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))\n\t\treturn -EOPNOTSUPP;\n\tsccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);\n\tif (!sccb)\n\t\treturn -ENOMEM;\n\tcopied = PAGE_SIZE -\n\t\tcopy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);\n\tif (offsetof(struct sccb_header, length) +\n\t    sizeof(sccb->length) > copied || sccb->length > copied) {\n\t\trc = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (sccb->length < 8) {\n\t\trc = -EINVAL;\n\t\tgoto out_free;\n\t}\n\trc = sclp_sync_request(ctl_sccb.cmdw, sccb);\n\tif (rc)\n\t\tgoto out_free;\n\tif (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))\n\t\trc = -EFAULT;\nout_free:\n\tfree_page((unsigned long) sccb);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n {\n \tstruct sclp_ctl_sccb ctl_sccb;\n \tstruct sccb_header *sccb;\n+\tunsigned long copied;\n \tint rc;\n \n \tif (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))\n@@ -11,14 +12,15 @@\n \tsccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);\n \tif (!sccb)\n \t\treturn -ENOMEM;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {\n+\tcopied = PAGE_SIZE -\n+\t\tcopy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);\n+\tif (offsetof(struct sccb_header, length) +\n+\t    sizeof(sccb->length) > copied || sccb->length > copied) {\n \t\trc = -EFAULT;\n \t\tgoto out_free;\n \t}\n-\tif (sccb->length > PAGE_SIZE || sccb->length < 8)\n-\t\treturn -EINVAL;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {\n-\t\trc = -EFAULT;\n+\tif (sccb->length < 8) {\n+\t\trc = -EINVAL;\n \t\tgoto out_free;\n \t}\n \trc = sclp_sync_request(ctl_sccb.cmdw, sccb);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {",
                "\tif (sccb->length > PAGE_SIZE || sccb->length < 8)",
                "\t\treturn -EINVAL;",
                "\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {",
                "\t\trc = -EFAULT;"
            ],
            "added_lines": [
                "\tunsigned long copied;",
                "\tcopied = PAGE_SIZE -",
                "\t\tcopy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);",
                "\tif (offsetof(struct sccb_header, length) +",
                "\t    sizeof(sccb->length) > copied || sccb->length > copied) {",
                "\tif (sccb->length < 8) {",
                "\t\trc = -EINVAL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-3744",
        "func_name": "android/create_pbuf",
        "description": "Buffer overflow in the create_pbuf function in btif/src/btif_hh.c in Bluetooth in Android 4.x before 4.4.4, 5.0.x before 5.0.2, 5.1.x before 5.1.1, and 6.x before 2016-07-01 allows remote attackers to gain privileges via a crafted pairing operation, aka internal bug 27930580.",
        "git_url": "https://android.googlesource.com/platform/system/bt/+/514139f4b40cbb035bb92f3e24d5a389d75db9e6",
        "commit_title": "DO NOT MERGE btif: check overflow on create_pbuf size",
        "commit_text": " Bug: 27930580 ",
        "func_before": "static BT_HDR *create_pbuf(UINT16 len, UINT8 *data)\n{\n    BT_HDR* p_buf = GKI_getbuf((UINT16) (len + BTA_HH_MIN_OFFSET + sizeof(BT_HDR)));\n\n    if (p_buf) {\n        UINT8* pbuf_data;\n\n        p_buf->len = len;\n        p_buf->offset = BTA_HH_MIN_OFFSET;\n\n        pbuf_data = (UINT8*) (p_buf + 1) + p_buf->offset;\n        memcpy(pbuf_data, data, len);\n    }\n    return p_buf;\n}",
        "func": "static BT_HDR *create_pbuf(UINT16 len, UINT8 *data)\n{\n    UINT16 buflen = (UINT16) (len + BTA_HH_MIN_OFFSET + sizeof(BT_HDR));\n    if (buflen < len) {\n      android_errorWriteWithInfoLog(0x534e4554, \"28672558\", -1, NULL, 0);\n      return NULL;\n    }\n    BT_HDR* p_buf = GKI_getbuf(buflen);\n\n    if (p_buf) {\n        UINT8* pbuf_data;\n\n        p_buf->len = len;\n        p_buf->offset = BTA_HH_MIN_OFFSET;\n\n        pbuf_data = (UINT8*) (p_buf + 1) + p_buf->offset;\n        memcpy(pbuf_data, data, len);\n    }\n    return p_buf;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,11 @@\n static BT_HDR *create_pbuf(UINT16 len, UINT8 *data)\n {\n-    BT_HDR* p_buf = GKI_getbuf((UINT16) (len + BTA_HH_MIN_OFFSET + sizeof(BT_HDR)));\n+    UINT16 buflen = (UINT16) (len + BTA_HH_MIN_OFFSET + sizeof(BT_HDR));\n+    if (buflen < len) {\n+      android_errorWriteWithInfoLog(0x534e4554, \"28672558\", -1, NULL, 0);\n+      return NULL;\n+    }\n+    BT_HDR* p_buf = GKI_getbuf(buflen);\n \n     if (p_buf) {\n         UINT8* pbuf_data;",
        "diff_line_info": {
            "deleted_lines": [
                "    BT_HDR* p_buf = GKI_getbuf((UINT16) (len + BTA_HH_MIN_OFFSET + sizeof(BT_HDR)));"
            ],
            "added_lines": [
                "    UINT16 buflen = (UINT16) (len + BTA_HH_MIN_OFFSET + sizeof(BT_HDR));",
                "    if (buflen < len) {",
                "      android_errorWriteWithInfoLog(0x534e4554, \"28672558\", -1, NULL, 0);",
                "      return NULL;",
                "    }",
                "    BT_HDR* p_buf = GKI_getbuf(buflen);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-3760",
        "func_name": "android/init",
        "description": "Bluetooth in Android 5.0.x before 5.0.2, 5.1.x before 5.1.1, and 6.x before 2016-07-01 allows local users to gain privileges by establishing a pairing that remains present during a session of the primary user, aka internal bug 27410683.",
        "git_url": "https://android.googlesource.com/platform/system/bt/+/37c88107679d36c419572732b4af6e18bb2f7dce",
        "commit_title": "Add guest mode functionality (2/3)",
        "commit_text": " Add a flag to enable() to start Bluetooth in restricted mode. In restricted mode, all devices that are paired during restricted mode are deleted upon leaving restricted mode. Right now restricted mode is only entered while a guest user is active.  Bug: 27410683 ",
        "func_before": "static future_t *init(void) {\n  pthread_mutex_init(&lock, NULL);\n  config = config_new(CONFIG_FILE_PATH);\n  if (!config) {\n    LOG_WARN(\"%s unable to load config file; attempting to transcode legacy file.\", __func__);\n    config = btif_config_transcode(LEGACY_CONFIG_FILE_PATH);\n    if (!config) {\n      LOG_WARN(\"%s unable to transcode legacy file, starting unconfigured.\", __func__);\n      config = config_new_empty();\n      if (!config) {\n        LOG_ERROR(\"%s unable to allocate a config object.\", __func__);\n        goto error;\n      }\n    }\n\n    if (config_save(config, CONFIG_FILE_PATH))\n      unlink(LEGACY_CONFIG_FILE_PATH);\n  }\n\n  btif_config_remove_unpaired(config);\n\n  // TODO(sharvil): use a non-wake alarm for this once we have\n  // API support for it. There's no need to wake the system to\n  // write back to disk.\n  alarm_timer = alarm_new();\n  if (!alarm_timer) {\n    LOG_ERROR(\"%s unable to create alarm.\", __func__);\n    goto error;\n  }\n\n  return future_new_immediate(FUTURE_SUCCESS);\n\nerror:;\n  alarm_free(alarm_timer);\n  config_free(config);\n  pthread_mutex_destroy(&lock);\n  alarm_timer = NULL;\n  config = NULL;\n  return future_new_immediate(FUTURE_FAIL);\n}",
        "func": "static future_t *init(void) {\n  pthread_mutex_init(&lock, NULL);\n  config = config_new(CONFIG_FILE_PATH);\n  if (!config) {\n    LOG_WARN(\"%s unable to load config file; attempting to transcode legacy file.\", __func__);\n    config = btif_config_transcode(LEGACY_CONFIG_FILE_PATH);\n    if (!config) {\n      LOG_WARN(\"%s unable to transcode legacy file, starting unconfigured.\", __func__);\n      config = config_new_empty();\n      if (!config) {\n        LOG_ERROR(\"%s unable to allocate a config object.\", __func__);\n        goto error;\n      }\n    }\n\n    if (config_save(config, CONFIG_FILE_PATH))\n      unlink(LEGACY_CONFIG_FILE_PATH);\n  }\n\n  btif_config_remove_unpaired(config);\n\n  // Cleanup temporary pairings if we have left guest mode\n  if (!is_restricted_mode())\n    btif_config_remove_restricted(config);\n\n  // TODO(sharvil): use a non-wake alarm for this once we have\n  // API support for it. There's no need to wake the system to\n  // write back to disk.\n  alarm_timer = alarm_new();\n  if (!alarm_timer) {\n    LOG_ERROR(\"%s unable to create alarm.\", __func__);\n    goto error;\n  }\n\n  return future_new_immediate(FUTURE_SUCCESS);\n\nerror:;\n  alarm_free(alarm_timer);\n  config_free(config);\n  pthread_mutex_destroy(&lock);\n  alarm_timer = NULL;\n  config = NULL;\n  return future_new_immediate(FUTURE_FAIL);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,6 +19,10 @@\n \n   btif_config_remove_unpaired(config);\n \n+  // Cleanup temporary pairings if we have left guest mode\n+  if (!is_restricted_mode())\n+    btif_config_remove_restricted(config);\n+\n   // TODO(sharvil): use a non-wake alarm for this once we have\n   // API support for it. There's no need to wake the system to\n   // write back to disk.",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  // Cleanup temporary pairings if we have left guest mode",
                "  if (!is_restricted_mode())",
                "    btif_config_remove_restricted(config);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2016-3760",
        "func_name": "android/btif_storage_add_bonded_device",
        "description": "Bluetooth in Android 5.0.x before 5.0.2, 5.1.x before 5.1.1, and 6.x before 2016-07-01 allows local users to gain privileges by establishing a pairing that remains present during a session of the primary user, aka internal bug 27410683.",
        "git_url": "https://android.googlesource.com/platform/system/bt/+/37c88107679d36c419572732b4af6e18bb2f7dce",
        "commit_title": "Add guest mode functionality (2/3)",
        "commit_text": " Add a flag to enable() to start Bluetooth in restricted mode. In restricted mode, all devices that are paired during restricted mode are deleted upon leaving restricted mode. Right now restricted mode is only entered while a guest user is active.  Bug: 27410683 ",
        "func_before": "bt_status_t btif_storage_add_bonded_device(bt_bdaddr_t *remote_bd_addr,\n                                           LINK_KEY link_key,\n                                           uint8_t key_type,\n                                           uint8_t pin_length)\n{\n    bdstr_t bdstr;\n    bdaddr_to_string(remote_bd_addr, bdstr, sizeof(bdstr));\n    int ret = btif_config_set_int(bdstr, \"LinkKeyType\", (int)key_type);\n    ret &= btif_config_set_int(bdstr, \"PinLength\", (int)pin_length);\n    ret &= btif_config_set_bin(bdstr, \"LinkKey\", link_key, sizeof(LINK_KEY));\n    /* write bonded info immediately */\n    btif_config_flush();\n    return ret ? BT_STATUS_SUCCESS : BT_STATUS_FAIL;\n}",
        "func": "bt_status_t btif_storage_add_bonded_device(bt_bdaddr_t *remote_bd_addr,\n                                           LINK_KEY link_key,\n                                           uint8_t key_type,\n                                           uint8_t pin_length)\n{\n    bdstr_t bdstr;\n    bdaddr_to_string(remote_bd_addr, bdstr, sizeof(bdstr));\n    int ret = btif_config_set_int(bdstr, \"LinkKeyType\", (int)key_type);\n    ret &= btif_config_set_int(bdstr, \"PinLength\", (int)pin_length);\n    ret &= btif_config_set_bin(bdstr, \"LinkKey\", link_key, sizeof(LINK_KEY));\n\n    if (is_restricted_mode()) {\n        BTIF_TRACE_WARNING(\"%s: '%s' pairing will be removed if unrestricted\",\n                         __func__, bdstr);\n        btif_config_set_int(bdstr, \"Restricted\", 1);\n    }\n\n    /* write bonded info immediately */\n    btif_config_flush();\n    return ret ? BT_STATUS_SUCCESS : BT_STATUS_FAIL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,13 @@\n     int ret = btif_config_set_int(bdstr, \"LinkKeyType\", (int)key_type);\n     ret &= btif_config_set_int(bdstr, \"PinLength\", (int)pin_length);\n     ret &= btif_config_set_bin(bdstr, \"LinkKey\", link_key, sizeof(LINK_KEY));\n+\n+    if (is_restricted_mode()) {\n+        BTIF_TRACE_WARNING(\"%s: '%s' pairing will be removed if unrestricted\",\n+                         __func__, bdstr);\n+        btif_config_set_int(bdstr, \"Restricted\", 1);\n+    }\n+\n     /* write bonded info immediately */\n     btif_config_flush();\n     return ret ? BT_STATUS_SUCCESS : BT_STATUS_FAIL;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    if (is_restricted_mode()) {",
                "        BTIF_TRACE_WARNING(\"%s: '%s' pairing will be removed if unrestricted\",",
                "                         __func__, bdstr);",
                "        btif_config_set_int(bdstr, \"Restricted\", 1);",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2016-3760",
        "func_name": "android/enable",
        "description": "Bluetooth in Android 5.0.x before 5.0.2, 5.1.x before 5.1.1, and 6.x before 2016-07-01 allows local users to gain privileges by establishing a pairing that remains present during a session of the primary user, aka internal bug 27410683.",
        "git_url": "https://android.googlesource.com/platform/system/bt/+/37c88107679d36c419572732b4af6e18bb2f7dce",
        "commit_title": "Add guest mode functionality (2/3)",
        "commit_text": " Add a flag to enable() to start Bluetooth in restricted mode. In restricted mode, all devices that are paired during restricted mode are deleted upon leaving restricted mode. Right now restricted mode is only entered while a guest user is active.  Bug: 27410683 ",
        "func_before": "static int enable(void) {\n  LOG_INFO(\"%s\", __func__);\n\n  if (!interface_ready())\n    return BT_STATUS_NOT_READY;\n\n  stack_manager_get_interface()->start_up_stack_async();\n  return BT_STATUS_SUCCESS;\n}",
        "func": "static int enable(bool start_restricted) {\n  LOG_INFO(LOG_TAG, \"%s: start restricted = %d\", __func__, start_restricted);\n\n  restricted_mode = start_restricted;\n\n  if (!interface_ready())\n    return BT_STATUS_NOT_READY;\n\n  stack_manager_get_interface()->start_up_stack_async();\n  return BT_STATUS_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,7 @@\n-static int enable(void) {\n-  LOG_INFO(\"%s\", __func__);\n+static int enable(bool start_restricted) {\n+  LOG_INFO(LOG_TAG, \"%s: start restricted = %d\", __func__, start_restricted);\n+\n+  restricted_mode = start_restricted;\n \n   if (!interface_ready())\n     return BT_STATUS_NOT_READY;",
        "diff_line_info": {
            "deleted_lines": [
                "static int enable(void) {",
                "  LOG_INFO(\"%s\", __func__);"
            ],
            "added_lines": [
                "static int enable(bool start_restricted) {",
                "  LOG_INFO(LOG_TAG, \"%s: start restricted = %d\", __func__, start_restricted);",
                "",
                "  restricted_mode = start_restricted;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-3760",
        "func_name": "android/remove_bond",
        "description": "Bluetooth in Android 5.0.x before 5.0.2, 5.1.x before 5.1.1, and 6.x before 2016-07-01 allows local users to gain privileges by establishing a pairing that remains present during a session of the primary user, aka internal bug 27410683.",
        "git_url": "https://android.googlesource.com/platform/system/bt/+/37c88107679d36c419572732b4af6e18bb2f7dce",
        "commit_title": "Add guest mode functionality (2/3)",
        "commit_text": " Add a flag to enable() to start Bluetooth in restricted mode. In restricted mode, all devices that are paired during restricted mode are deleted upon leaving restricted mode. Right now restricted mode is only entered while a guest user is active.  Bug: 27410683 ",
        "func_before": "static int remove_bond(const bt_bdaddr_t *bd_addr)\n{\n    /* sanity check */\n    if (interface_ready() == FALSE)\n        return BT_STATUS_NOT_READY;\n\n    return btif_dm_remove_bond(bd_addr);\n}",
        "func": "static int remove_bond(const bt_bdaddr_t *bd_addr)\n{\n    if (is_restricted_mode() && !btif_storage_is_restricted_device(bd_addr))\n        return BT_STATUS_SUCCESS;\n\n    /* sanity check */\n    if (interface_ready() == FALSE)\n        return BT_STATUS_NOT_READY;\n\n    return btif_dm_remove_bond(bd_addr);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,8 @@\n static int remove_bond(const bt_bdaddr_t *bd_addr)\n {\n+    if (is_restricted_mode() && !btif_storage_is_restricted_device(bd_addr))\n+        return BT_STATUS_SUCCESS;\n+\n     /* sanity check */\n     if (interface_ready() == FALSE)\n         return BT_STATUS_NOT_READY;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (is_restricted_mode() && !btif_storage_is_restricted_device(bd_addr))",
                "        return BT_STATUS_SUCCESS;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2016-3760",
        "func_name": "android/main",
        "description": "Bluetooth in Android 5.0.x before 5.0.2, 5.1.x before 5.1.1, and 6.x before 2016-07-01 allows local users to gain privileges by establishing a pairing that remains present during a session of the primary user, aka internal bug 27410683.",
        "git_url": "https://android.googlesource.com/platform/system/bt/+/37c88107679d36c419572732b4af6e18bb2f7dce",
        "commit_title": "Add guest mode functionality (2/3)",
        "commit_text": " Add a flag to enable() to start Bluetooth in restricted mode. In restricted mode, all devices that are paired during restricted mode are deleted upon leaving restricted mode. Right now restricted mode is only entered while a guest user is active.  Bug: 27410683 ",
        "func_before": "int main(int argc, char **argv) {\n  if (!parse_args(argc, argv)) {\n    usage(argv[0]);\n  }\n\n  if (bond && discoverable) {\n    fprintf(stderr, \"Can only select either bond or discoverable, not both\\n\");\n    usage(argv[0]);\n  }\n\n  if (sco_listen && sco_connect) {\n    fprintf(stderr, \"Can only select either sco_listen or sco_connect, not both\\n\");\n    usage(argv[0]);\n  }\n\n  if (!bond && !discover && !discoverable && !up && !get_name && !set_name && !sco_listen && !sco_connect) {\n    fprintf(stderr, \"Must specify one command\\n\");\n    usage(argv[0]);\n  }\n\n  if (signal(SIGINT, sig_handler) == SIG_ERR) {\n    fprintf(stderr, \"Will be unable to catch signals\\n\");\n  }\n\n  fprintf(stdout, \"Bringing up bluetooth adapter\\n\");\n  if (!hal_open(callbacks_get_adapter_struct())) {\n    fprintf(stderr, \"Unable to open Bluetooth HAL.\\n\");\n    return 1;\n  }\n\n  if (discover) {\n    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    fprintf(stdout, \"Starting to start discovery\\n\");\n    CALL_AND_WAIT(bt_interface->start_discovery(), discovery_state_changed);\n    fprintf(stdout, \"Started discovery for %d seconds\\n\", timeout_in_sec);\n\n    sleep(timeout_in_sec);\n\n    fprintf(stdout, \"Starting to cancel discovery\\n\");\n    CALL_AND_WAIT(bt_interface->cancel_discovery(), discovery_state_changed);\n    fprintf(stdout, \"Cancelled discovery after %d seconds\\n\", timeout_in_sec);\n  }\n\n  if (discoverable) {\n    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    bt_property_t *property = property_new_scan_mode(BT_SCAN_MODE_CONNECTABLE_DISCOVERABLE);\n\n    int rc = bt_interface->set_adapter_property(property);\n    fprintf(stdout, \"Set rc:%d device as discoverable for %d seconds\\n\", rc, timeout_in_sec);\n\n    sleep(timeout_in_sec);\n\n    property_free(property);\n  }\n\n   if (bond) {\n    if (bdaddr_is_empty(&bt_remote_bdaddr)) {\n      fprintf(stderr, \"Must specify a remote device address [ --bdaddr=xx:yy:zz:aa:bb:cc ]\\n\");\n      exit(1);\n    }\n\n    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    int rc = bt_interface->create_bond(&bt_remote_bdaddr, 0 /* UNKNOWN; Currently not documented :( */);\n    fprintf(stdout, \"Started bonding:%d for %d seconds\\n\", rc, timeout_in_sec);\n\n    sleep(timeout_in_sec);\n  }\n\n  if (up) {\n    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    fprintf(stdout, \"Waiting for %d seconds\\n\", timeout_in_sec);\n    sleep(timeout_in_sec);\n  }\n\n  if (get_name) {\n    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n    int error;\n    CALL_AND_WAIT(error = bt_interface->get_adapter_property(BT_PROPERTY_BDNAME), adapter_properties);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to get adapter property\\n\");\n      exit(1);\n    }\n    bt_property_t *property = adapter_get_property(BT_PROPERTY_BDNAME);\n    const bt_bdname_t *name = property_as_name(property);\n    if (name)\n      printf(\"Queried bluetooth device name:%s\\n\", name->name);\n    else\n      printf(\"No name\\n\");\n  }\n\n  if (set_name) {\n    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    bt_property_t *property = property_new_name(bd_name);\n    printf(\"Setting bluetooth device name to:%s\\n\", bd_name);\n    int error;\n    CALL_AND_WAIT(error = bt_interface->set_adapter_property(property), adapter_properties);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to set adapter property\\n\");\n      exit(1);\n    }\n    CALL_AND_WAIT(error = bt_interface->get_adapter_property(BT_PROPERTY_BDNAME), adapter_properties);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to get adapter property\\n\");\n      exit(1);\n    }\n    property_free(property);\n    sleep(timeout_in_sec);\n  }\n\n  if (sco_listen) {\n    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    bt_property_t *property = property_new_scan_mode(BT_SCAN_MODE_CONNECTABLE_DISCOVERABLE);\n    CALL_AND_WAIT(bt_interface->set_adapter_property(property), adapter_properties);\n    property_free(property);\n\n    const btsock_interface_t *sock = bt_interface->get_profile_interface(BT_PROFILE_SOCKETS_ID);\n\n    int rfcomm_fd = INVALID_FD;\n    int error = sock->listen(BTSOCK_RFCOMM, \"meow\", (const uint8_t *)&HFP_AG_UUID, 0, &rfcomm_fd, 0);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to listen for incoming RFCOMM socket: %d\\n\", error);\n      exit(1);\n    }\n\n    int sock_fd = INVALID_FD;\n    error = sock->listen(BTSOCK_SCO, NULL, NULL, 5, &sock_fd, 0);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to listen for incoming SCO sockets: %d\\n\", error);\n      exit(1);\n    }\n    fprintf(stdout, \"Waiting for incoming SCO connections...\\n\");\n    sleep(timeout_in_sec);\n  }\n\n  if (sco_connect) {\n    if (bdaddr_is_empty(&bt_remote_bdaddr)) {\n      fprintf(stderr, \"Must specify a remote device address [ --bdaddr=xx:yy:zz:aa:bb:cc ]\\n\");\n      exit(1);\n    }\n\n    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    const btsock_interface_t *sock = bt_interface->get_profile_interface(BT_PROFILE_SOCKETS_ID);\n\n    int rfcomm_fd = INVALID_FD;\n    int error = sock->connect(&bt_remote_bdaddr, BTSOCK_RFCOMM, (const uint8_t *)&HFP_AG_UUID, 0, &rfcomm_fd, 0);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to connect to RFCOMM socket: %d.\\n\", error);\n      exit(1);\n    }\n\n    WAIT(acl_state_changed);\n\n    fprintf(stdout, \"Establishing SCO connection...\\n\");\n\n    int sock_fd = INVALID_FD;\n    error = sock->connect(&bt_remote_bdaddr, BTSOCK_SCO, NULL, 5, &sock_fd, 0);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to connect to SCO socket: %d.\\n\", error);\n      exit(1);\n    }\n    sleep(timeout_in_sec);\n  }\n\n  CALL_AND_WAIT(bt_interface->disable(), adapter_state_changed);\n  fprintf(stdout, \"BT adapter is down\\n\");\n}",
        "func": "int main(int argc, char **argv) {\n  if (!parse_args(argc, argv)) {\n    usage(argv[0]);\n  }\n\n  if (bond && discoverable) {\n    fprintf(stderr, \"Can only select either bond or discoverable, not both\\n\");\n    usage(argv[0]);\n  }\n\n  if (sco_listen && sco_connect) {\n    fprintf(stderr, \"Can only select either sco_listen or sco_connect, not both\\n\");\n    usage(argv[0]);\n  }\n\n  if (!bond && !discover && !discoverable && !up && !get_name && !set_name && !sco_listen && !sco_connect) {\n    fprintf(stderr, \"Must specify one command\\n\");\n    usage(argv[0]);\n  }\n\n  if (signal(SIGINT, sig_handler) == SIG_ERR) {\n    fprintf(stderr, \"Will be unable to catch signals\\n\");\n  }\n\n  fprintf(stdout, \"Bringing up bluetooth adapter\\n\");\n  if (!hal_open(callbacks_get_adapter_struct())) {\n    fprintf(stderr, \"Unable to open Bluetooth HAL.\\n\");\n    return 1;\n  }\n\n  if (discover) {\n    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    fprintf(stdout, \"Starting to start discovery\\n\");\n    CALL_AND_WAIT(bt_interface->start_discovery(), discovery_state_changed);\n    fprintf(stdout, \"Started discovery for %d seconds\\n\", timeout_in_sec);\n\n    sleep(timeout_in_sec);\n\n    fprintf(stdout, \"Starting to cancel discovery\\n\");\n    CALL_AND_WAIT(bt_interface->cancel_discovery(), discovery_state_changed);\n    fprintf(stdout, \"Cancelled discovery after %d seconds\\n\", timeout_in_sec);\n  }\n\n  if (discoverable) {\n    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    bt_property_t *property = property_new_scan_mode(BT_SCAN_MODE_CONNECTABLE_DISCOVERABLE);\n\n    int rc = bt_interface->set_adapter_property(property);\n    fprintf(stdout, \"Set rc:%d device as discoverable for %d seconds\\n\", rc, timeout_in_sec);\n\n    sleep(timeout_in_sec);\n\n    property_free(property);\n  }\n\n   if (bond) {\n    if (bdaddr_is_empty(&bt_remote_bdaddr)) {\n      fprintf(stderr, \"Must specify a remote device address [ --bdaddr=xx:yy:zz:aa:bb:cc ]\\n\");\n      exit(1);\n    }\n\n    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    int rc = bt_interface->create_bond(&bt_remote_bdaddr, 0 /* UNKNOWN; Currently not documented :( */);\n    fprintf(stdout, \"Started bonding:%d for %d seconds\\n\", rc, timeout_in_sec);\n\n    sleep(timeout_in_sec);\n  }\n\n  if (up) {\n    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    fprintf(stdout, \"Waiting for %d seconds\\n\", timeout_in_sec);\n    sleep(timeout_in_sec);\n  }\n\n  if (get_name) {\n    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n    int error;\n    CALL_AND_WAIT(error = bt_interface->get_adapter_property(BT_PROPERTY_BDNAME), adapter_properties);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to get adapter property\\n\");\n      exit(1);\n    }\n    bt_property_t *property = adapter_get_property(BT_PROPERTY_BDNAME);\n    const bt_bdname_t *name = property_as_name(property);\n    if (name)\n      printf(\"Queried bluetooth device name:%s\\n\", name->name);\n    else\n      printf(\"No name\\n\");\n  }\n\n  if (set_name) {\n    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    bt_property_t *property = property_new_name(bd_name);\n    printf(\"Setting bluetooth device name to:%s\\n\", bd_name);\n    int error;\n    CALL_AND_WAIT(error = bt_interface->set_adapter_property(property), adapter_properties);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to set adapter property\\n\");\n      exit(1);\n    }\n    CALL_AND_WAIT(error = bt_interface->get_adapter_property(BT_PROPERTY_BDNAME), adapter_properties);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to get adapter property\\n\");\n      exit(1);\n    }\n    property_free(property);\n    sleep(timeout_in_sec);\n  }\n\n  if (sco_listen) {\n    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    bt_property_t *property = property_new_scan_mode(BT_SCAN_MODE_CONNECTABLE_DISCOVERABLE);\n    CALL_AND_WAIT(bt_interface->set_adapter_property(property), adapter_properties);\n    property_free(property);\n\n    const btsock_interface_t *sock = bt_interface->get_profile_interface(BT_PROFILE_SOCKETS_ID);\n\n    int rfcomm_fd = INVALID_FD;\n    int error = sock->listen(BTSOCK_RFCOMM, \"meow\", (const uint8_t *)&HFP_AG_UUID, 0, &rfcomm_fd, 0);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to listen for incoming RFCOMM socket: %d\\n\", error);\n      exit(1);\n    }\n\n    int sock_fd = INVALID_FD;\n    error = sock->listen(BTSOCK_SCO, NULL, NULL, 5, &sock_fd, 0);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to listen for incoming SCO sockets: %d\\n\", error);\n      exit(1);\n    }\n    fprintf(stdout, \"Waiting for incoming SCO connections...\\n\");\n    sleep(timeout_in_sec);\n  }\n\n  if (sco_connect) {\n    if (bdaddr_is_empty(&bt_remote_bdaddr)) {\n      fprintf(stderr, \"Must specify a remote device address [ --bdaddr=xx:yy:zz:aa:bb:cc ]\\n\");\n      exit(1);\n    }\n\n    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n    fprintf(stdout, \"BT adapter is up\\n\");\n\n    const btsock_interface_t *sock = bt_interface->get_profile_interface(BT_PROFILE_SOCKETS_ID);\n\n    int rfcomm_fd = INVALID_FD;\n    int error = sock->connect(&bt_remote_bdaddr, BTSOCK_RFCOMM, (const uint8_t *)&HFP_AG_UUID, 0, &rfcomm_fd, 0);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to connect to RFCOMM socket: %d.\\n\", error);\n      exit(1);\n    }\n\n    WAIT(acl_state_changed);\n\n    fprintf(stdout, \"Establishing SCO connection...\\n\");\n\n    int sock_fd = INVALID_FD;\n    error = sock->connect(&bt_remote_bdaddr, BTSOCK_SCO, NULL, 5, &sock_fd, 0);\n    if (error != BT_STATUS_SUCCESS) {\n      fprintf(stderr, \"Unable to connect to SCO socket: %d.\\n\", error);\n      exit(1);\n    }\n    sleep(timeout_in_sec);\n  }\n\n  CALL_AND_WAIT(bt_interface->disable(), adapter_state_changed);\n  fprintf(stdout, \"BT adapter is down\\n\");\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -29,7 +29,7 @@\n   }\n \n   if (discover) {\n-    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n+    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n     fprintf(stdout, \"BT adapter is up\\n\");\n \n     fprintf(stdout, \"Starting to start discovery\\n\");\n@@ -44,7 +44,7 @@\n   }\n \n   if (discoverable) {\n-    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n+    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n     fprintf(stdout, \"BT adapter is up\\n\");\n \n     bt_property_t *property = property_new_scan_mode(BT_SCAN_MODE_CONNECTABLE_DISCOVERABLE);\n@@ -63,7 +63,7 @@\n       exit(1);\n     }\n \n-    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n+    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n     fprintf(stdout, \"BT adapter is up\\n\");\n \n     int rc = bt_interface->create_bond(&bt_remote_bdaddr, 0 /* UNKNOWN; Currently not documented :( */);\n@@ -73,7 +73,7 @@\n   }\n \n   if (up) {\n-    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n+    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n     fprintf(stdout, \"BT adapter is up\\n\");\n \n     fprintf(stdout, \"Waiting for %d seconds\\n\", timeout_in_sec);\n@@ -81,7 +81,7 @@\n   }\n \n   if (get_name) {\n-    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n+    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n     fprintf(stdout, \"BT adapter is up\\n\");\n     int error;\n     CALL_AND_WAIT(error = bt_interface->get_adapter_property(BT_PROPERTY_BDNAME), adapter_properties);\n@@ -98,7 +98,7 @@\n   }\n \n   if (set_name) {\n-    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n+    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n     fprintf(stdout, \"BT adapter is up\\n\");\n \n     bt_property_t *property = property_new_name(bd_name);\n@@ -119,7 +119,7 @@\n   }\n \n   if (sco_listen) {\n-    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n+    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n     fprintf(stdout, \"BT adapter is up\\n\");\n \n     bt_property_t *property = property_new_scan_mode(BT_SCAN_MODE_CONNECTABLE_DISCOVERABLE);\n@@ -151,7 +151,7 @@\n       exit(1);\n     }\n \n-    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);\n+    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);\n     fprintf(stdout, \"BT adapter is up\\n\");\n \n     const btsock_interface_t *sock = bt_interface->get_profile_interface(BT_PROFILE_SOCKETS_ID);",
        "diff_line_info": {
            "deleted_lines": [
                "    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(), adapter_state_changed);"
            ],
            "added_lines": [
                "    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);",
                "    CALL_AND_WAIT(bt_interface->enable(false), adapter_state_changed);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-3760",
        "func_name": "android/enableNative",
        "description": "Bluetooth in Android 5.0.x before 5.0.2, 5.1.x before 5.1.1, and 6.x before 2016-07-01 allows local users to gain privileges by establishing a pairing that remains present during a session of the primary user, aka internal bug 27410683.",
        "git_url": "https://android.googlesource.com/platform/packages/apps/Bluetooth/+/122feb9a0b04290f55183ff2f0384c6c53756bd8",
        "commit_title": "Add guest mode functionality (3/3)",
        "commit_text": " Add a flag to enable() to start Bluetooth in restricted mode. In restricted mode, all devices that are paired during restricted mode are deleted upon leaving restricted mode. Right now restricted mode is only entered while a guest user is active.  Bug: 27410683 ",
        "func_before": "static jboolean enableNative(JNIEnv* env, jobject obj) {\n    ALOGV(\"%s:\",__FUNCTION__);\n\n    jboolean result = JNI_FALSE;\n    if (!sBluetoothInterface) return result;\n\n    int ret = sBluetoothInterface->enable();\n    result = (ret == BT_STATUS_SUCCESS || ret == BT_STATUS_DONE) ? JNI_TRUE : JNI_FALSE;\n    return result;\n}",
        "func": "static jboolean enableNative(JNIEnv* env, jobject obj, jboolean isGuest) {\n    ALOGV(\"%s:\",__FUNCTION__);\n\n    jboolean result = JNI_FALSE;\n    if (!sBluetoothInterface) return result;\n    int ret = sBluetoothInterface->enable(isGuest == JNI_TRUE ? 1 : 0);\n    result = (ret == BT_STATUS_SUCCESS || ret == BT_STATUS_DONE) ? JNI_TRUE : JNI_FALSE;\n    return result;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,9 @@\n-static jboolean enableNative(JNIEnv* env, jobject obj) {\n+static jboolean enableNative(JNIEnv* env, jobject obj, jboolean isGuest) {\n     ALOGV(\"%s:\",__FUNCTION__);\n \n     jboolean result = JNI_FALSE;\n     if (!sBluetoothInterface) return result;\n-\n-    int ret = sBluetoothInterface->enable();\n+    int ret = sBluetoothInterface->enable(isGuest == JNI_TRUE ? 1 : 0);\n     result = (ret == BT_STATUS_SUCCESS || ret == BT_STATUS_DONE) ? JNI_TRUE : JNI_FALSE;\n     return result;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static jboolean enableNative(JNIEnv* env, jobject obj) {",
                "",
                "    int ret = sBluetoothInterface->enable();"
            ],
            "added_lines": [
                "static jboolean enableNative(JNIEnv* env, jobject obj, jboolean isGuest) {",
                "    int ret = sBluetoothInterface->enable(isGuest == JNI_TRUE ? 1 : 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6136",
        "func_name": "linux-audit/audit-kernel/sched_feat_enable",
        "description": "Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/linux-audit/audit-kernel/commit/e73e81975f2447e6f556100cada64a18ec631cbb",
        "commit_title": "sched/debug: Fix potential deadlock when writing to sched_features",
        "commit_text": " The following lockdep report can be triggered by writing to /sys/kernel/debug/sched_features:    ======================================================   WARNING: possible circular locking dependency detected   4.18.0-rc6-00152-gcd3f77d74ac3-dirty #18 Not tainted   ------------------------------------------------------   sh/3358 is trying to acquire lock:   000000004ad3989d (cpu_hotplug_lock.rw_sem){++++}, at: static_key_enable+0x14/0x30   but task is already holding lock:   00000000c1b31a88 (&sb->s_type->i_mutex_key#3){+.+.}, at: sched_feat_write+0x160/0x428   which lock already depends on the new lock.   the existing dependency chain (in reverse order) is:   -> #3 (&sb->s_type->i_mutex_key#3){+.+.}:          lock_acquire+0xb8/0x148          down_write+0xac/0x140          start_creating+0x5c/0x168          debugfs_create_dir+0x18/0x220          opp_debug_register+0x8c/0x120          _add_opp_dev+0x104/0x1f8          dev_pm_opp_get_opp_table+0x174/0x340          _of_add_opp_table_v2+0x110/0x760          dev_pm_opp_of_add_table+0x5c/0x240          dev_pm_opp_of_cpumask_add_table+0x5c/0x100          cpufreq_init+0x160/0x430          cpufreq_online+0x1cc/0xe30          cpufreq_add_dev+0x78/0x198          subsys_interface_register+0x168/0x270          cpufreq_register_driver+0x1c8/0x278          dt_cpufreq_probe+0xdc/0x1b8          platform_drv_probe+0xb4/0x168          driver_probe_device+0x318/0x4b0          __device_attach_driver+0xfc/0x1f0          bus_for_each_drv+0xf8/0x180          __device_attach+0x164/0x200          device_initial_probe+0x10/0x18          bus_probe_device+0x110/0x178          device_add+0x6d8/0x908          platform_device_add+0x138/0x3d8          platform_device_register_full+0x1cc/0x1f8          cpufreq_dt_platdev_init+0x174/0x1bc          do_one_initcall+0xb8/0x310          kernel_init_freeable+0x4b8/0x56c          kernel_init+0x10/0x138          ret_from_fork+0x10/0x18   -> #2 (opp_table_lock){+.+.}:          lock_acquire+0xb8/0x148          __mutex_lock+0x104/0xf50          mutex_lock_nested+0x1c/0x28          _of_add_opp_table_v2+0xb4/0x760          dev_pm_opp_of_add_table+0x5c/0x240          dev_pm_opp_of_cpumask_add_table+0x5c/0x100          cpufreq_init+0x160/0x430          cpufreq_online+0x1cc/0xe30          cpufreq_add_dev+0x78/0x198          subsys_interface_register+0x168/0x270          cpufreq_register_driver+0x1c8/0x278          dt_cpufreq_probe+0xdc/0x1b8          platform_drv_probe+0xb4/0x168          driver_probe_device+0x318/0x4b0          __device_attach_driver+0xfc/0x1f0          bus_for_each_drv+0xf8/0x180          __device_attach+0x164/0x200          device_initial_probe+0x10/0x18          bus_probe_device+0x110/0x178          device_add+0x6d8/0x908          platform_device_add+0x138/0x3d8          platform_device_register_full+0x1cc/0x1f8          cpufreq_dt_platdev_init+0x174/0x1bc          do_one_initcall+0xb8/0x310          kernel_init_freeable+0x4b8/0x56c          kernel_init+0x10/0x138          ret_from_fork+0x10/0x18   -> #1 (subsys mutex#6){+.+.}:          lock_acquire+0xb8/0x148          __mutex_lock+0x104/0xf50          mutex_lock_nested+0x1c/0x28          subsys_interface_register+0xd8/0x270          cpufreq_register_driver+0x1c8/0x278          dt_cpufreq_probe+0xdc/0x1b8          platform_drv_probe+0xb4/0x168          driver_probe_device+0x318/0x4b0          __device_attach_driver+0xfc/0x1f0          bus_for_each_drv+0xf8/0x180          __device_attach+0x164/0x200          device_initial_probe+0x10/0x18          bus_probe_device+0x110/0x178          device_add+0x6d8/0x908          platform_device_add+0x138/0x3d8          platform_device_register_full+0x1cc/0x1f8          cpufreq_dt_platdev_init+0x174/0x1bc          do_one_initcall+0xb8/0x310          kernel_init_freeable+0x4b8/0x56c          kernel_init+0x10/0x138          ret_from_fork+0x10/0x18   -> #0 (cpu_hotplug_lock.rw_sem){++++}:          __lock_acquire+0x203c/0x21d0          lock_acquire+0xb8/0x148          cpus_read_lock+0x58/0x1c8          static_key_enable+0x14/0x30          sched_feat_write+0x314/0x428          full_proxy_write+0xa0/0x138          __vfs_write+0xd8/0x388          vfs_write+0xdc/0x318          ksys_write+0xb4/0x138          sys_write+0xc/0x18          __sys_trace_return+0x0/0x4   other info that might help us debug this:   Chain exists of:     cpu_hotplug_lock.rw_sem --> opp_table_lock --> &sb->s_type->i_mutex_key#3    Possible unsafe locking scenario:          CPU0                    CPU1          ----                    ----     lock(&sb->s_type->i_mutex_key#3);                                  lock(opp_table_lock);                                  lock(&sb->s_type->i_mutex_key#3);     lock(cpu_hotplug_lock.rw_sem);    *** DEADLOCK ***   2 locks held by sh/3358:    #0: 00000000a8c4b363 (sb_writers#10){.+.+}, at: vfs_write+0x238/0x318    #1: 00000000c1b31a88 (&sb->s_type->i_mutex_key#3){+.+.}, at: sched_feat_write+0x160/0x428   stack backtrace:   CPU: 5 PID: 3358 Comm: sh Not tainted 4.18.0-rc6-00152-gcd3f77d74ac3-dirty #18   Hardware name: Renesas H3ULCB Kingfisher board based on r8a7795 ES2.0+ (DT)   Call trace:    dump_backtrace+0x0/0x288    show_stack+0x14/0x20    dump_stack+0x13c/0x1ac    print_circular_bug.isra.10+0x270/0x438    check_prev_add.constprop.16+0x4dc/0xb98    __lock_acquire+0x203c/0x21d0    lock_acquire+0xb8/0x148    cpus_read_lock+0x58/0x1c8    static_key_enable+0x14/0x30    sched_feat_write+0x314/0x428    full_proxy_write+0xa0/0x138    __vfs_write+0xd8/0x388    vfs_write+0xdc/0x318    ksys_write+0xb4/0x138    sys_write+0xc/0x18    __sys_trace_return+0x0/0x4  This is because when loading the cpufreq_dt module we first acquire cpu_hotplug_lock.rw_sem lock, then in cpufreq_init(), we are taking the &sb->s_type->i_mutex_key lock.  But when writing to /sys/kernel/debug/sched_features, the cpu_hotplug_lock.rw_sem lock depends on the &sb->s_type->i_mutex_key lock.  To fix this bug, reverse the lock acquisition order when writing to sched_features, this way cpu_hotplug_lock.rw_sem no longer depends on &sb->s_type->i_mutex_key.  Cc: Eugeniu Rosca <erosca@de.adit-jv.com> Cc: George G. Davis <george_davis@mentor.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Thomas Gleixner <tglx@linutronix.de> Link: http://lkml.kernel.org/r/20180731121222.26195-1-jiada_wang@mentor.com",
        "func_before": "static void sched_feat_enable(int i)\n{\n\tstatic_key_enable(&sched_feat_keys[i]);\n}",
        "func": "static void sched_feat_enable(int i)\n{\n\tstatic_key_enable_cpuslocked(&sched_feat_keys[i]);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n static void sched_feat_enable(int i)\n {\n-\tstatic_key_enable(&sched_feat_keys[i]);\n+\tstatic_key_enable_cpuslocked(&sched_feat_keys[i]);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstatic_key_enable(&sched_feat_keys[i]);"
            ],
            "added_lines": [
                "\tstatic_key_enable_cpuslocked(&sched_feat_keys[i]);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6136",
        "func_name": "linux-audit/audit-kernel/sched_feat_write",
        "description": "Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/linux-audit/audit-kernel/commit/e73e81975f2447e6f556100cada64a18ec631cbb",
        "commit_title": "sched/debug: Fix potential deadlock when writing to sched_features",
        "commit_text": " The following lockdep report can be triggered by writing to /sys/kernel/debug/sched_features:    ======================================================   WARNING: possible circular locking dependency detected   4.18.0-rc6-00152-gcd3f77d74ac3-dirty #18 Not tainted   ------------------------------------------------------   sh/3358 is trying to acquire lock:   000000004ad3989d (cpu_hotplug_lock.rw_sem){++++}, at: static_key_enable+0x14/0x30   but task is already holding lock:   00000000c1b31a88 (&sb->s_type->i_mutex_key#3){+.+.}, at: sched_feat_write+0x160/0x428   which lock already depends on the new lock.   the existing dependency chain (in reverse order) is:   -> #3 (&sb->s_type->i_mutex_key#3){+.+.}:          lock_acquire+0xb8/0x148          down_write+0xac/0x140          start_creating+0x5c/0x168          debugfs_create_dir+0x18/0x220          opp_debug_register+0x8c/0x120          _add_opp_dev+0x104/0x1f8          dev_pm_opp_get_opp_table+0x174/0x340          _of_add_opp_table_v2+0x110/0x760          dev_pm_opp_of_add_table+0x5c/0x240          dev_pm_opp_of_cpumask_add_table+0x5c/0x100          cpufreq_init+0x160/0x430          cpufreq_online+0x1cc/0xe30          cpufreq_add_dev+0x78/0x198          subsys_interface_register+0x168/0x270          cpufreq_register_driver+0x1c8/0x278          dt_cpufreq_probe+0xdc/0x1b8          platform_drv_probe+0xb4/0x168          driver_probe_device+0x318/0x4b0          __device_attach_driver+0xfc/0x1f0          bus_for_each_drv+0xf8/0x180          __device_attach+0x164/0x200          device_initial_probe+0x10/0x18          bus_probe_device+0x110/0x178          device_add+0x6d8/0x908          platform_device_add+0x138/0x3d8          platform_device_register_full+0x1cc/0x1f8          cpufreq_dt_platdev_init+0x174/0x1bc          do_one_initcall+0xb8/0x310          kernel_init_freeable+0x4b8/0x56c          kernel_init+0x10/0x138          ret_from_fork+0x10/0x18   -> #2 (opp_table_lock){+.+.}:          lock_acquire+0xb8/0x148          __mutex_lock+0x104/0xf50          mutex_lock_nested+0x1c/0x28          _of_add_opp_table_v2+0xb4/0x760          dev_pm_opp_of_add_table+0x5c/0x240          dev_pm_opp_of_cpumask_add_table+0x5c/0x100          cpufreq_init+0x160/0x430          cpufreq_online+0x1cc/0xe30          cpufreq_add_dev+0x78/0x198          subsys_interface_register+0x168/0x270          cpufreq_register_driver+0x1c8/0x278          dt_cpufreq_probe+0xdc/0x1b8          platform_drv_probe+0xb4/0x168          driver_probe_device+0x318/0x4b0          __device_attach_driver+0xfc/0x1f0          bus_for_each_drv+0xf8/0x180          __device_attach+0x164/0x200          device_initial_probe+0x10/0x18          bus_probe_device+0x110/0x178          device_add+0x6d8/0x908          platform_device_add+0x138/0x3d8          platform_device_register_full+0x1cc/0x1f8          cpufreq_dt_platdev_init+0x174/0x1bc          do_one_initcall+0xb8/0x310          kernel_init_freeable+0x4b8/0x56c          kernel_init+0x10/0x138          ret_from_fork+0x10/0x18   -> #1 (subsys mutex#6){+.+.}:          lock_acquire+0xb8/0x148          __mutex_lock+0x104/0xf50          mutex_lock_nested+0x1c/0x28          subsys_interface_register+0xd8/0x270          cpufreq_register_driver+0x1c8/0x278          dt_cpufreq_probe+0xdc/0x1b8          platform_drv_probe+0xb4/0x168          driver_probe_device+0x318/0x4b0          __device_attach_driver+0xfc/0x1f0          bus_for_each_drv+0xf8/0x180          __device_attach+0x164/0x200          device_initial_probe+0x10/0x18          bus_probe_device+0x110/0x178          device_add+0x6d8/0x908          platform_device_add+0x138/0x3d8          platform_device_register_full+0x1cc/0x1f8          cpufreq_dt_platdev_init+0x174/0x1bc          do_one_initcall+0xb8/0x310          kernel_init_freeable+0x4b8/0x56c          kernel_init+0x10/0x138          ret_from_fork+0x10/0x18   -> #0 (cpu_hotplug_lock.rw_sem){++++}:          __lock_acquire+0x203c/0x21d0          lock_acquire+0xb8/0x148          cpus_read_lock+0x58/0x1c8          static_key_enable+0x14/0x30          sched_feat_write+0x314/0x428          full_proxy_write+0xa0/0x138          __vfs_write+0xd8/0x388          vfs_write+0xdc/0x318          ksys_write+0xb4/0x138          sys_write+0xc/0x18          __sys_trace_return+0x0/0x4   other info that might help us debug this:   Chain exists of:     cpu_hotplug_lock.rw_sem --> opp_table_lock --> &sb->s_type->i_mutex_key#3    Possible unsafe locking scenario:          CPU0                    CPU1          ----                    ----     lock(&sb->s_type->i_mutex_key#3);                                  lock(opp_table_lock);                                  lock(&sb->s_type->i_mutex_key#3);     lock(cpu_hotplug_lock.rw_sem);    *** DEADLOCK ***   2 locks held by sh/3358:    #0: 00000000a8c4b363 (sb_writers#10){.+.+}, at: vfs_write+0x238/0x318    #1: 00000000c1b31a88 (&sb->s_type->i_mutex_key#3){+.+.}, at: sched_feat_write+0x160/0x428   stack backtrace:   CPU: 5 PID: 3358 Comm: sh Not tainted 4.18.0-rc6-00152-gcd3f77d74ac3-dirty #18   Hardware name: Renesas H3ULCB Kingfisher board based on r8a7795 ES2.0+ (DT)   Call trace:    dump_backtrace+0x0/0x288    show_stack+0x14/0x20    dump_stack+0x13c/0x1ac    print_circular_bug.isra.10+0x270/0x438    check_prev_add.constprop.16+0x4dc/0xb98    __lock_acquire+0x203c/0x21d0    lock_acquire+0xb8/0x148    cpus_read_lock+0x58/0x1c8    static_key_enable+0x14/0x30    sched_feat_write+0x314/0x428    full_proxy_write+0xa0/0x138    __vfs_write+0xd8/0x388    vfs_write+0xdc/0x318    ksys_write+0xb4/0x138    sys_write+0xc/0x18    __sys_trace_return+0x0/0x4  This is because when loading the cpufreq_dt module we first acquire cpu_hotplug_lock.rw_sem lock, then in cpufreq_init(), we are taking the &sb->s_type->i_mutex_key lock.  But when writing to /sys/kernel/debug/sched_features, the cpu_hotplug_lock.rw_sem lock depends on the &sb->s_type->i_mutex_key lock.  To fix this bug, reverse the lock acquisition order when writing to sched_features, this way cpu_hotplug_lock.rw_sem no longer depends on &sb->s_type->i_mutex_key.  Cc: Eugeniu Rosca <erosca@de.adit-jv.com> Cc: George G. Davis <george_davis@mentor.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Thomas Gleixner <tglx@linutronix.de> Link: http://lkml.kernel.org/r/20180731121222.26195-1-jiada_wang@mentor.com",
        "func_before": "static ssize_t\nsched_feat_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tchar *cmp;\n\tint ret;\n\tstruct inode *inode;\n\n\tif (cnt > 63)\n\t\tcnt = 63;\n\n\tif (copy_from_user(&buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\tcmp = strstrip(buf);\n\n\t/* Ensure the static_key remains in a consistent state */\n\tinode = file_inode(filp);\n\tinode_lock(inode);\n\tret = sched_feat_set(cmp);\n\tinode_unlock(inode);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}",
        "func": "static ssize_t\nsched_feat_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tchar *cmp;\n\tint ret;\n\tstruct inode *inode;\n\n\tif (cnt > 63)\n\t\tcnt = 63;\n\n\tif (copy_from_user(&buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\tcmp = strstrip(buf);\n\n\t/* Ensure the static_key remains in a consistent state */\n\tinode = file_inode(filp);\n\tcpus_read_lock();\n\tinode_lock(inode);\n\tret = sched_feat_set(cmp);\n\tinode_unlock(inode);\n\tcpus_read_unlock();\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,9 +18,11 @@\n \n \t/* Ensure the static_key remains in a consistent state */\n \tinode = file_inode(filp);\n+\tcpus_read_lock();\n \tinode_lock(inode);\n \tret = sched_feat_set(cmp);\n \tinode_unlock(inode);\n+\tcpus_read_unlock();\n \tif (ret < 0)\n \t\treturn ret;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tcpus_read_lock();",
                "\tcpus_read_unlock();"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6136",
        "func_name": "linux-audit/audit-kernel/sched_feat_disable",
        "description": "Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/linux-audit/audit-kernel/commit/e73e81975f2447e6f556100cada64a18ec631cbb",
        "commit_title": "sched/debug: Fix potential deadlock when writing to sched_features",
        "commit_text": " The following lockdep report can be triggered by writing to /sys/kernel/debug/sched_features:    ======================================================   WARNING: possible circular locking dependency detected   4.18.0-rc6-00152-gcd3f77d74ac3-dirty #18 Not tainted   ------------------------------------------------------   sh/3358 is trying to acquire lock:   000000004ad3989d (cpu_hotplug_lock.rw_sem){++++}, at: static_key_enable+0x14/0x30   but task is already holding lock:   00000000c1b31a88 (&sb->s_type->i_mutex_key#3){+.+.}, at: sched_feat_write+0x160/0x428   which lock already depends on the new lock.   the existing dependency chain (in reverse order) is:   -> #3 (&sb->s_type->i_mutex_key#3){+.+.}:          lock_acquire+0xb8/0x148          down_write+0xac/0x140          start_creating+0x5c/0x168          debugfs_create_dir+0x18/0x220          opp_debug_register+0x8c/0x120          _add_opp_dev+0x104/0x1f8          dev_pm_opp_get_opp_table+0x174/0x340          _of_add_opp_table_v2+0x110/0x760          dev_pm_opp_of_add_table+0x5c/0x240          dev_pm_opp_of_cpumask_add_table+0x5c/0x100          cpufreq_init+0x160/0x430          cpufreq_online+0x1cc/0xe30          cpufreq_add_dev+0x78/0x198          subsys_interface_register+0x168/0x270          cpufreq_register_driver+0x1c8/0x278          dt_cpufreq_probe+0xdc/0x1b8          platform_drv_probe+0xb4/0x168          driver_probe_device+0x318/0x4b0          __device_attach_driver+0xfc/0x1f0          bus_for_each_drv+0xf8/0x180          __device_attach+0x164/0x200          device_initial_probe+0x10/0x18          bus_probe_device+0x110/0x178          device_add+0x6d8/0x908          platform_device_add+0x138/0x3d8          platform_device_register_full+0x1cc/0x1f8          cpufreq_dt_platdev_init+0x174/0x1bc          do_one_initcall+0xb8/0x310          kernel_init_freeable+0x4b8/0x56c          kernel_init+0x10/0x138          ret_from_fork+0x10/0x18   -> #2 (opp_table_lock){+.+.}:          lock_acquire+0xb8/0x148          __mutex_lock+0x104/0xf50          mutex_lock_nested+0x1c/0x28          _of_add_opp_table_v2+0xb4/0x760          dev_pm_opp_of_add_table+0x5c/0x240          dev_pm_opp_of_cpumask_add_table+0x5c/0x100          cpufreq_init+0x160/0x430          cpufreq_online+0x1cc/0xe30          cpufreq_add_dev+0x78/0x198          subsys_interface_register+0x168/0x270          cpufreq_register_driver+0x1c8/0x278          dt_cpufreq_probe+0xdc/0x1b8          platform_drv_probe+0xb4/0x168          driver_probe_device+0x318/0x4b0          __device_attach_driver+0xfc/0x1f0          bus_for_each_drv+0xf8/0x180          __device_attach+0x164/0x200          device_initial_probe+0x10/0x18          bus_probe_device+0x110/0x178          device_add+0x6d8/0x908          platform_device_add+0x138/0x3d8          platform_device_register_full+0x1cc/0x1f8          cpufreq_dt_platdev_init+0x174/0x1bc          do_one_initcall+0xb8/0x310          kernel_init_freeable+0x4b8/0x56c          kernel_init+0x10/0x138          ret_from_fork+0x10/0x18   -> #1 (subsys mutex#6){+.+.}:          lock_acquire+0xb8/0x148          __mutex_lock+0x104/0xf50          mutex_lock_nested+0x1c/0x28          subsys_interface_register+0xd8/0x270          cpufreq_register_driver+0x1c8/0x278          dt_cpufreq_probe+0xdc/0x1b8          platform_drv_probe+0xb4/0x168          driver_probe_device+0x318/0x4b0          __device_attach_driver+0xfc/0x1f0          bus_for_each_drv+0xf8/0x180          __device_attach+0x164/0x200          device_initial_probe+0x10/0x18          bus_probe_device+0x110/0x178          device_add+0x6d8/0x908          platform_device_add+0x138/0x3d8          platform_device_register_full+0x1cc/0x1f8          cpufreq_dt_platdev_init+0x174/0x1bc          do_one_initcall+0xb8/0x310          kernel_init_freeable+0x4b8/0x56c          kernel_init+0x10/0x138          ret_from_fork+0x10/0x18   -> #0 (cpu_hotplug_lock.rw_sem){++++}:          __lock_acquire+0x203c/0x21d0          lock_acquire+0xb8/0x148          cpus_read_lock+0x58/0x1c8          static_key_enable+0x14/0x30          sched_feat_write+0x314/0x428          full_proxy_write+0xa0/0x138          __vfs_write+0xd8/0x388          vfs_write+0xdc/0x318          ksys_write+0xb4/0x138          sys_write+0xc/0x18          __sys_trace_return+0x0/0x4   other info that might help us debug this:   Chain exists of:     cpu_hotplug_lock.rw_sem --> opp_table_lock --> &sb->s_type->i_mutex_key#3    Possible unsafe locking scenario:          CPU0                    CPU1          ----                    ----     lock(&sb->s_type->i_mutex_key#3);                                  lock(opp_table_lock);                                  lock(&sb->s_type->i_mutex_key#3);     lock(cpu_hotplug_lock.rw_sem);    *** DEADLOCK ***   2 locks held by sh/3358:    #0: 00000000a8c4b363 (sb_writers#10){.+.+}, at: vfs_write+0x238/0x318    #1: 00000000c1b31a88 (&sb->s_type->i_mutex_key#3){+.+.}, at: sched_feat_write+0x160/0x428   stack backtrace:   CPU: 5 PID: 3358 Comm: sh Not tainted 4.18.0-rc6-00152-gcd3f77d74ac3-dirty #18   Hardware name: Renesas H3ULCB Kingfisher board based on r8a7795 ES2.0+ (DT)   Call trace:    dump_backtrace+0x0/0x288    show_stack+0x14/0x20    dump_stack+0x13c/0x1ac    print_circular_bug.isra.10+0x270/0x438    check_prev_add.constprop.16+0x4dc/0xb98    __lock_acquire+0x203c/0x21d0    lock_acquire+0xb8/0x148    cpus_read_lock+0x58/0x1c8    static_key_enable+0x14/0x30    sched_feat_write+0x314/0x428    full_proxy_write+0xa0/0x138    __vfs_write+0xd8/0x388    vfs_write+0xdc/0x318    ksys_write+0xb4/0x138    sys_write+0xc/0x18    __sys_trace_return+0x0/0x4  This is because when loading the cpufreq_dt module we first acquire cpu_hotplug_lock.rw_sem lock, then in cpufreq_init(), we are taking the &sb->s_type->i_mutex_key lock.  But when writing to /sys/kernel/debug/sched_features, the cpu_hotplug_lock.rw_sem lock depends on the &sb->s_type->i_mutex_key lock.  To fix this bug, reverse the lock acquisition order when writing to sched_features, this way cpu_hotplug_lock.rw_sem no longer depends on &sb->s_type->i_mutex_key.  Cc: Eugeniu Rosca <erosca@de.adit-jv.com> Cc: George G. Davis <george_davis@mentor.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Thomas Gleixner <tglx@linutronix.de> Link: http://lkml.kernel.org/r/20180731121222.26195-1-jiada_wang@mentor.com",
        "func_before": "static void sched_feat_disable(int i)\n{\n\tstatic_key_disable(&sched_feat_keys[i]);\n}",
        "func": "static void sched_feat_disable(int i)\n{\n\tstatic_key_disable_cpuslocked(&sched_feat_keys[i]);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n static void sched_feat_disable(int i)\n {\n-\tstatic_key_disable(&sched_feat_keys[i]);\n+\tstatic_key_disable_cpuslocked(&sched_feat_keys[i]);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstatic_key_disable(&sched_feat_keys[i]);"
            ],
            "added_lines": [
                "\tstatic_key_disable_cpuslocked(&sched_feat_keys[i]);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6136",
        "func_name": "linux-audit/audit-kernel/dccp_v6_err",
        "description": "Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/linux-audit/audit-kernel/commit/45caeaa5ac0b4b11784ac6f932c0ad4c6b67cda0",
        "commit_title": "dccp/tcp: fix routing redirect race",
        "commit_text": " As Eric Dumazet pointed out this also needs to be fixed in IPv6. v2: Contains the IPv6 tcp/Ipv6 dccp patches as well.  We have seen a few incidents lately where a dst_enty has been freed with a dangling TCP socket reference (sk->sk_dst_cache) pointing to that dst_entry. If the conditions/timings are right a crash then ensues when the freed dst_entry is referenced later on. A Common crashing back trace is:   #8 [] page_fault at ffffffff8163e648     [exception RIP: __tcp_ack_snd_check+74] . .  #9 [] tcp_rcv_established at ffffffff81580b64 #10 [] tcp_v4_do_rcv at ffffffff8158b54a #11 [] tcp_v4_rcv at ffffffff8158cd02 #12 [] ip_local_deliver_finish at ffffffff815668f4 #13 [] ip_local_deliver at ffffffff81566bd9 #14 [] ip_rcv_finish at ffffffff8156656d #15 [] ip_rcv at ffffffff81566f06 #16 [] __netif_receive_skb_core at ffffffff8152b3a2 #17 [] __netif_receive_skb at ffffffff8152b608 #18 [] netif_receive_skb at ffffffff8152b690 #19 [] vmxnet3_rq_rx_complete at ffffffffa015eeaf [vmxnet3] #20 [] vmxnet3_poll_rx_only at ffffffffa015f32a [vmxnet3] #21 [] net_rx_action at ffffffff8152bac2 #22 [] __do_softirq at ffffffff81084b4f #23 [] call_softirq at ffffffff8164845c #24 [] do_softirq at ffffffff81016fc5 #25 [] irq_exit at ffffffff81084ee5 #26 [] do_IRQ at ffffffff81648ff8  Of course it may happen with other NIC drivers as well.  It's found the freed dst_entry here:   224 static bool tcp_in_quickack_mode(struct sock *sk)↩  225 {↩  226 ▹       const struct inet_connection_sock *icsk = inet_csk(sk);↩  227 ▹       const struct dst_entry *dst = __sk_dst_get(sk);↩  228 ↩  229 ▹       return (dst && dst_metric(dst, RTAX_QUICKACK)) ||↩  230 ▹       ▹       (icsk->icsk_ack.quick && !icsk->icsk_ack.pingpong);↩  231 }↩  But there are other backtraces attributed to the same freed dst_entry in netfilter code as well.  All the vmcores showed 2 significant clues:  - Remote hosts behind the default gateway had always been redirected to a different gateway. A rtable/dst_entry will be added for that host. Making more dst_entrys with lower reference counts. Making this more probable.  - All vmcores showed a postitive LockDroppedIcmps value, e.g:  LockDroppedIcmps                  267  A closer look at the tcp_v4_err() handler revealed that do_redirect() will run regardless of whether user space has the socket locked. This can result in a race condition where the same dst_entry cached in sk->sk_dst_entry can be decremented twice for the same socket via:  do_redirect()->__sk_dst_check()-> dst_release().  Which leads to the dst_entry being prematurely freed with another socket pointing to it via sk->sk_dst_cache and a subsequent crash.  To fix this skip do_redirect() if usespace has the socket locked. Instead let the redirect take place later when user space does not have the socket locked.  The dccp/IPv6 code is very similar in this respect, so fixing it there too.  As Eric Garver pointed out the following commit now invalidates routes. Which can set the dst->obsolete flag so that ipv4_dst_check() returns null and triggers the dst_release().  Cc: Eric Garver <egarver@redhat.com> Cc: Hannes Sowa <hsowa@redhat.com>",
        "func_before": "static void dccp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct dccp_hdr *dh;\n\tstruct dccp_sock *dp;\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tint err;\n\t__u64 seq;\n\tstruct net *net = dev_net(skb->dev);\n\n\t/* Only need dccph_dport & dccph_sport which are the first\n\t * 4 bytes in dccp header.\n\t * Our caller (icmpv6_notify()) already pulled 8 bytes for us.\n\t */\n\tBUILD_BUG_ON(offsetofend(struct dccp_hdr, dccph_sport) > 8);\n\tBUILD_BUG_ON(offsetofend(struct dccp_hdr, dccph_dport) > 8);\n\tdh = (struct dccp_hdr *)(skb->data + offset);\n\n\tsk = __inet6_lookup_established(net, &dccp_hashinfo,\n\t\t\t\t\t&hdr->daddr, dh->dccph_dport,\n\t\t\t\t\t&hdr->saddr, ntohs(dh->dccph_sport),\n\t\t\t\t\tinet6_iif(skb));\n\n\tif (!sk) {\n\t\t__ICMP6_INC_STATS(net, __in6_dev_get(skb->dev),\n\t\t\t\t  ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = dccp_hdr_seq(dh);\n\tif (sk->sk_state == DCCP_NEW_SYN_RECV)\n\t\treturn dccp_req_err(sk, seq);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == NDISC_REDIRECT) {\n\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\tif (dst)\n\t\t\tdst->ops->redirect(dst, sk, skb);\n\t\tgoto out;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tstruct dst_entry *dst = NULL;\n\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\tif ((1 << sk->sk_state) & (DCCPF_LISTEN | DCCPF_CLOSED))\n\t\t\tgoto out;\n\n\t\tdst = inet6_csk_update_pmtu(sk, ntohl(info));\n\t\tif (!dst)\n\t\t\tgoto out;\n\n\t\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst))\n\t\t\tdccp_sync_mss(sk, dst_mtu(dst));\n\t\tgoto out;\n\t}\n\n\ticmpv6_err_convert(type, code, &err);\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:  /* Cannot happen.\n\t\t\t       It can, it SYNs are crossed. --ANK */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\t__DCCP_INC_STATS(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\t\t\t/*\n\t\t\t * Wake people up to see the error\n\t\t\t * (see connect in sock.c)\n\t\t\t */\n\t\t\tsk->sk_error_report(sk);\n\t\t\tdccp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "func": "static void dccp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct dccp_hdr *dh;\n\tstruct dccp_sock *dp;\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tint err;\n\t__u64 seq;\n\tstruct net *net = dev_net(skb->dev);\n\n\t/* Only need dccph_dport & dccph_sport which are the first\n\t * 4 bytes in dccp header.\n\t * Our caller (icmpv6_notify()) already pulled 8 bytes for us.\n\t */\n\tBUILD_BUG_ON(offsetofend(struct dccp_hdr, dccph_sport) > 8);\n\tBUILD_BUG_ON(offsetofend(struct dccp_hdr, dccph_dport) > 8);\n\tdh = (struct dccp_hdr *)(skb->data + offset);\n\n\tsk = __inet6_lookup_established(net, &dccp_hashinfo,\n\t\t\t\t\t&hdr->daddr, dh->dccph_dport,\n\t\t\t\t\t&hdr->saddr, ntohs(dh->dccph_sport),\n\t\t\t\t\tinet6_iif(skb));\n\n\tif (!sk) {\n\t\t__ICMP6_INC_STATS(net, __in6_dev_get(skb->dev),\n\t\t\t\t  ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = dccp_hdr_seq(dh);\n\tif (sk->sk_state == DCCP_NEW_SYN_RECV)\n\t\treturn dccp_req_err(sk, seq);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == NDISC_REDIRECT) {\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\t\tif (dst)\n\t\t\t\tdst->ops->redirect(dst, sk, skb);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tstruct dst_entry *dst = NULL;\n\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\tif ((1 << sk->sk_state) & (DCCPF_LISTEN | DCCPF_CLOSED))\n\t\t\tgoto out;\n\n\t\tdst = inet6_csk_update_pmtu(sk, ntohl(info));\n\t\tif (!dst)\n\t\t\tgoto out;\n\n\t\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst))\n\t\t\tdccp_sync_mss(sk, dst_mtu(dst));\n\t\tgoto out;\n\t}\n\n\ticmpv6_err_convert(type, code, &err);\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:  /* Cannot happen.\n\t\t\t       It can, it SYNs are crossed. --ANK */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\t__DCCP_INC_STATS(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\t\t\t/*\n\t\t\t * Wake people up to see the error\n\t\t\t * (see connect in sock.c)\n\t\t\t */\n\t\t\tsk->sk_error_report(sk);\n\t\t\tdccp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -54,10 +54,12 @@\n \tnp = inet6_sk(sk);\n \n \tif (type == NDISC_REDIRECT) {\n-\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n+\t\tif (!sock_owned_by_user(sk)) {\n+\t\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n \n-\t\tif (dst)\n-\t\t\tdst->ops->redirect(dst, sk, skb);\n+\t\t\tif (dst)\n+\t\t\t\tdst->ops->redirect(dst, sk, skb);\n+\t\t}\n \t\tgoto out;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);",
                "\t\tif (dst)",
                "\t\t\tdst->ops->redirect(dst, sk, skb);"
            ],
            "added_lines": [
                "\t\tif (!sock_owned_by_user(sk)) {",
                "\t\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);",
                "\t\t\tif (dst)",
                "\t\t\t\tdst->ops->redirect(dst, sk, skb);",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6136",
        "func_name": "linux-audit/audit-kernel/dccp_v4_err",
        "description": "Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/linux-audit/audit-kernel/commit/45caeaa5ac0b4b11784ac6f932c0ad4c6b67cda0",
        "commit_title": "dccp/tcp: fix routing redirect race",
        "commit_text": " As Eric Dumazet pointed out this also needs to be fixed in IPv6. v2: Contains the IPv6 tcp/Ipv6 dccp patches as well.  We have seen a few incidents lately where a dst_enty has been freed with a dangling TCP socket reference (sk->sk_dst_cache) pointing to that dst_entry. If the conditions/timings are right a crash then ensues when the freed dst_entry is referenced later on. A Common crashing back trace is:   #8 [] page_fault at ffffffff8163e648     [exception RIP: __tcp_ack_snd_check+74] . .  #9 [] tcp_rcv_established at ffffffff81580b64 #10 [] tcp_v4_do_rcv at ffffffff8158b54a #11 [] tcp_v4_rcv at ffffffff8158cd02 #12 [] ip_local_deliver_finish at ffffffff815668f4 #13 [] ip_local_deliver at ffffffff81566bd9 #14 [] ip_rcv_finish at ffffffff8156656d #15 [] ip_rcv at ffffffff81566f06 #16 [] __netif_receive_skb_core at ffffffff8152b3a2 #17 [] __netif_receive_skb at ffffffff8152b608 #18 [] netif_receive_skb at ffffffff8152b690 #19 [] vmxnet3_rq_rx_complete at ffffffffa015eeaf [vmxnet3] #20 [] vmxnet3_poll_rx_only at ffffffffa015f32a [vmxnet3] #21 [] net_rx_action at ffffffff8152bac2 #22 [] __do_softirq at ffffffff81084b4f #23 [] call_softirq at ffffffff8164845c #24 [] do_softirq at ffffffff81016fc5 #25 [] irq_exit at ffffffff81084ee5 #26 [] do_IRQ at ffffffff81648ff8  Of course it may happen with other NIC drivers as well.  It's found the freed dst_entry here:   224 static bool tcp_in_quickack_mode(struct sock *sk)↩  225 {↩  226 ▹       const struct inet_connection_sock *icsk = inet_csk(sk);↩  227 ▹       const struct dst_entry *dst = __sk_dst_get(sk);↩  228 ↩  229 ▹       return (dst && dst_metric(dst, RTAX_QUICKACK)) ||↩  230 ▹       ▹       (icsk->icsk_ack.quick && !icsk->icsk_ack.pingpong);↩  231 }↩  But there are other backtraces attributed to the same freed dst_entry in netfilter code as well.  All the vmcores showed 2 significant clues:  - Remote hosts behind the default gateway had always been redirected to a different gateway. A rtable/dst_entry will be added for that host. Making more dst_entrys with lower reference counts. Making this more probable.  - All vmcores showed a postitive LockDroppedIcmps value, e.g:  LockDroppedIcmps                  267  A closer look at the tcp_v4_err() handler revealed that do_redirect() will run regardless of whether user space has the socket locked. This can result in a race condition where the same dst_entry cached in sk->sk_dst_entry can be decremented twice for the same socket via:  do_redirect()->__sk_dst_check()-> dst_release().  Which leads to the dst_entry being prematurely freed with another socket pointing to it via sk->sk_dst_cache and a subsequent crash.  To fix this skip do_redirect() if usespace has the socket locked. Instead let the redirect take place later when user space does not have the socket locked.  The dccp/IPv6 code is very similar in this respect, so fixing it there too.  As Eric Garver pointed out the following commit now invalidates routes. Which can set the dst->obsolete flag so that ipv4_dst_check() returns null and triggers the dst_release().  Cc: Eric Garver <egarver@redhat.com> Cc: Hannes Sowa <hsowa@redhat.com>",
        "func_before": "static void dccp_v4_err(struct sk_buff *skb, u32 info)\n{\n\tconst struct iphdr *iph = (struct iphdr *)skb->data;\n\tconst u8 offset = iph->ihl << 2;\n\tconst struct dccp_hdr *dh;\n\tstruct dccp_sock *dp;\n\tstruct inet_sock *inet;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct sock *sk;\n\t__u64 seq;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\t/* Only need dccph_dport & dccph_sport which are the first\n\t * 4 bytes in dccp header.\n\t * Our caller (icmp_socket_deliver()) already pulled 8 bytes for us.\n\t */\n\tBUILD_BUG_ON(offsetofend(struct dccp_hdr, dccph_sport) > 8);\n\tBUILD_BUG_ON(offsetofend(struct dccp_hdr, dccph_dport) > 8);\n\tdh = (struct dccp_hdr *)(skb->data + offset);\n\n\tsk = __inet_lookup_established(net, &dccp_hashinfo,\n\t\t\t\t       iph->daddr, dh->dccph_dport,\n\t\t\t\t       iph->saddr, ntohs(dh->dccph_sport),\n\t\t\t\t       inet_iif(skb));\n\tif (!sk) {\n\t\t__ICMP_INC_STATS(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = dccp_hdr_seq(dh);\n\tif (sk->sk_state == DCCP_NEW_SYN_RECV)\n\t\treturn dccp_req_err(sk, seq);\n\n\tbh_lock_sock(sk);\n\t/* If too many ICMPs get dropped on busy\n\t * servers this needs to be solved differently.\n\t */\n\tif (sock_owned_by_user(sk))\n\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase ICMP_REDIRECT:\n\t\tdccp_do_redirect(skb, sk);\n\t\tgoto out;\n\tcase ICMP_SOURCE_QUENCH:\n\t\t/* Just silently ignore these. */\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\n\t\tif (code == ICMP_FRAG_NEEDED) { /* PMTU discovery (RFC1191) */\n\t\t\tif (!sock_owned_by_user(sk))\n\t\t\t\tdccp_do_pmtu_discovery(sk, iph, info);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = icmp_err_convert[code].errno;\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (sk->sk_state) {\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\t__DCCP_INC_STATS(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\n\t\t\tsk->sk_error_report(sk);\n\n\t\t\tdccp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\t/* If we've already connected we will keep trying\n\t * until we time out, or the user gives up.\n\t *\n\t * rfc1122 4.2.3.9 allows to consider as hard errors\n\t * only PROTO_UNREACH and PORT_UNREACH (well, FRAG_FAILED too,\n\t * but it is obsoleted by pmtu discovery).\n\t *\n\t * Note, that in modern internet, where routing is unreliable\n\t * and in each dark corner broken firewalls sit, sending random\n\t * errors ordered by their masters even this two messages finally lose\n\t * their original sense (even Linux sends invalid PORT_UNREACHs)\n\t *\n\t * Now we are in compliance with RFCs.\n\t *\t\t\t\t\t\t\t--ANK (980905)\n\t */\n\n\tinet = inet_sk(sk);\n\tif (!sock_owned_by_user(sk) && inet->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else /* Only an error on timeout */\n\t\tsk->sk_err_soft = err;\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "func": "static void dccp_v4_err(struct sk_buff *skb, u32 info)\n{\n\tconst struct iphdr *iph = (struct iphdr *)skb->data;\n\tconst u8 offset = iph->ihl << 2;\n\tconst struct dccp_hdr *dh;\n\tstruct dccp_sock *dp;\n\tstruct inet_sock *inet;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct sock *sk;\n\t__u64 seq;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\t/* Only need dccph_dport & dccph_sport which are the first\n\t * 4 bytes in dccp header.\n\t * Our caller (icmp_socket_deliver()) already pulled 8 bytes for us.\n\t */\n\tBUILD_BUG_ON(offsetofend(struct dccp_hdr, dccph_sport) > 8);\n\tBUILD_BUG_ON(offsetofend(struct dccp_hdr, dccph_dport) > 8);\n\tdh = (struct dccp_hdr *)(skb->data + offset);\n\n\tsk = __inet_lookup_established(net, &dccp_hashinfo,\n\t\t\t\t       iph->daddr, dh->dccph_dport,\n\t\t\t\t       iph->saddr, ntohs(dh->dccph_sport),\n\t\t\t\t       inet_iif(skb));\n\tif (!sk) {\n\t\t__ICMP_INC_STATS(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = dccp_hdr_seq(dh);\n\tif (sk->sk_state == DCCP_NEW_SYN_RECV)\n\t\treturn dccp_req_err(sk, seq);\n\n\tbh_lock_sock(sk);\n\t/* If too many ICMPs get dropped on busy\n\t * servers this needs to be solved differently.\n\t */\n\tif (sock_owned_by_user(sk))\n\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase ICMP_REDIRECT:\n\t\tif (!sock_owned_by_user(sk))\n\t\t\tdccp_do_redirect(skb, sk);\n\t\tgoto out;\n\tcase ICMP_SOURCE_QUENCH:\n\t\t/* Just silently ignore these. */\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\n\t\tif (code == ICMP_FRAG_NEEDED) { /* PMTU discovery (RFC1191) */\n\t\t\tif (!sock_owned_by_user(sk))\n\t\t\t\tdccp_do_pmtu_discovery(sk, iph, info);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = icmp_err_convert[code].errno;\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (sk->sk_state) {\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\t__DCCP_INC_STATS(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\n\t\t\tsk->sk_error_report(sk);\n\n\t\t\tdccp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\t/* If we've already connected we will keep trying\n\t * until we time out, or the user gives up.\n\t *\n\t * rfc1122 4.2.3.9 allows to consider as hard errors\n\t * only PROTO_UNREACH and PORT_UNREACH (well, FRAG_FAILED too,\n\t * but it is obsoleted by pmtu discovery).\n\t *\n\t * Note, that in modern internet, where routing is unreliable\n\t * and in each dark corner broken firewalls sit, sending random\n\t * errors ordered by their masters even this two messages finally lose\n\t * their original sense (even Linux sends invalid PORT_UNREACHs)\n\t *\n\t * Now we are in compliance with RFCs.\n\t *\t\t\t\t\t\t\t--ANK (980905)\n\t */\n\n\tinet = inet_sk(sk);\n\tif (!sock_owned_by_user(sk) && inet->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else /* Only an error on timeout */\n\t\tsk->sk_err_soft = err;\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,7 +56,8 @@\n \n \tswitch (type) {\n \tcase ICMP_REDIRECT:\n-\t\tdccp_do_redirect(skb, sk);\n+\t\tif (!sock_owned_by_user(sk))\n+\t\t\tdccp_do_redirect(skb, sk);\n \t\tgoto out;\n \tcase ICMP_SOURCE_QUENCH:\n \t\t/* Just silently ignore these. */",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tdccp_do_redirect(skb, sk);"
            ],
            "added_lines": [
                "\t\tif (!sock_owned_by_user(sk))",
                "\t\t\tdccp_do_redirect(skb, sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6136",
        "func_name": "linux-audit/audit-kernel/tcp_v4_err",
        "description": "Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/linux-audit/audit-kernel/commit/45caeaa5ac0b4b11784ac6f932c0ad4c6b67cda0",
        "commit_title": "dccp/tcp: fix routing redirect race",
        "commit_text": " As Eric Dumazet pointed out this also needs to be fixed in IPv6. v2: Contains the IPv6 tcp/Ipv6 dccp patches as well.  We have seen a few incidents lately where a dst_enty has been freed with a dangling TCP socket reference (sk->sk_dst_cache) pointing to that dst_entry. If the conditions/timings are right a crash then ensues when the freed dst_entry is referenced later on. A Common crashing back trace is:   #8 [] page_fault at ffffffff8163e648     [exception RIP: __tcp_ack_snd_check+74] . .  #9 [] tcp_rcv_established at ffffffff81580b64 #10 [] tcp_v4_do_rcv at ffffffff8158b54a #11 [] tcp_v4_rcv at ffffffff8158cd02 #12 [] ip_local_deliver_finish at ffffffff815668f4 #13 [] ip_local_deliver at ffffffff81566bd9 #14 [] ip_rcv_finish at ffffffff8156656d #15 [] ip_rcv at ffffffff81566f06 #16 [] __netif_receive_skb_core at ffffffff8152b3a2 #17 [] __netif_receive_skb at ffffffff8152b608 #18 [] netif_receive_skb at ffffffff8152b690 #19 [] vmxnet3_rq_rx_complete at ffffffffa015eeaf [vmxnet3] #20 [] vmxnet3_poll_rx_only at ffffffffa015f32a [vmxnet3] #21 [] net_rx_action at ffffffff8152bac2 #22 [] __do_softirq at ffffffff81084b4f #23 [] call_softirq at ffffffff8164845c #24 [] do_softirq at ffffffff81016fc5 #25 [] irq_exit at ffffffff81084ee5 #26 [] do_IRQ at ffffffff81648ff8  Of course it may happen with other NIC drivers as well.  It's found the freed dst_entry here:   224 static bool tcp_in_quickack_mode(struct sock *sk)↩  225 {↩  226 ▹       const struct inet_connection_sock *icsk = inet_csk(sk);↩  227 ▹       const struct dst_entry *dst = __sk_dst_get(sk);↩  228 ↩  229 ▹       return (dst && dst_metric(dst, RTAX_QUICKACK)) ||↩  230 ▹       ▹       (icsk->icsk_ack.quick && !icsk->icsk_ack.pingpong);↩  231 }↩  But there are other backtraces attributed to the same freed dst_entry in netfilter code as well.  All the vmcores showed 2 significant clues:  - Remote hosts behind the default gateway had always been redirected to a different gateway. A rtable/dst_entry will be added for that host. Making more dst_entrys with lower reference counts. Making this more probable.  - All vmcores showed a postitive LockDroppedIcmps value, e.g:  LockDroppedIcmps                  267  A closer look at the tcp_v4_err() handler revealed that do_redirect() will run regardless of whether user space has the socket locked. This can result in a race condition where the same dst_entry cached in sk->sk_dst_entry can be decremented twice for the same socket via:  do_redirect()->__sk_dst_check()-> dst_release().  Which leads to the dst_entry being prematurely freed with another socket pointing to it via sk->sk_dst_cache and a subsequent crash.  To fix this skip do_redirect() if usespace has the socket locked. Instead let the redirect take place later when user space does not have the socket locked.  The dccp/IPv6 code is very similar in this respect, so fixing it there too.  As Eric Garver pointed out the following commit now invalidates routes. Which can set the dst->obsolete flag so that ipv4_dst_check() returns null and triggers the dst_release().  Cc: Eric Garver <egarver@redhat.com> Cc: Hannes Sowa <hsowa@redhat.com>",
        "func_before": "void tcp_v4_err(struct sk_buff *icmp_skb, u32 info)\n{\n\tconst struct iphdr *iph = (const struct iphdr *)icmp_skb->data;\n\tstruct tcphdr *th = (struct tcphdr *)(icmp_skb->data + (iph->ihl << 2));\n\tstruct inet_connection_sock *icsk;\n\tstruct tcp_sock *tp;\n\tstruct inet_sock *inet;\n\tconst int type = icmp_hdr(icmp_skb)->type;\n\tconst int code = icmp_hdr(icmp_skb)->code;\n\tstruct sock *sk;\n\tstruct sk_buff *skb;\n\tstruct request_sock *fastopen;\n\t__u32 seq, snd_una;\n\t__u32 remaining;\n\tint err;\n\tstruct net *net = dev_net(icmp_skb->dev);\n\n\tsk = __inet_lookup_established(net, &tcp_hashinfo, iph->daddr,\n\t\t\t\t       th->dest, iph->saddr, ntohs(th->source),\n\t\t\t\t       inet_iif(icmp_skb));\n\tif (!sk) {\n\t\t__ICMP_INC_STATS(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = ntohl(th->seq);\n\tif (sk->sk_state == TCP_NEW_SYN_RECV)\n\t\treturn tcp_req_err(sk, seq,\n\t\t\t\t  type == ICMP_PARAMETERPROB ||\n\t\t\t\t  type == ICMP_TIME_EXCEEDED ||\n\t\t\t\t  (type == ICMP_DEST_UNREACH &&\n\t\t\t\t   (code == ICMP_NET_UNREACH ||\n\t\t\t\t    code == ICMP_HOST_UNREACH)));\n\n\tbh_lock_sock(sk);\n\t/* If too many ICMPs get dropped on busy\n\t * servers this needs to be solved differently.\n\t * We do take care of PMTU discovery (RFC1191) special case :\n\t * we can receive locally generated ICMP messages while socket is held.\n\t */\n\tif (sock_owned_by_user(sk)) {\n\t\tif (!(type == ICMP_DEST_UNREACH && code == ICMP_FRAG_NEEDED))\n\t\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\t}\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (unlikely(iph->ttl < inet_sk(sk)->min_ttl)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto out;\n\t}\n\n\ticsk = inet_csk(sk);\n\ttp = tcp_sk(sk);\n\t/* XXX (TFO) - tp->snd_una should be ISN (tcp_create_openreq_child() */\n\tfastopen = tp->fastopen_rsk;\n\tsnd_una = fastopen ? tcp_rsk(fastopen)->snt_isn : tp->snd_una;\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, snd_una, tp->snd_nxt)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase ICMP_REDIRECT:\n\t\tdo_redirect(icmp_skb, sk);\n\t\tgoto out;\n\tcase ICMP_SOURCE_QUENCH:\n\t\t/* Just silently ignore these. */\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\n\t\tif (code == ICMP_FRAG_NEEDED) { /* PMTU discovery (RFC1191) */\n\t\t\t/* We are not interested in TCP_LISTEN and open_requests\n\t\t\t * (SYN-ACKs send out by Linux are always <576bytes so\n\t\t\t * they should go through unfragmented).\n\t\t\t */\n\t\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\t\tgoto out;\n\n\t\t\ttp->mtu_info = info;\n\t\t\tif (!sock_owned_by_user(sk)) {\n\t\t\t\ttcp_v4_mtu_reduced(sk);\n\t\t\t} else {\n\t\t\t\tif (!test_and_set_bit(TCP_MTU_REDUCED_DEFERRED, &sk->sk_tsq_flags))\n\t\t\t\t\tsock_hold(sk);\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = icmp_err_convert[code].errno;\n\t\t/* check if icmp_skb allows revert of backoff\n\t\t * (see draft-zimmermann-tcp-lcd) */\n\t\tif (code != ICMP_NET_UNREACH && code != ICMP_HOST_UNREACH)\n\t\t\tbreak;\n\t\tif (seq != tp->snd_una  || !icsk->icsk_retransmits ||\n\t\t    !icsk->icsk_backoff || fastopen)\n\t\t\tbreak;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tbreak;\n\n\t\ticsk->icsk_backoff--;\n\t\ticsk->icsk_rto = tp->srtt_us ? __tcp_set_rto(tp) :\n\t\t\t\t\t       TCP_TIMEOUT_INIT;\n\t\ticsk->icsk_rto = inet_csk_rto_backoff(icsk, TCP_RTO_MAX);\n\n\t\tskb = tcp_write_queue_head(sk);\n\t\tBUG_ON(!skb);\n\n\t\tremaining = icsk->icsk_rto -\n\t\t\t    min(icsk->icsk_rto,\n\t\t\t\ttcp_time_stamp - tcp_skb_timestamp(skb));\n\n\t\tif (remaining) {\n\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t\t  remaining, TCP_RTO_MAX);\n\t\t} else {\n\t\t\t/* RTO revert clocked out retransmission.\n\t\t\t * Will retransmit now */\n\t\t\ttcp_retransmit_timer(sk);\n\t\t}\n\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:\n\t\t/* Only in fast or simultaneous open. If a fast open socket is\n\t\t * is already accepted it is treated as a connected one below.\n\t\t */\n\t\tif (fastopen && !fastopen->sk)\n\t\t\tbreak;\n\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tsk->sk_err = err;\n\n\t\t\tsk->sk_error_report(sk);\n\n\t\t\ttcp_done(sk);\n\t\t} else {\n\t\t\tsk->sk_err_soft = err;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* If we've already connected we will keep trying\n\t * until we time out, or the user gives up.\n\t *\n\t * rfc1122 4.2.3.9 allows to consider as hard errors\n\t * only PROTO_UNREACH and PORT_UNREACH (well, FRAG_FAILED too,\n\t * but it is obsoleted by pmtu discovery).\n\t *\n\t * Note, that in modern internet, where routing is unreliable\n\t * and in each dark corner broken firewalls sit, sending random\n\t * errors ordered by their masters even this two messages finally lose\n\t * their original sense (even Linux sends invalid PORT_UNREACHs)\n\t *\n\t * Now we are in compliance with RFCs.\n\t *\t\t\t\t\t\t\t--ANK (980905)\n\t */\n\n\tinet = inet_sk(sk);\n\tif (!sock_owned_by_user(sk) && inet->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\t{ /* Only an error on timeout */\n\t\tsk->sk_err_soft = err;\n\t}\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "func": "void tcp_v4_err(struct sk_buff *icmp_skb, u32 info)\n{\n\tconst struct iphdr *iph = (const struct iphdr *)icmp_skb->data;\n\tstruct tcphdr *th = (struct tcphdr *)(icmp_skb->data + (iph->ihl << 2));\n\tstruct inet_connection_sock *icsk;\n\tstruct tcp_sock *tp;\n\tstruct inet_sock *inet;\n\tconst int type = icmp_hdr(icmp_skb)->type;\n\tconst int code = icmp_hdr(icmp_skb)->code;\n\tstruct sock *sk;\n\tstruct sk_buff *skb;\n\tstruct request_sock *fastopen;\n\t__u32 seq, snd_una;\n\t__u32 remaining;\n\tint err;\n\tstruct net *net = dev_net(icmp_skb->dev);\n\n\tsk = __inet_lookup_established(net, &tcp_hashinfo, iph->daddr,\n\t\t\t\t       th->dest, iph->saddr, ntohs(th->source),\n\t\t\t\t       inet_iif(icmp_skb));\n\tif (!sk) {\n\t\t__ICMP_INC_STATS(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = ntohl(th->seq);\n\tif (sk->sk_state == TCP_NEW_SYN_RECV)\n\t\treturn tcp_req_err(sk, seq,\n\t\t\t\t  type == ICMP_PARAMETERPROB ||\n\t\t\t\t  type == ICMP_TIME_EXCEEDED ||\n\t\t\t\t  (type == ICMP_DEST_UNREACH &&\n\t\t\t\t   (code == ICMP_NET_UNREACH ||\n\t\t\t\t    code == ICMP_HOST_UNREACH)));\n\n\tbh_lock_sock(sk);\n\t/* If too many ICMPs get dropped on busy\n\t * servers this needs to be solved differently.\n\t * We do take care of PMTU discovery (RFC1191) special case :\n\t * we can receive locally generated ICMP messages while socket is held.\n\t */\n\tif (sock_owned_by_user(sk)) {\n\t\tif (!(type == ICMP_DEST_UNREACH && code == ICMP_FRAG_NEEDED))\n\t\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\t}\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (unlikely(iph->ttl < inet_sk(sk)->min_ttl)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto out;\n\t}\n\n\ticsk = inet_csk(sk);\n\ttp = tcp_sk(sk);\n\t/* XXX (TFO) - tp->snd_una should be ISN (tcp_create_openreq_child() */\n\tfastopen = tp->fastopen_rsk;\n\tsnd_una = fastopen ? tcp_rsk(fastopen)->snt_isn : tp->snd_una;\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, snd_una, tp->snd_nxt)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase ICMP_REDIRECT:\n\t\tif (!sock_owned_by_user(sk))\n\t\t\tdo_redirect(icmp_skb, sk);\n\t\tgoto out;\n\tcase ICMP_SOURCE_QUENCH:\n\t\t/* Just silently ignore these. */\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\n\t\tif (code == ICMP_FRAG_NEEDED) { /* PMTU discovery (RFC1191) */\n\t\t\t/* We are not interested in TCP_LISTEN and open_requests\n\t\t\t * (SYN-ACKs send out by Linux are always <576bytes so\n\t\t\t * they should go through unfragmented).\n\t\t\t */\n\t\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\t\tgoto out;\n\n\t\t\ttp->mtu_info = info;\n\t\t\tif (!sock_owned_by_user(sk)) {\n\t\t\t\ttcp_v4_mtu_reduced(sk);\n\t\t\t} else {\n\t\t\t\tif (!test_and_set_bit(TCP_MTU_REDUCED_DEFERRED, &sk->sk_tsq_flags))\n\t\t\t\t\tsock_hold(sk);\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = icmp_err_convert[code].errno;\n\t\t/* check if icmp_skb allows revert of backoff\n\t\t * (see draft-zimmermann-tcp-lcd) */\n\t\tif (code != ICMP_NET_UNREACH && code != ICMP_HOST_UNREACH)\n\t\t\tbreak;\n\t\tif (seq != tp->snd_una  || !icsk->icsk_retransmits ||\n\t\t    !icsk->icsk_backoff || fastopen)\n\t\t\tbreak;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tbreak;\n\n\t\ticsk->icsk_backoff--;\n\t\ticsk->icsk_rto = tp->srtt_us ? __tcp_set_rto(tp) :\n\t\t\t\t\t       TCP_TIMEOUT_INIT;\n\t\ticsk->icsk_rto = inet_csk_rto_backoff(icsk, TCP_RTO_MAX);\n\n\t\tskb = tcp_write_queue_head(sk);\n\t\tBUG_ON(!skb);\n\n\t\tremaining = icsk->icsk_rto -\n\t\t\t    min(icsk->icsk_rto,\n\t\t\t\ttcp_time_stamp - tcp_skb_timestamp(skb));\n\n\t\tif (remaining) {\n\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t\t  remaining, TCP_RTO_MAX);\n\t\t} else {\n\t\t\t/* RTO revert clocked out retransmission.\n\t\t\t * Will retransmit now */\n\t\t\ttcp_retransmit_timer(sk);\n\t\t}\n\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:\n\t\t/* Only in fast or simultaneous open. If a fast open socket is\n\t\t * is already accepted it is treated as a connected one below.\n\t\t */\n\t\tif (fastopen && !fastopen->sk)\n\t\t\tbreak;\n\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tsk->sk_err = err;\n\n\t\t\tsk->sk_error_report(sk);\n\n\t\t\ttcp_done(sk);\n\t\t} else {\n\t\t\tsk->sk_err_soft = err;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* If we've already connected we will keep trying\n\t * until we time out, or the user gives up.\n\t *\n\t * rfc1122 4.2.3.9 allows to consider as hard errors\n\t * only PROTO_UNREACH and PORT_UNREACH (well, FRAG_FAILED too,\n\t * but it is obsoleted by pmtu discovery).\n\t *\n\t * Note, that in modern internet, where routing is unreliable\n\t * and in each dark corner broken firewalls sit, sending random\n\t * errors ordered by their masters even this two messages finally lose\n\t * their original sense (even Linux sends invalid PORT_UNREACHs)\n\t *\n\t * Now we are in compliance with RFCs.\n\t *\t\t\t\t\t\t\t--ANK (980905)\n\t */\n\n\tinet = inet_sk(sk);\n\tif (!sock_owned_by_user(sk) && inet->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\t{ /* Only an error on timeout */\n\t\tsk->sk_err_soft = err;\n\t}\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -66,7 +66,8 @@\n \n \tswitch (type) {\n \tcase ICMP_REDIRECT:\n-\t\tdo_redirect(icmp_skb, sk);\n+\t\tif (!sock_owned_by_user(sk))\n+\t\t\tdo_redirect(icmp_skb, sk);\n \t\tgoto out;\n \tcase ICMP_SOURCE_QUENCH:\n \t\t/* Just silently ignore these. */",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tdo_redirect(icmp_skb, sk);"
            ],
            "added_lines": [
                "\t\tif (!sock_owned_by_user(sk))",
                "\t\t\tdo_redirect(icmp_skb, sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6136",
        "func_name": "linux-audit/audit-kernel/tcp_v6_err",
        "description": "Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/linux-audit/audit-kernel/commit/45caeaa5ac0b4b11784ac6f932c0ad4c6b67cda0",
        "commit_title": "dccp/tcp: fix routing redirect race",
        "commit_text": " As Eric Dumazet pointed out this also needs to be fixed in IPv6. v2: Contains the IPv6 tcp/Ipv6 dccp patches as well.  We have seen a few incidents lately where a dst_enty has been freed with a dangling TCP socket reference (sk->sk_dst_cache) pointing to that dst_entry. If the conditions/timings are right a crash then ensues when the freed dst_entry is referenced later on. A Common crashing back trace is:   #8 [] page_fault at ffffffff8163e648     [exception RIP: __tcp_ack_snd_check+74] . .  #9 [] tcp_rcv_established at ffffffff81580b64 #10 [] tcp_v4_do_rcv at ffffffff8158b54a #11 [] tcp_v4_rcv at ffffffff8158cd02 #12 [] ip_local_deliver_finish at ffffffff815668f4 #13 [] ip_local_deliver at ffffffff81566bd9 #14 [] ip_rcv_finish at ffffffff8156656d #15 [] ip_rcv at ffffffff81566f06 #16 [] __netif_receive_skb_core at ffffffff8152b3a2 #17 [] __netif_receive_skb at ffffffff8152b608 #18 [] netif_receive_skb at ffffffff8152b690 #19 [] vmxnet3_rq_rx_complete at ffffffffa015eeaf [vmxnet3] #20 [] vmxnet3_poll_rx_only at ffffffffa015f32a [vmxnet3] #21 [] net_rx_action at ffffffff8152bac2 #22 [] __do_softirq at ffffffff81084b4f #23 [] call_softirq at ffffffff8164845c #24 [] do_softirq at ffffffff81016fc5 #25 [] irq_exit at ffffffff81084ee5 #26 [] do_IRQ at ffffffff81648ff8  Of course it may happen with other NIC drivers as well.  It's found the freed dst_entry here:   224 static bool tcp_in_quickack_mode(struct sock *sk)↩  225 {↩  226 ▹       const struct inet_connection_sock *icsk = inet_csk(sk);↩  227 ▹       const struct dst_entry *dst = __sk_dst_get(sk);↩  228 ↩  229 ▹       return (dst && dst_metric(dst, RTAX_QUICKACK)) ||↩  230 ▹       ▹       (icsk->icsk_ack.quick && !icsk->icsk_ack.pingpong);↩  231 }↩  But there are other backtraces attributed to the same freed dst_entry in netfilter code as well.  All the vmcores showed 2 significant clues:  - Remote hosts behind the default gateway had always been redirected to a different gateway. A rtable/dst_entry will be added for that host. Making more dst_entrys with lower reference counts. Making this more probable.  - All vmcores showed a postitive LockDroppedIcmps value, e.g:  LockDroppedIcmps                  267  A closer look at the tcp_v4_err() handler revealed that do_redirect() will run regardless of whether user space has the socket locked. This can result in a race condition where the same dst_entry cached in sk->sk_dst_entry can be decremented twice for the same socket via:  do_redirect()->__sk_dst_check()-> dst_release().  Which leads to the dst_entry being prematurely freed with another socket pointing to it via sk->sk_dst_cache and a subsequent crash.  To fix this skip do_redirect() if usespace has the socket locked. Instead let the redirect take place later when user space does not have the socket locked.  The dccp/IPv6 code is very similar in this respect, so fixing it there too.  As Eric Garver pointed out the following commit now invalidates routes. Which can set the dst->obsolete flag so that ipv4_dst_check() returns null and triggers the dst_release().  Cc: Eric Garver <egarver@redhat.com> Cc: Hannes Sowa <hsowa@redhat.com>",
        "func_before": "static void tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct tcphdr *th = (struct tcphdr *)(skb->data+offset);\n\tstruct net *net = dev_net(skb->dev);\n\tstruct request_sock *fastopen;\n\tstruct ipv6_pinfo *np;\n\tstruct tcp_sock *tp;\n\t__u32 seq, snd_una;\n\tstruct sock *sk;\n\tbool fatal;\n\tint err;\n\n\tsk = __inet6_lookup_established(net, &tcp_hashinfo,\n\t\t\t\t\t&hdr->daddr, th->dest,\n\t\t\t\t\t&hdr->saddr, ntohs(th->source),\n\t\t\t\t\tskb->dev->ifindex);\n\n\tif (!sk) {\n\t\t__ICMP6_INC_STATS(net, __in6_dev_get(skb->dev),\n\t\t\t\t  ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = ntohl(th->seq);\n\tfatal = icmpv6_err_convert(type, code, &err);\n\tif (sk->sk_state == TCP_NEW_SYN_RECV)\n\t\treturn tcp_req_err(sk, seq, fatal);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk) && type != ICMPV6_PKT_TOOBIG)\n\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (ipv6_hdr(skb)->hop_limit < inet6_sk(sk)->min_hopcount) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto out;\n\t}\n\n\ttp = tcp_sk(sk);\n\t/* XXX (TFO) - tp->snd_una should be ISN (tcp_create_openreq_child() */\n\tfastopen = tp->fastopen_rsk;\n\tsnd_una = fastopen ? tcp_rsk(fastopen)->snt_isn : tp->snd_una;\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, snd_una, tp->snd_nxt)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == NDISC_REDIRECT) {\n\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\tif (dst)\n\t\t\tdst->ops->redirect(dst, sk, skb);\n\t\tgoto out;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\t/* We are not interested in TCP_LISTEN and open_requests\n\t\t * (SYN-ACKs send out by Linux are always <576bytes so\n\t\t * they should go through unfragmented).\n\t\t */\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\tgoto out;\n\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\n\t\ttp->mtu_info = ntohl(info);\n\t\tif (!sock_owned_by_user(sk))\n\t\t\ttcp_v6_mtu_reduced(sk);\n\t\telse if (!test_and_set_bit(TCP_MTU_REDUCED_DEFERRED,\n\t\t\t\t\t   &sk->sk_tsq_flags))\n\t\t\tsock_hold(sk);\n\t\tgoto out;\n\t}\n\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:\n\t\t/* Only in fast or simultaneous open. If a fast open socket is\n\t\t * is already accepted it is treated as a connected one below.\n\t\t */\n\t\tif (fastopen && !fastopen->sk)\n\t\t\tbreak;\n\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tsk->sk_err = err;\n\t\t\tsk->sk_error_report(sk);\t\t/* Wake people up to see the error (see connect in sock.c) */\n\n\t\t\ttcp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "func": "static void tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct tcphdr *th = (struct tcphdr *)(skb->data+offset);\n\tstruct net *net = dev_net(skb->dev);\n\tstruct request_sock *fastopen;\n\tstruct ipv6_pinfo *np;\n\tstruct tcp_sock *tp;\n\t__u32 seq, snd_una;\n\tstruct sock *sk;\n\tbool fatal;\n\tint err;\n\n\tsk = __inet6_lookup_established(net, &tcp_hashinfo,\n\t\t\t\t\t&hdr->daddr, th->dest,\n\t\t\t\t\t&hdr->saddr, ntohs(th->source),\n\t\t\t\t\tskb->dev->ifindex);\n\n\tif (!sk) {\n\t\t__ICMP6_INC_STATS(net, __in6_dev_get(skb->dev),\n\t\t\t\t  ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = ntohl(th->seq);\n\tfatal = icmpv6_err_convert(type, code, &err);\n\tif (sk->sk_state == TCP_NEW_SYN_RECV)\n\t\treturn tcp_req_err(sk, seq, fatal);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk) && type != ICMPV6_PKT_TOOBIG)\n\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (ipv6_hdr(skb)->hop_limit < inet6_sk(sk)->min_hopcount) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto out;\n\t}\n\n\ttp = tcp_sk(sk);\n\t/* XXX (TFO) - tp->snd_una should be ISN (tcp_create_openreq_child() */\n\tfastopen = tp->fastopen_rsk;\n\tsnd_una = fastopen ? tcp_rsk(fastopen)->snt_isn : tp->snd_una;\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, snd_una, tp->snd_nxt)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == NDISC_REDIRECT) {\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\t\tif (dst)\n\t\t\t\tdst->ops->redirect(dst, sk, skb);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\t/* We are not interested in TCP_LISTEN and open_requests\n\t\t * (SYN-ACKs send out by Linux are always <576bytes so\n\t\t * they should go through unfragmented).\n\t\t */\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\tgoto out;\n\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\n\t\ttp->mtu_info = ntohl(info);\n\t\tif (!sock_owned_by_user(sk))\n\t\t\ttcp_v6_mtu_reduced(sk);\n\t\telse if (!test_and_set_bit(TCP_MTU_REDUCED_DEFERRED,\n\t\t\t\t\t   &sk->sk_tsq_flags))\n\t\t\tsock_hold(sk);\n\t\tgoto out;\n\t}\n\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:\n\t\t/* Only in fast or simultaneous open. If a fast open socket is\n\t\t * is already accepted it is treated as a connected one below.\n\t\t */\n\t\tif (fastopen && !fastopen->sk)\n\t\t\tbreak;\n\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tsk->sk_err = err;\n\t\t\tsk->sk_error_report(sk);\t\t/* Wake people up to see the error (see connect in sock.c) */\n\n\t\t\ttcp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -57,10 +57,12 @@\n \tnp = inet6_sk(sk);\n \n \tif (type == NDISC_REDIRECT) {\n-\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n+\t\tif (!sock_owned_by_user(sk)) {\n+\t\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n \n-\t\tif (dst)\n-\t\t\tdst->ops->redirect(dst, sk, skb);\n+\t\t\tif (dst)\n+\t\t\t\tdst->ops->redirect(dst, sk, skb);\n+\t\t}\n \t\tgoto out;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);",
                "\t\tif (dst)",
                "\t\t\tdst->ops->redirect(dst, sk, skb);"
            ],
            "added_lines": [
                "\t\tif (!sock_owned_by_user(sk)) {",
                "\t\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);",
                "\t\t\tif (dst)",
                "\t\t\t\tdst->ops->redirect(dst, sk, skb);",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6156",
        "func_name": "torvalds/linux/ec_device_ioctl_xcmd",
        "description": "Race condition in the ec_device_ioctl_xcmd function in drivers/platform/chrome/cros_ec_dev.c in the Linux kernel before 4.7 allows local users to cause a denial of service (out-of-bounds array access) by changing a certain size value, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/torvalds/linux/commit/096cdc6f52225835ff503f987a0d68ef770bb78e",
        "commit_title": "platform/chrome: cros_ec_dev - double fetch bug in ioctl",
        "commit_text": " We verify \"u_cmd.outsize\" and \"u_cmd.insize\" but we need to make sure that those values have not changed between the two copy_from_user() calls.  Otherwise it could lead to a buffer overflow.  Additionally, cros_ec_cmd_xfer() can set s_cmd->insize to a lower value. We should use the new smaller value so we don't copy too much data to the user.  Cc: <stable@vger.kernel.org> # v4.2+",
        "func_before": "static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n{\n\tlong ret;\n\tstruct cros_ec_command u_cmd;\n\tstruct cros_ec_command *s_cmd;\n\n\tif (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))\n\t\treturn -EFAULT;\n\n\tif ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||\n\t    (u_cmd.insize > EC_MAX_MSG_BYTES))\n\t\treturn -EINVAL;\n\n\ts_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),\n\t\t\tGFP_KERNEL);\n\tif (!s_cmd)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\ts_cmd->command += ec->cmd_offset;\n\tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n\t/* Only copy data to userland if data was received. */\n\tif (ret < 0)\n\t\tgoto exit;\n\n\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))\n\t\tret = -EFAULT;\nexit:\n\tkfree(s_cmd);\n\treturn ret;\n}",
        "func": "static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n{\n\tlong ret;\n\tstruct cros_ec_command u_cmd;\n\tstruct cros_ec_command *s_cmd;\n\n\tif (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))\n\t\treturn -EFAULT;\n\n\tif ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||\n\t    (u_cmd.insize > EC_MAX_MSG_BYTES))\n\t\treturn -EINVAL;\n\n\ts_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),\n\t\t\tGFP_KERNEL);\n\tif (!s_cmd)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tif (u_cmd.outsize != s_cmd->outsize ||\n\t    u_cmd.insize != s_cmd->insize) {\n\t\tret = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\ts_cmd->command += ec->cmd_offset;\n\tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n\t/* Only copy data to userland if data was received. */\n\tif (ret < 0)\n\t\tgoto exit;\n\n\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n\t\tret = -EFAULT;\nexit:\n\tkfree(s_cmd);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,13 +21,19 @@\n \t\tgoto exit;\n \t}\n \n+\tif (u_cmd.outsize != s_cmd->outsize ||\n+\t    u_cmd.insize != s_cmd->insize) {\n+\t\tret = -EINVAL;\n+\t\tgoto exit;\n+\t}\n+\n \ts_cmd->command += ec->cmd_offset;\n \tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n \t/* Only copy data to userland if data was received. */\n \tif (ret < 0)\n \t\tgoto exit;\n \n-\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))\n+\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n \t\tret = -EFAULT;\n exit:\n \tkfree(s_cmd);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))"
            ],
            "added_lines": [
                "\tif (u_cmd.outsize != s_cmd->outsize ||",
                "\t    u_cmd.insize != s_cmd->insize) {",
                "\t\tret = -EINVAL;",
                "\t\tgoto exit;",
                "\t}",
                "",
                "\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6516",
        "func_name": "torvalds/linux/ioctl_file_dedupe_range",
        "description": "Race condition in the ioctl_file_dedupe_range function in fs/ioctl.c in the Linux kernel through 4.7 allows local users to cause a denial of service (heap-based buffer overflow) or possibly gain privileges by changing a certain count value, aka a \"double fetch\" vulnerability.",
        "git_url": "https://github.com/torvalds/linux/commit/10eec60ce79187686e052092e5383c99b4420a20",
        "commit_title": "vfs: ioctl: prevent double-fetch in dedupe ioctl",
        "commit_text": " This prevents a double-fetch from user space that can lead to to an undersized allocation and heap overflow. ",
        "func_before": "static long ioctl_file_dedupe_range(struct file *file, void __user *arg)\n{\n\tstruct file_dedupe_range __user *argp = arg;\n\tstruct file_dedupe_range *same = NULL;\n\tint ret;\n\tunsigned long size;\n\tu16 count;\n\n\tif (get_user(count, &argp->dest_count)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tsize = offsetof(struct file_dedupe_range __user, info[count]);\n\n\tsame = memdup_user(argp, size);\n\tif (IS_ERR(same)) {\n\t\tret = PTR_ERR(same);\n\t\tsame = NULL;\n\t\tgoto out;\n\t}\n\n\tret = vfs_dedupe_file_range(file, same);\n\tif (ret)\n\t\tgoto out;\n\n\tret = copy_to_user(argp, same, size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(same);\n\treturn ret;\n}",
        "func": "static long ioctl_file_dedupe_range(struct file *file, void __user *arg)\n{\n\tstruct file_dedupe_range __user *argp = arg;\n\tstruct file_dedupe_range *same = NULL;\n\tint ret;\n\tunsigned long size;\n\tu16 count;\n\n\tif (get_user(count, &argp->dest_count)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tsize = offsetof(struct file_dedupe_range __user, info[count]);\n\n\tsame = memdup_user(argp, size);\n\tif (IS_ERR(same)) {\n\t\tret = PTR_ERR(same);\n\t\tsame = NULL;\n\t\tgoto out;\n\t}\n\n\tsame->dest_count = count;\n\tret = vfs_dedupe_file_range(file, same);\n\tif (ret)\n\t\tgoto out;\n\n\tret = copy_to_user(argp, same, size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(same);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,6 +20,7 @@\n \t\tgoto out;\n \t}\n \n+\tsame->dest_count = count;\n \tret = vfs_dedupe_file_range(file, same);\n \tif (ret)\n \t\tgoto out;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tsame->dest_count = count;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-7777",
        "func_name": "xen-project/xen/hvmemul_get_fpu",
        "description": "Xen 4.7.x and earlier does not properly honor CR0.TS and CR0.EM, which allows local x86 HVM guest OS users to read or modify FPU, MMX, or XMM register state information belonging to arbitrary tasks on the guest by modifying an instruction while the hypervisor is preparing to emulate it.",
        "git_url": "https://github.com/xen-project/xen/commit/3ab81e01104d7c05e239f3bd9329c4fc87a463f4",
        "commit_title": "x86emul: honor guest CR0.TS and CR0.EM",
        "commit_text": " We must not emulate any instructions accessing respective registers when either of these flags is set in the guest view of the register, or else we may do so on data not belonging to the guest's current task.  Being architecturally required behavior, the logic gets placed in the instruction emulator instead of hvmemul_get_fpu(). It should be noted, though, that hvmemul_get_fpu() being the only current handler for the get_fpu() callback, we don't have an active problem with CR4: Both CR4.OSFXSR and CR4.OSXSAVE get handled as necessary by that function.  This is XSA-190. ",
        "func_before": "static int hvmemul_get_fpu(\n    void (*exception_callback)(void *, struct cpu_user_regs *),\n    void *exception_callback_arg,\n    enum x86_emulate_fpu_type type,\n    struct x86_emulate_ctxt *ctxt)\n{\n    struct vcpu *curr = current;\n\n    switch ( type )\n    {\n    case X86EMUL_FPU_fpu:\n        break;\n    case X86EMUL_FPU_mmx:\n        if ( !cpu_has_mmx )\n            return X86EMUL_UNHANDLEABLE;\n        break;\n    case X86EMUL_FPU_xmm:\n        if ( (curr->arch.hvm_vcpu.guest_cr[0] & X86_CR0_EM) ||\n             !(curr->arch.hvm_vcpu.guest_cr[4] & X86_CR4_OSFXSR) )\n            return X86EMUL_UNHANDLEABLE;\n        break;\n    case X86EMUL_FPU_ymm:\n        if ( !(curr->arch.hvm_vcpu.guest_cr[0] & X86_CR0_PE) ||\n             (ctxt->regs->eflags & X86_EFLAGS_VM) ||\n             !(curr->arch.hvm_vcpu.guest_cr[4] & X86_CR4_OSXSAVE) ||\n             !(curr->arch.xcr0 & XSTATE_SSE) ||\n             !(curr->arch.xcr0 & XSTATE_YMM) )\n            return X86EMUL_UNHANDLEABLE;\n        break;\n    default:\n        return X86EMUL_UNHANDLEABLE;\n    }\n\n    if ( !curr->fpu_dirtied )\n        hvm_funcs.fpu_dirty_intercept();\n\n    curr->arch.hvm_vcpu.fpu_exception_callback = exception_callback;\n    curr->arch.hvm_vcpu.fpu_exception_callback_arg = exception_callback_arg;\n\n    return X86EMUL_OKAY;\n}",
        "func": "static int hvmemul_get_fpu(\n    void (*exception_callback)(void *, struct cpu_user_regs *),\n    void *exception_callback_arg,\n    enum x86_emulate_fpu_type type,\n    struct x86_emulate_ctxt *ctxt)\n{\n    struct vcpu *curr = current;\n\n    switch ( type )\n    {\n    case X86EMUL_FPU_fpu:\n    case X86EMUL_FPU_wait:\n        break;\n    case X86EMUL_FPU_mmx:\n        if ( !cpu_has_mmx )\n            return X86EMUL_UNHANDLEABLE;\n        break;\n    case X86EMUL_FPU_xmm:\n        if ( !(curr->arch.hvm_vcpu.guest_cr[4] & X86_CR4_OSFXSR) )\n            return X86EMUL_UNHANDLEABLE;\n        break;\n    case X86EMUL_FPU_ymm:\n        if ( !(curr->arch.hvm_vcpu.guest_cr[0] & X86_CR0_PE) ||\n             (ctxt->regs->eflags & X86_EFLAGS_VM) ||\n             !(curr->arch.hvm_vcpu.guest_cr[4] & X86_CR4_OSXSAVE) ||\n             !(curr->arch.xcr0 & XSTATE_SSE) ||\n             !(curr->arch.xcr0 & XSTATE_YMM) )\n            return X86EMUL_UNHANDLEABLE;\n        break;\n    default:\n        return X86EMUL_UNHANDLEABLE;\n    }\n\n    if ( !curr->fpu_dirtied )\n        hvm_funcs.fpu_dirty_intercept();\n\n    curr->arch.hvm_vcpu.fpu_exception_callback = exception_callback;\n    curr->arch.hvm_vcpu.fpu_exception_callback_arg = exception_callback_arg;\n\n    return X86EMUL_OKAY;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,14 +9,14 @@\n     switch ( type )\n     {\n     case X86EMUL_FPU_fpu:\n+    case X86EMUL_FPU_wait:\n         break;\n     case X86EMUL_FPU_mmx:\n         if ( !cpu_has_mmx )\n             return X86EMUL_UNHANDLEABLE;\n         break;\n     case X86EMUL_FPU_xmm:\n-        if ( (curr->arch.hvm_vcpu.guest_cr[0] & X86_CR0_EM) ||\n-             !(curr->arch.hvm_vcpu.guest_cr[4] & X86_CR4_OSFXSR) )\n+        if ( !(curr->arch.hvm_vcpu.guest_cr[4] & X86_CR4_OSFXSR) )\n             return X86EMUL_UNHANDLEABLE;\n         break;\n     case X86EMUL_FPU_ymm:",
        "diff_line_info": {
            "deleted_lines": [
                "        if ( (curr->arch.hvm_vcpu.guest_cr[0] & X86_CR0_EM) ||",
                "             !(curr->arch.hvm_vcpu.guest_cr[4] & X86_CR4_OSFXSR) )"
            ],
            "added_lines": [
                "    case X86EMUL_FPU_wait:",
                "        if ( !(curr->arch.hvm_vcpu.guest_cr[4] & X86_CR4_OSFXSR) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-5195",
        "func_name": "torvalds/linux/follow_page_pte",
        "description": "Race condition in mm/gup.c in the Linux kernel 2.x through 4.x before 4.8.3 allows local users to gain privileges by leveraging incorrect handling of a copy-on-write (COW) feature to write to a read-only memory mapping, as exploited in the wild in October 2016, aka \"Dirty COW.\"",
        "git_url": "https://github.com/torvalds/linux/commit/19be0eaffa3ac7d8eb6784ad9bdbc7d67ed8e619",
        "commit_title": "mm: remove gup_flags FOLL_WRITE games from __get_user_pages()",
        "commit_text": " This is an ancient bug that was actually attempted to be fixed once (badly) by me eleven years ago in commit 4ceb5db9757a (\"Fix get_user_pages() race for write access\") but that was then undone due to problems on s390 by commit f33ea7f404e5 (\"fix get_user_pages bug\").  In the meantime, the s390 situation has long been fixed, and we can now fix it by checking the pte_dirty() bit properly (and do it better).  The s390 dirty bit was implemented in abf09bed3cce (\"s390/mm: implement software dirty bits\") which made it into v3.9.  Earlier kernels will have to look at the page state itself.  Also, the VM has become more scalable, and what used a purely theoretical race back then has become easier to trigger.  To fix it, we introduce a new internal FOLL_COW flag to mark the \"yes, we already did a COW\" rather than play racy games with FOLL_WRITE that is very fundamental, and then use the pte dirty flag to validate that the FOLL_COW flag is still valid.  Reported-and-tested-by: Phil \"not Paul\" Oester <kernel@linuxace.com> Cc: Andy Lutomirski <luto@kernel.org> Cc: Kees Cook <keescook@chromium.org> Cc: Oleg Nesterov <oleg@redhat.com> Cc: Willy Tarreau <w@1wt.eu> Cc: Nick Piggin <npiggin@gmail.com> Cc: Greg Thelen <gthelen@google.com> Cc: stable@vger.kernel.org",
        "func_before": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap = NULL;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\tif ((flags & FOLL_WRITE) && !pte_write(pte)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\treturn NULL;\n\t}\n\n\tpage = vm_normal_page(vma, address, pte);\n\tif (!page && pte_devmap(pte) && (flags & FOLL_GET)) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET case since\n\t\t * they are only valid while holding the pgmap reference.\n\t\t */\n\t\tpgmap = get_dev_pagemap(pte_pfn(pte), NULL);\n\t\tif (pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (flags & FOLL_SPLIT && PageTransCompound(page)) {\n\t\tint ret;\n\t\tget_page(page);\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tlock_page(page);\n\t\tret = split_huge_page(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tgoto retry;\n\t}\n\n\tif (flags & FOLL_GET) {\n\t\tget_page(page);\n\n\t\t/* drop the pgmap reference now that we hold the page */\n\t\tif (pgmap) {\n\t\t\tput_dev_pagemap(pgmap);\n\t\t\tpgmap = NULL;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/* Do not mlock pte-mapped THP */\n\t\tif (PageTransCompound(page))\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * The preliminary mapping check is mainly to avoid the\n\t\t * pointless overhead of lock_page on the ZERO_PAGE\n\t\t * which might bounce very badly if there is contention.\n\t\t *\n\t\t * If the page is already locked, we don't need to\n\t\t * handle it now - vmscan will handle it later if and\n\t\t * when it attempts to reclaim the page.\n\t\t */\n\t\tif (page->mapping && trylock_page(page)) {\n\t\t\tlru_add_drain();  /* push cached pages to LRU */\n\t\t\t/*\n\t\t\t * Because we lock page here, and migration is\n\t\t\t * blocked by the pte's page reference, and we\n\t\t\t * know the page is still mapped, we don't even\n\t\t\t * need to check for file-cache page truncation.\n\t\t\t */\n\t\t\tmlock_vma_page(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
        "func": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap = NULL;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\tif ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\treturn NULL;\n\t}\n\n\tpage = vm_normal_page(vma, address, pte);\n\tif (!page && pte_devmap(pte) && (flags & FOLL_GET)) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET case since\n\t\t * they are only valid while holding the pgmap reference.\n\t\t */\n\t\tpgmap = get_dev_pagemap(pte_pfn(pte), NULL);\n\t\tif (pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (flags & FOLL_SPLIT && PageTransCompound(page)) {\n\t\tint ret;\n\t\tget_page(page);\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tlock_page(page);\n\t\tret = split_huge_page(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tgoto retry;\n\t}\n\n\tif (flags & FOLL_GET) {\n\t\tget_page(page);\n\n\t\t/* drop the pgmap reference now that we hold the page */\n\t\tif (pgmap) {\n\t\t\tput_dev_pagemap(pgmap);\n\t\t\tpgmap = NULL;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/* Do not mlock pte-mapped THP */\n\t\tif (PageTransCompound(page))\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * The preliminary mapping check is mainly to avoid the\n\t\t * pointless overhead of lock_page on the ZERO_PAGE\n\t\t * which might bounce very badly if there is contention.\n\t\t *\n\t\t * If the page is already locked, we don't need to\n\t\t * handle it now - vmscan will handle it later if and\n\t\t * when it attempts to reclaim the page.\n\t\t */\n\t\tif (page->mapping && trylock_page(page)) {\n\t\t\tlru_add_drain();  /* push cached pages to LRU */\n\t\t\t/*\n\t\t\t * Because we lock page here, and migration is\n\t\t\t * blocked by the pte's page reference, and we\n\t\t\t * know the page is still mapped, we don't even\n\t\t\t * need to check for file-cache page truncation.\n\t\t\t */\n\t\t\tmlock_vma_page(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -33,7 +33,7 @@\n \t}\n \tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n \t\tgoto no_page;\n-\tif ((flags & FOLL_WRITE) && !pte_write(pte)) {\n+\tif ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {\n \t\tpte_unmap_unlock(ptep, ptl);\n \t\treturn NULL;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif ((flags & FOLL_WRITE) && !pte_write(pte)) {"
            ],
            "added_lines": [
                "\tif ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-5195",
        "func_name": "torvalds/linux/faultin_page",
        "description": "Race condition in mm/gup.c in the Linux kernel 2.x through 4.x before 4.8.3 allows local users to gain privileges by leveraging incorrect handling of a copy-on-write (COW) feature to write to a read-only memory mapping, as exploited in the wild in October 2016, aka \"Dirty COW.\"",
        "git_url": "https://github.com/torvalds/linux/commit/19be0eaffa3ac7d8eb6784ad9bdbc7d67ed8e619",
        "commit_title": "mm: remove gup_flags FOLL_WRITE games from __get_user_pages()",
        "commit_text": " This is an ancient bug that was actually attempted to be fixed once (badly) by me eleven years ago in commit 4ceb5db9757a (\"Fix get_user_pages() race for write access\") but that was then undone due to problems on s390 by commit f33ea7f404e5 (\"fix get_user_pages bug\").  In the meantime, the s390 situation has long been fixed, and we can now fix it by checking the pte_dirty() bit properly (and do it better).  The s390 dirty bit was implemented in abf09bed3cce (\"s390/mm: implement software dirty bits\") which made it into v3.9.  Earlier kernels will have to look at the page state itself.  Also, the VM has become more scalable, and what used a purely theoretical race back then has become easier to trigger.  To fix it, we introduce a new internal FOLL_COW flag to mark the \"yes, we already did a COW\" rather than play racy games with FOLL_WRITE that is very fundamental, and then use the pte dirty flag to validate that the FOLL_COW flag is still valid.  Reported-and-tested-by: Phil \"not Paul\" Oester <kernel@linuxace.com> Cc: Andy Lutomirski <luto@kernel.org> Cc: Kees Cook <keescook@chromium.org> Cc: Oleg Nesterov <oleg@redhat.com> Cc: Willy Tarreau <w@1wt.eu> Cc: Nick Piggin <npiggin@gmail.com> Cc: Greg Thelen <gthelen@google.com> Cc: stable@vger.kernel.org",
        "func_before": "static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, int *nonblocking)\n{\n\tunsigned int fault_flags = 0;\n\tint ret;\n\n\t/* mlock all present pages, but do not fault in new pages */\n\tif ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)\n\t\treturn -ENOENT;\n\t/* For mm_populate(), just skip the stack guard page. */\n\tif ((*flags & FOLL_POPULATE) &&\n\t\t\t(stack_guard_page_start(vma, address) ||\n\t\t\t stack_guard_page_end(vma, address + PAGE_SIZE)))\n\t\treturn -ENOENT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (nonblocking)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\tVM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tif (ret & VM_FAULT_ERROR) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\treturn -ENOMEM;\n\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\t\treturn *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;\n\t\tif (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\t\treturn -EFAULT;\n\t\tBUG();\n\t}\n\n\tif (tsk) {\n\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (nonblocking)\n\t\t\t*nonblocking = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t\t*flags &= ~FOLL_WRITE;\n\treturn 0;\n}",
        "func": "static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, int *nonblocking)\n{\n\tunsigned int fault_flags = 0;\n\tint ret;\n\n\t/* mlock all present pages, but do not fault in new pages */\n\tif ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)\n\t\treturn -ENOENT;\n\t/* For mm_populate(), just skip the stack guard page. */\n\tif ((*flags & FOLL_POPULATE) &&\n\t\t\t(stack_guard_page_start(vma, address) ||\n\t\t\t stack_guard_page_end(vma, address + PAGE_SIZE)))\n\t\treturn -ENOENT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (nonblocking)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\tVM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tif (ret & VM_FAULT_ERROR) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\treturn -ENOMEM;\n\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\t\treturn *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;\n\t\tif (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\t\treturn -EFAULT;\n\t\tBUG();\n\t}\n\n\tif (tsk) {\n\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (nonblocking)\n\t\t\t*nonblocking = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t        *flags |= FOLL_COW;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -59,6 +59,6 @@\n \t * reCOWed by userspace write).\n \t */\n \tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n-\t\t*flags &= ~FOLL_WRITE;\n+\t        *flags |= FOLL_COW;\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t*flags &= ~FOLL_WRITE;"
            ],
            "added_lines": [
                "\t        *flags |= FOLL_COW;"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-8963",
        "func_name": "torvalds/linux/swevent_hlist_get_cpu",
        "description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation.",
        "git_url": "https://github.com/torvalds/linux/commit/12ca6ad2e3a896256f086497a7c7406a547ee373",
        "commit_title": "perf: Fix race in swevent hash",
        "commit_text": " There's a race on CPU unplug where we free the swevent hash array while it can still have events on. This will result in a use-after-free which is BAD.  Simply do not free the hash array on unplug. This leaves the thing around and no use-after-free takes place.  When the last swevent dies, we do a for_each_possible_cpu() iteration anyway to clean these up, at which time we'll free it, so no leakage will occur.  Cc: Arnaldo Carvalho de Melo <acme@redhat.com> Cc: Frederic Weisbecker <fweisbec@gmail.com> Cc: Jiri Olsa <jolsa@redhat.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Stephane Eranian <eranian@google.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: Vince Weaver <vincent.weaver@maine.edu>",
        "func_before": "static int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}",
        "func": "static int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,6 @@\n \tint err = 0;\n \n \tmutex_lock(&swhash->hlist_mutex);\n-\n \tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n \t\tstruct swevent_hlist *hlist;\n ",
        "diff_line_info": {
            "deleted_lines": [
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2015-8963",
        "func_name": "torvalds/linux/perf_swevent_add",
        "description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation.",
        "git_url": "https://github.com/torvalds/linux/commit/12ca6ad2e3a896256f086497a7c7406a547ee373",
        "commit_title": "perf: Fix race in swevent hash",
        "commit_text": " There's a race on CPU unplug where we free the swevent hash array while it can still have events on. This will result in a use-after-free which is BAD.  Simply do not free the hash array on unplug. This leaves the thing around and no use-after-free takes place.  When the last swevent dies, we do a for_each_possible_cpu() iteration anyway to clean these up, at which time we'll free it, so no leakage will occur.  Cc: Arnaldo Carvalho de Melo <acme@redhat.com> Cc: Frederic Weisbecker <fweisbec@gmail.com> Cc: Jiri Olsa <jolsa@redhat.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Stephane Eranian <eranian@google.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: Vince Weaver <vincent.weaver@maine.edu>",
        "func_before": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (!head) {\n\t\t/*\n\t\t * We can race with cpu hotplug code. Do not\n\t\t * WARN if the cpu just got unplugged.\n\t\t */\n\t\tWARN_ON_ONCE(swhash->online);\n\t\treturn -EINVAL;\n\t}\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}",
        "func": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (WARN_ON_ONCE(!head))\n\t\treturn -EINVAL;\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,14 +12,8 @@\n \thwc->state = !(flags & PERF_EF_START);\n \n \thead = find_swevent_head(swhash, event);\n-\tif (!head) {\n-\t\t/*\n-\t\t * We can race with cpu hotplug code. Do not\n-\t\t * WARN if the cpu just got unplugged.\n-\t\t */\n-\t\tWARN_ON_ONCE(swhash->online);\n+\tif (WARN_ON_ONCE(!head))\n \t\treturn -EINVAL;\n-\t}\n \n \thlist_add_head_rcu(&event->hlist_entry, head);\n \tperf_event_update_userpage(event);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!head) {",
                "\t\t/*",
                "\t\t * We can race with cpu hotplug code. Do not",
                "\t\t * WARN if the cpu just got unplugged.",
                "\t\t */",
                "\t\tWARN_ON_ONCE(swhash->online);",
                "\t}"
            ],
            "added_lines": [
                "\tif (WARN_ON_ONCE(!head))"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-8963",
        "func_name": "torvalds/linux/perf_event_exit_cpu",
        "description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation.",
        "git_url": "https://github.com/torvalds/linux/commit/12ca6ad2e3a896256f086497a7c7406a547ee373",
        "commit_title": "perf: Fix race in swevent hash",
        "commit_text": " There's a race on CPU unplug where we free the swevent hash array while it can still have events on. This will result in a use-after-free which is BAD.  Simply do not free the hash array on unplug. This leaves the thing around and no use-after-free takes place.  When the last swevent dies, we do a for_each_possible_cpu() iteration anyway to clean these up, at which time we'll free it, so no leakage will occur.  Cc: Arnaldo Carvalho de Melo <acme@redhat.com> Cc: Frederic Weisbecker <fweisbec@gmail.com> Cc: Jiri Olsa <jolsa@redhat.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Stephane Eranian <eranian@google.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: Vince Weaver <vincent.weaver@maine.edu>",
        "func_before": "static void perf_event_exit_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tperf_event_exit_cpu_context(cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tswhash->online = false;\n\tswevent_hlist_release(swhash);\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
        "func": "static void perf_event_exit_cpu(int cpu)\n{\n\tperf_event_exit_cpu_context(cpu);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,4 @@\n static void perf_event_exit_cpu(int cpu)\n {\n-\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n-\n \tperf_event_exit_cpu_context(cpu);\n-\n-\tmutex_lock(&swhash->hlist_mutex);\n-\tswhash->online = false;\n-\tswevent_hlist_release(swhash);\n-\tmutex_unlock(&swhash->hlist_mutex);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);",
                "",
                "",
                "\tmutex_lock(&swhash->hlist_mutex);",
                "\tswhash->online = false;",
                "\tswevent_hlist_release(swhash);",
                "\tmutex_unlock(&swhash->hlist_mutex);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2015-8963",
        "func_name": "torvalds/linux/perf_event_init_cpu",
        "description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation.",
        "git_url": "https://github.com/torvalds/linux/commit/12ca6ad2e3a896256f086497a7c7406a547ee373",
        "commit_title": "perf: Fix race in swevent hash",
        "commit_text": " There's a race on CPU unplug where we free the swevent hash array while it can still have events on. This will result in a use-after-free which is BAD.  Simply do not free the hash array on unplug. This leaves the thing around and no use-after-free takes place.  When the last swevent dies, we do a for_each_possible_cpu() iteration anyway to clean these up, at which time we'll free it, so no leakage will occur.  Cc: Arnaldo Carvalho de Melo <acme@redhat.com> Cc: Frederic Weisbecker <fweisbec@gmail.com> Cc: Jiri Olsa <jolsa@redhat.com> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Stephane Eranian <eranian@google.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: Vince Weaver <vincent.weaver@maine.edu>",
        "func_before": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tswhash->online = true;\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
        "func": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,6 @@\n \tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n \n \tmutex_lock(&swhash->hlist_mutex);\n-\tswhash->online = true;\n \tif (swhash->hlist_refcount > 0) {\n \t\tstruct swevent_hlist *hlist;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tswhash->online = true;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2016-7911",
        "func_name": "torvalds/linux/get_task_ioprio",
        "description": "Race condition in the get_task_ioprio function in block/ioprio.c in the Linux kernel before 4.6.6 allows local users to gain privileges or cause a denial of service (use-after-free) via a crafted ioprio_get system call.",
        "git_url": "https://github.com/torvalds/linux/commit/8ba8682107ee2ca3347354e018865d8e1967c5f4",
        "commit_title": "block: fix use-after-free in sys_ioprio_get()",
        "commit_text": " get_task_ioprio() accesses the task->io_context without holding the task lock and thus can race with exit_io_context(), leading to a use-after-free. The reproducer below hits this within a few seconds on my 4-core QEMU VM:  #define _GNU_SOURCE #include <assert.h> #include <unistd.h> #include <sys/syscall.h> #include <sys/wait.h>  int main(int argc, char **argv) { \tpid_t pid, child; \tlong nproc, i;  \t/* ioprio_set(IOPRIO_WHO_PROCESS, 0, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0)); */ \tsyscall(SYS_ioprio_set, 1, 0, 0x6000);  \tnproc = sysconf(_SC_NPROCESSORS_ONLN);  \tfor (i = 0; i < nproc; i++) { \t\tpid = fork(); \t\tassert(pid != -1); \t\tif (pid == 0) { \t\t\tfor (;;) { \t\t\t\tpid = fork(); \t\t\t\tassert(pid != -1); \t\t\t\tif (pid == 0) { \t\t\t\t\t_exit(0); \t\t\t\t} else { \t\t\t\t\tchild = wait(NULL); \t\t\t\t\tassert(child == pid); \t\t\t\t} \t\t\t} \t\t}  \t\tpid = fork(); \t\tassert(pid != -1); \t\tif (pid == 0) { \t\t\tfor (;;) { \t\t\t\t/* ioprio_get(IOPRIO_WHO_PGRP, 0); */ \t\t\t\tsyscall(SYS_ioprio_get, 2, 0); \t\t\t} \t\t} \t}  \tfor (;;) { \t\t/* ioprio_get(IOPRIO_WHO_PGRP, 0); */ \t\tsyscall(SYS_ioprio_get, 2, 0); \t}  \treturn 0; }  This gets us KASAN dumps like this:  [   35.526914] ================================================================== [   35.530009] BUG: KASAN: out-of-bounds in get_task_ioprio+0x7b/0x90 at addr ffff880066f34e6c [   35.530009] Read of size 2 by task ioprio-gpf/363 [   35.530009] ============================================================================= [   35.530009] BUG blkdev_ioc (Not tainted): kasan: bad access detected [   35.530009] -----------------------------------------------------------------------------  [   35.530009] Disabling lock debugging due to kernel taint [   35.530009] INFO: Allocated in create_task_io_context+0x2b/0x370 age=0 cpu=0 pid=360 [   35.530009] \t___slab_alloc+0x55d/0x5a0 [   35.530009] \t__slab_alloc.isra.20+0x2b/0x40 [   35.530009] \tkmem_cache_alloc_node+0x84/0x200 [   35.530009] \tcreate_task_io_context+0x2b/0x370 [   35.530009] \tget_task_io_context+0x92/0xb0 [   35.530009] \tcopy_process.part.8+0x5029/0x5660 [   35.530009] \t_do_fork+0x155/0x7e0 [   35.530009] \tSyS_clone+0x19/0x20 [   35.530009] \tdo_syscall_64+0x195/0x3a0 [   35.530009] \treturn_from_SYSCALL_64+0x0/0x6a [   35.530009] INFO: Freed in put_io_context+0xe7/0x120 age=0 cpu=0 pid=1060 [   35.530009] \t__slab_free+0x27b/0x3d0 [   35.530009] \tkmem_cache_free+0x1fb/0x220 [   35.530009] \tput_io_context+0xe7/0x120 [   35.530009] \tput_io_context_active+0x238/0x380 [   35.530009] \texit_io_context+0x66/0x80 [   35.530009] \tdo_exit+0x158e/0x2b90 [   35.530009] \tdo_group_exit+0xe5/0x2b0 [   35.530009] \tSyS_exit_group+0x1d/0x20 [   35.530009] \tentry_SYSCALL_64_fastpath+0x1a/0xa4 [   35.530009] INFO: Slab 0xffffea00019bcd00 objects=20 used=4 fp=0xffff880066f34ff0 flags=0x1fffe0000004080 [   35.530009] INFO: Object 0xffff880066f34e58 @offset=3672 fp=0x0000000000000001 [   35.530009] ==================================================================  Fix it by grabbing the task lock while we poke at the io_context.  Cc: stable@vger.kernel.org",
        "func_before": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\nout:\n\treturn ret;\n}",
        "func": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\ttask_lock(p);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\n\ttask_unlock(p);\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,8 +6,10 @@\n \tif (ret)\n \t\tgoto out;\n \tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n+\ttask_lock(p);\n \tif (p->io_context)\n \t\tret = p->io_context->ioprio;\n+\ttask_unlock(p);\n out:\n \treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\ttask_lock(p);",
                "\ttask_unlock(p);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-7916",
        "func_name": "torvalds/linux/environ_read",
        "description": "Race condition in the environ_read function in fs/proc/base.c in the Linux kernel before 4.5.4 allows local users to obtain sensitive information from kernel memory by reading a /proc/*/environ file during a process-setup time interval in which environment-variable copying is incomplete.",
        "git_url": "https://github.com/torvalds/linux/commit/8148a73c9901a8794a50f950083c00ccf97d43b3",
        "commit_title": "proc: prevent accessing /proc/<PID>/environ until it's ready",
        "commit_text": " If /proc/<PID>/environ gets read before the envp[] array is fully set up in create_{aout,elf,elf_fdpic,flat}_tables(), we might end up trying to read more bytes than are actually written, as env_start will already be set but env_end will still be zero, making the range calculation underflow, allowing to read beyond the end of what has been written.  Fix this as it is done for /proc/<PID>/cmdline by testing env_end for zero.  It is, apparently, intentionally set last in create_*_tables().  This bug was found by the PaX size_overflow plugin that detected the arithmetic underflow of 'this_len = env_end - (env_start + src)' when env_end is still zero.  The expected consequence is that userland trying to access /proc/<PID>/environ of a not yet fully set up process may get inconsistent data as we're in the middle of copying in the environment variables.  Cc: Emese Revfy <re.emese@gmail.com> Cc: Pax Team <pageexec@freemail.hu> Cc: Al Viro <viro@zeniv.linux.org.uk> Cc: Mateusz Guzik <mguzik@redhat.com> Cc: Alexey Dobriyan <adobriyan@gmail.com> Cc: Cyrill Gorcunov <gorcunov@openvz.org> Cc: Jarod Wilson <jarod@redhat.com>",
        "func_before": "static ssize_t environ_read(struct file *file, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos)\n{\n\tchar *page;\n\tunsigned long src = *ppos;\n\tint ret = 0;\n\tstruct mm_struct *mm = file->private_data;\n\tunsigned long env_start, env_end;\n\n\tif (!mm)\n\t\treturn 0;\n\n\tpage = (char *)__get_free_page(GFP_TEMPORARY);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tret = 0;\n\tif (!atomic_inc_not_zero(&mm->mm_users))\n\t\tgoto free;\n\n\tdown_read(&mm->mmap_sem);\n\tenv_start = mm->env_start;\n\tenv_end = mm->env_end;\n\tup_read(&mm->mmap_sem);\n\n\twhile (count > 0) {\n\t\tsize_t this_len, max_len;\n\t\tint retval;\n\n\t\tif (src >= (env_end - env_start))\n\t\t\tbreak;\n\n\t\tthis_len = env_end - (env_start + src);\n\n\t\tmax_len = min_t(size_t, PAGE_SIZE, count);\n\t\tthis_len = min(max_len, this_len);\n\n\t\tretval = access_remote_vm(mm, (env_start + src),\n\t\t\tpage, this_len, 0);\n\n\t\tif (retval <= 0) {\n\t\t\tret = retval;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_to_user(buf, page, retval)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret += retval;\n\t\tsrc += retval;\n\t\tbuf += retval;\n\t\tcount -= retval;\n\t}\n\t*ppos = src;\n\tmmput(mm);\n\nfree:\n\tfree_page((unsigned long) page);\n\treturn ret;\n}",
        "func": "static ssize_t environ_read(struct file *file, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos)\n{\n\tchar *page;\n\tunsigned long src = *ppos;\n\tint ret = 0;\n\tstruct mm_struct *mm = file->private_data;\n\tunsigned long env_start, env_end;\n\n\t/* Ensure the process spawned far enough to have an environment. */\n\tif (!mm || !mm->env_end)\n\t\treturn 0;\n\n\tpage = (char *)__get_free_page(GFP_TEMPORARY);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tret = 0;\n\tif (!atomic_inc_not_zero(&mm->mm_users))\n\t\tgoto free;\n\n\tdown_read(&mm->mmap_sem);\n\tenv_start = mm->env_start;\n\tenv_end = mm->env_end;\n\tup_read(&mm->mmap_sem);\n\n\twhile (count > 0) {\n\t\tsize_t this_len, max_len;\n\t\tint retval;\n\n\t\tif (src >= (env_end - env_start))\n\t\t\tbreak;\n\n\t\tthis_len = env_end - (env_start + src);\n\n\t\tmax_len = min_t(size_t, PAGE_SIZE, count);\n\t\tthis_len = min(max_len, this_len);\n\n\t\tretval = access_remote_vm(mm, (env_start + src),\n\t\t\tpage, this_len, 0);\n\n\t\tif (retval <= 0) {\n\t\t\tret = retval;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_to_user(buf, page, retval)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret += retval;\n\t\tsrc += retval;\n\t\tbuf += retval;\n\t\tcount -= retval;\n\t}\n\t*ppos = src;\n\tmmput(mm);\n\nfree:\n\tfree_page((unsigned long) page);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,8 @@\n \tstruct mm_struct *mm = file->private_data;\n \tunsigned long env_start, env_end;\n \n-\tif (!mm)\n+\t/* Ensure the process spawned far enough to have an environment. */\n+\tif (!mm || !mm->env_end)\n \t\treturn 0;\n \n \tpage = (char *)__get_free_page(GFP_TEMPORARY);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!mm)"
            ],
            "added_lines": [
                "\t/* Ensure the process spawned far enough to have an environment. */",
                "\tif (!mm || !mm->env_end)"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-8655",
        "func_name": "torvalds/linux/packet_set_ring",
        "description": "Race condition in net/packet/af_packet.c in the Linux kernel through 4.8.12 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging the CAP_NET_RAW capability to change a socket version, related to the packet_set_ring and packet_setsockopt functions.",
        "git_url": "https://github.com/torvalds/linux/commit/84ac7260236a49c79eede91617700174c2c19b0c",
        "commit_title": "packet: fix race condition in packet_set_ring",
        "commit_text": " When packet_set_ring creates a ring buffer it will initialize a struct timer_list if the packet version is TPACKET_V3. This value can then be raced by a different thread calling setsockopt to set the version to TPACKET_V1 before packet_set_ring has finished.  This leads to a use-after-free on a function pointer in the struct timer_list when the socket is closed as the previously initialized timer will not be deleted.  The bug is fixed by taking lock_sock(sk) in packet_setsockopt when changing the packet version while also taking the lock at the start of packet_set_ring. ",
        "func_before": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n\tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n\t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n\t\tgoto out;\n\t}\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    (int)(req->tp_block_size -\n\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t/* Transmit path is not supported. We checked\n\t\t * it above but just being paranoid\n\t\t */\n\t\t\tif (!tx_ring)\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\trelease_sock(sk);\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\treturn err;\n}",
        "func": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\tlock_sock(sk);\n\t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n\tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n\t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n\t\tgoto out;\n\t}\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    (int)(req->tp_block_size -\n\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t/* Transmit path is not supported. We checked\n\t\t * it above but just being paranoid\n\t\t */\n\t\t\tif (!tx_ring)\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,6 +11,7 @@\n \t/* Added to avoid minimal code churn */\n \tstruct tpacket_req *req = &req_u->req;\n \n+\tlock_sock(sk);\n \t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n \tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n \t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n@@ -92,7 +93,6 @@\n \t\t\tgoto out;\n \t}\n \n-\tlock_sock(sk);\n \n \t/* Detach socket from network */\n \tspin_lock(&po->bind_lock);\n@@ -141,10 +141,10 @@\n \t\tif (!tx_ring)\n \t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n \t}\n-\trelease_sock(sk);\n \n \tif (pg_vec)\n \t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\n out:\n+\trelease_sock(sk);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tlock_sock(sk);",
                "\trelease_sock(sk);"
            ],
            "added_lines": [
                "\tlock_sock(sk);",
                "\trelease_sock(sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-8655",
        "func_name": "torvalds/linux/packet_setsockopt",
        "description": "Race condition in net/packet/af_packet.c in the Linux kernel through 4.8.12 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging the CAP_NET_RAW capability to change a socket version, related to the packet_set_ring and packet_setsockopt functions.",
        "git_url": "https://github.com/torvalds/linux/commit/84ac7260236a49c79eede91617700174c2c19b0c",
        "commit_title": "packet: fix race condition in packet_set_ring",
        "commit_text": " When packet_set_ring creates a ring buffer it will initialize a struct timer_list if the packet version is TPACKET_V3. This value can then be raced by a different thread calling setsockopt to set the version to TPACKET_V1 before packet_set_ring has finished.  This leads to a use-after-free on a function pointer in the struct timer_list when the socket is closed as the previously initialized timer will not be deleted.  The bug is fixed by taking lock_sock(sk) in packet_setsockopt when changing the packet version while also taking the lock at the start of packet_set_ring. ",
        "func_before": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_version = val;\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
        "func": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -71,19 +71,25 @@\n \n \t\tif (optlen != sizeof(val))\n \t\t\treturn -EINVAL;\n-\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n-\t\t\treturn -EBUSY;\n \t\tif (copy_from_user(&val, optval, sizeof(val)))\n \t\t\treturn -EFAULT;\n \t\tswitch (val) {\n \t\tcase TPACKET_V1:\n \t\tcase TPACKET_V2:\n \t\tcase TPACKET_V3:\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\treturn -EINVAL;\n+\t\t}\n+\t\tlock_sock(sk);\n+\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n+\t\t\tret = -EBUSY;\n+\t\t} else {\n \t\t\tpo->tp_version = val;\n-\t\t\treturn 0;\n-\t\tdefault:\n-\t\t\treturn -EINVAL;\n+\t\t\tret = 0;\n \t\t}\n+\t\trelease_sock(sk);\n+\t\treturn ret;\n \t}\n \tcase PACKET_RESERVE:\n \t{",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)",
                "\t\t\treturn -EBUSY;",
                "\t\t\treturn 0;",
                "\t\tdefault:",
                "\t\t\treturn -EINVAL;"
            ],
            "added_lines": [
                "\t\t\tbreak;",
                "\t\tdefault:",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "\t\tlock_sock(sk);",
                "\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {",
                "\t\t\tret = -EBUSY;",
                "\t\t} else {",
                "\t\t\tret = 0;",
                "\t\trelease_sock(sk);",
                "\t\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6663",
        "func_name": "mysql/mysql-server/my_redel",
        "description": "Race condition in Oracle MySQL before 5.5.52, 5.6.x before 5.6.33, 5.7.x before 5.7.15, and 8.x before 8.0.1; MariaDB before 5.5.52, 10.0.x before 10.0.28, and 10.1.x before 10.1.18; Percona Server before 5.5.51-38.2, 5.6.x before 5.6.32-78-1, and 5.7.x before 5.7.14-8; and Percona XtraDB Cluster before 5.5.41-37.0, 5.6.x before 5.6.32-25.17, and 5.7.x before 5.7.14-26.17 allows local users with certain permissions to gain privileges by leveraging use of my_copystat by REPAIR TABLE to repair a MyISAM table.",
        "git_url": "https://github.com/mysql/mysql-server/commit/4e5473862e6852b0f3802b0cd0c6fa10b5253291",
        "commit_title": "Bug#24388746: PRIVILEGE ESCALATION AND RACE CONDITION USING CREATE TABLE",
        "commit_text": " During REPAIR TABLE of a MyISAM table, a temporary data file (.TMD) is created. When repair finishes, this file is renamed to the original .MYD file. The problem was that during this rename, we copied the stats from the old file to the new file with chmod/chown. If a user managed to replace the temporary file before chmod/chown was executed, it was possible to get an arbitrary file with the privileges of the mysql user.  This patch fixes the problem by not copying stats from the old file to the new file. This is not needed as the new file was created with the correct stats. This fix only changes server behavior - external utilities such as myisamchk still does chmod/chown.  No test case provided since the problem involves synchronization with file system operations.",
        "func_before": "int my_redel(const char *org_name, const char *tmp_name, myf MyFlags)\n{\n  int error=1;\n  DBUG_ENTER(\"my_redel\");\n  DBUG_PRINT(\"my\",(\"org_name: '%s' tmp_name: '%s'  MyFlags: %d\",\n\t\t   org_name,tmp_name,MyFlags));\n\n  if (my_copystat(org_name,tmp_name,MyFlags) < 0)\n    goto end;\n  if (MyFlags & MY_REDEL_MAKE_BACKUP)\n  {\n    char name_buff[FN_REFLEN+20];    \n    char ext[20];\n    ext[0]='-';\n    get_date(ext+1,2+4,(time_t) 0);\n    strmov(strend(ext),REDEL_EXT);\n    if (my_rename(org_name, fn_format(name_buff, org_name, \"\", ext, 2),\n\t\t  MyFlags))\n      goto end;\n  }\n  else if (my_delete_allow_opened(org_name, MyFlags))\n      goto end;\n  if (my_rename(tmp_name,org_name,MyFlags))\n    goto end;\n\n  error=0;\nend:\n  DBUG_RETURN(error);\n}",
        "func": "int my_redel(const char *org_name, const char *tmp_name, myf MyFlags)\n{\n  int error=1;\n  DBUG_ENTER(\"my_redel\");\n  DBUG_PRINT(\"my\",(\"org_name: '%s' tmp_name: '%s'  MyFlags: %d\",\n\t\t   org_name,tmp_name,MyFlags));\n\n  if (!(MyFlags & MY_REDEL_NO_COPY_STAT))\n  {\n    if (my_copystat(org_name,tmp_name,MyFlags) < 0)\n      goto end;\n  }\n  if (MyFlags & MY_REDEL_MAKE_BACKUP)\n  {\n    char name_buff[FN_REFLEN+20];    \n    char ext[20];\n    ext[0]='-';\n    get_date(ext+1,2+4,(time_t) 0);\n    strmov(strend(ext),REDEL_EXT);\n    if (my_rename(org_name, fn_format(name_buff, org_name, \"\", ext, 2),\n\t\t  MyFlags))\n      goto end;\n  }\n  else if (my_delete_allow_opened(org_name, MyFlags))\n      goto end;\n  if (my_rename(tmp_name,org_name,MyFlags))\n    goto end;\n\n  error=0;\nend:\n  DBUG_RETURN(error);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,8 +5,11 @@\n   DBUG_PRINT(\"my\",(\"org_name: '%s' tmp_name: '%s'  MyFlags: %d\",\n \t\t   org_name,tmp_name,MyFlags));\n \n-  if (my_copystat(org_name,tmp_name,MyFlags) < 0)\n-    goto end;\n+  if (!(MyFlags & MY_REDEL_NO_COPY_STAT))\n+  {\n+    if (my_copystat(org_name,tmp_name,MyFlags) < 0)\n+      goto end;\n+  }\n   if (MyFlags & MY_REDEL_MAKE_BACKUP)\n   {\n     char name_buff[FN_REFLEN+20];    ",
        "diff_line_info": {
            "deleted_lines": [
                "  if (my_copystat(org_name,tmp_name,MyFlags) < 0)",
                "    goto end;"
            ],
            "added_lines": [
                "  if (!(MyFlags & MY_REDEL_NO_COPY_STAT))",
                "  {",
                "    if (my_copystat(org_name,tmp_name,MyFlags) < 0)",
                "      goto end;",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6663",
        "func_name": "mysql/mysql-server/myisamchk",
        "description": "Race condition in Oracle MySQL before 5.5.52, 5.6.x before 5.6.33, 5.7.x before 5.7.15, and 8.x before 8.0.1; MariaDB before 5.5.52, 10.0.x before 10.0.28, and 10.1.x before 10.1.18; Percona Server before 5.5.51-38.2, 5.6.x before 5.6.32-78-1, and 5.7.x before 5.7.14-8; and Percona XtraDB Cluster before 5.5.41-37.0, 5.6.x before 5.6.32-25.17, and 5.7.x before 5.7.14-26.17 allows local users with certain permissions to gain privileges by leveraging use of my_copystat by REPAIR TABLE to repair a MyISAM table.",
        "git_url": "https://github.com/mysql/mysql-server/commit/4e5473862e6852b0f3802b0cd0c6fa10b5253291",
        "commit_title": "Bug#24388746: PRIVILEGE ESCALATION AND RACE CONDITION USING CREATE TABLE",
        "commit_text": " During REPAIR TABLE of a MyISAM table, a temporary data file (.TMD) is created. When repair finishes, this file is renamed to the original .MYD file. The problem was that during this rename, we copied the stats from the old file to the new file with chmod/chown. If a user managed to replace the temporary file before chmod/chown was executed, it was possible to get an arbitrary file with the privileges of the mysql user.  This patch fixes the problem by not copying stats from the old file to the new file. This is not needed as the new file was created with the correct stats. This fix only changes server behavior - external utilities such as myisamchk still does chmod/chown.  No test case provided since the problem involves synchronization with file system operations.",
        "func_before": "static int myisamchk(MI_CHECK *param, char * filename)\n{\n  int error,lock_type,recreate;\n  int rep_quick= param->testflag & (T_QUICK | T_FORCE_UNIQUENESS);\n  MI_INFO *info;\n  File datafile;\n  char llbuff[22],llbuff2[22];\n  my_bool state_updated=0;\n  MYISAM_SHARE *share;\n  DBUG_ENTER(\"myisamchk\");\n\n  param->out_flag=error=param->warning_printed=param->error_printed=\n    recreate=0;\n  datafile=0;\n  param->isam_file_name=filename;\t\t/* For error messages */\n  if (!(info=mi_open(filename,\n\t\t     (param->testflag & (T_DESCRIPT | T_READONLY)) ?\n\t\t     O_RDONLY : O_RDWR,\n\t\t     HA_OPEN_FOR_REPAIR |\n\t\t     ((param->testflag & T_WAIT_FOREVER) ?\n\t\t      HA_OPEN_WAIT_IF_LOCKED :\n\t\t      (param->testflag & T_DESCRIPT) ?\n\t\t      HA_OPEN_IGNORE_IF_LOCKED : HA_OPEN_ABORT_IF_LOCKED))))\n  {\n    /* Avoid twice printing of isam file name */\n    param->error_printed=1;\n    switch (my_errno) {\n    case HA_ERR_CRASHED:\n      mi_check_print_error(param,\"'%s' doesn't have a correct index definition. You need to recreate it before you can do a repair\",filename);\n      break;\n    case HA_ERR_NOT_A_TABLE:\n      mi_check_print_error(param,\"'%s' is not a MyISAM-table\",filename);\n      break;\n    case HA_ERR_CRASHED_ON_USAGE:\n      mi_check_print_error(param,\"'%s' is marked as crashed\",filename);\n      break;\n    case HA_ERR_CRASHED_ON_REPAIR:\n      mi_check_print_error(param,\"'%s' is marked as crashed after last repair\",filename);\n      break;\n    case HA_ERR_OLD_FILE:\n      mi_check_print_error(param,\"'%s' is an old type of MyISAM-table\", filename);\n      break;\n    case HA_ERR_END_OF_FILE:\n      mi_check_print_error(param,\"Couldn't read complete header from '%s'\", filename);\n      break;\n    case EAGAIN:\n      mi_check_print_error(param,\"'%s' is locked. Use -w to wait until unlocked\",filename);\n      break;\n    case ENOENT:\n      mi_check_print_error(param,\"File '%s' doesn't exist\",filename);\n      break;\n    case EACCES:\n      mi_check_print_error(param,\"You don't have permission to use '%s'\",filename);\n      break;\n    default:\n      mi_check_print_error(param,\"%d when opening MyISAM-table '%s'\",\n\t\t  my_errno,filename);\n      break;\n    }\n    DBUG_RETURN(1);\n  }\n  share=info->s;\n  share->options&= ~HA_OPTION_READ_ONLY_DATA; /* We are modifing it */\n  share->tot_locks-= share->r_locks;\n  share->r_locks=0;\n\n  /*\n    Skip the checking of the file if:\n    We are using --fast and the table is closed properly\n    We are using --check-only-changed-tables and the table hasn't changed\n  */\n  if (param->testflag & (T_FAST | T_CHECK_ONLY_CHANGED))\n  {\n    my_bool need_to_check= mi_is_crashed(info) || share->state.open_count != 0;\n\n    if ((param->testflag & (T_REP_ANY | T_SORT_RECORDS)) &&\n\t((share->state.changed & (STATE_CHANGED | STATE_CRASHED |\n\t\t\t\t  STATE_CRASHED_ON_REPAIR) ||\n\t  !(param->testflag & T_CHECK_ONLY_CHANGED))))\n      need_to_check=1;\n\n    if (info->s->base.keys && info->state->records)\n    {\n      if ((param->testflag & T_STATISTICS) &&\n          (share->state.changed & STATE_NOT_ANALYZED))\n        need_to_check=1;\n      if ((param->testflag & T_SORT_INDEX) &&\n          (share->state.changed & STATE_NOT_SORTED_PAGES))\n        need_to_check=1;\n      if ((param->testflag & T_REP_BY_SORT) &&\n          (share->state.changed & STATE_NOT_OPTIMIZED_KEYS))\n        need_to_check=1;\n    }\n    if ((param->testflag & T_CHECK_ONLY_CHANGED) &&\n\t(share->state.changed & (STATE_CHANGED | STATE_CRASHED |\n\t\t\t\t STATE_CRASHED_ON_REPAIR)))\n      need_to_check=1;\n    if (!need_to_check)\n    {\n      if (!(param->testflag & T_SILENT) || param->testflag & T_INFO)\n\tprintf(\"MyISAM file: %s is already checked\\n\",filename);\n      if (mi_close(info))\n      {\n\tmi_check_print_error(param,\"%d when closing MyISAM-table '%s'\",\n\t\t\t     my_errno,filename);\n\tDBUG_RETURN(1);\n      }\n      DBUG_RETURN(0);\n    }\n  }\n  if ((param->testflag & (T_REP_ANY | T_STATISTICS |\n\t\t\t  T_SORT_RECORDS | T_SORT_INDEX)) &&\n      (((param->testflag & T_UNPACK) &&\n\tshare->data_file_type == COMPRESSED_RECORD) ||\n       mi_uint2korr(share->state.header.state_info_length) !=\n       MI_STATE_INFO_SIZE ||\n       mi_uint2korr(share->state.header.base_info_length) !=\n       MI_BASE_INFO_SIZE ||\n       mi_is_any_intersect_keys_active(param->keys_in_use, share->base.keys,\n                                       ~share->state.key_map) ||\n       test_if_almost_full(info) ||\n       info->s->state.header.file_version[3] != myisam_file_magic[3] ||\n       (set_collation &&\n        set_collation->number != share->state.header.language) ||\n       myisam_block_size != MI_KEY_BLOCK_LENGTH))\n  {\n    if (set_collation)\n      param->language= set_collation->number;\n    if (recreate_table(param, &info,filename))\n    {\n      (void) fprintf(stderr,\n\t\t   \"MyISAM-table '%s' is not fixed because of errors\\n\",\n\t      filename);\n      return(-1);\n    }\n    recreate=1;\n    if (!(param->testflag & T_REP_ANY))\n    {\n      param->testflag|=T_REP_BY_SORT;\t\t/* if only STATISTICS */\n      if (!(param->testflag & T_SILENT))\n\tprintf(\"- '%s' has old table-format. Recreating index\\n\",filename);\n      rep_quick|=T_QUICK;\n    }\n    share=info->s;\n    share->tot_locks-= share->r_locks;\n    share->r_locks=0;\n  }\n\n  if (param->testflag & T_DESCRIPT)\n  {\n    param->total_files++;\n    param->total_records+=info->state->records;\n    param->total_deleted+=info->state->del;\n    descript(param, info, filename);\n  }\n  else\n  {\n    if (!stopwords_inited++)\n      ft_init_stopwords();\n\n    if (!(param->testflag & T_READONLY))\n      lock_type = F_WRLCK;\t\t\t/* table is changed */\n    else\n      lock_type= F_RDLCK;\n    if (info->lock_type == F_RDLCK)\n      info->lock_type=F_UNLCK;\t\t\t/* Read only table */\n    if (_mi_readinfo(info,lock_type,0))\n    {\n      mi_check_print_error(param,\"Can't lock indexfile of '%s', error: %d\",\n\t\t  filename,my_errno);\n      param->error_printed=0;\n      goto end2;\n    }\n    /*\n      _mi_readinfo() has locked the table.\n      We mark the table as locked (without doing file locks) to be able to\n      use functions that only works on locked tables (like row caching).\n    */\n    mi_lock_database(info, F_EXTRA_LCK);\n    datafile=info->dfile;\n\n    if (param->testflag & (T_REP_ANY | T_SORT_RECORDS | T_SORT_INDEX))\n    {\n      if (param->testflag & T_REP_ANY)\n      {\n\tulonglong tmp=share->state.key_map;\n\tmi_copy_keys_active(share->state.key_map, share->base.keys,\n                            param->keys_in_use);\n\tif (tmp != share->state.key_map)\n\t  info->update|=HA_STATE_CHANGED;\n      }\n      if (rep_quick && chk_del(param, info, param->testflag & ~T_VERBOSE))\n      {\n\tif (param->testflag & T_FORCE_CREATE)\n\t{\n\t  rep_quick=0;\n\t  mi_check_print_info(param,\"Creating new data file\\n\");\n\t}\n\telse\n\t{\n\t  error=1;\n\t  mi_check_print_error(param,\n\t\t\t       \"Quick-recover aborted; Run recovery without switch 'q'\");\n\t}\n      }\n      if (!error)\n      {\n\tif ((param->testflag & (T_REP_BY_SORT | T_REP_PARALLEL)) &&\n\t    (mi_is_any_key_active(share->state.key_map) ||\n\t     (rep_quick && !param->keys_in_use && !recreate)) &&\n\t    mi_test_if_sort_rep(info, info->state->records,\n\t\t\t\tinfo->s->state.key_map,\n\t\t\t\tparam->force_sort))\n\t{\n          if (param->testflag & T_REP_BY_SORT)\n            error=mi_repair_by_sort(param,info,filename,rep_quick);\n          else\n            error=mi_repair_parallel(param,info,filename,rep_quick);\n\t  state_updated=1;\n\t}\n\telse if (param->testflag & T_REP_ANY)\n\t  error=mi_repair(param, info,filename,rep_quick);\n      }\n      if (!error && param->testflag & T_SORT_RECORDS)\n      {\n\t/*\n\t  The data file is nowadays reopened in the repair code so we should\n\t  soon remove the following reopen-code\n\t*/\n#ifndef TO_BE_REMOVED\n\tif (param->out_flag & O_NEW_DATA)\n\t{\t\t\t/* Change temp file to org file */\n\t  (void) my_close(info->dfile,MYF(MY_WME)); /* Close new file */\n\t  error|=change_to_newfile(filename, MI_NAME_DEXT, DATA_TMP_EXT, MYF(0));\n\t  if (mi_open_datafile(info,info->s, NULL, -1))\n\t    error=1;\n\t  param->out_flag&= ~O_NEW_DATA; /* We are using new datafile */\n\t  param->read_cache.file=info->dfile;\n\t}\n#endif\n\tif (! error)\n\t{\n\t  uint key;\n\t  /*\n\t    We can't update the index in mi_sort_records if we have a\n\t    prefix compressed or fulltext index\n\t  */\n\t  my_bool update_index=1;\n\t  for (key=0 ; key < share->base.keys; key++)\n\t    if (share->keyinfo[key].flag & (HA_BINARY_PACK_KEY|HA_FULLTEXT))\n\t      update_index=0;\n\n\t  error=mi_sort_records(param,info,filename,param->opt_sort_key,\n                             /* what is the following parameter for ? */\n\t\t\t\t(my_bool) !(param->testflag & T_REP),\n\t\t\t\tupdate_index);\n\t  datafile=info->dfile;\t/* This is now locked */\n\t  if (!error && !update_index)\n\t  {\n\t    if (param->verbose)\n\t      puts(\"Table had a compressed index;  We must now recreate the index\");\n\t    error=mi_repair_by_sort(param,info,filename,1);\n\t  }\n\t}\n      }\n      if (!error && param->testflag & T_SORT_INDEX)\n\terror=mi_sort_index(param,info,filename);\n      if (!error)\n\tshare->state.changed&= ~(STATE_CHANGED | STATE_CRASHED |\n\t\t\t\t STATE_CRASHED_ON_REPAIR);\n      else\n\tmi_mark_crashed(info);\n    }\n    else if ((param->testflag & T_CHECK) || !(param->testflag & T_AUTO_INC))\n    {\n      if (!(param->testflag & T_SILENT) || param->testflag & T_INFO)\n\tprintf(\"Checking MyISAM file: %s\\n\",filename);\n      if (!(param->testflag & T_SILENT))\n\tprintf(\"Data records: %7s   Deleted blocks: %7s\\n\",\n\t       llstr(info->state->records,llbuff),\n\t       llstr(info->state->del,llbuff2));\n      error =chk_status(param,info);\n      mi_intersect_keys_active(share->state.key_map, param->keys_in_use);\n      error =chk_size(param,info);\n      if (!error || !(param->testflag & (T_FAST | T_FORCE_CREATE)))\n\terror|=chk_del(param, info,param->testflag);\n      if ((!error || (!(param->testflag & (T_FAST | T_FORCE_CREATE)) &&\n\t\t      !param->start_check_pos)))\n      {\n\terror|=chk_key(param, info);\n\tif (!error && (param->testflag & (T_STATISTICS | T_AUTO_INC)))\n\t  error=update_state_info(param, info,\n\t\t\t\t  ((param->testflag & T_STATISTICS) ?\n\t\t\t\t   UPDATE_STAT : 0) |\n\t\t\t\t  ((param->testflag & T_AUTO_INC) ?\n\t\t\t\t   UPDATE_AUTO_INC : 0));\n      }\n      if ((!rep_quick && !error) ||\n\t  !(param->testflag & (T_FAST | T_FORCE_CREATE)))\n      {\n\tif (param->testflag & (T_EXTEND | T_MEDIUM))\n\t  (void) init_key_cache(dflt_key_cache,opt_key_cache_block_size,\n                              param->use_buffers, 0, 0);\n\t(void) init_io_cache(&param->read_cache,datafile,\n\t\t\t   (uint) param->read_buffer_length,\n\t\t\t   READ_CACHE,\n\t\t\t   (param->start_check_pos ?\n\t\t\t    param->start_check_pos :\n\t\t\t    share->pack.header_length),\n\t\t\t   1,\n\t\t\t   MYF(MY_WME));\n\tlock_memory(param);\n\tif ((info->s->options & (HA_OPTION_PACK_RECORD |\n\t\t\t\t HA_OPTION_COMPRESS_RECORD)) ||\n\t    (param->testflag & (T_EXTEND | T_MEDIUM)))\n\t  error|=chk_data_link(param, info, param->testflag & T_EXTEND);\n\terror|=flush_blocks(param, share->key_cache, share->kfile);\n\t(void) end_io_cache(&param->read_cache);\n      }\n      if (!error)\n      {\n\tif ((share->state.changed & STATE_CHANGED) &&\n\t    (param->testflag & T_UPDATE_STATE))\n\t  info->update|=HA_STATE_CHANGED | HA_STATE_ROW_CHANGED;\n\tshare->state.changed&= ~(STATE_CHANGED | STATE_CRASHED |\n\t\t\t\t STATE_CRASHED_ON_REPAIR);\n      }\n      else if (!mi_is_crashed(info) &&\n\t       (param->testflag & T_UPDATE_STATE))\n      {\t\t\t\t\t\t/* Mark crashed */\n\tmi_mark_crashed(info);\n\tinfo->update|=HA_STATE_CHANGED | HA_STATE_ROW_CHANGED;\n      }\n    }\n  }\n  if ((param->testflag & T_AUTO_INC) ||\n      ((param->testflag & T_REP_ANY) && info->s->base.auto_key))\n    update_auto_increment_key(param, info,\n\t\t\t      (my_bool) !test(param->testflag & T_AUTO_INC));\n\n  if (!(param->testflag & T_DESCRIPT))\n  {\n    if (info->update & HA_STATE_CHANGED && ! (param->testflag & T_READONLY))\n      error|=update_state_info(param, info,\n\t\t\t       UPDATE_OPEN_COUNT |\n\t\t\t       (((param->testflag & T_REP_ANY) ?\n\t\t\t\t UPDATE_TIME : 0) |\n\t\t\t\t(state_updated ? UPDATE_STAT : 0) |\n\t\t\t\t((param->testflag & T_SORT_RECORDS) ?\n\t\t\t\t UPDATE_SORT : 0)));\n    (void) lock_file(param, share->kfile,0L,F_UNLCK,\"indexfile\",filename);\n    info->update&= ~HA_STATE_CHANGED;\n  }\n  mi_lock_database(info, F_UNLCK);\nend2:\n  if (mi_close(info))\n  {\n    mi_check_print_error(param,\"%d when closing MyISAM-table '%s'\",my_errno,filename);\n    DBUG_RETURN(1);\n  }\n  if (error == 0)\n  {\n    if (param->out_flag & O_NEW_DATA)\n      error|=change_to_newfile(filename,MI_NAME_DEXT,DATA_TMP_EXT,\n\t\t\t       ((param->testflag & T_BACKUP_DATA) ?\n\t\t\t\tMYF(MY_REDEL_MAKE_BACKUP) : MYF(0)));\n    if (param->out_flag & O_NEW_INDEX)\n      error|=change_to_newfile(filename, MI_NAME_IEXT, INDEX_TMP_EXT, MYF(0));\n  }\n  (void) fflush(stdout); (void) fflush(stderr);\n  if (param->error_printed)\n  {\n    if (param->testflag & (T_REP_ANY | T_SORT_RECORDS | T_SORT_INDEX))\n    {\n      (void) fprintf(stderr,\n\t\t   \"MyISAM-table '%s' is not fixed because of errors\\n\",\n\t\t   filename);\n      if (param->testflag & T_REP_ANY)\n\t(void) fprintf(stderr,\n\t\t     \"Try fixing it by using the --safe-recover (-o), the --force (-f) option or by not using the --quick (-q) flag\\n\");\n    }\n    else if (!(param->error_printed & 2) &&\n\t     !(param->testflag & T_FORCE_CREATE))\n      (void) fprintf(stderr,\n      \"MyISAM-table '%s' is corrupted\\nFix it using switch \\\"-r\\\" or \\\"-o\\\"\\n\",\n\t      filename);\n  }\n  else if (param->warning_printed &&\n\t   ! (param->testflag & (T_REP_ANY | T_SORT_RECORDS | T_SORT_INDEX |\n\t\t\t  T_FORCE_CREATE)))\n    (void) fprintf(stderr, \"MyISAM-table '%s' is usable but should be fixed\\n\",\n\t\t filename);\n  (void) fflush(stderr);\n  DBUG_RETURN(error);\n}",
        "func": "static int myisamchk(MI_CHECK *param, char * filename)\n{\n  int error,lock_type,recreate;\n  int rep_quick= param->testflag & (T_QUICK | T_FORCE_UNIQUENESS);\n  MI_INFO *info;\n  File datafile;\n  char llbuff[22],llbuff2[22];\n  my_bool state_updated=0;\n  MYISAM_SHARE *share;\n  DBUG_ENTER(\"myisamchk\");\n\n  param->out_flag=error=param->warning_printed=param->error_printed=\n    recreate=0;\n  datafile=0;\n  param->isam_file_name=filename;\t\t/* For error messages */\n  if (!(info=mi_open(filename,\n\t\t     (param->testflag & (T_DESCRIPT | T_READONLY)) ?\n\t\t     O_RDONLY : O_RDWR,\n\t\t     HA_OPEN_FOR_REPAIR |\n\t\t     ((param->testflag & T_WAIT_FOREVER) ?\n\t\t      HA_OPEN_WAIT_IF_LOCKED :\n\t\t      (param->testflag & T_DESCRIPT) ?\n\t\t      HA_OPEN_IGNORE_IF_LOCKED : HA_OPEN_ABORT_IF_LOCKED))))\n  {\n    /* Avoid twice printing of isam file name */\n    param->error_printed=1;\n    switch (my_errno) {\n    case HA_ERR_CRASHED:\n      mi_check_print_error(param,\"'%s' doesn't have a correct index definition. You need to recreate it before you can do a repair\",filename);\n      break;\n    case HA_ERR_NOT_A_TABLE:\n      mi_check_print_error(param,\"'%s' is not a MyISAM-table\",filename);\n      break;\n    case HA_ERR_CRASHED_ON_USAGE:\n      mi_check_print_error(param,\"'%s' is marked as crashed\",filename);\n      break;\n    case HA_ERR_CRASHED_ON_REPAIR:\n      mi_check_print_error(param,\"'%s' is marked as crashed after last repair\",filename);\n      break;\n    case HA_ERR_OLD_FILE:\n      mi_check_print_error(param,\"'%s' is an old type of MyISAM-table\", filename);\n      break;\n    case HA_ERR_END_OF_FILE:\n      mi_check_print_error(param,\"Couldn't read complete header from '%s'\", filename);\n      break;\n    case EAGAIN:\n      mi_check_print_error(param,\"'%s' is locked. Use -w to wait until unlocked\",filename);\n      break;\n    case ENOENT:\n      mi_check_print_error(param,\"File '%s' doesn't exist\",filename);\n      break;\n    case EACCES:\n      mi_check_print_error(param,\"You don't have permission to use '%s'\",filename);\n      break;\n    default:\n      mi_check_print_error(param,\"%d when opening MyISAM-table '%s'\",\n\t\t  my_errno,filename);\n      break;\n    }\n    DBUG_RETURN(1);\n  }\n  share=info->s;\n  share->options&= ~HA_OPTION_READ_ONLY_DATA; /* We are modifing it */\n  share->tot_locks-= share->r_locks;\n  share->r_locks=0;\n\n  /*\n    Skip the checking of the file if:\n    We are using --fast and the table is closed properly\n    We are using --check-only-changed-tables and the table hasn't changed\n  */\n  if (param->testflag & (T_FAST | T_CHECK_ONLY_CHANGED))\n  {\n    my_bool need_to_check= mi_is_crashed(info) || share->state.open_count != 0;\n\n    if ((param->testflag & (T_REP_ANY | T_SORT_RECORDS)) &&\n\t((share->state.changed & (STATE_CHANGED | STATE_CRASHED |\n\t\t\t\t  STATE_CRASHED_ON_REPAIR) ||\n\t  !(param->testflag & T_CHECK_ONLY_CHANGED))))\n      need_to_check=1;\n\n    if (info->s->base.keys && info->state->records)\n    {\n      if ((param->testflag & T_STATISTICS) &&\n          (share->state.changed & STATE_NOT_ANALYZED))\n        need_to_check=1;\n      if ((param->testflag & T_SORT_INDEX) &&\n          (share->state.changed & STATE_NOT_SORTED_PAGES))\n        need_to_check=1;\n      if ((param->testflag & T_REP_BY_SORT) &&\n          (share->state.changed & STATE_NOT_OPTIMIZED_KEYS))\n        need_to_check=1;\n    }\n    if ((param->testflag & T_CHECK_ONLY_CHANGED) &&\n\t(share->state.changed & (STATE_CHANGED | STATE_CRASHED |\n\t\t\t\t STATE_CRASHED_ON_REPAIR)))\n      need_to_check=1;\n    if (!need_to_check)\n    {\n      if (!(param->testflag & T_SILENT) || param->testflag & T_INFO)\n\tprintf(\"MyISAM file: %s is already checked\\n\",filename);\n      if (mi_close(info))\n      {\n\tmi_check_print_error(param,\"%d when closing MyISAM-table '%s'\",\n\t\t\t     my_errno,filename);\n\tDBUG_RETURN(1);\n      }\n      DBUG_RETURN(0);\n    }\n  }\n  if ((param->testflag & (T_REP_ANY | T_STATISTICS |\n\t\t\t  T_SORT_RECORDS | T_SORT_INDEX)) &&\n      (((param->testflag & T_UNPACK) &&\n\tshare->data_file_type == COMPRESSED_RECORD) ||\n       mi_uint2korr(share->state.header.state_info_length) !=\n       MI_STATE_INFO_SIZE ||\n       mi_uint2korr(share->state.header.base_info_length) !=\n       MI_BASE_INFO_SIZE ||\n       mi_is_any_intersect_keys_active(param->keys_in_use, share->base.keys,\n                                       ~share->state.key_map) ||\n       test_if_almost_full(info) ||\n       info->s->state.header.file_version[3] != myisam_file_magic[3] ||\n       (set_collation &&\n        set_collation->number != share->state.header.language) ||\n       myisam_block_size != MI_KEY_BLOCK_LENGTH))\n  {\n    if (set_collation)\n      param->language= set_collation->number;\n    if (recreate_table(param, &info,filename))\n    {\n      (void) fprintf(stderr,\n\t\t   \"MyISAM-table '%s' is not fixed because of errors\\n\",\n\t      filename);\n      return(-1);\n    }\n    recreate=1;\n    if (!(param->testflag & T_REP_ANY))\n    {\n      param->testflag|=T_REP_BY_SORT;\t\t/* if only STATISTICS */\n      if (!(param->testflag & T_SILENT))\n\tprintf(\"- '%s' has old table-format. Recreating index\\n\",filename);\n      rep_quick|=T_QUICK;\n    }\n    share=info->s;\n    share->tot_locks-= share->r_locks;\n    share->r_locks=0;\n  }\n\n  if (param->testflag & T_DESCRIPT)\n  {\n    param->total_files++;\n    param->total_records+=info->state->records;\n    param->total_deleted+=info->state->del;\n    descript(param, info, filename);\n  }\n  else\n  {\n    if (!stopwords_inited++)\n      ft_init_stopwords();\n\n    if (!(param->testflag & T_READONLY))\n      lock_type = F_WRLCK;\t\t\t/* table is changed */\n    else\n      lock_type= F_RDLCK;\n    if (info->lock_type == F_RDLCK)\n      info->lock_type=F_UNLCK;\t\t\t/* Read only table */\n    if (_mi_readinfo(info,lock_type,0))\n    {\n      mi_check_print_error(param,\"Can't lock indexfile of '%s', error: %d\",\n\t\t  filename,my_errno);\n      param->error_printed=0;\n      goto end2;\n    }\n    /*\n      _mi_readinfo() has locked the table.\n      We mark the table as locked (without doing file locks) to be able to\n      use functions that only works on locked tables (like row caching).\n    */\n    mi_lock_database(info, F_EXTRA_LCK);\n    datafile=info->dfile;\n\n    if (param->testflag & (T_REP_ANY | T_SORT_RECORDS | T_SORT_INDEX))\n    {\n      if (param->testflag & T_REP_ANY)\n      {\n\tulonglong tmp=share->state.key_map;\n\tmi_copy_keys_active(share->state.key_map, share->base.keys,\n                            param->keys_in_use);\n\tif (tmp != share->state.key_map)\n\t  info->update|=HA_STATE_CHANGED;\n      }\n      if (rep_quick && chk_del(param, info, param->testflag & ~T_VERBOSE))\n      {\n\tif (param->testflag & T_FORCE_CREATE)\n\t{\n\t  rep_quick=0;\n\t  mi_check_print_info(param,\"Creating new data file\\n\");\n\t}\n\telse\n\t{\n\t  error=1;\n\t  mi_check_print_error(param,\n\t\t\t       \"Quick-recover aborted; Run recovery without switch 'q'\");\n\t}\n      }\n      if (!error)\n      {\n\tif ((param->testflag & (T_REP_BY_SORT | T_REP_PARALLEL)) &&\n\t    (mi_is_any_key_active(share->state.key_map) ||\n\t     (rep_quick && !param->keys_in_use && !recreate)) &&\n\t    mi_test_if_sort_rep(info, info->state->records,\n\t\t\t\tinfo->s->state.key_map,\n\t\t\t\tparam->force_sort))\n\t{\n          /*\n            The new file might not be created with the right stats depending\n            on how myisamchk is run, so we must copy file stats from old to new.\n          */\n          if (param->testflag & T_REP_BY_SORT)\n            error= mi_repair_by_sort(param, info, filename, rep_quick, FALSE);\n          else\n            error= mi_repair_parallel(param, info, filename, rep_quick, FALSE);\n\t  state_updated=1;\n\t}\n\telse if (param->testflag & T_REP_ANY)\n\t  error= mi_repair(param, info, filename, rep_quick, FALSE);\n      }\n      if (!error && param->testflag & T_SORT_RECORDS)\n      {\n\t/*\n\t  The data file is nowadays reopened in the repair code so we should\n\t  soon remove the following reopen-code\n\t*/\n#ifndef TO_BE_REMOVED\n\tif (param->out_flag & O_NEW_DATA)\n\t{\t\t\t/* Change temp file to org file */\n\t  (void) my_close(info->dfile,MYF(MY_WME)); /* Close new file */\n\t  error|=change_to_newfile(filename, MI_NAME_DEXT, DATA_TMP_EXT, MYF(0));\n\t  if (mi_open_datafile(info,info->s, NULL, -1))\n\t    error=1;\n\t  param->out_flag&= ~O_NEW_DATA; /* We are using new datafile */\n\t  param->read_cache.file=info->dfile;\n\t}\n#endif\n\tif (! error)\n\t{\n\t  uint key;\n\t  /*\n\t    We can't update the index in mi_sort_records if we have a\n\t    prefix compressed or fulltext index\n\t  */\n\t  my_bool update_index=1;\n\t  for (key=0 ; key < share->base.keys; key++)\n\t    if (share->keyinfo[key].flag & (HA_BINARY_PACK_KEY|HA_FULLTEXT))\n\t      update_index=0;\n\n\t  error=mi_sort_records(param,info,filename,param->opt_sort_key,\n                             /* what is the following parameter for ? */\n\t\t\t\t(my_bool) !(param->testflag & T_REP),\n\t\t\t\tupdate_index);\n\t  datafile=info->dfile;\t/* This is now locked */\n\t  if (!error && !update_index)\n\t  {\n\t    if (param->verbose)\n\t      puts(\"Table had a compressed index;  We must now recreate the index\");\n\t    error= mi_repair_by_sort(param, info, filename, 1, FALSE);\n\t  }\n\t}\n      }\n      if (!error && param->testflag & T_SORT_INDEX)\n\terror= mi_sort_index(param, info, filename, FALSE);\n      if (!error)\n\tshare->state.changed&= ~(STATE_CHANGED | STATE_CRASHED |\n\t\t\t\t STATE_CRASHED_ON_REPAIR);\n      else\n\tmi_mark_crashed(info);\n    }\n    else if ((param->testflag & T_CHECK) || !(param->testflag & T_AUTO_INC))\n    {\n      if (!(param->testflag & T_SILENT) || param->testflag & T_INFO)\n\tprintf(\"Checking MyISAM file: %s\\n\",filename);\n      if (!(param->testflag & T_SILENT))\n\tprintf(\"Data records: %7s   Deleted blocks: %7s\\n\",\n\t       llstr(info->state->records,llbuff),\n\t       llstr(info->state->del,llbuff2));\n      error =chk_status(param,info);\n      mi_intersect_keys_active(share->state.key_map, param->keys_in_use);\n      error =chk_size(param,info);\n      if (!error || !(param->testflag & (T_FAST | T_FORCE_CREATE)))\n\terror|=chk_del(param, info,param->testflag);\n      if ((!error || (!(param->testflag & (T_FAST | T_FORCE_CREATE)) &&\n\t\t      !param->start_check_pos)))\n      {\n\terror|=chk_key(param, info);\n\tif (!error && (param->testflag & (T_STATISTICS | T_AUTO_INC)))\n\t  error=update_state_info(param, info,\n\t\t\t\t  ((param->testflag & T_STATISTICS) ?\n\t\t\t\t   UPDATE_STAT : 0) |\n\t\t\t\t  ((param->testflag & T_AUTO_INC) ?\n\t\t\t\t   UPDATE_AUTO_INC : 0));\n      }\n      if ((!rep_quick && !error) ||\n\t  !(param->testflag & (T_FAST | T_FORCE_CREATE)))\n      {\n\tif (param->testflag & (T_EXTEND | T_MEDIUM))\n\t  (void) init_key_cache(dflt_key_cache,opt_key_cache_block_size,\n                              param->use_buffers, 0, 0);\n\t(void) init_io_cache(&param->read_cache,datafile,\n\t\t\t   (uint) param->read_buffer_length,\n\t\t\t   READ_CACHE,\n\t\t\t   (param->start_check_pos ?\n\t\t\t    param->start_check_pos :\n\t\t\t    share->pack.header_length),\n\t\t\t   1,\n\t\t\t   MYF(MY_WME));\n\tlock_memory(param);\n\tif ((info->s->options & (HA_OPTION_PACK_RECORD |\n\t\t\t\t HA_OPTION_COMPRESS_RECORD)) ||\n\t    (param->testflag & (T_EXTEND | T_MEDIUM)))\n\t  error|=chk_data_link(param, info, param->testflag & T_EXTEND);\n\terror|=flush_blocks(param, share->key_cache, share->kfile);\n\t(void) end_io_cache(&param->read_cache);\n      }\n      if (!error)\n      {\n\tif ((share->state.changed & STATE_CHANGED) &&\n\t    (param->testflag & T_UPDATE_STATE))\n\t  info->update|=HA_STATE_CHANGED | HA_STATE_ROW_CHANGED;\n\tshare->state.changed&= ~(STATE_CHANGED | STATE_CRASHED |\n\t\t\t\t STATE_CRASHED_ON_REPAIR);\n      }\n      else if (!mi_is_crashed(info) &&\n\t       (param->testflag & T_UPDATE_STATE))\n      {\t\t\t\t\t\t/* Mark crashed */\n\tmi_mark_crashed(info);\n\tinfo->update|=HA_STATE_CHANGED | HA_STATE_ROW_CHANGED;\n      }\n    }\n  }\n  if ((param->testflag & T_AUTO_INC) ||\n      ((param->testflag & T_REP_ANY) && info->s->base.auto_key))\n    update_auto_increment_key(param, info,\n\t\t\t      (my_bool) !test(param->testflag & T_AUTO_INC));\n\n  if (!(param->testflag & T_DESCRIPT))\n  {\n    if (info->update & HA_STATE_CHANGED && ! (param->testflag & T_READONLY))\n      error|=update_state_info(param, info,\n\t\t\t       UPDATE_OPEN_COUNT |\n\t\t\t       (((param->testflag & T_REP_ANY) ?\n\t\t\t\t UPDATE_TIME : 0) |\n\t\t\t\t(state_updated ? UPDATE_STAT : 0) |\n\t\t\t\t((param->testflag & T_SORT_RECORDS) ?\n\t\t\t\t UPDATE_SORT : 0)));\n    (void) lock_file(param, share->kfile,0L,F_UNLCK,\"indexfile\",filename);\n    info->update&= ~HA_STATE_CHANGED;\n  }\n  mi_lock_database(info, F_UNLCK);\nend2:\n  if (mi_close(info))\n  {\n    mi_check_print_error(param,\"%d when closing MyISAM-table '%s'\",my_errno,filename);\n    DBUG_RETURN(1);\n  }\n  if (error == 0)\n  {\n    if (param->out_flag & O_NEW_DATA)\n      error|=change_to_newfile(filename,MI_NAME_DEXT,DATA_TMP_EXT,\n\t\t\t       ((param->testflag & T_BACKUP_DATA) ?\n\t\t\t\tMYF(MY_REDEL_MAKE_BACKUP) : MYF(0)));\n    if (param->out_flag & O_NEW_INDEX)\n      error|=change_to_newfile(filename, MI_NAME_IEXT, INDEX_TMP_EXT, MYF(0));\n  }\n  (void) fflush(stdout); (void) fflush(stderr);\n  if (param->error_printed)\n  {\n    if (param->testflag & (T_REP_ANY | T_SORT_RECORDS | T_SORT_INDEX))\n    {\n      (void) fprintf(stderr,\n\t\t   \"MyISAM-table '%s' is not fixed because of errors\\n\",\n\t\t   filename);\n      if (param->testflag & T_REP_ANY)\n\t(void) fprintf(stderr,\n\t\t     \"Try fixing it by using the --safe-recover (-o), the --force (-f) option or by not using the --quick (-q) flag\\n\");\n    }\n    else if (!(param->error_printed & 2) &&\n\t     !(param->testflag & T_FORCE_CREATE))\n      (void) fprintf(stderr,\n      \"MyISAM-table '%s' is corrupted\\nFix it using switch \\\"-r\\\" or \\\"-o\\\"\\n\",\n\t      filename);\n  }\n  else if (param->warning_printed &&\n\t   ! (param->testflag & (T_REP_ANY | T_SORT_RECORDS | T_SORT_INDEX |\n\t\t\t  T_FORCE_CREATE)))\n    (void) fprintf(stderr, \"MyISAM-table '%s' is usable but should be fixed\\n\",\n\t\t filename);\n  (void) fflush(stderr);\n  DBUG_RETURN(error);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -212,14 +212,18 @@\n \t\t\t\tinfo->s->state.key_map,\n \t\t\t\tparam->force_sort))\n \t{\n+          /*\n+            The new file might not be created with the right stats depending\n+            on how myisamchk is run, so we must copy file stats from old to new.\n+          */\n           if (param->testflag & T_REP_BY_SORT)\n-            error=mi_repair_by_sort(param,info,filename,rep_quick);\n+            error= mi_repair_by_sort(param, info, filename, rep_quick, FALSE);\n           else\n-            error=mi_repair_parallel(param,info,filename,rep_quick);\n+            error= mi_repair_parallel(param, info, filename, rep_quick, FALSE);\n \t  state_updated=1;\n \t}\n \telse if (param->testflag & T_REP_ANY)\n-\t  error=mi_repair(param, info,filename,rep_quick);\n+\t  error= mi_repair(param, info, filename, rep_quick, FALSE);\n       }\n       if (!error && param->testflag & T_SORT_RECORDS)\n       {\n@@ -259,12 +263,12 @@\n \t  {\n \t    if (param->verbose)\n \t      puts(\"Table had a compressed index;  We must now recreate the index\");\n-\t    error=mi_repair_by_sort(param,info,filename,1);\n+\t    error= mi_repair_by_sort(param, info, filename, 1, FALSE);\n \t  }\n \t}\n       }\n       if (!error && param->testflag & T_SORT_INDEX)\n-\terror=mi_sort_index(param,info,filename);\n+\terror= mi_sort_index(param, info, filename, FALSE);\n       if (!error)\n \tshare->state.changed&= ~(STATE_CHANGED | STATE_CRASHED |\n \t\t\t\t STATE_CRASHED_ON_REPAIR);",
        "diff_line_info": {
            "deleted_lines": [
                "            error=mi_repair_by_sort(param,info,filename,rep_quick);",
                "            error=mi_repair_parallel(param,info,filename,rep_quick);",
                "\t  error=mi_repair(param, info,filename,rep_quick);",
                "\t    error=mi_repair_by_sort(param,info,filename,1);",
                "\terror=mi_sort_index(param,info,filename);"
            ],
            "added_lines": [
                "          /*",
                "            The new file might not be created with the right stats depending",
                "            on how myisamchk is run, so we must copy file stats from old to new.",
                "          */",
                "            error= mi_repair_by_sort(param, info, filename, rep_quick, FALSE);",
                "            error= mi_repair_parallel(param, info, filename, rep_quick, FALSE);",
                "\t  error= mi_repair(param, info, filename, rep_quick, FALSE);",
                "\t    error= mi_repair_by_sort(param, info, filename, 1, FALSE);",
                "\terror= mi_sort_index(param, info, filename, FALSE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6663",
        "func_name": "mysql/mysql-server/ha_myisam::repair",
        "description": "Race condition in Oracle MySQL before 5.5.52, 5.6.x before 5.6.33, 5.7.x before 5.7.15, and 8.x before 8.0.1; MariaDB before 5.5.52, 10.0.x before 10.0.28, and 10.1.x before 10.1.18; Percona Server before 5.5.51-38.2, 5.6.x before 5.6.32-78-1, and 5.7.x before 5.7.14-8; and Percona XtraDB Cluster before 5.5.41-37.0, 5.6.x before 5.6.32-25.17, and 5.7.x before 5.7.14-26.17 allows local users with certain permissions to gain privileges by leveraging use of my_copystat by REPAIR TABLE to repair a MyISAM table.",
        "git_url": "https://github.com/mysql/mysql-server/commit/4e5473862e6852b0f3802b0cd0c6fa10b5253291",
        "commit_title": "Bug#24388746: PRIVILEGE ESCALATION AND RACE CONDITION USING CREATE TABLE",
        "commit_text": " During REPAIR TABLE of a MyISAM table, a temporary data file (.TMD) is created. When repair finishes, this file is renamed to the original .MYD file. The problem was that during this rename, we copied the stats from the old file to the new file with chmod/chown. If a user managed to replace the temporary file before chmod/chown was executed, it was possible to get an arbitrary file with the privileges of the mysql user.  This patch fixes the problem by not copying stats from the old file to the new file. This is not needed as the new file was created with the correct stats. This fix only changes server behavior - external utilities such as myisamchk still does chmod/chown.  No test case provided since the problem involves synchronization with file system operations.",
        "func_before": "int ha_myisam::repair(THD *thd, MI_CHECK &param, bool do_optimize)\n{\n  int error=0;\n  uint local_testflag=param.testflag;\n  bool optimize_done= !do_optimize, statistics_done=0;\n  const char *old_proc_info=thd->proc_info;\n  char fixed_name[FN_REFLEN];\n  MYISAM_SHARE* share = file->s;\n  ha_rows rows= file->state->records;\n  DBUG_ENTER(\"ha_myisam::repair\");\n\n  param.db_name=    table->s->db.str;\n  param.table_name= table->alias;\n  param.using_global_keycache = 1;\n  param.thd= thd;\n  param.tmpdir= &mysql_tmpdir_list;\n  param.out_flag= 0;\n  strmov(fixed_name,file->filename);\n\n  // Release latches since this can take a long time\n  ha_release_temporary_latches(thd);\n\n  // Don't lock tables if we have used LOCK TABLE\n  if (! thd->locked_tables_mode &&\n      mi_lock_database(file, table->s->tmp_table ? F_EXTRA_LCK : F_WRLCK))\n  {\n    mi_check_print_error(&param,ER(ER_CANT_LOCK),my_errno);\n    DBUG_RETURN(HA_ADMIN_FAILED);\n  }\n\n  if (!do_optimize ||\n      ((file->state->del || share->state.split != file->state->records) &&\n       (!(param.testflag & T_QUICK) ||\n\t!(share->state.changed & STATE_NOT_OPTIMIZED_KEYS))))\n  {\n    ulonglong key_map= ((local_testflag & T_CREATE_MISSING_KEYS) ?\n\t\t\tmi_get_mask_all_keys_active(share->base.keys) :\n\t\t\tshare->state.key_map);\n    uint testflag=param.testflag;\n#ifdef HAVE_MMAP\n    bool remap= test(share->file_map);\n    /*\n      mi_repair*() functions family use file I/O even if memory\n      mapping is available.\n\n      Since mixing mmap I/O and file I/O may cause various artifacts,\n      memory mapping must be disabled.\n    */\n    if (remap)\n      mi_munmap_file(file);\n#endif\n    if (mi_test_if_sort_rep(file,file->state->records,key_map,0) &&\n\t(local_testflag & T_REP_BY_SORT))\n    {\n      local_testflag|= T_STATISTICS;\n      param.testflag|= T_STATISTICS;\t\t// We get this for free\n      statistics_done=1;\n      if (THDVAR(thd, repair_threads)>1)\n      {\n        char buf[40];\n        /* TODO: respect myisam_repair_threads variable */\n        my_snprintf(buf, 40, \"Repair with %d threads\", my_count_bits(key_map));\n        thd_proc_info(thd, buf);\n        error = mi_repair_parallel(&param, file, fixed_name,\n            param.testflag & T_QUICK);\n        thd_proc_info(thd, \"Repair done\"); // to reset proc_info, as\n                                      // it was pointing to local buffer\n      }\n      else\n      {\n        thd_proc_info(thd, \"Repair by sorting\");\n        error = mi_repair_by_sort(&param, file, fixed_name,\n            param.testflag & T_QUICK);\n      }\n    }\n    else\n    {\n      thd_proc_info(thd, \"Repair with keycache\");\n      param.testflag &= ~T_REP_BY_SORT;\n      error=  mi_repair(&param, file, fixed_name,\n\t\t\tparam.testflag & T_QUICK);\n    }\n#ifdef HAVE_MMAP\n    if (remap)\n      mi_dynmap_file(file, file->state->data_file_length);\n#endif\n    param.testflag=testflag;\n    optimize_done=1;\n  }\n  if (!error)\n  {\n    if ((local_testflag & T_SORT_INDEX) &&\n\t(share->state.changed & STATE_NOT_SORTED_PAGES))\n    {\n      optimize_done=1;\n      thd_proc_info(thd, \"Sorting index\");\n      error=mi_sort_index(&param,file,fixed_name);\n    }\n    if (!statistics_done && (local_testflag & T_STATISTICS))\n    {\n      if (share->state.changed & STATE_NOT_ANALYZED)\n      {\n\toptimize_done=1;\n\tthd_proc_info(thd, \"Analyzing\");\n\terror = chk_key(&param, file);\n      }\n      else\n\tlocal_testflag&= ~T_STATISTICS;\t\t// Don't update statistics\n    }\n  }\n  thd_proc_info(thd, \"Saving state\");\n  if (!error)\n  {\n    if ((share->state.changed & STATE_CHANGED) || mi_is_crashed(file))\n    {\n      share->state.changed&= ~(STATE_CHANGED | STATE_CRASHED |\n\t\t\t       STATE_CRASHED_ON_REPAIR);\n      file->update|=HA_STATE_CHANGED | HA_STATE_ROW_CHANGED;\n    }\n    /*\n      the following 'if', thought conceptually wrong,\n      is a useful optimization nevertheless.\n    */\n    if (file->state != &file->s->state.state)\n      file->s->state.state = *file->state;\n    if (file->s->base.auto_key)\n      update_auto_increment_key(&param, file, 1);\n    if (optimize_done)\n      error = update_state_info(&param, file,\n\t\t\t\tUPDATE_TIME | UPDATE_OPEN_COUNT |\n\t\t\t\t(local_testflag &\n\t\t\t\t T_STATISTICS ? UPDATE_STAT : 0));\n    info(HA_STATUS_NO_LOCK | HA_STATUS_TIME | HA_STATUS_VARIABLE |\n\t HA_STATUS_CONST);\n    if (rows != file->state->records && ! (param.testflag & T_VERY_SILENT))\n    {\n      char llbuff[22],llbuff2[22];\n      mi_check_print_warning(&param,\"Number of rows changed from %s to %s\",\n\t\t\t     llstr(rows,llbuff),\n\t\t\t     llstr(file->state->records,llbuff2));\n    }\n  }\n  else\n  {\n    mi_mark_crashed_on_repair(file);\n    file->update |= HA_STATE_CHANGED | HA_STATE_ROW_CHANGED;\n    update_state_info(&param, file, 0);\n  }\n  thd_proc_info(thd, old_proc_info);\n  if (! thd->locked_tables_mode)\n    mi_lock_database(file,F_UNLCK);\n  DBUG_RETURN(error ? HA_ADMIN_FAILED :\n\t      !optimize_done ? HA_ADMIN_ALREADY_DONE : HA_ADMIN_OK);\n}",
        "func": "int ha_myisam::repair(THD *thd, MI_CHECK &param, bool do_optimize)\n{\n  int error=0;\n  uint local_testflag=param.testflag;\n  bool optimize_done= !do_optimize, statistics_done=0;\n  const char *old_proc_info=thd->proc_info;\n  char fixed_name[FN_REFLEN];\n  MYISAM_SHARE* share = file->s;\n  ha_rows rows= file->state->records;\n  DBUG_ENTER(\"ha_myisam::repair\");\n\n  param.db_name=    table->s->db.str;\n  param.table_name= table->alias;\n  param.using_global_keycache = 1;\n  param.thd= thd;\n  param.tmpdir= &mysql_tmpdir_list;\n  param.out_flag= 0;\n  strmov(fixed_name,file->filename);\n\n  // Release latches since this can take a long time\n  ha_release_temporary_latches(thd);\n\n  // Don't lock tables if we have used LOCK TABLE\n  if (! thd->locked_tables_mode &&\n      mi_lock_database(file, table->s->tmp_table ? F_EXTRA_LCK : F_WRLCK))\n  {\n    mi_check_print_error(&param,ER(ER_CANT_LOCK),my_errno);\n    DBUG_RETURN(HA_ADMIN_FAILED);\n  }\n\n  if (!do_optimize ||\n      ((file->state->del || share->state.split != file->state->records) &&\n       (!(param.testflag & T_QUICK) ||\n\t!(share->state.changed & STATE_NOT_OPTIMIZED_KEYS))))\n  {\n    ulonglong key_map= ((local_testflag & T_CREATE_MISSING_KEYS) ?\n\t\t\tmi_get_mask_all_keys_active(share->base.keys) :\n\t\t\tshare->state.key_map);\n    uint testflag=param.testflag;\n#ifdef HAVE_MMAP\n    bool remap= test(share->file_map);\n    /*\n      mi_repair*() functions family use file I/O even if memory\n      mapping is available.\n\n      Since mixing mmap I/O and file I/O may cause various artifacts,\n      memory mapping must be disabled.\n    */\n    if (remap)\n      mi_munmap_file(file);\n#endif\n    if (mi_test_if_sort_rep(file,file->state->records,key_map,0) &&\n\t(local_testflag & T_REP_BY_SORT))\n    {\n      local_testflag|= T_STATISTICS;\n      param.testflag|= T_STATISTICS;\t\t// We get this for free\n      statistics_done=1;\n      if (THDVAR(thd, repair_threads)>1)\n      {\n        char buf[40];\n        /* TODO: respect myisam_repair_threads variable */\n        my_snprintf(buf, 40, \"Repair with %d threads\", my_count_bits(key_map));\n        thd_proc_info(thd, buf);\n        /*\n          The new file is created with the right stats, so we can skip\n          copying file stats from old to new.\n        */\n        error = mi_repair_parallel(&param, file, fixed_name,\n                                   param.testflag & T_QUICK, TRUE);\n        thd_proc_info(thd, \"Repair done\"); // to reset proc_info, as\n                                      // it was pointing to local buffer\n      }\n      else\n      {\n        thd_proc_info(thd, \"Repair by sorting\");\n        /*\n          The new file is created with the right stats, so we can skip\n          copying file stats from old to new.\n        */\n        error = mi_repair_by_sort(&param, file, fixed_name,\n                                  param.testflag & T_QUICK, TRUE);\n      }\n    }\n    else\n    {\n      thd_proc_info(thd, \"Repair with keycache\");\n      param.testflag &= ~T_REP_BY_SORT;\n      /*\n        The new file is created with the right stats, so we can skip\n        copying file stats from old to new.\n      */\n      error=  mi_repair(&param, file, fixed_name,\n\t\t\tparam.testflag & T_QUICK, TRUE);\n    }\n#ifdef HAVE_MMAP\n    if (remap)\n      mi_dynmap_file(file, file->state->data_file_length);\n#endif\n    param.testflag=testflag;\n    optimize_done=1;\n  }\n  if (!error)\n  {\n    if ((local_testflag & T_SORT_INDEX) &&\n\t(share->state.changed & STATE_NOT_SORTED_PAGES))\n    {\n      optimize_done=1;\n      thd_proc_info(thd, \"Sorting index\");\n      /*\n        The new file is created with the right stats, so we can skip\n        copying file stats from old to new.\n      */\n      error=mi_sort_index(&param,file,fixed_name, TRUE);\n    }\n    if (!statistics_done && (local_testflag & T_STATISTICS))\n    {\n      if (share->state.changed & STATE_NOT_ANALYZED)\n      {\n\toptimize_done=1;\n\tthd_proc_info(thd, \"Analyzing\");\n\terror = chk_key(&param, file);\n      }\n      else\n\tlocal_testflag&= ~T_STATISTICS;\t\t// Don't update statistics\n    }\n  }\n  thd_proc_info(thd, \"Saving state\");\n  if (!error)\n  {\n    if ((share->state.changed & STATE_CHANGED) || mi_is_crashed(file))\n    {\n      share->state.changed&= ~(STATE_CHANGED | STATE_CRASHED |\n\t\t\t       STATE_CRASHED_ON_REPAIR);\n      file->update|=HA_STATE_CHANGED | HA_STATE_ROW_CHANGED;\n    }\n    /*\n      the following 'if', thought conceptually wrong,\n      is a useful optimization nevertheless.\n    */\n    if (file->state != &file->s->state.state)\n      file->s->state.state = *file->state;\n    if (file->s->base.auto_key)\n      update_auto_increment_key(&param, file, 1);\n    if (optimize_done)\n      error = update_state_info(&param, file,\n\t\t\t\tUPDATE_TIME | UPDATE_OPEN_COUNT |\n\t\t\t\t(local_testflag &\n\t\t\t\t T_STATISTICS ? UPDATE_STAT : 0));\n    info(HA_STATUS_NO_LOCK | HA_STATUS_TIME | HA_STATUS_VARIABLE |\n\t HA_STATUS_CONST);\n    if (rows != file->state->records && ! (param.testflag & T_VERY_SILENT))\n    {\n      char llbuff[22],llbuff2[22];\n      mi_check_print_warning(&param,\"Number of rows changed from %s to %s\",\n\t\t\t     llstr(rows,llbuff),\n\t\t\t     llstr(file->state->records,llbuff2));\n    }\n  }\n  else\n  {\n    mi_mark_crashed_on_repair(file);\n    file->update |= HA_STATE_CHANGED | HA_STATE_ROW_CHANGED;\n    update_state_info(&param, file, 0);\n  }\n  thd_proc_info(thd, old_proc_info);\n  if (! thd->locked_tables_mode)\n    mi_lock_database(file,F_UNLCK);\n  DBUG_RETURN(error ? HA_ADMIN_FAILED :\n\t      !optimize_done ? HA_ADMIN_ALREADY_DONE : HA_ADMIN_OK);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -61,24 +61,36 @@\n         /* TODO: respect myisam_repair_threads variable */\n         my_snprintf(buf, 40, \"Repair with %d threads\", my_count_bits(key_map));\n         thd_proc_info(thd, buf);\n+        /*\n+          The new file is created with the right stats, so we can skip\n+          copying file stats from old to new.\n+        */\n         error = mi_repair_parallel(&param, file, fixed_name,\n-            param.testflag & T_QUICK);\n+                                   param.testflag & T_QUICK, TRUE);\n         thd_proc_info(thd, \"Repair done\"); // to reset proc_info, as\n                                       // it was pointing to local buffer\n       }\n       else\n       {\n         thd_proc_info(thd, \"Repair by sorting\");\n+        /*\n+          The new file is created with the right stats, so we can skip\n+          copying file stats from old to new.\n+        */\n         error = mi_repair_by_sort(&param, file, fixed_name,\n-            param.testflag & T_QUICK);\n+                                  param.testflag & T_QUICK, TRUE);\n       }\n     }\n     else\n     {\n       thd_proc_info(thd, \"Repair with keycache\");\n       param.testflag &= ~T_REP_BY_SORT;\n+      /*\n+        The new file is created with the right stats, so we can skip\n+        copying file stats from old to new.\n+      */\n       error=  mi_repair(&param, file, fixed_name,\n-\t\t\tparam.testflag & T_QUICK);\n+\t\t\tparam.testflag & T_QUICK, TRUE);\n     }\n #ifdef HAVE_MMAP\n     if (remap)\n@@ -94,7 +106,11 @@\n     {\n       optimize_done=1;\n       thd_proc_info(thd, \"Sorting index\");\n-      error=mi_sort_index(&param,file,fixed_name);\n+      /*\n+        The new file is created with the right stats, so we can skip\n+        copying file stats from old to new.\n+      */\n+      error=mi_sort_index(&param,file,fixed_name, TRUE);\n     }\n     if (!statistics_done && (local_testflag & T_STATISTICS))\n     {",
        "diff_line_info": {
            "deleted_lines": [
                "            param.testflag & T_QUICK);",
                "            param.testflag & T_QUICK);",
                "\t\t\tparam.testflag & T_QUICK);",
                "      error=mi_sort_index(&param,file,fixed_name);"
            ],
            "added_lines": [
                "        /*",
                "          The new file is created with the right stats, so we can skip",
                "          copying file stats from old to new.",
                "        */",
                "                                   param.testflag & T_QUICK, TRUE);",
                "        /*",
                "          The new file is created with the right stats, so we can skip",
                "          copying file stats from old to new.",
                "        */",
                "                                  param.testflag & T_QUICK, TRUE);",
                "      /*",
                "        The new file is created with the right stats, so we can skip",
                "        copying file stats from old to new.",
                "      */",
                "\t\t\tparam.testflag & T_QUICK, TRUE);",
                "      /*",
                "        The new file is created with the right stats, so we can skip",
                "        copying file stats from old to new.",
                "      */",
                "      error=mi_sort_index(&param,file,fixed_name, TRUE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6663",
        "func_name": "mysql/mysql-server/mi_repair_parallel",
        "description": "Race condition in Oracle MySQL before 5.5.52, 5.6.x before 5.6.33, 5.7.x before 5.7.15, and 8.x before 8.0.1; MariaDB before 5.5.52, 10.0.x before 10.0.28, and 10.1.x before 10.1.18; Percona Server before 5.5.51-38.2, 5.6.x before 5.6.32-78-1, and 5.7.x before 5.7.14-8; and Percona XtraDB Cluster before 5.5.41-37.0, 5.6.x before 5.6.32-25.17, and 5.7.x before 5.7.14-26.17 allows local users with certain permissions to gain privileges by leveraging use of my_copystat by REPAIR TABLE to repair a MyISAM table.",
        "git_url": "https://github.com/mysql/mysql-server/commit/4e5473862e6852b0f3802b0cd0c6fa10b5253291",
        "commit_title": "Bug#24388746: PRIVILEGE ESCALATION AND RACE CONDITION USING CREATE TABLE",
        "commit_text": " During REPAIR TABLE of a MyISAM table, a temporary data file (.TMD) is created. When repair finishes, this file is renamed to the original .MYD file. The problem was that during this rename, we copied the stats from the old file to the new file with chmod/chown. If a user managed to replace the temporary file before chmod/chown was executed, it was possible to get an arbitrary file with the privileges of the mysql user.  This patch fixes the problem by not copying stats from the old file to the new file. This is not needed as the new file was created with the correct stats. This fix only changes server behavior - external utilities such as myisamchk still does chmod/chown.  No test case provided since the problem involves synchronization with file system operations.",
        "func_before": "int mi_repair_parallel(MI_CHECK *param, register MI_INFO *info,\n\t\t\tconst char * name, int rep_quick)\n{\n  int got_error;\n  uint i,key, total_key_length, istep;\n  ulong rec_length;\n  ha_rows start_records;\n  my_off_t new_header_length,del;\n  File new_file;\n  MI_SORT_PARAM *sort_param=0;\n  MYISAM_SHARE *share=info->s;\n  ulong   *rec_per_key_part;\n  HA_KEYSEG *keyseg;\n  char llbuff[22];\n  IO_CACHE new_data_cache; /* For non-quick repair. */\n  IO_CACHE_SHARE io_share;\n  SORT_INFO sort_info;\n  ulonglong UNINIT_VAR(key_map);\n  pthread_attr_t thr_attr;\n  ulong max_pack_reclength;\n  int error;\n  DBUG_ENTER(\"mi_repair_parallel\");\n\n  start_records=info->state->records;\n  got_error=1;\n  new_file= -1;\n  new_header_length=(param->testflag & T_UNPACK) ? 0 :\n    share->pack.header_length;\n  if (!(param->testflag & T_SILENT))\n  {\n    printf(\"- parallel recovering (with sort) MyISAM-table '%s'\\n\",name);\n    printf(\"Data records: %s\\n\", llstr(start_records,llbuff));\n  }\n  param->testflag|=T_REP; /* for easy checking */\n\n  if (info->s->options & (HA_OPTION_CHECKSUM | HA_OPTION_COMPRESS_RECORD))\n    param->testflag|=T_CALC_CHECKSUM;\n\n  /*\n    Quick repair (not touching data file, rebuilding indexes):\n    {\n      Read  cache is (MI_CHECK *param)->read_cache using info->dfile.\n    }\n\n    Non-quick repair (rebuilding data file and indexes):\n    {\n      Master thread:\n\n        Read  cache is (MI_CHECK *param)->read_cache using info->dfile.\n        Write cache is (MI_INFO   *info)->rec_cache  using new_file.\n\n      Slave threads:\n\n        Read  cache is new_data_cache synced to master rec_cache.\n\n      The final assignment of the filedescriptor for rec_cache is done\n      after the cache creation.\n\n      Don't check file size on new_data_cache, as the resulting file size\n      is not known yet.\n\n      As rec_cache and new_data_cache are synced, write_buffer_length is\n      used for the read cache 'new_data_cache'. Both start at the same\n      position 'new_header_length'.\n    }\n  */\n  DBUG_PRINT(\"info\", (\"is quick repair: %d\", rep_quick));\n  bzero((char*)&sort_info,sizeof(sort_info));\n  /* Initialize pthread structures before goto err. */\n  mysql_mutex_init(mi_key_mutex_MI_SORT_INFO_mutex,\n                   &sort_info.mutex, MY_MUTEX_INIT_FAST);\n  mysql_cond_init(mi_key_cond_MI_SORT_INFO_cond, &sort_info.cond, 0);\n  mysql_mutex_init(mi_key_mutex_MI_CHECK_print_msg,\n                   &param->print_msg_mutex, MY_MUTEX_INIT_FAST);\n  param->need_print_msg_lock= 1;\n\n  if (!(sort_info.key_block=\n\talloc_key_blocks(param, (uint) param->sort_key_blocks,\n\t\t\t share->base.max_key_block_length)) ||\n      init_io_cache(&param->read_cache, info->dfile,\n                    (uint) param->read_buffer_length,\n                    READ_CACHE, share->pack.header_length, 1, MYF(MY_WME)) ||\n      (!rep_quick &&\n       (init_io_cache(&info->rec_cache, info->dfile,\n                      (uint) param->write_buffer_length,\n                      WRITE_CACHE, new_header_length, 1,\n                      MYF(MY_WME | MY_WAIT_IF_FULL) & param->myf_rw) ||\n        init_io_cache(&new_data_cache, -1,\n                      (uint) param->write_buffer_length,\n                      READ_CACHE, new_header_length, 1,\n                      MYF(MY_WME | MY_DONT_CHECK_FILESIZE)))))\n    goto err;\n  sort_info.key_block_end=sort_info.key_block+param->sort_key_blocks;\n  info->opt_flag|=WRITE_CACHE_USED;\n  info->rec_cache.file=info->dfile;         /* for sort_delete_record */\n\n  if (!rep_quick)\n  {\n    /* Get real path for data file */\n    if ((new_file= mysql_file_create(mi_key_file_datatmp,\n                                     fn_format(param->temp_filename,\n                                               share->data_file_name, \"\",\n                                               DATA_TMP_EXT, 2+4),\n                                     0, param->tmpfile_createflag,\n                                     MYF(0))) < 0)\n    {\n      mi_check_print_error(param,\"Can't create new tempfile: '%s'\",\n\t\t\t   param->temp_filename);\n      goto err;\n    }\n    if (new_header_length &&\n        filecopy(param, new_file,info->dfile,0L,new_header_length,\n\t\t \"datafile-header\"))\n      goto err;\n    if (param->testflag & T_UNPACK)\n    {\n      share->options&= ~HA_OPTION_COMPRESS_RECORD;\n      mi_int2store(share->state.header.options,share->options);\n    }\n    share->state.dellink= HA_OFFSET_ERROR;\n    info->rec_cache.file=new_file;\n  }\n\n  info->update= (short) (HA_STATE_CHANGED | HA_STATE_ROW_CHANGED);\n\n  /* Optionally drop indexes and optionally modify the key_map. */\n  mi_drop_all_indexes(param, info, FALSE);\n  key_map= share->state.key_map;\n  if (param->testflag & T_CREATE_MISSING_KEYS)\n  {\n    /* Invert the copied key_map to recreate all disabled indexes. */\n    key_map= ~key_map;\n  }\n\n  sort_info.info=info;\n  sort_info.param = param;\n\n  set_data_file_type(&sort_info, share);\n  sort_info.dupp=0;\n  sort_info.buff=0;\n  param->read_cache.end_of_file=sort_info.filelength=\n    mysql_file_seek(param->read_cache.file, 0L, MY_SEEK_END, MYF(0));\n\n  if (share->data_file_type == DYNAMIC_RECORD)\n    rec_length=max(share->base.min_pack_length+1,share->base.min_block_length);\n  else if (share->data_file_type == COMPRESSED_RECORD)\n    rec_length=share->base.min_block_length;\n  else\n    rec_length=share->base.pack_reclength;\n  /*\n    +1 below is required hack for parallel repair mode.\n    The info->state->records value, that is compared later\n    to sort_info.max_records and cannot exceed it, is\n    increased in sort_key_write. In mi_repair_by_sort, sort_key_write\n    is called after sort_key_read, where the comparison is performed,\n    but in parallel mode master thread can call sort_key_write\n    before some other repair thread calls sort_key_read.\n    Furthermore I'm not even sure +1 would be enough.\n    May be sort_info.max_records shold be always set to max value in\n    parallel mode.\n  */\n  sort_info.max_records=\n    ((param->testflag & T_CREATE_MISSING_KEYS) ? info->state->records + 1:\n     (ha_rows) (sort_info.filelength/rec_length+1));\n\n  del=info->state->del;\n  param->glob_crc=0;\n  /* for compressed tables */\n  max_pack_reclength= share->base.pack_reclength;\n  if (share->options & HA_OPTION_COMPRESS_RECORD)\n    set_if_bigger(max_pack_reclength, share->max_pack_length);\n  if (!(sort_param=(MI_SORT_PARAM *)\n        my_malloc((uint) share->base.keys *\n\t\t  (sizeof(MI_SORT_PARAM) + max_pack_reclength),\n\t\t  MYF(MY_ZEROFILL))))\n  {\n    mi_check_print_error(param,\"Not enough memory for key!\");\n    goto err;\n  }\n  total_key_length=0;\n  rec_per_key_part= param->rec_per_key_part;\n  info->state->records=info->state->del=share->state.split=0;\n  info->state->empty=0;\n\n  for (i=key=0, istep=1 ; key < share->base.keys ;\n       rec_per_key_part+=sort_param[i].keyinfo->keysegs, i+=istep, key++)\n  {\n    sort_param[i].key=key;\n    sort_param[i].keyinfo=share->keyinfo+key;\n    sort_param[i].seg=sort_param[i].keyinfo->seg;\n    /*\n      Skip this index if it is marked disabled in the copied\n      (and possibly inverted) key_map.\n    */\n    if (! mi_is_key_active(key_map, key))\n    {\n      /* Remember old statistics for key */\n      memcpy((char*) rec_per_key_part,\n\t     (char*) (share->state.rec_per_key_part+\n\t\t      (uint) (rec_per_key_part - param->rec_per_key_part)),\n\t     sort_param[i].keyinfo->keysegs*sizeof(*rec_per_key_part));\n      istep=0;\n      continue;\n    }\n    istep=1;\n    if ((!(param->testflag & T_SILENT)))\n      printf (\"- Fixing index %d\\n\",key+1);\n    if (sort_param[i].keyinfo->flag & HA_FULLTEXT)\n    {\n      sort_param[i].key_read=sort_ft_key_read;\n      sort_param[i].key_write=sort_ft_key_write;\n    }\n    else\n    {\n      sort_param[i].key_read=sort_key_read;\n      sort_param[i].key_write=sort_key_write;\n    }\n    sort_param[i].key_cmp=sort_key_cmp;\n    sort_param[i].lock_in_memory=lock_memory;\n    sort_param[i].tmpdir=param->tmpdir;\n    sort_param[i].sort_info=&sort_info;\n    sort_param[i].master=0;\n    sort_param[i].fix_datafile=0;\n    sort_param[i].calc_checksum= 0;\n\n    sort_param[i].filepos=new_header_length;\n    sort_param[i].max_pos=sort_param[i].pos=share->pack.header_length;\n\n    sort_param[i].record= (((uchar *)(sort_param+share->base.keys))+\n\t\t\t   (max_pack_reclength * i));\n    if (!mi_alloc_rec_buff(info, -1, &sort_param[i].rec_buff))\n    {\n      mi_check_print_error(param,\"Not enough memory!\");\n      goto err;\n    }\n\n    sort_param[i].key_length=share->rec_reflength;\n    for (keyseg=sort_param[i].seg; keyseg->type != HA_KEYTYPE_END;\n\t keyseg++)\n    {\n      sort_param[i].key_length+=keyseg->length;\n      if (keyseg->flag & HA_SPACE_PACK)\n        sort_param[i].key_length+=get_pack_length(keyseg->length);\n      if (keyseg->flag & (HA_BLOB_PART | HA_VAR_LENGTH_PART))\n        sort_param[i].key_length+=2 + test(keyseg->length >= 127);\n      if (keyseg->flag & HA_NULL_PART)\n        sort_param[i].key_length++;\n    }\n    total_key_length+=sort_param[i].key_length;\n\n    if (sort_param[i].keyinfo->flag & HA_FULLTEXT)\n    {\n      uint ft_max_word_len_for_sort=FT_MAX_WORD_LEN_FOR_SORT*\n                                    sort_param[i].keyinfo->seg->charset->mbmaxlen;\n      sort_param[i].key_length+=ft_max_word_len_for_sort-HA_FT_MAXBYTELEN;\n      init_alloc_root(&sort_param[i].wordroot, FTPARSER_MEMROOT_ALLOC_SIZE, 0);\n    }\n  }\n  sort_info.total_keys=i;\n  sort_param[0].master= 1;\n  sort_param[0].fix_datafile= (my_bool)(! rep_quick);\n  sort_param[0].calc_checksum= test(param->testflag & T_CALC_CHECKSUM);\n\n  if (!ftparser_alloc_param(info))\n    goto err;\n\n  sort_info.got_error=0;\n  mysql_mutex_lock(&sort_info.mutex);\n\n  /*\n    Initialize the I/O cache share for use with the read caches and, in\n    case of non-quick repair, the write cache. When all threads join on\n    the cache lock, the writer copies the write cache contents to the\n    read caches.\n  */\n  if (i > 1)\n  {\n    if (rep_quick)\n      init_io_cache_share(&param->read_cache, &io_share, NULL, i);\n    else\n      init_io_cache_share(&new_data_cache, &io_share, &info->rec_cache, i);\n  }\n  else\n    io_share.total_threads= 0; /* share not used */\n\n  (void) pthread_attr_init(&thr_attr);\n  (void) pthread_attr_setdetachstate(&thr_attr,PTHREAD_CREATE_DETACHED);\n\n  for (i=0 ; i < sort_info.total_keys ; i++)\n  {\n    /*\n      Copy the properly initialized IO_CACHE structure so that every\n      thread has its own copy. In quick mode param->read_cache is shared\n      for use by all threads. In non-quick mode all threads but the\n      first copy the shared new_data_cache, which is synchronized to the\n      write cache of the first thread. The first thread copies\n      param->read_cache, which is not shared.\n    */\n    sort_param[i].read_cache= ((rep_quick || !i) ? param->read_cache :\n                               new_data_cache);\n    DBUG_PRINT(\"io_cache_share\", (\"thread: %u  read_cache: 0x%lx\",\n                                  i, (long) &sort_param[i].read_cache));\n\n    /*\n      two approaches: the same amount of memory for each thread\n      or the memory for the same number of keys for each thread...\n      In the second one all the threads will fill their sort_buffers\n      (and call write_keys) at the same time, putting more stress on i/o.\n    */\n    sort_param[i].sortbuff_size=\n#ifndef USING_SECOND_APPROACH\n      param->sort_buffer_length/sort_info.total_keys;\n#else\n      param->sort_buffer_length*sort_param[i].key_length/total_key_length;\n#endif\n    if ((error= mysql_thread_create(mi_key_thread_find_all_keys,\n                                    &sort_param[i].thr, &thr_attr,\n                                    thr_find_all_keys,\n                                    (void *) (sort_param+i))))\n    {\n      mi_check_print_error(param,\"Cannot start a repair thread (errno= %d)\",\n                           error);\n      /* Cleanup: Detach from the share. Avoid others to be blocked. */\n      if (io_share.total_threads)\n        remove_io_thread(&sort_param[i].read_cache);\n      DBUG_PRINT(\"error\", (\"Cannot start a repair thread\"));\n      sort_info.got_error=1;\n    }\n    else\n      sort_info.threads_running++;\n  }\n  (void) pthread_attr_destroy(&thr_attr);\n\n  /* waiting for all threads to finish */\n  while (sort_info.threads_running)\n    mysql_cond_wait(&sort_info.cond, &sort_info.mutex);\n  mysql_mutex_unlock(&sort_info.mutex);\n\n  if ((got_error= thr_write_keys(sort_param)))\n  {\n    param->retry_repair=1;\n    goto err;\n  }\n  got_error=1;\t\t\t\t/* Assume the following may go wrong */\n\n  if (sort_param[0].fix_datafile)\n  {\n    /*\n      Append some nuls to the end of a memory mapped file. Destroy the\n      write cache. The master thread did already detach from the share\n      by remove_io_thread() in sort.c:thr_find_all_keys().\n    */\n    if (write_data_suffix(&sort_info,1) || end_io_cache(&info->rec_cache))\n      goto err;\n    if (param->testflag & T_SAFE_REPAIR)\n    {\n      /* Don't repair if we loosed more than one row */\n      if (info->state->records+1 < start_records)\n      {\n        info->state->records=start_records;\n        goto err;\n      }\n    }\n    share->state.state.data_file_length= info->state->data_file_length=\n      sort_param->filepos;\n    /* Only whole records */\n    share->state.version=(ulong) time((time_t*) 0);\n\n    /*\n      Exchange the data file descriptor of the table, so that we use the\n      new file from now on.\n     */\n    mysql_file_close(info->dfile, MYF(0));\n    info->dfile=new_file;\n\n    share->data_file_type=sort_info.new_data_file_type;\n    share->pack.header_length=(ulong) new_header_length;\n  }\n  else\n    info->state->data_file_length=sort_param->max_pos;\n\n  if (rep_quick && del+sort_info.dupp != info->state->del)\n  {\n    mi_check_print_error(param,\"Couldn't fix table with quick recovery: Found wrong number of deleted records\");\n    mi_check_print_error(param,\"Run recovery again without -q\");\n    param->retry_repair=1;\n    param->testflag|=T_RETRY_WITHOUT_QUICK;\n    goto err;\n  }\n\n  if (rep_quick & T_FORCE_UNIQUENESS)\n  {\n    my_off_t skr=info->state->data_file_length+\n      (share->options & HA_OPTION_COMPRESS_RECORD ?\n       MEMMAP_EXTRA_MARGIN : 0);\n#ifdef USE_RELOC\n    if (share->data_file_type == STATIC_RECORD &&\n\tskr < share->base.reloc*share->base.min_pack_length)\n      skr=share->base.reloc*share->base.min_pack_length;\n#endif\n    if (skr != sort_info.filelength)\n      if (mysql_file_chsize(info->dfile, skr, 0, MYF(0)))\n\tmi_check_print_warning(param,\n\t\t\t       \"Can't change size of datafile,  error: %d\",\n\t\t\t       my_errno);\n  }\n  if (param->testflag & T_CALC_CHECKSUM)\n    info->state->checksum=param->glob_crc;\n\n  if (mysql_file_chsize(share->kfile, info->state->key_file_length, 0, MYF(0)))\n    mi_check_print_warning(param,\n\t\t\t   \"Can't change size of indexfile, error: %d\", my_errno);\n\n  if (!(param->testflag & T_SILENT))\n  {\n    if (start_records != info->state->records)\n      printf(\"Data records: %s\\n\", llstr(info->state->records,llbuff));\n    if (sort_info.dupp)\n      mi_check_print_warning(param,\n\t\t\t     \"%s records have been removed\",\n\t\t\t     llstr(sort_info.dupp,llbuff));\n  }\n  got_error=0;\n\n  if (&share->state.state != info->state)\n    memcpy(&share->state.state, info->state, sizeof(*info->state));\n\nerr:\n  got_error|= flush_blocks(param, share->key_cache, share->kfile);\n  /*\n    Destroy the write cache. The master thread did already detach from\n    the share by remove_io_thread() or it was not yet started (if the\n    error happend before creating the thread).\n  */\n  (void) end_io_cache(&info->rec_cache);\n  /*\n    Destroy the new data cache in case of non-quick repair. All slave\n    threads did either detach from the share by remove_io_thread()\n    already or they were not yet started (if the error happend before\n    creating the threads).\n  */\n  if (!rep_quick)\n    (void) end_io_cache(&new_data_cache);\n  if (!got_error)\n  {\n    /* Replace the actual file with the temporary file */\n    if (new_file >= 0)\n    {\n      mysql_file_close(new_file, MYF(0));\n      info->dfile=new_file= -1;\n      if (change_to_newfile(share->data_file_name, MI_NAME_DEXT, DATA_TMP_EXT,\n\t\t\t    (param->testflag & T_BACKUP_DATA ?\n\t\t\t     MYF(MY_REDEL_MAKE_BACKUP): MYF(0))) ||\n\t  mi_open_datafile(info,share,name,-1))\n\tgot_error=1;\n    }\n  }\n  if (got_error)\n  {\n    if (! param->error_printed)\n      mi_check_print_error(param,\"%d when fixing table\",my_errno);\n    if (new_file >= 0)\n    {\n      (void) mysql_file_close(new_file, MYF(0));\n      (void) mysql_file_delete(mi_key_file_datatmp,\n                               param->temp_filename, MYF(MY_WME));\n      if (info->dfile == new_file) /* Retry with key cache */\n        if (unlikely(mi_open_datafile(info, share, name, -1)))\n          param->retry_repair= 0; /* Safety */\n    }\n    mi_mark_crashed_on_repair(info);\n  }\n  else if (key_map == share->state.key_map)\n    share->state.changed&= ~STATE_NOT_OPTIMIZED_KEYS;\n  share->state.changed|=STATE_NOT_SORTED_PAGES;\n\n  mysql_cond_destroy(&sort_info.cond);\n  mysql_mutex_destroy(&sort_info.mutex);\n  mysql_mutex_destroy(&param->print_msg_mutex);\n  param->need_print_msg_lock= 0;\n\n  my_free(sort_info.ft_buf);\n  my_free(sort_info.key_block);\n  my_free(sort_param);\n  my_free(sort_info.buff);\n  (void) end_io_cache(&param->read_cache);\n  info->opt_flag&= ~(READ_CACHE_USED | WRITE_CACHE_USED);\n  if (!got_error && (param->testflag & T_UNPACK))\n  {\n    share->state.header.options[0]&= (uchar) ~HA_OPTION_COMPRESS_RECORD;\n    share->pack.header_length=0;\n  }\n  DBUG_RETURN(got_error);\n}",
        "func": "int mi_repair_parallel(MI_CHECK *param, register MI_INFO *info,\n                       const char * name, int rep_quick, my_bool no_copy_stat)\n{\n  int got_error;\n  uint i,key, total_key_length, istep;\n  ulong rec_length;\n  ha_rows start_records;\n  my_off_t new_header_length,del;\n  File new_file;\n  MI_SORT_PARAM *sort_param=0;\n  MYISAM_SHARE *share=info->s;\n  ulong   *rec_per_key_part;\n  HA_KEYSEG *keyseg;\n  char llbuff[22];\n  IO_CACHE new_data_cache; /* For non-quick repair. */\n  IO_CACHE_SHARE io_share;\n  SORT_INFO sort_info;\n  ulonglong UNINIT_VAR(key_map);\n  pthread_attr_t thr_attr;\n  ulong max_pack_reclength;\n  int error;\n  DBUG_ENTER(\"mi_repair_parallel\");\n\n  start_records=info->state->records;\n  got_error=1;\n  new_file= -1;\n  new_header_length=(param->testflag & T_UNPACK) ? 0 :\n    share->pack.header_length;\n  if (!(param->testflag & T_SILENT))\n  {\n    printf(\"- parallel recovering (with sort) MyISAM-table '%s'\\n\",name);\n    printf(\"Data records: %s\\n\", llstr(start_records,llbuff));\n  }\n  param->testflag|=T_REP; /* for easy checking */\n\n  if (info->s->options & (HA_OPTION_CHECKSUM | HA_OPTION_COMPRESS_RECORD))\n    param->testflag|=T_CALC_CHECKSUM;\n\n  /*\n    Quick repair (not touching data file, rebuilding indexes):\n    {\n      Read  cache is (MI_CHECK *param)->read_cache using info->dfile.\n    }\n\n    Non-quick repair (rebuilding data file and indexes):\n    {\n      Master thread:\n\n        Read  cache is (MI_CHECK *param)->read_cache using info->dfile.\n        Write cache is (MI_INFO   *info)->rec_cache  using new_file.\n\n      Slave threads:\n\n        Read  cache is new_data_cache synced to master rec_cache.\n\n      The final assignment of the filedescriptor for rec_cache is done\n      after the cache creation.\n\n      Don't check file size on new_data_cache, as the resulting file size\n      is not known yet.\n\n      As rec_cache and new_data_cache are synced, write_buffer_length is\n      used for the read cache 'new_data_cache'. Both start at the same\n      position 'new_header_length'.\n    }\n  */\n  DBUG_PRINT(\"info\", (\"is quick repair: %d\", rep_quick));\n  bzero((char*)&sort_info,sizeof(sort_info));\n  /* Initialize pthread structures before goto err. */\n  mysql_mutex_init(mi_key_mutex_MI_SORT_INFO_mutex,\n                   &sort_info.mutex, MY_MUTEX_INIT_FAST);\n  mysql_cond_init(mi_key_cond_MI_SORT_INFO_cond, &sort_info.cond, 0);\n  mysql_mutex_init(mi_key_mutex_MI_CHECK_print_msg,\n                   &param->print_msg_mutex, MY_MUTEX_INIT_FAST);\n  param->need_print_msg_lock= 1;\n\n  if (!(sort_info.key_block=\n\talloc_key_blocks(param, (uint) param->sort_key_blocks,\n\t\t\t share->base.max_key_block_length)) ||\n      init_io_cache(&param->read_cache, info->dfile,\n                    (uint) param->read_buffer_length,\n                    READ_CACHE, share->pack.header_length, 1, MYF(MY_WME)) ||\n      (!rep_quick &&\n       (init_io_cache(&info->rec_cache, info->dfile,\n                      (uint) param->write_buffer_length,\n                      WRITE_CACHE, new_header_length, 1,\n                      MYF(MY_WME | MY_WAIT_IF_FULL) & param->myf_rw) ||\n        init_io_cache(&new_data_cache, -1,\n                      (uint) param->write_buffer_length,\n                      READ_CACHE, new_header_length, 1,\n                      MYF(MY_WME | MY_DONT_CHECK_FILESIZE)))))\n    goto err;\n  sort_info.key_block_end=sort_info.key_block+param->sort_key_blocks;\n  info->opt_flag|=WRITE_CACHE_USED;\n  info->rec_cache.file=info->dfile;         /* for sort_delete_record */\n\n  if (!rep_quick)\n  {\n    /* Get real path for data file */\n    if ((new_file= mysql_file_create(mi_key_file_datatmp,\n                                     fn_format(param->temp_filename,\n                                               share->data_file_name, \"\",\n                                               DATA_TMP_EXT, 2+4),\n                                     0, param->tmpfile_createflag,\n                                     MYF(0))) < 0)\n    {\n      mi_check_print_error(param,\"Can't create new tempfile: '%s'\",\n\t\t\t   param->temp_filename);\n      goto err;\n    }\n    if (new_header_length &&\n        filecopy(param, new_file,info->dfile,0L,new_header_length,\n\t\t \"datafile-header\"))\n      goto err;\n    if (param->testflag & T_UNPACK)\n    {\n      share->options&= ~HA_OPTION_COMPRESS_RECORD;\n      mi_int2store(share->state.header.options,share->options);\n    }\n    share->state.dellink= HA_OFFSET_ERROR;\n    info->rec_cache.file=new_file;\n  }\n\n  info->update= (short) (HA_STATE_CHANGED | HA_STATE_ROW_CHANGED);\n\n  /* Optionally drop indexes and optionally modify the key_map. */\n  mi_drop_all_indexes(param, info, FALSE);\n  key_map= share->state.key_map;\n  if (param->testflag & T_CREATE_MISSING_KEYS)\n  {\n    /* Invert the copied key_map to recreate all disabled indexes. */\n    key_map= ~key_map;\n  }\n\n  sort_info.info=info;\n  sort_info.param = param;\n\n  set_data_file_type(&sort_info, share);\n  sort_info.dupp=0;\n  sort_info.buff=0;\n  param->read_cache.end_of_file=sort_info.filelength=\n    mysql_file_seek(param->read_cache.file, 0L, MY_SEEK_END, MYF(0));\n\n  if (share->data_file_type == DYNAMIC_RECORD)\n    rec_length=max(share->base.min_pack_length+1,share->base.min_block_length);\n  else if (share->data_file_type == COMPRESSED_RECORD)\n    rec_length=share->base.min_block_length;\n  else\n    rec_length=share->base.pack_reclength;\n  /*\n    +1 below is required hack for parallel repair mode.\n    The info->state->records value, that is compared later\n    to sort_info.max_records and cannot exceed it, is\n    increased in sort_key_write. In mi_repair_by_sort, sort_key_write\n    is called after sort_key_read, where the comparison is performed,\n    but in parallel mode master thread can call sort_key_write\n    before some other repair thread calls sort_key_read.\n    Furthermore I'm not even sure +1 would be enough.\n    May be sort_info.max_records shold be always set to max value in\n    parallel mode.\n  */\n  sort_info.max_records=\n    ((param->testflag & T_CREATE_MISSING_KEYS) ? info->state->records + 1:\n     (ha_rows) (sort_info.filelength/rec_length+1));\n\n  del=info->state->del;\n  param->glob_crc=0;\n  /* for compressed tables */\n  max_pack_reclength= share->base.pack_reclength;\n  if (share->options & HA_OPTION_COMPRESS_RECORD)\n    set_if_bigger(max_pack_reclength, share->max_pack_length);\n  if (!(sort_param=(MI_SORT_PARAM *)\n        my_malloc((uint) share->base.keys *\n\t\t  (sizeof(MI_SORT_PARAM) + max_pack_reclength),\n\t\t  MYF(MY_ZEROFILL))))\n  {\n    mi_check_print_error(param,\"Not enough memory for key!\");\n    goto err;\n  }\n  total_key_length=0;\n  rec_per_key_part= param->rec_per_key_part;\n  info->state->records=info->state->del=share->state.split=0;\n  info->state->empty=0;\n\n  for (i=key=0, istep=1 ; key < share->base.keys ;\n       rec_per_key_part+=sort_param[i].keyinfo->keysegs, i+=istep, key++)\n  {\n    sort_param[i].key=key;\n    sort_param[i].keyinfo=share->keyinfo+key;\n    sort_param[i].seg=sort_param[i].keyinfo->seg;\n    /*\n      Skip this index if it is marked disabled in the copied\n      (and possibly inverted) key_map.\n    */\n    if (! mi_is_key_active(key_map, key))\n    {\n      /* Remember old statistics for key */\n      memcpy((char*) rec_per_key_part,\n\t     (char*) (share->state.rec_per_key_part+\n\t\t      (uint) (rec_per_key_part - param->rec_per_key_part)),\n\t     sort_param[i].keyinfo->keysegs*sizeof(*rec_per_key_part));\n      istep=0;\n      continue;\n    }\n    istep=1;\n    if ((!(param->testflag & T_SILENT)))\n      printf (\"- Fixing index %d\\n\",key+1);\n    if (sort_param[i].keyinfo->flag & HA_FULLTEXT)\n    {\n      sort_param[i].key_read=sort_ft_key_read;\n      sort_param[i].key_write=sort_ft_key_write;\n    }\n    else\n    {\n      sort_param[i].key_read=sort_key_read;\n      sort_param[i].key_write=sort_key_write;\n    }\n    sort_param[i].key_cmp=sort_key_cmp;\n    sort_param[i].lock_in_memory=lock_memory;\n    sort_param[i].tmpdir=param->tmpdir;\n    sort_param[i].sort_info=&sort_info;\n    sort_param[i].master=0;\n    sort_param[i].fix_datafile=0;\n    sort_param[i].calc_checksum= 0;\n\n    sort_param[i].filepos=new_header_length;\n    sort_param[i].max_pos=sort_param[i].pos=share->pack.header_length;\n\n    sort_param[i].record= (((uchar *)(sort_param+share->base.keys))+\n\t\t\t   (max_pack_reclength * i));\n    if (!mi_alloc_rec_buff(info, -1, &sort_param[i].rec_buff))\n    {\n      mi_check_print_error(param,\"Not enough memory!\");\n      goto err;\n    }\n\n    sort_param[i].key_length=share->rec_reflength;\n    for (keyseg=sort_param[i].seg; keyseg->type != HA_KEYTYPE_END;\n\t keyseg++)\n    {\n      sort_param[i].key_length+=keyseg->length;\n      if (keyseg->flag & HA_SPACE_PACK)\n        sort_param[i].key_length+=get_pack_length(keyseg->length);\n      if (keyseg->flag & (HA_BLOB_PART | HA_VAR_LENGTH_PART))\n        sort_param[i].key_length+=2 + test(keyseg->length >= 127);\n      if (keyseg->flag & HA_NULL_PART)\n        sort_param[i].key_length++;\n    }\n    total_key_length+=sort_param[i].key_length;\n\n    if (sort_param[i].keyinfo->flag & HA_FULLTEXT)\n    {\n      uint ft_max_word_len_for_sort=FT_MAX_WORD_LEN_FOR_SORT*\n                                    sort_param[i].keyinfo->seg->charset->mbmaxlen;\n      sort_param[i].key_length+=ft_max_word_len_for_sort-HA_FT_MAXBYTELEN;\n      init_alloc_root(&sort_param[i].wordroot, FTPARSER_MEMROOT_ALLOC_SIZE, 0);\n    }\n  }\n  sort_info.total_keys=i;\n  sort_param[0].master= 1;\n  sort_param[0].fix_datafile= (my_bool)(! rep_quick);\n  sort_param[0].calc_checksum= test(param->testflag & T_CALC_CHECKSUM);\n\n  if (!ftparser_alloc_param(info))\n    goto err;\n\n  sort_info.got_error=0;\n  mysql_mutex_lock(&sort_info.mutex);\n\n  /*\n    Initialize the I/O cache share for use with the read caches and, in\n    case of non-quick repair, the write cache. When all threads join on\n    the cache lock, the writer copies the write cache contents to the\n    read caches.\n  */\n  if (i > 1)\n  {\n    if (rep_quick)\n      init_io_cache_share(&param->read_cache, &io_share, NULL, i);\n    else\n      init_io_cache_share(&new_data_cache, &io_share, &info->rec_cache, i);\n  }\n  else\n    io_share.total_threads= 0; /* share not used */\n\n  (void) pthread_attr_init(&thr_attr);\n  (void) pthread_attr_setdetachstate(&thr_attr,PTHREAD_CREATE_DETACHED);\n\n  for (i=0 ; i < sort_info.total_keys ; i++)\n  {\n    /*\n      Copy the properly initialized IO_CACHE structure so that every\n      thread has its own copy. In quick mode param->read_cache is shared\n      for use by all threads. In non-quick mode all threads but the\n      first copy the shared new_data_cache, which is synchronized to the\n      write cache of the first thread. The first thread copies\n      param->read_cache, which is not shared.\n    */\n    sort_param[i].read_cache= ((rep_quick || !i) ? param->read_cache :\n                               new_data_cache);\n    DBUG_PRINT(\"io_cache_share\", (\"thread: %u  read_cache: 0x%lx\",\n                                  i, (long) &sort_param[i].read_cache));\n\n    /*\n      two approaches: the same amount of memory for each thread\n      or the memory for the same number of keys for each thread...\n      In the second one all the threads will fill their sort_buffers\n      (and call write_keys) at the same time, putting more stress on i/o.\n    */\n    sort_param[i].sortbuff_size=\n#ifndef USING_SECOND_APPROACH\n      param->sort_buffer_length/sort_info.total_keys;\n#else\n      param->sort_buffer_length*sort_param[i].key_length/total_key_length;\n#endif\n    if ((error= mysql_thread_create(mi_key_thread_find_all_keys,\n                                    &sort_param[i].thr, &thr_attr,\n                                    thr_find_all_keys,\n                                    (void *) (sort_param+i))))\n    {\n      mi_check_print_error(param,\"Cannot start a repair thread (errno= %d)\",\n                           error);\n      /* Cleanup: Detach from the share. Avoid others to be blocked. */\n      if (io_share.total_threads)\n        remove_io_thread(&sort_param[i].read_cache);\n      DBUG_PRINT(\"error\", (\"Cannot start a repair thread\"));\n      sort_info.got_error=1;\n    }\n    else\n      sort_info.threads_running++;\n  }\n  (void) pthread_attr_destroy(&thr_attr);\n\n  /* waiting for all threads to finish */\n  while (sort_info.threads_running)\n    mysql_cond_wait(&sort_info.cond, &sort_info.mutex);\n  mysql_mutex_unlock(&sort_info.mutex);\n\n  if ((got_error= thr_write_keys(sort_param)))\n  {\n    param->retry_repair=1;\n    goto err;\n  }\n  got_error=1;\t\t\t\t/* Assume the following may go wrong */\n\n  if (sort_param[0].fix_datafile)\n  {\n    /*\n      Append some nuls to the end of a memory mapped file. Destroy the\n      write cache. The master thread did already detach from the share\n      by remove_io_thread() in sort.c:thr_find_all_keys().\n    */\n    if (write_data_suffix(&sort_info,1) || end_io_cache(&info->rec_cache))\n      goto err;\n    if (param->testflag & T_SAFE_REPAIR)\n    {\n      /* Don't repair if we loosed more than one row */\n      if (info->state->records+1 < start_records)\n      {\n        info->state->records=start_records;\n        goto err;\n      }\n    }\n    share->state.state.data_file_length= info->state->data_file_length=\n      sort_param->filepos;\n    /* Only whole records */\n    share->state.version=(ulong) time((time_t*) 0);\n\n    /*\n      Exchange the data file descriptor of the table, so that we use the\n      new file from now on.\n     */\n    mysql_file_close(info->dfile, MYF(0));\n    info->dfile=new_file;\n\n    share->data_file_type=sort_info.new_data_file_type;\n    share->pack.header_length=(ulong) new_header_length;\n  }\n  else\n    info->state->data_file_length=sort_param->max_pos;\n\n  if (rep_quick && del+sort_info.dupp != info->state->del)\n  {\n    mi_check_print_error(param,\"Couldn't fix table with quick recovery: Found wrong number of deleted records\");\n    mi_check_print_error(param,\"Run recovery again without -q\");\n    param->retry_repair=1;\n    param->testflag|=T_RETRY_WITHOUT_QUICK;\n    goto err;\n  }\n\n  if (rep_quick & T_FORCE_UNIQUENESS)\n  {\n    my_off_t skr=info->state->data_file_length+\n      (share->options & HA_OPTION_COMPRESS_RECORD ?\n       MEMMAP_EXTRA_MARGIN : 0);\n#ifdef USE_RELOC\n    if (share->data_file_type == STATIC_RECORD &&\n\tskr < share->base.reloc*share->base.min_pack_length)\n      skr=share->base.reloc*share->base.min_pack_length;\n#endif\n    if (skr != sort_info.filelength)\n      if (mysql_file_chsize(info->dfile, skr, 0, MYF(0)))\n\tmi_check_print_warning(param,\n\t\t\t       \"Can't change size of datafile,  error: %d\",\n\t\t\t       my_errno);\n  }\n  if (param->testflag & T_CALC_CHECKSUM)\n    info->state->checksum=param->glob_crc;\n\n  if (mysql_file_chsize(share->kfile, info->state->key_file_length, 0, MYF(0)))\n    mi_check_print_warning(param,\n\t\t\t   \"Can't change size of indexfile, error: %d\", my_errno);\n\n  if (!(param->testflag & T_SILENT))\n  {\n    if (start_records != info->state->records)\n      printf(\"Data records: %s\\n\", llstr(info->state->records,llbuff));\n    if (sort_info.dupp)\n      mi_check_print_warning(param,\n\t\t\t     \"%s records have been removed\",\n\t\t\t     llstr(sort_info.dupp,llbuff));\n  }\n  got_error=0;\n\n  if (&share->state.state != info->state)\n    memcpy(&share->state.state, info->state, sizeof(*info->state));\n\nerr:\n  got_error|= flush_blocks(param, share->key_cache, share->kfile);\n  /*\n    Destroy the write cache. The master thread did already detach from\n    the share by remove_io_thread() or it was not yet started (if the\n    error happend before creating the thread).\n  */\n  (void) end_io_cache(&info->rec_cache);\n  /*\n    Destroy the new data cache in case of non-quick repair. All slave\n    threads did either detach from the share by remove_io_thread()\n    already or they were not yet started (if the error happend before\n    creating the threads).\n  */\n  if (!rep_quick)\n    (void) end_io_cache(&new_data_cache);\n  if (!got_error)\n  {\n    /* Replace the actual file with the temporary file */\n    if (new_file >= 0)\n    {\n      myf flags= 0;\n      if (param->testflag & T_BACKUP_DATA)\n        flags |= MY_REDEL_MAKE_BACKUP;\n      if (no_copy_stat)\n        flags |= MY_REDEL_NO_COPY_STAT;\n      mysql_file_close(new_file, MYF(0));\n      info->dfile=new_file= -1;\n      if (change_to_newfile(share->data_file_name, MI_NAME_DEXT, DATA_TMP_EXT,\n\t\t\t    flags) ||\n\t  mi_open_datafile(info,share,name,-1))\n\tgot_error=1;\n    }\n  }\n  if (got_error)\n  {\n    if (! param->error_printed)\n      mi_check_print_error(param,\"%d when fixing table\",my_errno);\n    if (new_file >= 0)\n    {\n      (void) mysql_file_close(new_file, MYF(0));\n      (void) mysql_file_delete(mi_key_file_datatmp,\n                               param->temp_filename, MYF(MY_WME));\n      if (info->dfile == new_file) /* Retry with key cache */\n        if (unlikely(mi_open_datafile(info, share, name, -1)))\n          param->retry_repair= 0; /* Safety */\n    }\n    mi_mark_crashed_on_repair(info);\n  }\n  else if (key_map == share->state.key_map)\n    share->state.changed&= ~STATE_NOT_OPTIMIZED_KEYS;\n  share->state.changed|=STATE_NOT_SORTED_PAGES;\n\n  mysql_cond_destroy(&sort_info.cond);\n  mysql_mutex_destroy(&sort_info.mutex);\n  mysql_mutex_destroy(&param->print_msg_mutex);\n  param->need_print_msg_lock= 0;\n\n  my_free(sort_info.ft_buf);\n  my_free(sort_info.key_block);\n  my_free(sort_param);\n  my_free(sort_info.buff);\n  (void) end_io_cache(&param->read_cache);\n  info->opt_flag&= ~(READ_CACHE_USED | WRITE_CACHE_USED);\n  if (!got_error && (param->testflag & T_UNPACK))\n  {\n    share->state.header.options[0]&= (uchar) ~HA_OPTION_COMPRESS_RECORD;\n    share->pack.header_length=0;\n  }\n  DBUG_RETURN(got_error);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n int mi_repair_parallel(MI_CHECK *param, register MI_INFO *info,\n-\t\t\tconst char * name, int rep_quick)\n+                       const char * name, int rep_quick, my_bool no_copy_stat)\n {\n   int got_error;\n   uint i,key, total_key_length, istep;\n@@ -446,11 +446,15 @@\n     /* Replace the actual file with the temporary file */\n     if (new_file >= 0)\n     {\n+      myf flags= 0;\n+      if (param->testflag & T_BACKUP_DATA)\n+        flags |= MY_REDEL_MAKE_BACKUP;\n+      if (no_copy_stat)\n+        flags |= MY_REDEL_NO_COPY_STAT;\n       mysql_file_close(new_file, MYF(0));\n       info->dfile=new_file= -1;\n       if (change_to_newfile(share->data_file_name, MI_NAME_DEXT, DATA_TMP_EXT,\n-\t\t\t    (param->testflag & T_BACKUP_DATA ?\n-\t\t\t     MYF(MY_REDEL_MAKE_BACKUP): MYF(0))) ||\n+\t\t\t    flags) ||\n \t  mi_open_datafile(info,share,name,-1))\n \tgot_error=1;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tconst char * name, int rep_quick)",
                "\t\t\t    (param->testflag & T_BACKUP_DATA ?",
                "\t\t\t     MYF(MY_REDEL_MAKE_BACKUP): MYF(0))) ||"
            ],
            "added_lines": [
                "                       const char * name, int rep_quick, my_bool no_copy_stat)",
                "      myf flags= 0;",
                "      if (param->testflag & T_BACKUP_DATA)",
                "        flags |= MY_REDEL_MAKE_BACKUP;",
                "      if (no_copy_stat)",
                "        flags |= MY_REDEL_NO_COPY_STAT;",
                "\t\t\t    flags) ||"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6663",
        "func_name": "mysql/mysql-server/mi_repair",
        "description": "Race condition in Oracle MySQL before 5.5.52, 5.6.x before 5.6.33, 5.7.x before 5.7.15, and 8.x before 8.0.1; MariaDB before 5.5.52, 10.0.x before 10.0.28, and 10.1.x before 10.1.18; Percona Server before 5.5.51-38.2, 5.6.x before 5.6.32-78-1, and 5.7.x before 5.7.14-8; and Percona XtraDB Cluster before 5.5.41-37.0, 5.6.x before 5.6.32-25.17, and 5.7.x before 5.7.14-26.17 allows local users with certain permissions to gain privileges by leveraging use of my_copystat by REPAIR TABLE to repair a MyISAM table.",
        "git_url": "https://github.com/mysql/mysql-server/commit/4e5473862e6852b0f3802b0cd0c6fa10b5253291",
        "commit_title": "Bug#24388746: PRIVILEGE ESCALATION AND RACE CONDITION USING CREATE TABLE",
        "commit_text": " During REPAIR TABLE of a MyISAM table, a temporary data file (.TMD) is created. When repair finishes, this file is renamed to the original .MYD file. The problem was that during this rename, we copied the stats from the old file to the new file with chmod/chown. If a user managed to replace the temporary file before chmod/chown was executed, it was possible to get an arbitrary file with the privileges of the mysql user.  This patch fixes the problem by not copying stats from the old file to the new file. This is not needed as the new file was created with the correct stats. This fix only changes server behavior - external utilities such as myisamchk still does chmod/chown.  No test case provided since the problem involves synchronization with file system operations.",
        "func_before": "int mi_repair(MI_CHECK *param, register MI_INFO *info,\n\t      char * name, int rep_quick)\n{\n  int error,got_error;\n  ha_rows start_records,new_header_length;\n  my_off_t del;\n  File new_file;\n  MYISAM_SHARE *share=info->s;\n  char llbuff[22],llbuff2[22];\n  SORT_INFO sort_info;\n  MI_SORT_PARAM sort_param;\n  DBUG_ENTER(\"mi_repair\");\n\n  bzero((char *)&sort_info, sizeof(sort_info));\n  bzero((char *)&sort_param, sizeof(sort_param));\n  start_records=info->state->records;\n  new_header_length=(param->testflag & T_UNPACK) ? 0L :\n    share->pack.header_length;\n  got_error=1;\n  new_file= -1;\n  sort_param.sort_info=&sort_info;\n\n  if (!(param->testflag & T_SILENT))\n  {\n    printf(\"- recovering (with keycache) MyISAM-table '%s'\\n\",name);\n    printf(\"Data records: %s\\n\", llstr(info->state->records,llbuff));\n  }\n  param->testflag|=T_REP; /* for easy checking */\n\n  if (info->s->options & (HA_OPTION_CHECKSUM | HA_OPTION_COMPRESS_RECORD))\n    param->testflag|=T_CALC_CHECKSUM;\n\n  DBUG_ASSERT(param->use_buffers < SIZE_T_MAX);\n\n  if (!param->using_global_keycache)\n    (void) init_key_cache(dflt_key_cache, param->key_cache_block_size,\n                        param->use_buffers, 0, 0);\n\n  if (init_io_cache(&param->read_cache,info->dfile,\n\t\t    (uint) param->read_buffer_length,\n\t\t    READ_CACHE,share->pack.header_length,1,MYF(MY_WME)))\n  {\n    bzero(&info->rec_cache,sizeof(info->rec_cache));\n    goto err;\n  }\n  if (!rep_quick)\n    if (init_io_cache(&info->rec_cache,-1,(uint) param->write_buffer_length,\n\t\t      WRITE_CACHE, new_header_length, 1,\n\t\t      MYF(MY_WME | MY_WAIT_IF_FULL)))\n      goto err;\n  info->opt_flag|=WRITE_CACHE_USED;\n  if (!mi_alloc_rec_buff(info, -1, &sort_param.record) ||\n      !mi_alloc_rec_buff(info, -1, &sort_param.rec_buff))\n  {\n    mi_check_print_error(param, \"Not enough memory for extra record\");\n    goto err;\n  }\n\n  if (!rep_quick)\n  {\n    /* Get real path for data file */\n    if ((new_file= mysql_file_create(mi_key_file_datatmp,\n                                     fn_format(param->temp_filename,\n                                               share->data_file_name, \"\",\n                                               DATA_TMP_EXT, 2+4),\n                                     0, param->tmpfile_createflag,\n                                     MYF(0))) < 0)\n    {\n      mi_check_print_error(param,\"Can't create new tempfile: '%s'\",\n\t\t\t   param->temp_filename);\n      goto err;\n    }\n    if (new_header_length &&\n        filecopy(param,new_file,info->dfile,0L,new_header_length,\n\t\t \"datafile-header\"))\n      goto err;\n    info->s->state.dellink= HA_OFFSET_ERROR;\n    info->rec_cache.file=new_file;\n    if (param->testflag & T_UNPACK)\n    {\n      share->options&= ~HA_OPTION_COMPRESS_RECORD;\n      mi_int2store(share->state.header.options,share->options);\n    }\n  }\n  sort_info.info=info;\n  sort_info.param = param;\n  sort_param.read_cache=param->read_cache;\n  sort_param.pos=sort_param.max_pos=share->pack.header_length;\n  sort_param.filepos=new_header_length;\n  param->read_cache.end_of_file=sort_info.filelength=\n    mysql_file_seek(info->dfile, 0L, MY_SEEK_END, MYF(0));\n  sort_info.dupp=0;\n  sort_param.fix_datafile= (my_bool) (! rep_quick);\n  sort_param.master=1;\n  sort_info.max_records= ~(ha_rows) 0;\n\n  set_data_file_type(&sort_info, share);\n  del=info->state->del;\n  info->state->records=info->state->del=share->state.split=0;\n  info->state->empty=0;\n  param->glob_crc=0;\n  if (param->testflag & T_CALC_CHECKSUM)\n    sort_param.calc_checksum= 1;\n\n  info->update= (short) (HA_STATE_CHANGED | HA_STATE_ROW_CHANGED);\n\n  /* This function always recreates all enabled indexes. */\n  if (param->testflag & T_CREATE_MISSING_KEYS)\n    mi_set_all_keys_active(share->state.key_map, share->base.keys);\n  mi_drop_all_indexes(param, info, TRUE);\n\n  lock_memory(param);\t\t\t/* Everything is alloced */\n\n  /* Re-create all keys, which are set in key_map. */\n  while (!(error=sort_get_next_record(&sort_param)))\n  {\n    if (writekeys(&sort_param))\n    {\n      if (my_errno != HA_ERR_FOUND_DUPP_KEY)\n\tgoto err;\n      DBUG_DUMP(\"record\",(uchar*) sort_param.record,share->base.pack_reclength);\n      mi_check_print_info(param,\"Duplicate key %2d for record at %10s against new record at %10s\",\n\t\t\t  info->errkey+1,\n\t\t\t  llstr(sort_param.start_recpos,llbuff),\n\t\t\t  llstr(info->dupp_key_pos,llbuff2));\n      if (param->testflag & T_VERBOSE)\n      {\n\t(void) _mi_make_key(info,(uint) info->errkey,info->lastkey,\n\t\t\t  sort_param.record,0L);\n\t_mi_print_key(stdout,share->keyinfo[info->errkey].seg,info->lastkey,\n\t\t      USE_WHOLE_KEY);\n      }\n      sort_info.dupp++;\n      if ((param->testflag & (T_FORCE_UNIQUENESS|T_QUICK)) == T_QUICK)\n      {\n        param->testflag|=T_RETRY_WITHOUT_QUICK;\n\tparam->error_printed=1;\n\tgoto err;\n      }\n      continue;\n    }\n    if (sort_write_record(&sort_param))\n      goto err;\n  }\n  if (error > 0 || write_data_suffix(&sort_info, (my_bool)!rep_quick) ||\n      flush_io_cache(&info->rec_cache) || param->read_cache.error < 0)\n    goto err;\n\n  if (param->testflag & T_WRITE_LOOP)\n  {\n    (void) fputs(\"          \\r\",stdout); (void) fflush(stdout);\n  }\n  if (mysql_file_chsize(share->kfile, info->state->key_file_length, 0, MYF(0)))\n  {\n    mi_check_print_warning(param,\n\t\t\t   \"Can't change size of indexfile, error: %d\",\n\t\t\t   my_errno);\n    goto err;\n  }\n\n  if (rep_quick && del+sort_info.dupp != info->state->del)\n  {\n    mi_check_print_error(param,\"Couldn't fix table with quick recovery: Found wrong number of deleted records\");\n    mi_check_print_error(param,\"Run recovery again without -q\");\n    got_error=1;\n    param->retry_repair=1;\n    param->testflag|=T_RETRY_WITHOUT_QUICK;\n    goto err;\n  }\n  if (param->testflag & T_SAFE_REPAIR)\n  {\n    /* Don't repair if we loosed more than one row */\n    if (info->state->records+1 < start_records)\n    {\n      info->state->records=start_records;\n      got_error=1;\n      goto err;\n    }\n  }\n\n  if (!rep_quick)\n  {\n    mysql_file_close(info->dfile, MYF(0));\n    info->dfile=new_file;\n    info->state->data_file_length=sort_param.filepos;\n    share->state.version=(ulong) time((time_t*) 0);\t/* Force reopen */\n  }\n  else\n  {\n    info->state->data_file_length=sort_param.max_pos;\n  }\n  if (param->testflag & T_CALC_CHECKSUM)\n    info->state->checksum=param->glob_crc;\n\n  if (!(param->testflag & T_SILENT))\n  {\n    if (start_records != info->state->records)\n      printf(\"Data records: %s\\n\", llstr(info->state->records,llbuff));\n    if (sort_info.dupp)\n      mi_check_print_warning(param,\n\t\t\t     \"%s records have been removed\",\n\t\t\t     llstr(sort_info.dupp,llbuff));\n  }\n\n  got_error=0;\n  /* If invoked by external program that uses thr_lock */\n  if (&share->state.state != info->state)\n    memcpy( &share->state.state, info->state, sizeof(*info->state));\n\nerr:\n  if (!got_error)\n  {\n    /* Replace the actual file with the temporary file */\n    if (new_file >= 0)\n    {\n      mysql_file_close(new_file, MYF(0));\n      info->dfile=new_file= -1;\n      /*\n        On Windows, the old data file cannot be deleted if it is either\n        open, or memory mapped. Closing the file won't remove the memory\n        map implicilty on Windows. We closed the data file, but we keep\n        the MyISAM table open. A memory map will be closed on the final\n        mi_close() only. So we need to unmap explicitly here. After\n        renaming the new file under the hook, we couldn't use the map of\n        the old file any more anyway.\n      */\n      if (info->s->file_map)\n      {\n        (void) my_munmap((char*) info->s->file_map,\n                         (size_t) info->s->mmaped_length);\n        info->s->file_map= NULL;\n      }\n      if (change_to_newfile(share->data_file_name, MI_NAME_DEXT, DATA_TMP_EXT,\n\t\t\t    (param->testflag & T_BACKUP_DATA ?\n\t\t\t     MYF(MY_REDEL_MAKE_BACKUP): MYF(0))) ||\n\t  mi_open_datafile(info,share,name,-1))\n\tgot_error=1;\n\n      param->retry_repair= 0;\n    }\n  }\n  if (got_error)\n  {\n    if (! param->error_printed)\n      mi_check_print_error(param,\"%d for record at pos %s\",my_errno,\n\t\t  llstr(sort_param.start_recpos,llbuff));\n    if (new_file >= 0)\n    {\n      (void) mysql_file_close(new_file, MYF(0));\n      (void) mysql_file_delete(mi_key_file_datatmp,\n                               param->temp_filename, MYF(MY_WME));\n      info->rec_cache.file=-1; /* don't flush data to new_file, it's closed */\n    }\n    mi_mark_crashed_on_repair(info);\n  }\n  my_free(mi_get_rec_buff_ptr(info, sort_param.rec_buff));\n  my_free(mi_get_rec_buff_ptr(info, sort_param.record));\n  my_free(sort_info.buff);\n  (void) end_io_cache(&param->read_cache);\n  info->opt_flag&= ~(READ_CACHE_USED | WRITE_CACHE_USED);\n  (void) end_io_cache(&info->rec_cache);\n  got_error|=flush_blocks(param, share->key_cache, share->kfile);\n  if (!got_error && param->testflag & T_UNPACK)\n  {\n    share->state.header.options[0]&= (uchar) ~HA_OPTION_COMPRESS_RECORD;\n    share->pack.header_length=0;\n    share->data_file_type=sort_info.new_data_file_type;\n  }\n  share->state.changed|= (STATE_NOT_OPTIMIZED_KEYS | STATE_NOT_SORTED_PAGES |\n\t\t\t  STATE_NOT_ANALYZED);\n  DBUG_RETURN(got_error);\n}",
        "func": "int mi_repair(MI_CHECK *param, register MI_INFO *info,\n\t      char * name, int rep_quick, my_bool no_copy_stat)\n{\n  int error,got_error;\n  ha_rows start_records,new_header_length;\n  my_off_t del;\n  File new_file;\n  MYISAM_SHARE *share=info->s;\n  char llbuff[22],llbuff2[22];\n  SORT_INFO sort_info;\n  MI_SORT_PARAM sort_param;\n  DBUG_ENTER(\"mi_repair\");\n\n  bzero((char *)&sort_info, sizeof(sort_info));\n  bzero((char *)&sort_param, sizeof(sort_param));\n  start_records=info->state->records;\n  new_header_length=(param->testflag & T_UNPACK) ? 0L :\n    share->pack.header_length;\n  got_error=1;\n  new_file= -1;\n  sort_param.sort_info=&sort_info;\n\n  if (!(param->testflag & T_SILENT))\n  {\n    printf(\"- recovering (with keycache) MyISAM-table '%s'\\n\",name);\n    printf(\"Data records: %s\\n\", llstr(info->state->records,llbuff));\n  }\n  param->testflag|=T_REP; /* for easy checking */\n\n  if (info->s->options & (HA_OPTION_CHECKSUM | HA_OPTION_COMPRESS_RECORD))\n    param->testflag|=T_CALC_CHECKSUM;\n\n  DBUG_ASSERT(param->use_buffers < SIZE_T_MAX);\n\n  if (!param->using_global_keycache)\n    (void) init_key_cache(dflt_key_cache, param->key_cache_block_size,\n                        param->use_buffers, 0, 0);\n\n  if (init_io_cache(&param->read_cache,info->dfile,\n\t\t    (uint) param->read_buffer_length,\n\t\t    READ_CACHE,share->pack.header_length,1,MYF(MY_WME)))\n  {\n    bzero(&info->rec_cache,sizeof(info->rec_cache));\n    goto err;\n  }\n  if (!rep_quick)\n    if (init_io_cache(&info->rec_cache,-1,(uint) param->write_buffer_length,\n\t\t      WRITE_CACHE, new_header_length, 1,\n\t\t      MYF(MY_WME | MY_WAIT_IF_FULL)))\n      goto err;\n  info->opt_flag|=WRITE_CACHE_USED;\n  if (!mi_alloc_rec_buff(info, -1, &sort_param.record) ||\n      !mi_alloc_rec_buff(info, -1, &sort_param.rec_buff))\n  {\n    mi_check_print_error(param, \"Not enough memory for extra record\");\n    goto err;\n  }\n\n  if (!rep_quick)\n  {\n    /* Get real path for data file */\n    if ((new_file= mysql_file_create(mi_key_file_datatmp,\n                                     fn_format(param->temp_filename,\n                                               share->data_file_name, \"\",\n                                               DATA_TMP_EXT, 2+4),\n                                     0, param->tmpfile_createflag,\n                                     MYF(0))) < 0)\n    {\n      mi_check_print_error(param,\"Can't create new tempfile: '%s'\",\n\t\t\t   param->temp_filename);\n      goto err;\n    }\n    if (new_header_length &&\n        filecopy(param,new_file,info->dfile,0L,new_header_length,\n\t\t \"datafile-header\"))\n      goto err;\n    info->s->state.dellink= HA_OFFSET_ERROR;\n    info->rec_cache.file=new_file;\n    if (param->testflag & T_UNPACK)\n    {\n      share->options&= ~HA_OPTION_COMPRESS_RECORD;\n      mi_int2store(share->state.header.options,share->options);\n    }\n  }\n  sort_info.info=info;\n  sort_info.param = param;\n  sort_param.read_cache=param->read_cache;\n  sort_param.pos=sort_param.max_pos=share->pack.header_length;\n  sort_param.filepos=new_header_length;\n  param->read_cache.end_of_file=sort_info.filelength=\n    mysql_file_seek(info->dfile, 0L, MY_SEEK_END, MYF(0));\n  sort_info.dupp=0;\n  sort_param.fix_datafile= (my_bool) (! rep_quick);\n  sort_param.master=1;\n  sort_info.max_records= ~(ha_rows) 0;\n\n  set_data_file_type(&sort_info, share);\n  del=info->state->del;\n  info->state->records=info->state->del=share->state.split=0;\n  info->state->empty=0;\n  param->glob_crc=0;\n  if (param->testflag & T_CALC_CHECKSUM)\n    sort_param.calc_checksum= 1;\n\n  info->update= (short) (HA_STATE_CHANGED | HA_STATE_ROW_CHANGED);\n\n  /* This function always recreates all enabled indexes. */\n  if (param->testflag & T_CREATE_MISSING_KEYS)\n    mi_set_all_keys_active(share->state.key_map, share->base.keys);\n  mi_drop_all_indexes(param, info, TRUE);\n\n  lock_memory(param);\t\t\t/* Everything is alloced */\n\n  /* Re-create all keys, which are set in key_map. */\n  while (!(error=sort_get_next_record(&sort_param)))\n  {\n    if (writekeys(&sort_param))\n    {\n      if (my_errno != HA_ERR_FOUND_DUPP_KEY)\n\tgoto err;\n      DBUG_DUMP(\"record\",(uchar*) sort_param.record,share->base.pack_reclength);\n      mi_check_print_info(param,\"Duplicate key %2d for record at %10s against new record at %10s\",\n\t\t\t  info->errkey+1,\n\t\t\t  llstr(sort_param.start_recpos,llbuff),\n\t\t\t  llstr(info->dupp_key_pos,llbuff2));\n      if (param->testflag & T_VERBOSE)\n      {\n\t(void) _mi_make_key(info,(uint) info->errkey,info->lastkey,\n\t\t\t  sort_param.record,0L);\n\t_mi_print_key(stdout,share->keyinfo[info->errkey].seg,info->lastkey,\n\t\t      USE_WHOLE_KEY);\n      }\n      sort_info.dupp++;\n      if ((param->testflag & (T_FORCE_UNIQUENESS|T_QUICK)) == T_QUICK)\n      {\n        param->testflag|=T_RETRY_WITHOUT_QUICK;\n\tparam->error_printed=1;\n\tgoto err;\n      }\n      continue;\n    }\n    if (sort_write_record(&sort_param))\n      goto err;\n  }\n  if (error > 0 || write_data_suffix(&sort_info, (my_bool)!rep_quick) ||\n      flush_io_cache(&info->rec_cache) || param->read_cache.error < 0)\n    goto err;\n\n  if (param->testflag & T_WRITE_LOOP)\n  {\n    (void) fputs(\"          \\r\",stdout); (void) fflush(stdout);\n  }\n  if (mysql_file_chsize(share->kfile, info->state->key_file_length, 0, MYF(0)))\n  {\n    mi_check_print_warning(param,\n\t\t\t   \"Can't change size of indexfile, error: %d\",\n\t\t\t   my_errno);\n    goto err;\n  }\n\n  if (rep_quick && del+sort_info.dupp != info->state->del)\n  {\n    mi_check_print_error(param,\"Couldn't fix table with quick recovery: Found wrong number of deleted records\");\n    mi_check_print_error(param,\"Run recovery again without -q\");\n    got_error=1;\n    param->retry_repair=1;\n    param->testflag|=T_RETRY_WITHOUT_QUICK;\n    goto err;\n  }\n  if (param->testflag & T_SAFE_REPAIR)\n  {\n    /* Don't repair if we loosed more than one row */\n    if (info->state->records+1 < start_records)\n    {\n      info->state->records=start_records;\n      got_error=1;\n      goto err;\n    }\n  }\n\n  if (!rep_quick)\n  {\n    mysql_file_close(info->dfile, MYF(0));\n    info->dfile=new_file;\n    info->state->data_file_length=sort_param.filepos;\n    share->state.version=(ulong) time((time_t*) 0);\t/* Force reopen */\n  }\n  else\n  {\n    info->state->data_file_length=sort_param.max_pos;\n  }\n  if (param->testflag & T_CALC_CHECKSUM)\n    info->state->checksum=param->glob_crc;\n\n  if (!(param->testflag & T_SILENT))\n  {\n    if (start_records != info->state->records)\n      printf(\"Data records: %s\\n\", llstr(info->state->records,llbuff));\n    if (sort_info.dupp)\n      mi_check_print_warning(param,\n\t\t\t     \"%s records have been removed\",\n\t\t\t     llstr(sort_info.dupp,llbuff));\n  }\n\n  got_error=0;\n  /* If invoked by external program that uses thr_lock */\n  if (&share->state.state != info->state)\n    memcpy( &share->state.state, info->state, sizeof(*info->state));\n\nerr:\n  if (!got_error)\n  {\n    /* Replace the actual file with the temporary file */\n    if (new_file >= 0)\n    {\n      myf flags= 0;\n      if (param->testflag & T_BACKUP_DATA)\n        flags |= MY_REDEL_MAKE_BACKUP;\n      if (no_copy_stat)\n        flags |= MY_REDEL_NO_COPY_STAT;\n      mysql_file_close(new_file, MYF(0));\n      info->dfile=new_file= -1;\n      /*\n        On Windows, the old data file cannot be deleted if it is either\n        open, or memory mapped. Closing the file won't remove the memory\n        map implicilty on Windows. We closed the data file, but we keep\n        the MyISAM table open. A memory map will be closed on the final\n        mi_close() only. So we need to unmap explicitly here. After\n        renaming the new file under the hook, we couldn't use the map of\n        the old file any more anyway.\n      */\n      if (info->s->file_map)\n      {\n        (void) my_munmap((char*) info->s->file_map,\n                         (size_t) info->s->mmaped_length);\n        info->s->file_map= NULL;\n      }\n      if (change_to_newfile(share->data_file_name, MI_NAME_DEXT, DATA_TMP_EXT,\n                            flags) ||\n\t  mi_open_datafile(info,share,name,-1))\n\tgot_error=1;\n\n      param->retry_repair= 0;\n    }\n  }\n  if (got_error)\n  {\n    if (! param->error_printed)\n      mi_check_print_error(param,\"%d for record at pos %s\",my_errno,\n\t\t  llstr(sort_param.start_recpos,llbuff));\n    if (new_file >= 0)\n    {\n      (void) mysql_file_close(new_file, MYF(0));\n      (void) mysql_file_delete(mi_key_file_datatmp,\n                               param->temp_filename, MYF(MY_WME));\n      info->rec_cache.file=-1; /* don't flush data to new_file, it's closed */\n    }\n    mi_mark_crashed_on_repair(info);\n  }\n  my_free(mi_get_rec_buff_ptr(info, sort_param.rec_buff));\n  my_free(mi_get_rec_buff_ptr(info, sort_param.record));\n  my_free(sort_info.buff);\n  (void) end_io_cache(&param->read_cache);\n  info->opt_flag&= ~(READ_CACHE_USED | WRITE_CACHE_USED);\n  (void) end_io_cache(&info->rec_cache);\n  got_error|=flush_blocks(param, share->key_cache, share->kfile);\n  if (!got_error && param->testflag & T_UNPACK)\n  {\n    share->state.header.options[0]&= (uchar) ~HA_OPTION_COMPRESS_RECORD;\n    share->pack.header_length=0;\n    share->data_file_type=sort_info.new_data_file_type;\n  }\n  share->state.changed|= (STATE_NOT_OPTIMIZED_KEYS | STATE_NOT_SORTED_PAGES |\n\t\t\t  STATE_NOT_ANALYZED);\n  DBUG_RETURN(got_error);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n int mi_repair(MI_CHECK *param, register MI_INFO *info,\n-\t      char * name, int rep_quick)\n+\t      char * name, int rep_quick, my_bool no_copy_stat)\n {\n   int error,got_error;\n   ha_rows start_records,new_header_length;\n@@ -213,6 +213,11 @@\n     /* Replace the actual file with the temporary file */\n     if (new_file >= 0)\n     {\n+      myf flags= 0;\n+      if (param->testflag & T_BACKUP_DATA)\n+        flags |= MY_REDEL_MAKE_BACKUP;\n+      if (no_copy_stat)\n+        flags |= MY_REDEL_NO_COPY_STAT;\n       mysql_file_close(new_file, MYF(0));\n       info->dfile=new_file= -1;\n       /*\n@@ -231,8 +236,7 @@\n         info->s->file_map= NULL;\n       }\n       if (change_to_newfile(share->data_file_name, MI_NAME_DEXT, DATA_TMP_EXT,\n-\t\t\t    (param->testflag & T_BACKUP_DATA ?\n-\t\t\t     MYF(MY_REDEL_MAKE_BACKUP): MYF(0))) ||\n+                            flags) ||\n \t  mi_open_datafile(info,share,name,-1))\n \tgot_error=1;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t      char * name, int rep_quick)",
                "\t\t\t    (param->testflag & T_BACKUP_DATA ?",
                "\t\t\t     MYF(MY_REDEL_MAKE_BACKUP): MYF(0))) ||"
            ],
            "added_lines": [
                "\t      char * name, int rep_quick, my_bool no_copy_stat)",
                "      myf flags= 0;",
                "      if (param->testflag & T_BACKUP_DATA)",
                "        flags |= MY_REDEL_MAKE_BACKUP;",
                "      if (no_copy_stat)",
                "        flags |= MY_REDEL_NO_COPY_STAT;",
                "                            flags) ||"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6663",
        "func_name": "mysql/mysql-server/mi_sort_index",
        "description": "Race condition in Oracle MySQL before 5.5.52, 5.6.x before 5.6.33, 5.7.x before 5.7.15, and 8.x before 8.0.1; MariaDB before 5.5.52, 10.0.x before 10.0.28, and 10.1.x before 10.1.18; Percona Server before 5.5.51-38.2, 5.6.x before 5.6.32-78-1, and 5.7.x before 5.7.14-8; and Percona XtraDB Cluster before 5.5.41-37.0, 5.6.x before 5.6.32-25.17, and 5.7.x before 5.7.14-26.17 allows local users with certain permissions to gain privileges by leveraging use of my_copystat by REPAIR TABLE to repair a MyISAM table.",
        "git_url": "https://github.com/mysql/mysql-server/commit/4e5473862e6852b0f3802b0cd0c6fa10b5253291",
        "commit_title": "Bug#24388746: PRIVILEGE ESCALATION AND RACE CONDITION USING CREATE TABLE",
        "commit_text": " During REPAIR TABLE of a MyISAM table, a temporary data file (.TMD) is created. When repair finishes, this file is renamed to the original .MYD file. The problem was that during this rename, we copied the stats from the old file to the new file with chmod/chown. If a user managed to replace the temporary file before chmod/chown was executed, it was possible to get an arbitrary file with the privileges of the mysql user.  This patch fixes the problem by not copying stats from the old file to the new file. This is not needed as the new file was created with the correct stats. This fix only changes server behavior - external utilities such as myisamchk still does chmod/chown.  No test case provided since the problem involves synchronization with file system operations.",
        "func_before": "int mi_sort_index(MI_CHECK *param, register MI_INFO *info, char * name)\n{\n  reg2 uint key;\n  reg1 MI_KEYDEF *keyinfo;\n  File new_file;\n  my_off_t index_pos[HA_MAX_POSSIBLE_KEY];\n  uint r_locks,w_locks;\n  int old_lock;\n  MYISAM_SHARE *share=info->s;\n  MI_STATE_INFO old_state;\n  DBUG_ENTER(\"mi_sort_index\");\n\n  /* cannot sort index files with R-tree indexes */\n  for (key= 0,keyinfo= &share->keyinfo[0]; key < share->base.keys ;\n       key++,keyinfo++)\n    if (keyinfo->key_alg == HA_KEY_ALG_RTREE)\n      DBUG_RETURN(0);\n\n  if (!(param->testflag & T_SILENT))\n    printf(\"- Sorting index for MyISAM-table '%s'\\n\",name);\n\n  /* Get real path for index file */\n  fn_format(param->temp_filename,name,\"\", MI_NAME_IEXT,2+4+32);\n  if ((new_file= mysql_file_create(mi_key_file_datatmp,\n                                   fn_format(param->temp_filename,\n                                             param->temp_filename,\n                                             \"\", INDEX_TMP_EXT, 2+4),\n                                   0, param->tmpfile_createflag, MYF(0))) <= 0)\n  {\n    mi_check_print_error(param,\"Can't create new tempfile: '%s'\",\n\t\t\t param->temp_filename);\n    DBUG_RETURN(-1);\n  }\n  if (filecopy(param, new_file,share->kfile,0L,\n\t       (ulong) share->base.keystart, \"headerblock\"))\n    goto err;\n\n  param->new_file_pos=share->base.keystart;\n  for (key= 0,keyinfo= &share->keyinfo[0]; key < share->base.keys ;\n       key++,keyinfo++)\n  {\n    if (! mi_is_key_active(info->s->state.key_map, key))\n      continue;\n\n    if (share->state.key_root[key] != HA_OFFSET_ERROR)\n    {\n      index_pos[key]=param->new_file_pos;\t/* Write first block here */\n      if (sort_one_index(param,info,keyinfo,share->state.key_root[key],\n\t\t\t new_file))\n\tgoto err;\n    }\n    else\n      index_pos[key]= HA_OFFSET_ERROR;\t\t/* No blocks */\n  }\n\n  /* Flush key cache for this file if we are calling this outside myisamchk */\n  flush_key_blocks(share->key_cache,share->kfile, FLUSH_IGNORE_CHANGED);\n\n  share->state.version=(ulong) time((time_t*) 0);\n  old_state= share->state;\t\t\t/* save state if not stored */\n  r_locks=   share->r_locks;\n  w_locks=   share->w_locks;\n  old_lock=  info->lock_type;\n\n\t/* Put same locks as old file */\n  share->r_locks= share->w_locks= share->tot_locks= 0;\n  (void) _mi_writeinfo(info,WRITEINFO_UPDATE_KEYFILE);\n  (void) mysql_file_close(share->kfile, MYF(MY_WME));\n  share->kfile = -1;\n  (void) mysql_file_close(new_file, MYF(MY_WME));\n  if (change_to_newfile(share->index_file_name, MI_NAME_IEXT, INDEX_TMP_EXT,\n\t\t\tMYF(0)) ||\n      mi_open_keyfile(share))\n    goto err2;\n  info->lock_type= F_UNLCK;\t\t\t/* Force mi_readinfo to lock */\n  _mi_readinfo(info,F_WRLCK,0);\t\t\t/* Will lock the table */\n  info->lock_type=  old_lock;\n  share->r_locks=   r_locks;\n  share->w_locks=   w_locks;\n  share->tot_locks= r_locks+w_locks;\n  share->state=     old_state;\t\t\t/* Restore old state */\n\n  info->state->key_file_length=param->new_file_pos;\n  info->update= (short) (HA_STATE_CHANGED | HA_STATE_ROW_CHANGED);\n  for (key=0 ; key < info->s->base.keys ; key++)\n    info->s->state.key_root[key]=index_pos[key];\n  for (key=0 ; key < info->s->state.header.max_block_size_index ; key++)\n    info->s->state.key_del[key]=  HA_OFFSET_ERROR;\n\n  info->s->state.changed&= ~STATE_NOT_SORTED_PAGES;\n  DBUG_RETURN(0);\n\nerr:\n  (void) mysql_file_close(new_file, MYF(MY_WME));\nerr2:\n  (void) mysql_file_delete(mi_key_file_datatmp,\n                           param->temp_filename, MYF(MY_WME));\n  DBUG_RETURN(-1);\n}",
        "func": "int mi_sort_index(MI_CHECK *param, register MI_INFO *info, char * name,\n                  my_bool no_copy_stat)\n{\n  reg2 uint key;\n  reg1 MI_KEYDEF *keyinfo;\n  File new_file;\n  my_off_t index_pos[HA_MAX_POSSIBLE_KEY];\n  uint r_locks,w_locks;\n  int old_lock;\n  MYISAM_SHARE *share=info->s;\n  MI_STATE_INFO old_state;\n  DBUG_ENTER(\"mi_sort_index\");\n\n  /* cannot sort index files with R-tree indexes */\n  for (key= 0,keyinfo= &share->keyinfo[0]; key < share->base.keys ;\n       key++,keyinfo++)\n    if (keyinfo->key_alg == HA_KEY_ALG_RTREE)\n      DBUG_RETURN(0);\n\n  if (!(param->testflag & T_SILENT))\n    printf(\"- Sorting index for MyISAM-table '%s'\\n\",name);\n\n  /* Get real path for index file */\n  fn_format(param->temp_filename,name,\"\", MI_NAME_IEXT,2+4+32);\n  if ((new_file= mysql_file_create(mi_key_file_datatmp,\n                                   fn_format(param->temp_filename,\n                                             param->temp_filename,\n                                             \"\", INDEX_TMP_EXT, 2+4),\n                                   0, param->tmpfile_createflag, MYF(0))) <= 0)\n  {\n    mi_check_print_error(param,\"Can't create new tempfile: '%s'\",\n\t\t\t param->temp_filename);\n    DBUG_RETURN(-1);\n  }\n  if (filecopy(param, new_file,share->kfile,0L,\n\t       (ulong) share->base.keystart, \"headerblock\"))\n    goto err;\n\n  param->new_file_pos=share->base.keystart;\n  for (key= 0,keyinfo= &share->keyinfo[0]; key < share->base.keys ;\n       key++,keyinfo++)\n  {\n    if (! mi_is_key_active(info->s->state.key_map, key))\n      continue;\n\n    if (share->state.key_root[key] != HA_OFFSET_ERROR)\n    {\n      index_pos[key]=param->new_file_pos;\t/* Write first block here */\n      if (sort_one_index(param,info,keyinfo,share->state.key_root[key],\n\t\t\t new_file))\n\tgoto err;\n    }\n    else\n      index_pos[key]= HA_OFFSET_ERROR;\t\t/* No blocks */\n  }\n\n  /* Flush key cache for this file if we are calling this outside myisamchk */\n  flush_key_blocks(share->key_cache,share->kfile, FLUSH_IGNORE_CHANGED);\n\n  share->state.version=(ulong) time((time_t*) 0);\n  old_state= share->state;\t\t\t/* save state if not stored */\n  r_locks=   share->r_locks;\n  w_locks=   share->w_locks;\n  old_lock=  info->lock_type;\n\n\t/* Put same locks as old file */\n  share->r_locks= share->w_locks= share->tot_locks= 0;\n  (void) _mi_writeinfo(info,WRITEINFO_UPDATE_KEYFILE);\n  (void) mysql_file_close(share->kfile, MYF(MY_WME));\n  share->kfile = -1;\n  (void) mysql_file_close(new_file, MYF(MY_WME));\n  if (change_to_newfile(share->index_file_name, MI_NAME_IEXT, INDEX_TMP_EXT,\n\t\t\tno_copy_stat ? MYF(MY_REDEL_NO_COPY_STAT) : MYF(0)) ||\n      mi_open_keyfile(share))\n    goto err2;\n  info->lock_type= F_UNLCK;\t\t\t/* Force mi_readinfo to lock */\n  _mi_readinfo(info,F_WRLCK,0);\t\t\t/* Will lock the table */\n  info->lock_type=  old_lock;\n  share->r_locks=   r_locks;\n  share->w_locks=   w_locks;\n  share->tot_locks= r_locks+w_locks;\n  share->state=     old_state;\t\t\t/* Restore old state */\n\n  info->state->key_file_length=param->new_file_pos;\n  info->update= (short) (HA_STATE_CHANGED | HA_STATE_ROW_CHANGED);\n  for (key=0 ; key < info->s->base.keys ; key++)\n    info->s->state.key_root[key]=index_pos[key];\n  for (key=0 ; key < info->s->state.header.max_block_size_index ; key++)\n    info->s->state.key_del[key]=  HA_OFFSET_ERROR;\n\n  info->s->state.changed&= ~STATE_NOT_SORTED_PAGES;\n  DBUG_RETURN(0);\n\nerr:\n  (void) mysql_file_close(new_file, MYF(MY_WME));\nerr2:\n  (void) mysql_file_delete(mi_key_file_datatmp,\n                           param->temp_filename, MYF(MY_WME));\n  DBUG_RETURN(-1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n-int mi_sort_index(MI_CHECK *param, register MI_INFO *info, char * name)\n+int mi_sort_index(MI_CHECK *param, register MI_INFO *info, char * name,\n+                  my_bool no_copy_stat)\n {\n   reg2 uint key;\n   reg1 MI_KEYDEF *keyinfo;\n@@ -69,7 +70,7 @@\n   share->kfile = -1;\n   (void) mysql_file_close(new_file, MYF(MY_WME));\n   if (change_to_newfile(share->index_file_name, MI_NAME_IEXT, INDEX_TMP_EXT,\n-\t\t\tMYF(0)) ||\n+\t\t\tno_copy_stat ? MYF(MY_REDEL_NO_COPY_STAT) : MYF(0)) ||\n       mi_open_keyfile(share))\n     goto err2;\n   info->lock_type= F_UNLCK;\t\t\t/* Force mi_readinfo to lock */",
        "diff_line_info": {
            "deleted_lines": [
                "int mi_sort_index(MI_CHECK *param, register MI_INFO *info, char * name)",
                "\t\t\tMYF(0)) ||"
            ],
            "added_lines": [
                "int mi_sort_index(MI_CHECK *param, register MI_INFO *info, char * name,",
                "                  my_bool no_copy_stat)",
                "\t\t\tno_copy_stat ? MYF(MY_REDEL_NO_COPY_STAT) : MYF(0)) ||"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6663",
        "func_name": "mysql/mysql-server/mi_repair_by_sort",
        "description": "Race condition in Oracle MySQL before 5.5.52, 5.6.x before 5.6.33, 5.7.x before 5.7.15, and 8.x before 8.0.1; MariaDB before 5.5.52, 10.0.x before 10.0.28, and 10.1.x before 10.1.18; Percona Server before 5.5.51-38.2, 5.6.x before 5.6.32-78-1, and 5.7.x before 5.7.14-8; and Percona XtraDB Cluster before 5.5.41-37.0, 5.6.x before 5.6.32-25.17, and 5.7.x before 5.7.14-26.17 allows local users with certain permissions to gain privileges by leveraging use of my_copystat by REPAIR TABLE to repair a MyISAM table.",
        "git_url": "https://github.com/mysql/mysql-server/commit/4e5473862e6852b0f3802b0cd0c6fa10b5253291",
        "commit_title": "Bug#24388746: PRIVILEGE ESCALATION AND RACE CONDITION USING CREATE TABLE",
        "commit_text": " During REPAIR TABLE of a MyISAM table, a temporary data file (.TMD) is created. When repair finishes, this file is renamed to the original .MYD file. The problem was that during this rename, we copied the stats from the old file to the new file with chmod/chown. If a user managed to replace the temporary file before chmod/chown was executed, it was possible to get an arbitrary file with the privileges of the mysql user.  This patch fixes the problem by not copying stats from the old file to the new file. This is not needed as the new file was created with the correct stats. This fix only changes server behavior - external utilities such as myisamchk still does chmod/chown.  No test case provided since the problem involves synchronization with file system operations.",
        "func_before": "int mi_repair_by_sort(MI_CHECK *param, register MI_INFO *info,\n\t\t      const char * name, int rep_quick)\n{\n  int got_error;\n  uint i;\n  ulong length;\n  ha_rows start_records;\n  my_off_t new_header_length,del;\n  File new_file;\n  MI_SORT_PARAM sort_param;\n  MYISAM_SHARE *share=info->s;\n  HA_KEYSEG *keyseg;\n  ulong   *rec_per_key_part;\n  char llbuff[22];\n  SORT_INFO sort_info;\n  ulonglong UNINIT_VAR(key_map);\n  DBUG_ENTER(\"mi_repair_by_sort\");\n\n  start_records=info->state->records;\n  got_error=1;\n  new_file= -1;\n  new_header_length=(param->testflag & T_UNPACK) ? 0 :\n    share->pack.header_length;\n  if (!(param->testflag & T_SILENT))\n  {\n    printf(\"- recovering (with sort) MyISAM-table '%s'\\n\",name);\n    printf(\"Data records: %s\\n\", llstr(start_records,llbuff));\n  }\n  param->testflag|=T_REP; /* for easy checking */\n\n  if (info->s->options & (HA_OPTION_CHECKSUM | HA_OPTION_COMPRESS_RECORD))\n    param->testflag|=T_CALC_CHECKSUM;\n\n  bzero((char*)&sort_info,sizeof(sort_info));\n  bzero((char *)&sort_param, sizeof(sort_param));\n  if (!(sort_info.key_block=\n\talloc_key_blocks(param,\n\t\t\t (uint) param->sort_key_blocks,\n\t\t\t share->base.max_key_block_length))\n      || init_io_cache(&param->read_cache,info->dfile,\n\t\t       (uint) param->read_buffer_length,\n\t\t       READ_CACHE,share->pack.header_length,1,MYF(MY_WME)) ||\n      (! rep_quick &&\n       init_io_cache(&info->rec_cache,info->dfile,\n\t\t     (uint) param->write_buffer_length,\n\t\t     WRITE_CACHE,new_header_length,1,\n\t\t     MYF(MY_WME | MY_WAIT_IF_FULL) & param->myf_rw)))\n    goto err;\n  sort_info.key_block_end=sort_info.key_block+param->sort_key_blocks;\n  info->opt_flag|=WRITE_CACHE_USED;\n  info->rec_cache.file=info->dfile;\t\t/* for sort_delete_record */\n\n  if (!mi_alloc_rec_buff(info, -1, &sort_param.record) ||\n      !mi_alloc_rec_buff(info, -1, &sort_param.rec_buff))\n  {\n    mi_check_print_error(param, \"Not enough memory for extra record\");\n    goto err;\n  }\n  if (!rep_quick)\n  {\n    /* Get real path for data file */\n    if ((new_file= mysql_file_create(mi_key_file_datatmp,\n                                     fn_format(param->temp_filename,\n                                               share->data_file_name, \"\",\n                                               DATA_TMP_EXT, 2+4),\n                                     0, param->tmpfile_createflag,\n                                     MYF(0))) < 0)\n    {\n      mi_check_print_error(param,\"Can't create new tempfile: '%s'\",\n\t\t\t   param->temp_filename);\n      goto err;\n    }\n    if (new_header_length &&\n        filecopy(param, new_file,info->dfile,0L,new_header_length,\n\t\t \"datafile-header\"))\n      goto err;\n    if (param->testflag & T_UNPACK)\n    {\n      share->options&= ~HA_OPTION_COMPRESS_RECORD;\n      mi_int2store(share->state.header.options,share->options);\n    }\n    share->state.dellink= HA_OFFSET_ERROR;\n    info->rec_cache.file=new_file;\n  }\n\n  info->update= (short) (HA_STATE_CHANGED | HA_STATE_ROW_CHANGED);\n\n  /* Optionally drop indexes and optionally modify the key_map. */\n  mi_drop_all_indexes(param, info, FALSE);\n  key_map= share->state.key_map;\n  if (param->testflag & T_CREATE_MISSING_KEYS)\n  {\n    /* Invert the copied key_map to recreate all disabled indexes. */\n    key_map= ~key_map;\n  }\n\n  sort_info.info=info;\n  sort_info.param = param;\n\n  set_data_file_type(&sort_info, share);\n  sort_param.filepos=new_header_length;\n  sort_info.dupp=0;\n  sort_info.buff=0;\n  param->read_cache.end_of_file=sort_info.filelength=\n    mysql_file_seek(param->read_cache.file, 0L, MY_SEEK_END, MYF(0));\n\n  sort_param.wordlist=NULL;\n  init_alloc_root(&sort_param.wordroot, FTPARSER_MEMROOT_ALLOC_SIZE, 0);\n\n  if (share->data_file_type == DYNAMIC_RECORD)\n    length=max(share->base.min_pack_length+1,share->base.min_block_length);\n  else if (share->data_file_type == COMPRESSED_RECORD)\n    length=share->base.min_block_length;\n  else\n    length=share->base.pack_reclength;\n  sort_info.max_records=\n    ((param->testflag & T_CREATE_MISSING_KEYS) ? info->state->records :\n     (ha_rows) (sort_info.filelength/length+1));\n  sort_param.key_cmp=sort_key_cmp;\n  sort_param.lock_in_memory=lock_memory;\n  sort_param.tmpdir=param->tmpdir;\n  sort_param.sort_info=&sort_info;\n  sort_param.fix_datafile= (my_bool) (! rep_quick);\n  sort_param.master =1;\n  \n  del=info->state->del;\n  param->glob_crc=0;\n  if (param->testflag & T_CALC_CHECKSUM)\n    sort_param.calc_checksum= 1;\n\n  rec_per_key_part= param->rec_per_key_part;\n  for (sort_param.key=0 ; sort_param.key < share->base.keys ;\n       rec_per_key_part+=sort_param.keyinfo->keysegs, sort_param.key++)\n  {\n    sort_param.read_cache=param->read_cache;\n    sort_param.keyinfo=share->keyinfo+sort_param.key;\n    sort_param.seg=sort_param.keyinfo->seg;\n    /*\n      Skip this index if it is marked disabled in the copied\n      (and possibly inverted) key_map.\n    */\n    if (! mi_is_key_active(key_map, sort_param.key))\n    {\n      /* Remember old statistics for key */\n      memcpy((char*) rec_per_key_part,\n\t     (char*) (share->state.rec_per_key_part +\n\t\t      (uint) (rec_per_key_part - param->rec_per_key_part)),\n\t     sort_param.keyinfo->keysegs*sizeof(*rec_per_key_part));\n      DBUG_PRINT(\"repair\", (\"skipping seemingly disabled index #: %u\",\n                            sort_param.key));\n      continue;\n    }\n\n    if ((!(param->testflag & T_SILENT)))\n      printf (\"- Fixing index %d\\n\",sort_param.key+1);\n    sort_param.max_pos=sort_param.pos=share->pack.header_length;\n    keyseg=sort_param.seg;\n    bzero((char*) sort_param.unique,sizeof(sort_param.unique));\n    sort_param.key_length=share->rec_reflength;\n    for (i=0 ; keyseg[i].type != HA_KEYTYPE_END; i++)\n    {\n      sort_param.key_length+=keyseg[i].length;\n      if (keyseg[i].flag & HA_SPACE_PACK)\n\tsort_param.key_length+=get_pack_length(keyseg[i].length);\n      if (keyseg[i].flag & (HA_BLOB_PART | HA_VAR_LENGTH_PART))\n\tsort_param.key_length+=2 + test(keyseg[i].length >= 127);\n      if (keyseg[i].flag & HA_NULL_PART)\n\tsort_param.key_length++;\n    }\n    info->state->records=info->state->del=share->state.split=0;\n    info->state->empty=0;\n\n    if (sort_param.keyinfo->flag & HA_FULLTEXT)\n    {\n      uint ft_max_word_len_for_sort=FT_MAX_WORD_LEN_FOR_SORT*\n                                    sort_param.keyinfo->seg->charset->mbmaxlen;\n      sort_param.key_length+=ft_max_word_len_for_sort-HA_FT_MAXBYTELEN;\n      /*\n        fulltext indexes may have much more entries than the\n        number of rows in the table. We estimate the number here.\n      */\n      if (sort_param.keyinfo->parser == &ft_default_parser)\n      {\n        /*\n          for built-in parser the number of generated index entries\n          cannot be larger than the size of the data file divided\n          by the minimal word's length\n        */\n        sort_info.max_records=\n          (ha_rows) (sort_info.filelength/ft_min_word_len+1);\n      }\n      else\n      {\n        /*\n          for external plugin parser we cannot tell anything at all :(\n          so, we'll use all the sort memory and start from ~10 buffpeks.\n          (see _create_index_by_sort)\n        */\n        sort_info.max_records= 10 *\n                               max(param->sort_buffer_length, MIN_SORT_BUFFER) /\n                               sort_param.key_length;\n      }\n\n      sort_param.key_read=sort_ft_key_read;\n      sort_param.key_write=sort_ft_key_write;\n    }\n    else\n    {\n      sort_param.key_read=sort_key_read;\n      sort_param.key_write=sort_key_write;\n    }\n\n    if (_create_index_by_sort(&sort_param,\n\t\t\t      (my_bool) (!(param->testflag & T_VERBOSE)),\n                              param->sort_buffer_length))\n    {\n      param->retry_repair=1;\n      goto err;\n    }\n    /* No need to calculate checksum again. */\n    sort_param.calc_checksum= 0;\n    free_root(&sort_param.wordroot, MYF(0));\n\n    /* Set for next loop */\n    sort_info.max_records= (ha_rows) info->state->records;\n\n    if (param->testflag & T_STATISTICS)\n      update_key_parts(sort_param.keyinfo, rec_per_key_part, sort_param.unique,\n                       param->stats_method == MI_STATS_METHOD_IGNORE_NULLS?\n                       sort_param.notnull: NULL,\n                       (ulonglong) info->state->records);\n    /* Enable this index in the permanent (not the copied) key_map. */\n    mi_set_key_active(share->state.key_map, sort_param.key);\n    DBUG_PRINT(\"repair\", (\"set enabled index #: %u\", sort_param.key));\n\n    if (sort_param.fix_datafile)\n    {\n      param->read_cache.end_of_file=sort_param.filepos;\n      if (write_data_suffix(&sort_info,1) || end_io_cache(&info->rec_cache))\n\tgoto err;\n      if (param->testflag & T_SAFE_REPAIR)\n      {\n\t/* Don't repair if we loosed more than one row */\n\tif (info->state->records+1 < start_records)\n\t{\n\t  info->state->records=start_records;\n\t  goto err;\n\t}\n      }\n      share->state.state.data_file_length = info->state->data_file_length=\n\tsort_param.filepos;\n      /* Only whole records */\n      share->state.version=(ulong) time((time_t*) 0);\n      mysql_file_close(info->dfile, MYF(0));\n      info->dfile=new_file;\n      share->data_file_type=sort_info.new_data_file_type;\n      share->pack.header_length=(ulong) new_header_length;\n      sort_param.fix_datafile=0;\n    }\n    else\n      info->state->data_file_length=sort_param.max_pos;\n\n    param->read_cache.file=info->dfile;\t\t/* re-init read cache */\n    reinit_io_cache(&param->read_cache,READ_CACHE,share->pack.header_length,\n                    1,1);\n  }\n\n  if (param->testflag & T_WRITE_LOOP)\n  {\n    (void) fputs(\"          \\r\",stdout); (void) fflush(stdout);\n  }\n\n  if (rep_quick && del+sort_info.dupp != info->state->del)\n  {\n    mi_check_print_error(param,\"Couldn't fix table with quick recovery: Found wrong number of deleted records\");\n    mi_check_print_error(param,\"Run recovery again without -q\");\n    got_error=1;\n    param->retry_repair=1;\n    param->testflag|=T_RETRY_WITHOUT_QUICK;\n    goto err;\n  }\n\n  if (rep_quick & T_FORCE_UNIQUENESS)\n  {\n    my_off_t skr=info->state->data_file_length+\n      (share->options & HA_OPTION_COMPRESS_RECORD ?\n       MEMMAP_EXTRA_MARGIN : 0);\n#ifdef USE_RELOC\n    if (share->data_file_type == STATIC_RECORD &&\n\tskr < share->base.reloc*share->base.min_pack_length)\n      skr=share->base.reloc*share->base.min_pack_length;\n#endif\n    if (skr != sort_info.filelength)\n      if (mysql_file_chsize(info->dfile, skr, 0, MYF(0)))\n\tmi_check_print_warning(param,\n\t\t\t       \"Can't change size of datafile,  error: %d\",\n\t\t\t       my_errno);\n  }\n  if (param->testflag & T_CALC_CHECKSUM)\n    info->state->checksum=param->glob_crc;\n\n  if (mysql_file_chsize(share->kfile, info->state->key_file_length, 0, MYF(0)))\n    mi_check_print_warning(param,\n\t\t\t   \"Can't change size of indexfile, error: %d\",\n\t\t\t   my_errno);\n\n  if (!(param->testflag & T_SILENT))\n  {\n    if (start_records != info->state->records)\n      printf(\"Data records: %s\\n\", llstr(info->state->records,llbuff));\n    if (sort_info.dupp)\n      mi_check_print_warning(param,\n\t\t\t     \"%s records have been removed\",\n\t\t\t     llstr(sort_info.dupp,llbuff));\n  }\n  got_error=0;\n\n  if (&share->state.state != info->state)\n    memcpy( &share->state.state, info->state, sizeof(*info->state));\n\nerr:\n  got_error|= flush_blocks(param, share->key_cache, share->kfile);\n  (void) end_io_cache(&info->rec_cache);\n  if (!got_error)\n  {\n    /* Replace the actual file with the temporary file */\n    if (new_file >= 0)\n    {\n      mysql_file_close(new_file, MYF(0));\n      info->dfile=new_file= -1;\n      if (change_to_newfile(share->data_file_name,MI_NAME_DEXT, DATA_TMP_EXT,\n\t\t\t    (param->testflag & T_BACKUP_DATA ?\n\t\t\t     MYF(MY_REDEL_MAKE_BACKUP): MYF(0))) ||\n\t  mi_open_datafile(info,share,name,-1))\n\tgot_error=1;\n    }\n  }\n  if (got_error)\n  {\n    if (! param->error_printed)\n      mi_check_print_error(param,\"%d when fixing table\",my_errno);\n    if (new_file >= 0)\n    {\n      (void) mysql_file_close(new_file, MYF(0));\n      (void) mysql_file_delete(mi_key_file_datatmp,\n                               param->temp_filename, MYF(MY_WME));\n      if (info->dfile == new_file) /* Retry with key cache */\n        if (unlikely(mi_open_datafile(info, share, name, -1)))\n          param->retry_repair= 0; /* Safety */\n    }\n    mi_mark_crashed_on_repair(info);\n  }\n  else if (key_map == share->state.key_map)\n    share->state.changed&= ~STATE_NOT_OPTIMIZED_KEYS;\n  share->state.changed|=STATE_NOT_SORTED_PAGES;\n\n  my_free(mi_get_rec_buff_ptr(info, sort_param.rec_buff));\n  my_free(mi_get_rec_buff_ptr(info, sort_param.record));\n  my_free(sort_info.key_block);\n  my_free(sort_info.ft_buf);\n  my_free(sort_info.buff);\n  (void) end_io_cache(&param->read_cache);\n  info->opt_flag&= ~(READ_CACHE_USED | WRITE_CACHE_USED);\n  if (!got_error && (param->testflag & T_UNPACK))\n  {\n    share->state.header.options[0]&= (uchar) ~HA_OPTION_COMPRESS_RECORD;\n    share->pack.header_length=0;\n  }\n  DBUG_RETURN(got_error);\n}",
        "func": "int mi_repair_by_sort(MI_CHECK *param, register MI_INFO *info,\n\t\t      const char * name, int rep_quick, my_bool no_copy_stat)\n{\n  int got_error;\n  uint i;\n  ulong length;\n  ha_rows start_records;\n  my_off_t new_header_length,del;\n  File new_file;\n  MI_SORT_PARAM sort_param;\n  MYISAM_SHARE *share=info->s;\n  HA_KEYSEG *keyseg;\n  ulong   *rec_per_key_part;\n  char llbuff[22];\n  SORT_INFO sort_info;\n  ulonglong UNINIT_VAR(key_map);\n  DBUG_ENTER(\"mi_repair_by_sort\");\n\n  start_records=info->state->records;\n  got_error=1;\n  new_file= -1;\n  new_header_length=(param->testflag & T_UNPACK) ? 0 :\n    share->pack.header_length;\n  if (!(param->testflag & T_SILENT))\n  {\n    printf(\"- recovering (with sort) MyISAM-table '%s'\\n\",name);\n    printf(\"Data records: %s\\n\", llstr(start_records,llbuff));\n  }\n  param->testflag|=T_REP; /* for easy checking */\n\n  if (info->s->options & (HA_OPTION_CHECKSUM | HA_OPTION_COMPRESS_RECORD))\n    param->testflag|=T_CALC_CHECKSUM;\n\n  bzero((char*)&sort_info,sizeof(sort_info));\n  bzero((char *)&sort_param, sizeof(sort_param));\n  if (!(sort_info.key_block=\n\talloc_key_blocks(param,\n\t\t\t (uint) param->sort_key_blocks,\n\t\t\t share->base.max_key_block_length))\n      || init_io_cache(&param->read_cache,info->dfile,\n\t\t       (uint) param->read_buffer_length,\n\t\t       READ_CACHE,share->pack.header_length,1,MYF(MY_WME)) ||\n      (! rep_quick &&\n       init_io_cache(&info->rec_cache,info->dfile,\n\t\t     (uint) param->write_buffer_length,\n\t\t     WRITE_CACHE,new_header_length,1,\n\t\t     MYF(MY_WME | MY_WAIT_IF_FULL) & param->myf_rw)))\n    goto err;\n  sort_info.key_block_end=sort_info.key_block+param->sort_key_blocks;\n  info->opt_flag|=WRITE_CACHE_USED;\n  info->rec_cache.file=info->dfile;\t\t/* for sort_delete_record */\n\n  if (!mi_alloc_rec_buff(info, -1, &sort_param.record) ||\n      !mi_alloc_rec_buff(info, -1, &sort_param.rec_buff))\n  {\n    mi_check_print_error(param, \"Not enough memory for extra record\");\n    goto err;\n  }\n  if (!rep_quick)\n  {\n    /* Get real path for data file */\n    if ((new_file= mysql_file_create(mi_key_file_datatmp,\n                                     fn_format(param->temp_filename,\n                                               share->data_file_name, \"\",\n                                               DATA_TMP_EXT, 2+4),\n                                     0, param->tmpfile_createflag,\n                                     MYF(0))) < 0)\n    {\n      mi_check_print_error(param,\"Can't create new tempfile: '%s'\",\n\t\t\t   param->temp_filename);\n      goto err;\n    }\n    if (new_header_length &&\n        filecopy(param, new_file,info->dfile,0L,new_header_length,\n\t\t \"datafile-header\"))\n      goto err;\n    if (param->testflag & T_UNPACK)\n    {\n      share->options&= ~HA_OPTION_COMPRESS_RECORD;\n      mi_int2store(share->state.header.options,share->options);\n    }\n    share->state.dellink= HA_OFFSET_ERROR;\n    info->rec_cache.file=new_file;\n  }\n\n  info->update= (short) (HA_STATE_CHANGED | HA_STATE_ROW_CHANGED);\n\n  /* Optionally drop indexes and optionally modify the key_map. */\n  mi_drop_all_indexes(param, info, FALSE);\n  key_map= share->state.key_map;\n  if (param->testflag & T_CREATE_MISSING_KEYS)\n  {\n    /* Invert the copied key_map to recreate all disabled indexes. */\n    key_map= ~key_map;\n  }\n\n  sort_info.info=info;\n  sort_info.param = param;\n\n  set_data_file_type(&sort_info, share);\n  sort_param.filepos=new_header_length;\n  sort_info.dupp=0;\n  sort_info.buff=0;\n  param->read_cache.end_of_file=sort_info.filelength=\n    mysql_file_seek(param->read_cache.file, 0L, MY_SEEK_END, MYF(0));\n\n  sort_param.wordlist=NULL;\n  init_alloc_root(&sort_param.wordroot, FTPARSER_MEMROOT_ALLOC_SIZE, 0);\n\n  if (share->data_file_type == DYNAMIC_RECORD)\n    length=max(share->base.min_pack_length+1,share->base.min_block_length);\n  else if (share->data_file_type == COMPRESSED_RECORD)\n    length=share->base.min_block_length;\n  else\n    length=share->base.pack_reclength;\n  sort_info.max_records=\n    ((param->testflag & T_CREATE_MISSING_KEYS) ? info->state->records :\n     (ha_rows) (sort_info.filelength/length+1));\n  sort_param.key_cmp=sort_key_cmp;\n  sort_param.lock_in_memory=lock_memory;\n  sort_param.tmpdir=param->tmpdir;\n  sort_param.sort_info=&sort_info;\n  sort_param.fix_datafile= (my_bool) (! rep_quick);\n  sort_param.master =1;\n  \n  del=info->state->del;\n  param->glob_crc=0;\n  if (param->testflag & T_CALC_CHECKSUM)\n    sort_param.calc_checksum= 1;\n\n  rec_per_key_part= param->rec_per_key_part;\n  for (sort_param.key=0 ; sort_param.key < share->base.keys ;\n       rec_per_key_part+=sort_param.keyinfo->keysegs, sort_param.key++)\n  {\n    sort_param.read_cache=param->read_cache;\n    sort_param.keyinfo=share->keyinfo+sort_param.key;\n    sort_param.seg=sort_param.keyinfo->seg;\n    /*\n      Skip this index if it is marked disabled in the copied\n      (and possibly inverted) key_map.\n    */\n    if (! mi_is_key_active(key_map, sort_param.key))\n    {\n      /* Remember old statistics for key */\n      memcpy((char*) rec_per_key_part,\n\t     (char*) (share->state.rec_per_key_part +\n\t\t      (uint) (rec_per_key_part - param->rec_per_key_part)),\n\t     sort_param.keyinfo->keysegs*sizeof(*rec_per_key_part));\n      DBUG_PRINT(\"repair\", (\"skipping seemingly disabled index #: %u\",\n                            sort_param.key));\n      continue;\n    }\n\n    if ((!(param->testflag & T_SILENT)))\n      printf (\"- Fixing index %d\\n\",sort_param.key+1);\n    sort_param.max_pos=sort_param.pos=share->pack.header_length;\n    keyseg=sort_param.seg;\n    bzero((char*) sort_param.unique,sizeof(sort_param.unique));\n    sort_param.key_length=share->rec_reflength;\n    for (i=0 ; keyseg[i].type != HA_KEYTYPE_END; i++)\n    {\n      sort_param.key_length+=keyseg[i].length;\n      if (keyseg[i].flag & HA_SPACE_PACK)\n\tsort_param.key_length+=get_pack_length(keyseg[i].length);\n      if (keyseg[i].flag & (HA_BLOB_PART | HA_VAR_LENGTH_PART))\n\tsort_param.key_length+=2 + test(keyseg[i].length >= 127);\n      if (keyseg[i].flag & HA_NULL_PART)\n\tsort_param.key_length++;\n    }\n    info->state->records=info->state->del=share->state.split=0;\n    info->state->empty=0;\n\n    if (sort_param.keyinfo->flag & HA_FULLTEXT)\n    {\n      uint ft_max_word_len_for_sort=FT_MAX_WORD_LEN_FOR_SORT*\n                                    sort_param.keyinfo->seg->charset->mbmaxlen;\n      sort_param.key_length+=ft_max_word_len_for_sort-HA_FT_MAXBYTELEN;\n      /*\n        fulltext indexes may have much more entries than the\n        number of rows in the table. We estimate the number here.\n      */\n      if (sort_param.keyinfo->parser == &ft_default_parser)\n      {\n        /*\n          for built-in parser the number of generated index entries\n          cannot be larger than the size of the data file divided\n          by the minimal word's length\n        */\n        sort_info.max_records=\n          (ha_rows) (sort_info.filelength/ft_min_word_len+1);\n      }\n      else\n      {\n        /*\n          for external plugin parser we cannot tell anything at all :(\n          so, we'll use all the sort memory and start from ~10 buffpeks.\n          (see _create_index_by_sort)\n        */\n        sort_info.max_records= 10 *\n                               max(param->sort_buffer_length, MIN_SORT_BUFFER) /\n                               sort_param.key_length;\n      }\n\n      sort_param.key_read=sort_ft_key_read;\n      sort_param.key_write=sort_ft_key_write;\n    }\n    else\n    {\n      sort_param.key_read=sort_key_read;\n      sort_param.key_write=sort_key_write;\n    }\n\n    if (_create_index_by_sort(&sort_param,\n\t\t\t      (my_bool) (!(param->testflag & T_VERBOSE)),\n                              param->sort_buffer_length))\n    {\n      param->retry_repair=1;\n      goto err;\n    }\n    /* No need to calculate checksum again. */\n    sort_param.calc_checksum= 0;\n    free_root(&sort_param.wordroot, MYF(0));\n\n    /* Set for next loop */\n    sort_info.max_records= (ha_rows) info->state->records;\n\n    if (param->testflag & T_STATISTICS)\n      update_key_parts(sort_param.keyinfo, rec_per_key_part, sort_param.unique,\n                       param->stats_method == MI_STATS_METHOD_IGNORE_NULLS?\n                       sort_param.notnull: NULL,\n                       (ulonglong) info->state->records);\n    /* Enable this index in the permanent (not the copied) key_map. */\n    mi_set_key_active(share->state.key_map, sort_param.key);\n    DBUG_PRINT(\"repair\", (\"set enabled index #: %u\", sort_param.key));\n\n    if (sort_param.fix_datafile)\n    {\n      param->read_cache.end_of_file=sort_param.filepos;\n      if (write_data_suffix(&sort_info,1) || end_io_cache(&info->rec_cache))\n\tgoto err;\n      if (param->testflag & T_SAFE_REPAIR)\n      {\n\t/* Don't repair if we loosed more than one row */\n\tif (info->state->records+1 < start_records)\n\t{\n\t  info->state->records=start_records;\n\t  goto err;\n\t}\n      }\n      share->state.state.data_file_length = info->state->data_file_length=\n\tsort_param.filepos;\n      /* Only whole records */\n      share->state.version=(ulong) time((time_t*) 0);\n      mysql_file_close(info->dfile, MYF(0));\n      info->dfile=new_file;\n      share->data_file_type=sort_info.new_data_file_type;\n      share->pack.header_length=(ulong) new_header_length;\n      sort_param.fix_datafile=0;\n    }\n    else\n      info->state->data_file_length=sort_param.max_pos;\n\n    param->read_cache.file=info->dfile;\t\t/* re-init read cache */\n    reinit_io_cache(&param->read_cache,READ_CACHE,share->pack.header_length,\n                    1,1);\n  }\n\n  if (param->testflag & T_WRITE_LOOP)\n  {\n    (void) fputs(\"          \\r\",stdout); (void) fflush(stdout);\n  }\n\n  if (rep_quick && del+sort_info.dupp != info->state->del)\n  {\n    mi_check_print_error(param,\"Couldn't fix table with quick recovery: Found wrong number of deleted records\");\n    mi_check_print_error(param,\"Run recovery again without -q\");\n    got_error=1;\n    param->retry_repair=1;\n    param->testflag|=T_RETRY_WITHOUT_QUICK;\n    goto err;\n  }\n\n  if (rep_quick & T_FORCE_UNIQUENESS)\n  {\n    my_off_t skr=info->state->data_file_length+\n      (share->options & HA_OPTION_COMPRESS_RECORD ?\n       MEMMAP_EXTRA_MARGIN : 0);\n#ifdef USE_RELOC\n    if (share->data_file_type == STATIC_RECORD &&\n\tskr < share->base.reloc*share->base.min_pack_length)\n      skr=share->base.reloc*share->base.min_pack_length;\n#endif\n    if (skr != sort_info.filelength)\n      if (mysql_file_chsize(info->dfile, skr, 0, MYF(0)))\n\tmi_check_print_warning(param,\n\t\t\t       \"Can't change size of datafile,  error: %d\",\n\t\t\t       my_errno);\n  }\n  if (param->testflag & T_CALC_CHECKSUM)\n    info->state->checksum=param->glob_crc;\n\n  if (mysql_file_chsize(share->kfile, info->state->key_file_length, 0, MYF(0)))\n    mi_check_print_warning(param,\n\t\t\t   \"Can't change size of indexfile, error: %d\",\n\t\t\t   my_errno);\n\n  if (!(param->testflag & T_SILENT))\n  {\n    if (start_records != info->state->records)\n      printf(\"Data records: %s\\n\", llstr(info->state->records,llbuff));\n    if (sort_info.dupp)\n      mi_check_print_warning(param,\n\t\t\t     \"%s records have been removed\",\n\t\t\t     llstr(sort_info.dupp,llbuff));\n  }\n  got_error=0;\n\n  if (&share->state.state != info->state)\n    memcpy( &share->state.state, info->state, sizeof(*info->state));\n\nerr:\n  got_error|= flush_blocks(param, share->key_cache, share->kfile);\n  (void) end_io_cache(&info->rec_cache);\n  if (!got_error)\n  {\n    /* Replace the actual file with the temporary file */\n    if (new_file >= 0)\n    {\n      myf flags= 0;\n      if (param->testflag & T_BACKUP_DATA)\n        flags |= MY_REDEL_MAKE_BACKUP;\n      if (no_copy_stat)\n        flags |= MY_REDEL_NO_COPY_STAT;\n      mysql_file_close(new_file, MYF(0));\n      info->dfile=new_file= -1;\n      if (change_to_newfile(share->data_file_name,MI_NAME_DEXT, DATA_TMP_EXT,\n                            flags) ||\n\t  mi_open_datafile(info,share,name,-1))\n\tgot_error=1;\n    }\n  }\n  if (got_error)\n  {\n    if (! param->error_printed)\n      mi_check_print_error(param,\"%d when fixing table\",my_errno);\n    if (new_file >= 0)\n    {\n      (void) mysql_file_close(new_file, MYF(0));\n      (void) mysql_file_delete(mi_key_file_datatmp,\n                               param->temp_filename, MYF(MY_WME));\n      if (info->dfile == new_file) /* Retry with key cache */\n        if (unlikely(mi_open_datafile(info, share, name, -1)))\n          param->retry_repair= 0; /* Safety */\n    }\n    mi_mark_crashed_on_repair(info);\n  }\n  else if (key_map == share->state.key_map)\n    share->state.changed&= ~STATE_NOT_OPTIMIZED_KEYS;\n  share->state.changed|=STATE_NOT_SORTED_PAGES;\n\n  my_free(mi_get_rec_buff_ptr(info, sort_param.rec_buff));\n  my_free(mi_get_rec_buff_ptr(info, sort_param.record));\n  my_free(sort_info.key_block);\n  my_free(sort_info.ft_buf);\n  my_free(sort_info.buff);\n  (void) end_io_cache(&param->read_cache);\n  info->opt_flag&= ~(READ_CACHE_USED | WRITE_CACHE_USED);\n  if (!got_error && (param->testflag & T_UNPACK))\n  {\n    share->state.header.options[0]&= (uchar) ~HA_OPTION_COMPRESS_RECORD;\n    share->pack.header_length=0;\n  }\n  DBUG_RETURN(got_error);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n int mi_repair_by_sort(MI_CHECK *param, register MI_INFO *info,\n-\t\t      const char * name, int rep_quick)\n+\t\t      const char * name, int rep_quick, my_bool no_copy_stat)\n {\n   int got_error;\n   uint i;\n@@ -326,11 +326,15 @@\n     /* Replace the actual file with the temporary file */\n     if (new_file >= 0)\n     {\n+      myf flags= 0;\n+      if (param->testflag & T_BACKUP_DATA)\n+        flags |= MY_REDEL_MAKE_BACKUP;\n+      if (no_copy_stat)\n+        flags |= MY_REDEL_NO_COPY_STAT;\n       mysql_file_close(new_file, MYF(0));\n       info->dfile=new_file= -1;\n       if (change_to_newfile(share->data_file_name,MI_NAME_DEXT, DATA_TMP_EXT,\n-\t\t\t    (param->testflag & T_BACKUP_DATA ?\n-\t\t\t     MYF(MY_REDEL_MAKE_BACKUP): MYF(0))) ||\n+                            flags) ||\n \t  mi_open_datafile(info,share,name,-1))\n \tgot_error=1;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t      const char * name, int rep_quick)",
                "\t\t\t    (param->testflag & T_BACKUP_DATA ?",
                "\t\t\t     MYF(MY_REDEL_MAKE_BACKUP): MYF(0))) ||"
            ],
            "added_lines": [
                "\t\t      const char * name, int rep_quick, my_bool no_copy_stat)",
                "      myf flags= 0;",
                "      if (param->testflag & T_BACKUP_DATA)",
                "        flags |= MY_REDEL_MAKE_BACKUP;",
                "      if (no_copy_stat)",
                "        flags |= MY_REDEL_NO_COPY_STAT;",
                "                            flags) ||"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6663",
        "func_name": "MariaDB/server/my_redel",
        "description": "Race condition in Oracle MySQL before 5.5.52, 5.6.x before 5.6.33, 5.7.x before 5.7.15, and 8.x before 8.0.1; MariaDB before 5.5.52, 10.0.x before 10.0.28, and 10.1.x before 10.1.18; Percona Server before 5.5.51-38.2, 5.6.x before 5.6.32-78-1, and 5.7.x before 5.7.14-8; and Percona XtraDB Cluster before 5.5.41-37.0, 5.6.x before 5.6.32-25.17, and 5.7.x before 5.7.14-26.17 allows local users with certain permissions to gain privileges by leveraging use of my_copystat by REPAIR TABLE to repair a MyISAM table.",
        "git_url": "https://github.com/MariaDB/server/commit/347eeefbfc658c8531878218487d729f4e020805",
        "commit_title": "don't use my_copystat in the server",
        "commit_text": " it was supposed to be used in command-line tools only. Different fix for 4e5473862e:  Bug#24388746: PRIVILEGE ESCALATION AND RACE CONDITION USING CREATE TABLE",
        "func_before": "int my_redel(const char *org_name, const char *tmp_name,\n             time_t backup_time_stamp, myf MyFlags)\n{\n  int error=1;\n  DBUG_ENTER(\"my_redel\");\n  DBUG_PRINT(\"my\",(\"org_name: '%s' tmp_name: '%s'  MyFlags: %d\",\n\t\t   org_name,tmp_name,MyFlags));\n\n  if (my_copystat(org_name,tmp_name,MyFlags) < 0)\n    goto end;\n  if (MyFlags & MY_REDEL_MAKE_BACKUP)\n  {\n    char name_buff[FN_REFLEN + MY_BACKUP_NAME_EXTRA_LENGTH];    \n    my_create_backup_name(name_buff, org_name, backup_time_stamp);\n    if (my_rename(org_name, name_buff, MyFlags))\n      goto end;\n  }\n  else if (my_delete(org_name, MyFlags))\n      goto end;\n  if (my_rename(tmp_name,org_name,MyFlags))\n    goto end;\n\n  error=0;\nend:\n  DBUG_RETURN(error);\n}",
        "func": "int my_redel(const char *org_name, const char *tmp_name,\n             time_t backup_time_stamp, myf MyFlags)\n{\n  int error=1;\n  DBUG_ENTER(\"my_redel\");\n  DBUG_PRINT(\"my\",(\"org_name: '%s' tmp_name: '%s'  MyFlags: %d\",\n\t\t   org_name,tmp_name,MyFlags));\n\n  if (!my_disable_copystat_in_redel &&\n      my_copystat(org_name,tmp_name,MyFlags) < 0)\n    goto end;\n  if (MyFlags & MY_REDEL_MAKE_BACKUP)\n  {\n    char name_buff[FN_REFLEN + MY_BACKUP_NAME_EXTRA_LENGTH];    \n    my_create_backup_name(name_buff, org_name, backup_time_stamp);\n    if (my_rename(org_name, name_buff, MyFlags))\n      goto end;\n  }\n  else if (my_delete(org_name, MyFlags))\n      goto end;\n  if (my_rename(tmp_name,org_name,MyFlags))\n    goto end;\n\n  error=0;\nend:\n  DBUG_RETURN(error);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,8 @@\n   DBUG_PRINT(\"my\",(\"org_name: '%s' tmp_name: '%s'  MyFlags: %d\",\n \t\t   org_name,tmp_name,MyFlags));\n \n-  if (my_copystat(org_name,tmp_name,MyFlags) < 0)\n+  if (!my_disable_copystat_in_redel &&\n+      my_copystat(org_name,tmp_name,MyFlags) < 0)\n     goto end;\n   if (MyFlags & MY_REDEL_MAKE_BACKUP)\n   {",
        "diff_line_info": {
            "deleted_lines": [
                "  if (my_copystat(org_name,tmp_name,MyFlags) < 0)"
            ],
            "added_lines": [
                "  if (!my_disable_copystat_in_redel &&",
                "      my_copystat(org_name,tmp_name,MyFlags) < 0)"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6663",
        "func_name": "MariaDB/server/init_common_variables",
        "description": "Race condition in Oracle MySQL before 5.5.52, 5.6.x before 5.6.33, 5.7.x before 5.7.15, and 8.x before 8.0.1; MariaDB before 5.5.52, 10.0.x before 10.0.28, and 10.1.x before 10.1.18; Percona Server before 5.5.51-38.2, 5.6.x before 5.6.32-78-1, and 5.7.x before 5.7.14-8; and Percona XtraDB Cluster before 5.5.41-37.0, 5.6.x before 5.6.32-25.17, and 5.7.x before 5.7.14-26.17 allows local users with certain permissions to gain privileges by leveraging use of my_copystat by REPAIR TABLE to repair a MyISAM table.",
        "git_url": "https://github.com/MariaDB/server/commit/347eeefbfc658c8531878218487d729f4e020805",
        "commit_title": "don't use my_copystat in the server",
        "commit_text": " it was supposed to be used in command-line tools only. Different fix for 4e5473862e:  Bug#24388746: PRIVILEGE ESCALATION AND RACE CONDITION USING CREATE TABLE",
        "func_before": "static int init_common_variables()\n{\n  umask(((~my_umask) & 0666));\n  my_decimal_set_zero(&decimal_zero); // set decimal_zero constant;\n\n  tzset();\t\t\t// Set tzname\n\n  sf_leaking_memory= 0; // no memory leaks from now on\n\n  max_system_variables.pseudo_thread_id= (ulong)~0;\n  server_start_time= flush_status_time= my_time(0);\n\n  rpl_filter= new Rpl_filter;\n  binlog_filter= new Rpl_filter;\n  if (!rpl_filter || !binlog_filter)\n  {\n    sql_perror(\"Could not allocate replication and binlog filters\");\n    return 1;\n  }\n\n  if (init_thread_environment() ||\n      mysql_init_variables())\n    return 1;\n\n  if (ignore_db_dirs_init())\n    return 1;\n\n#ifdef HAVE_TZNAME\n  struct tm tm_tmp;\n  localtime_r(&server_start_time,&tm_tmp);\n  const char *tz_name=  tzname[tm_tmp.tm_isdst != 0 ? 1 : 0];\n#ifdef _WIN32\n  /*\n    Time zone name may be localized and contain non-ASCII characters,\n    Convert from ANSI encoding to UTF8.\n  */\n  wchar_t wtz_name[sizeof(system_time_zone)];\n  mbstowcs(wtz_name, tz_name, sizeof(system_time_zone)-1);\n  WideCharToMultiByte(CP_UTF8,0, wtz_name, -1, system_time_zone, \n    sizeof(system_time_zone) - 1, NULL, NULL);\n#else\n  strmake_buf(system_time_zone, tz_name);\n#endif /* _WIN32 */\n#endif /* HAVE_TZNAME */\n\n  /*\n    We set SYSTEM time zone as reasonable default and\n    also for failure of my_tz_init() and bootstrap mode.\n    If user explicitly set time zone with --default-time-zone\n    option we will change this value in my_tz_init().\n  */\n  global_system_variables.time_zone= my_tz_SYSTEM;\n\n#ifdef HAVE_PSI_INTERFACE\n  /*\n    Complete the mysql_bin_log initialization.\n    Instrumentation keys are known only after the performance schema initialization,\n    and can not be set in the MYSQL_BIN_LOG constructor (called before main()).\n  */\n  mysql_bin_log.set_psi_keys(key_BINLOG_LOCK_index,\n                             key_BINLOG_update_cond,\n                             key_file_binlog,\n                             key_file_binlog_index,\n                             key_BINLOG_COND_queue_busy);\n#endif\n\n  /*\n    Init mutexes for the global MYSQL_BIN_LOG objects.\n    As safe_mutex depends on what MY_INIT() does, we can't init the mutexes of\n    global MYSQL_BIN_LOGs in their constructors, because then they would be\n    inited before MY_INIT(). So we do it here.\n  */\n  mysql_bin_log.init_pthread_objects();\n\n  /* TODO: remove this when my_time_t is 64 bit compatible */\n  if (!IS_TIME_T_VALID_FOR_TIMESTAMP(server_start_time))\n  {\n    sql_print_error(\"This MySQL server doesn't support dates later then 2038\");\n    return 1;\n  }\n\n  if (gethostname(glob_hostname,sizeof(glob_hostname)) < 0)\n  {\n    /*\n      Get hostname of computer (used by 'show variables') and as default\n      basename for the pid file if --log-basename is not given.\n    */\n    strmake(glob_hostname, STRING_WITH_LEN(\"localhost\"));\n    sql_print_warning(\"gethostname failed, using '%s' as hostname\",\n                        glob_hostname);\n    opt_log_basename= const_cast<char *>(\"mysql\");\n  }\n  else\n    opt_log_basename= glob_hostname;\n\n  if (!*pidfile_name)\n  {\n    strmake(pidfile_name, opt_log_basename, sizeof(pidfile_name)-5);\n    strmov(fn_ext(pidfile_name),\".pid\");\t\t// Add proper extension\n  }\n\n  /*\n    The default-storage-engine entry in my_long_options should have a\n    non-null default value. It was earlier intialized as\n    (longlong)\"MyISAM\" in my_long_options but this triggered a\n    compiler error in the Sun Studio 12 compiler. As a work-around we\n    set the def_value member to 0 in my_long_options and initialize it\n    to the correct value here.\n\n    From MySQL 5.5 onwards, the default storage engine is InnoDB\n    (except in the embedded server, where the default continues to\n    be MyISAM)\n  */\n#if defined(WITH_INNOBASE_STORAGE_ENGINE) || defined(WITH_XTRADB_STORAGE_ENGINE)\n  default_storage_engine= const_cast<char *>(\"InnoDB\");\n#else\n  default_storage_engine= const_cast<char *>(\"MyISAM\");\n#endif\n\n  /*\n    Add server status variables to the dynamic list of\n    status variables that is shown by SHOW STATUS.\n    Later, in plugin_init, and mysql_install_plugin\n    new entries could be added to that list.\n  */\n  if (add_status_vars(status_vars))\n    return 1; // an error was already reported\n\n#ifndef DBUG_OFF\n  /*\n    We have few debug-only commands in com_status_vars, only visible in debug\n    builds. for simplicity we enable the assert only in debug builds\n\n    There are 8 Com_ variables which don't have corresponding SQLCOM_ values:\n    (TODO strictly speaking they shouldn't be here, should not have Com_ prefix\n    that is. Perhaps Stmt_ ? Comstmt_ ? Prepstmt_ ?)\n\n      Com_admin_commands       => com_other\n      Com_stmt_close           => com_stmt_close\n      Com_stmt_execute         => com_stmt_execute\n      Com_stmt_fetch           => com_stmt_fetch\n      Com_stmt_prepare         => com_stmt_prepare\n      Com_stmt_reprepare       => com_stmt_reprepare\n      Com_stmt_reset           => com_stmt_reset\n      Com_stmt_send_long_data  => com_stmt_send_long_data\n\n    With this correction the number of Com_ variables (number of elements in\n    the array, excluding the last element - terminator) must match the number\n    of SQLCOM_ constants.\n  */\n  compile_time_assert(sizeof(com_status_vars)/sizeof(com_status_vars[0]) - 1 ==\n                     SQLCOM_END + 8);\n#endif\n\n  if (get_options(&remaining_argc, &remaining_argv))\n    return 1;\n  set_server_version();\n\n  if (!opt_help)\n    sql_print_information(\"%s (mysqld %s) starting as process %lu ...\",\n                          my_progname, server_version, (ulong) getpid());\n\n#ifndef EMBEDDED_LIBRARY\n  if (opt_abort && !opt_verbose)\n    unireg_abort(0);\n#endif /*!EMBEDDED_LIBRARY*/\n\n  DBUG_PRINT(\"info\",(\"%s  Ver %s for %s on %s\\n\",my_progname,\n\t\t     server_version, SYSTEM_TYPE,MACHINE_TYPE));\n\n#ifdef HAVE_LARGE_PAGES\n  /* Initialize large page size */\n  if (opt_large_pages && (opt_large_page_size= my_get_large_page_size()))\n  {\n      DBUG_PRINT(\"info\", (\"Large page set, large_page_size = %d\",\n                 opt_large_page_size));\n      my_use_large_pages= 1;\n      my_large_page_size= opt_large_page_size;\n  }\n  else\n  {\n    opt_large_pages= 0;\n    /* \n       Either not configured to use large pages or Linux haven't\n       been compiled with large page support\n    */\n  }\n#endif /* HAVE_LARGE_PAGES */\n#ifdef HAVE_SOLARIS_LARGE_PAGES\n#define LARGE_PAGESIZE (4*1024*1024)  /* 4MB */\n#define SUPER_LARGE_PAGESIZE (256*1024*1024)  /* 256MB */\n  if (opt_large_pages)\n  {\n  /*\n    tell the kernel that we want to use 4/256MB page for heap storage\n    and also for the stack. We use 4 MByte as default and if the\n    super-large-page is set we increase it to 256 MByte. 256 MByte\n    is for server installations with GBytes of RAM memory where\n    the MySQL Server will have page caches and other memory regions\n    measured in a number of GBytes.\n    We use as big pages as possible which isn't bigger than the above\n    desired page sizes.\n  */\n   int nelem;\n   size_t max_desired_page_size;\n   if (opt_super_large_pages)\n     max_desired_page_size= SUPER_LARGE_PAGESIZE;\n   else\n     max_desired_page_size= LARGE_PAGESIZE;\n   nelem = getpagesizes(NULL, 0);\n   if (nelem > 0)\n   {\n     size_t *pagesize = (size_t *) malloc(sizeof(size_t) * nelem);\n     if (pagesize != NULL && getpagesizes(pagesize, nelem) > 0)\n     {\n       size_t max_page_size= 0;\n       for (int i= 0; i < nelem; i++)\n       {\n         if (pagesize[i] > max_page_size &&\n             pagesize[i] <= max_desired_page_size)\n            max_page_size= pagesize[i];\n       }\n       free(pagesize);\n       if (max_page_size > 0)\n       {\n         struct memcntl_mha mpss;\n\n         mpss.mha_cmd= MHA_MAPSIZE_BSSBRK;\n         mpss.mha_pagesize= max_page_size;\n         mpss.mha_flags= 0;\n         memcntl(NULL, 0, MC_HAT_ADVISE, (caddr_t)&mpss, 0, 0);\n         mpss.mha_cmd= MHA_MAPSIZE_STACK;\n         memcntl(NULL, 0, MC_HAT_ADVISE, (caddr_t)&mpss, 0, 0);\n       }\n     }\n   }\n  }\n#endif /* HAVE_SOLARIS_LARGE_PAGES */\n\n  /* connections and databases needs lots of files */\n  {\n    uint files, wanted_files, max_open_files;\n\n    /* MyISAM requires two file handles per table. */\n    wanted_files= (10 + max_connections + extra_max_connections +\n                   table_cache_size*2);\n    /*\n      We are trying to allocate no less than max_connections*5 file\n      handles (i.e. we are trying to set the limit so that they will\n      be available).  In addition, we allocate no less than how much\n      was already allocated.  However below we report a warning and\n      recompute values only if we got less file handles than were\n      explicitly requested.  No warning and re-computation occur if we\n      can't get max_connections*5 but still got no less than was\n      requested (value of wanted_files).\n    */\n    max_open_files= max(max(wanted_files,\n                            (max_connections + extra_max_connections)*5),\n                        open_files_limit);\n    files= my_set_max_open_files(max_open_files);\n\n    if (files < wanted_files)\n    {\n      if (!open_files_limit)\n      {\n        /*\n          If we have requested too much file handles than we bring\n          max_connections in supported bounds.\n        */\n        max_connections= (ulong) min(files-10-TABLE_OPEN_CACHE_MIN*2,\n                                     max_connections);\n        /*\n          Decrease table_cache_size according to max_connections, but\n          not below TABLE_OPEN_CACHE_MIN.  Outer min() ensures that we\n          never increase table_cache_size automatically (that could\n          happen if max_connections is decreased above).\n        */\n        table_cache_size= (ulong) min(max((files-10-max_connections)/2,\n                                          TABLE_OPEN_CACHE_MIN),\n                                      table_cache_size);\n\tDBUG_PRINT(\"warning\",\n\t\t   (\"Changed limits: max_open_files: %u  max_connections: %ld  table_cache: %ld\",\n\t\t    files, max_connections, table_cache_size));\n\tif (global_system_variables.log_warnings)\n\t  sql_print_warning(\"Changed limits: max_open_files: %u  max_connections: %ld  table_cache: %ld\",\n\t\t\tfiles, max_connections, table_cache_size);\n      }\n      else if (global_system_variables.log_warnings)\n\tsql_print_warning(\"Could not increase number of max_open_files to more than %u (request: %u)\", files, wanted_files);\n    }\n    open_files_limit= files;\n  }\n  unireg_init(opt_specialflag); /* Set up extern variabels */\n  if (!(my_default_lc_messages=\n        my_locale_by_name(lc_messages)))\n  {\n    sql_print_error(\"Unknown locale: '%s'\", lc_messages);\n    return 1;\n  }\n  global_system_variables.lc_messages= my_default_lc_messages;\n  if (init_errmessage())\t/* Read error messages from file */\n    return 1;\n  init_client_errs();\n  mysql_library_init(unused,unused,unused); /* for replication */\n  lex_init();\n  if (item_create_init())\n    return 1;\n  item_init();\n#ifndef EMBEDDED_LIBRARY\n  my_regex_init(&my_charset_latin1, check_enough_stack_size);\n  my_string_stack_guard= check_enough_stack_size;\n#else\n  my_regex_init(&my_charset_latin1, NULL);\n#endif\n  /*\n    Process a comma-separated character set list and choose\n    the first available character set. This is mostly for\n    test purposes, to be able to start \"mysqld\" even if\n    the requested character set is not available (see bug#18743).\n  */\n  for (;;)\n  {\n    char *next_character_set_name= strchr(default_character_set_name, ',');\n    if (next_character_set_name)\n      *next_character_set_name++= '\\0';\n    if (!(default_charset_info=\n          get_charset_by_csname(default_character_set_name,\n                                MY_CS_PRIMARY, MYF(MY_WME))))\n    {\n      if (next_character_set_name)\n      {\n        default_character_set_name= next_character_set_name;\n        default_collation_name= 0;          // Ignore collation\n      }\n      else\n        return 1;                           // Eof of the list\n    }\n    else\n      break;\n  }\n\n  if (default_collation_name)\n  {\n    CHARSET_INFO *default_collation;\n    default_collation= get_charset_by_name(default_collation_name, MYF(0));\n    if (!default_collation)\n    {\n      sql_print_error(ER_DEFAULT(ER_UNKNOWN_COLLATION), default_collation_name);\n      return 1;\n    }\n    if (!my_charset_same(default_charset_info, default_collation))\n    {\n      sql_print_error(ER_DEFAULT(ER_COLLATION_CHARSET_MISMATCH),\n\t\t      default_collation_name,\n\t\t      default_charset_info->csname);\n      return 1;\n    }\n    default_charset_info= default_collation;\n  }\n  /* Set collactions that depends on the default collation */\n  global_system_variables.collation_server=\t default_charset_info;\n  global_system_variables.collation_database=\t default_charset_info;\n  global_system_variables.collation_connection=  default_charset_info;\n  global_system_variables.character_set_results= default_charset_info;\n  global_system_variables.character_set_client=  default_charset_info;\n\n  if (!(character_set_filesystem=\n        get_charset_by_csname(character_set_filesystem_name,\n                              MY_CS_PRIMARY, MYF(MY_WME))))\n    return 1;\n  global_system_variables.character_set_filesystem= character_set_filesystem;\n\n  if (!(my_default_lc_time_names=\n        my_locale_by_name(lc_time_names_name)))\n  {\n    sql_print_error(\"Unknown locale: '%s'\", lc_time_names_name);\n    return 1;\n  }\n  global_system_variables.lc_time_names= my_default_lc_time_names;\n\n  /* check log options and issue warnings if needed */\n  if (opt_log && opt_logname && *opt_logname &&\n      !(log_output_options & (LOG_FILE | LOG_NONE)))\n    sql_print_warning(\"Although a path was specified for the \"\n                      \"--log option, log tables are used. \"\n                      \"To enable logging to files use the --log-output option.\");\n\n  if (opt_slow_log && opt_slow_logname && *opt_slow_logname &&\n      !(log_output_options & (LOG_FILE | LOG_NONE)))\n    sql_print_warning(\"Although a path was specified for the \"\n                      \"--log-slow-queries option, log tables are used. \"\n                      \"To enable logging to files use the --log-output=file option.\");\n\n  if (!opt_logname || !*opt_logname)\n    make_default_log_name(&opt_logname, \".log\", false);\n  if (!opt_slow_logname || !*opt_slow_logname)\n    make_default_log_name(&opt_slow_logname, \"-slow.log\", false);\n\n#if defined(ENABLED_DEBUG_SYNC)\n  /* Initialize the debug sync facility. See debug_sync.cc. */\n  if (debug_sync_init())\n    return 1; /* purecov: tested */\n#endif /* defined(ENABLED_DEBUG_SYNC) */\n\n#if (ENABLE_TEMP_POOL)\n  if (use_temp_pool && bitmap_init(&temp_pool,0,1024,1))\n    return 1;\n#else\n  use_temp_pool= 0;\n#endif\n\n  if (my_dboptions_cache_init())\n    return 1;\n\n  /*\n    Ensure that lower_case_table_names is set on system where we have case\n    insensitive names.  If this is not done the users MyISAM tables will\n    get corrupted if accesses with names of different case.\n  */\n  DBUG_PRINT(\"info\", (\"lower_case_table_names: %d\", lower_case_table_names));\n  lower_case_file_system= test_if_case_insensitive(mysql_real_data_home);\n  if (!lower_case_table_names && lower_case_file_system == 1)\n  {\n    if (lower_case_table_names_used)\n    {\n#if MYSQL_VERSION_ID < 100100\n      if (global_system_variables.log_warnings)\n        sql_print_warning(\"You have forced lower_case_table_names to 0 through \"\n                          \"a command-line option, even though your file system \"\n                          \"'%s' is case insensitive.  This means that you can \"\n                          \"corrupt your tables if you access them using names \"\n                          \"with different letter case. You should consider \"\n                          \"changing lower_case_table_names to 1 or 2\",\n                          mysql_real_data_home);\n#else\n      sql_print_error(\"The server option 'lower_case_table_names' is \"\n                      \"configured to use case sensitive table names but the \"\n                      \"data directory resides on a case-insensitive file system. \"\n                      \"Please use a case sensitive file system for your data \"\n                      \"directory or switch to a case-insensitive table name \"\n                      \"mode.\");\n#endif\n      return 1;\n    }\n    else\n    {\n      if (global_system_variables.log_warnings)\n\tsql_print_warning(\"Setting lower_case_table_names=2 because file system for %s is case insensitive\", mysql_real_data_home);\n      lower_case_table_names= 2;\n    }\n  }\n  else if (lower_case_table_names == 2 &&\n           !(lower_case_file_system= (lower_case_file_system == 1)))\n  {\n    if (global_system_variables.log_warnings)\n      sql_print_warning(\"lower_case_table_names was set to 2, even though your \"\n                        \"the file system '%s' is case sensitive.  Now setting \"\n                        \"lower_case_table_names to 0 to avoid future problems.\",\n\t\t\tmysql_real_data_home);\n    lower_case_table_names= 0;\n  }\n  else\n  {\n    lower_case_file_system= (lower_case_file_system == 1);\n  }\n\n  /* Reset table_alias_charset, now that lower_case_table_names is set. */\n  table_alias_charset= (lower_case_table_names ?\n\t\t\tfiles_charset_info :\n\t\t\t&my_charset_bin);\n\n  if (ignore_db_dirs_process_additions())\n  {\n    sql_print_error(\"An error occurred while storing ignore_db_dirs to a hash.\");\n    return 1;\n  }\n\n  return 0;\n}",
        "func": "static int init_common_variables()\n{\n  umask(((~my_umask) & 0666));\n  my_decimal_set_zero(&decimal_zero); // set decimal_zero constant;\n\n  tzset();\t\t\t// Set tzname\n\n  sf_leaking_memory= 0; // no memory leaks from now on\n\n  max_system_variables.pseudo_thread_id= (ulong)~0;\n  server_start_time= flush_status_time= my_time(0);\n  my_disable_copystat_in_redel= 1;\n\n  rpl_filter= new Rpl_filter;\n  binlog_filter= new Rpl_filter;\n  if (!rpl_filter || !binlog_filter)\n  {\n    sql_perror(\"Could not allocate replication and binlog filters\");\n    return 1;\n  }\n\n  if (init_thread_environment() ||\n      mysql_init_variables())\n    return 1;\n\n  if (ignore_db_dirs_init())\n    return 1;\n\n#ifdef HAVE_TZNAME\n  struct tm tm_tmp;\n  localtime_r(&server_start_time,&tm_tmp);\n  const char *tz_name=  tzname[tm_tmp.tm_isdst != 0 ? 1 : 0];\n#ifdef _WIN32\n  /*\n    Time zone name may be localized and contain non-ASCII characters,\n    Convert from ANSI encoding to UTF8.\n  */\n  wchar_t wtz_name[sizeof(system_time_zone)];\n  mbstowcs(wtz_name, tz_name, sizeof(system_time_zone)-1);\n  WideCharToMultiByte(CP_UTF8,0, wtz_name, -1, system_time_zone, \n    sizeof(system_time_zone) - 1, NULL, NULL);\n#else\n  strmake_buf(system_time_zone, tz_name);\n#endif /* _WIN32 */\n#endif /* HAVE_TZNAME */\n\n  /*\n    We set SYSTEM time zone as reasonable default and\n    also for failure of my_tz_init() and bootstrap mode.\n    If user explicitly set time zone with --default-time-zone\n    option we will change this value in my_tz_init().\n  */\n  global_system_variables.time_zone= my_tz_SYSTEM;\n\n#ifdef HAVE_PSI_INTERFACE\n  /*\n    Complete the mysql_bin_log initialization.\n    Instrumentation keys are known only after the performance schema initialization,\n    and can not be set in the MYSQL_BIN_LOG constructor (called before main()).\n  */\n  mysql_bin_log.set_psi_keys(key_BINLOG_LOCK_index,\n                             key_BINLOG_update_cond,\n                             key_file_binlog,\n                             key_file_binlog_index,\n                             key_BINLOG_COND_queue_busy);\n#endif\n\n  /*\n    Init mutexes for the global MYSQL_BIN_LOG objects.\n    As safe_mutex depends on what MY_INIT() does, we can't init the mutexes of\n    global MYSQL_BIN_LOGs in their constructors, because then they would be\n    inited before MY_INIT(). So we do it here.\n  */\n  mysql_bin_log.init_pthread_objects();\n\n  /* TODO: remove this when my_time_t is 64 bit compatible */\n  if (!IS_TIME_T_VALID_FOR_TIMESTAMP(server_start_time))\n  {\n    sql_print_error(\"This MySQL server doesn't support dates later then 2038\");\n    return 1;\n  }\n\n  if (gethostname(glob_hostname,sizeof(glob_hostname)) < 0)\n  {\n    /*\n      Get hostname of computer (used by 'show variables') and as default\n      basename for the pid file if --log-basename is not given.\n    */\n    strmake(glob_hostname, STRING_WITH_LEN(\"localhost\"));\n    sql_print_warning(\"gethostname failed, using '%s' as hostname\",\n                        glob_hostname);\n    opt_log_basename= const_cast<char *>(\"mysql\");\n  }\n  else\n    opt_log_basename= glob_hostname;\n\n  if (!*pidfile_name)\n  {\n    strmake(pidfile_name, opt_log_basename, sizeof(pidfile_name)-5);\n    strmov(fn_ext(pidfile_name),\".pid\");\t\t// Add proper extension\n  }\n\n  /*\n    The default-storage-engine entry in my_long_options should have a\n    non-null default value. It was earlier intialized as\n    (longlong)\"MyISAM\" in my_long_options but this triggered a\n    compiler error in the Sun Studio 12 compiler. As a work-around we\n    set the def_value member to 0 in my_long_options and initialize it\n    to the correct value here.\n\n    From MySQL 5.5 onwards, the default storage engine is InnoDB\n    (except in the embedded server, where the default continues to\n    be MyISAM)\n  */\n#if defined(WITH_INNOBASE_STORAGE_ENGINE) || defined(WITH_XTRADB_STORAGE_ENGINE)\n  default_storage_engine= const_cast<char *>(\"InnoDB\");\n#else\n  default_storage_engine= const_cast<char *>(\"MyISAM\");\n#endif\n\n  /*\n    Add server status variables to the dynamic list of\n    status variables that is shown by SHOW STATUS.\n    Later, in plugin_init, and mysql_install_plugin\n    new entries could be added to that list.\n  */\n  if (add_status_vars(status_vars))\n    return 1; // an error was already reported\n\n#ifndef DBUG_OFF\n  /*\n    We have few debug-only commands in com_status_vars, only visible in debug\n    builds. for simplicity we enable the assert only in debug builds\n\n    There are 8 Com_ variables which don't have corresponding SQLCOM_ values:\n    (TODO strictly speaking they shouldn't be here, should not have Com_ prefix\n    that is. Perhaps Stmt_ ? Comstmt_ ? Prepstmt_ ?)\n\n      Com_admin_commands       => com_other\n      Com_stmt_close           => com_stmt_close\n      Com_stmt_execute         => com_stmt_execute\n      Com_stmt_fetch           => com_stmt_fetch\n      Com_stmt_prepare         => com_stmt_prepare\n      Com_stmt_reprepare       => com_stmt_reprepare\n      Com_stmt_reset           => com_stmt_reset\n      Com_stmt_send_long_data  => com_stmt_send_long_data\n\n    With this correction the number of Com_ variables (number of elements in\n    the array, excluding the last element - terminator) must match the number\n    of SQLCOM_ constants.\n  */\n  compile_time_assert(sizeof(com_status_vars)/sizeof(com_status_vars[0]) - 1 ==\n                     SQLCOM_END + 8);\n#endif\n\n  if (get_options(&remaining_argc, &remaining_argv))\n    return 1;\n  set_server_version();\n\n  if (!opt_help)\n    sql_print_information(\"%s (mysqld %s) starting as process %lu ...\",\n                          my_progname, server_version, (ulong) getpid());\n\n#ifndef EMBEDDED_LIBRARY\n  if (opt_abort && !opt_verbose)\n    unireg_abort(0);\n#endif /*!EMBEDDED_LIBRARY*/\n\n  DBUG_PRINT(\"info\",(\"%s  Ver %s for %s on %s\\n\",my_progname,\n\t\t     server_version, SYSTEM_TYPE,MACHINE_TYPE));\n\n#ifdef HAVE_LARGE_PAGES\n  /* Initialize large page size */\n  if (opt_large_pages && (opt_large_page_size= my_get_large_page_size()))\n  {\n      DBUG_PRINT(\"info\", (\"Large page set, large_page_size = %d\",\n                 opt_large_page_size));\n      my_use_large_pages= 1;\n      my_large_page_size= opt_large_page_size;\n  }\n  else\n  {\n    opt_large_pages= 0;\n    /* \n       Either not configured to use large pages or Linux haven't\n       been compiled with large page support\n    */\n  }\n#endif /* HAVE_LARGE_PAGES */\n#ifdef HAVE_SOLARIS_LARGE_PAGES\n#define LARGE_PAGESIZE (4*1024*1024)  /* 4MB */\n#define SUPER_LARGE_PAGESIZE (256*1024*1024)  /* 256MB */\n  if (opt_large_pages)\n  {\n  /*\n    tell the kernel that we want to use 4/256MB page for heap storage\n    and also for the stack. We use 4 MByte as default and if the\n    super-large-page is set we increase it to 256 MByte. 256 MByte\n    is for server installations with GBytes of RAM memory where\n    the MySQL Server will have page caches and other memory regions\n    measured in a number of GBytes.\n    We use as big pages as possible which isn't bigger than the above\n    desired page sizes.\n  */\n   int nelem;\n   size_t max_desired_page_size;\n   if (opt_super_large_pages)\n     max_desired_page_size= SUPER_LARGE_PAGESIZE;\n   else\n     max_desired_page_size= LARGE_PAGESIZE;\n   nelem = getpagesizes(NULL, 0);\n   if (nelem > 0)\n   {\n     size_t *pagesize = (size_t *) malloc(sizeof(size_t) * nelem);\n     if (pagesize != NULL && getpagesizes(pagesize, nelem) > 0)\n     {\n       size_t max_page_size= 0;\n       for (int i= 0; i < nelem; i++)\n       {\n         if (pagesize[i] > max_page_size &&\n             pagesize[i] <= max_desired_page_size)\n            max_page_size= pagesize[i];\n       }\n       free(pagesize);\n       if (max_page_size > 0)\n       {\n         struct memcntl_mha mpss;\n\n         mpss.mha_cmd= MHA_MAPSIZE_BSSBRK;\n         mpss.mha_pagesize= max_page_size;\n         mpss.mha_flags= 0;\n         memcntl(NULL, 0, MC_HAT_ADVISE, (caddr_t)&mpss, 0, 0);\n         mpss.mha_cmd= MHA_MAPSIZE_STACK;\n         memcntl(NULL, 0, MC_HAT_ADVISE, (caddr_t)&mpss, 0, 0);\n       }\n     }\n   }\n  }\n#endif /* HAVE_SOLARIS_LARGE_PAGES */\n\n  /* connections and databases needs lots of files */\n  {\n    uint files, wanted_files, max_open_files;\n\n    /* MyISAM requires two file handles per table. */\n    wanted_files= (10 + max_connections + extra_max_connections +\n                   table_cache_size*2);\n    /*\n      We are trying to allocate no less than max_connections*5 file\n      handles (i.e. we are trying to set the limit so that they will\n      be available).  In addition, we allocate no less than how much\n      was already allocated.  However below we report a warning and\n      recompute values only if we got less file handles than were\n      explicitly requested.  No warning and re-computation occur if we\n      can't get max_connections*5 but still got no less than was\n      requested (value of wanted_files).\n    */\n    max_open_files= max(max(wanted_files,\n                            (max_connections + extra_max_connections)*5),\n                        open_files_limit);\n    files= my_set_max_open_files(max_open_files);\n\n    if (files < wanted_files)\n    {\n      if (!open_files_limit)\n      {\n        /*\n          If we have requested too much file handles than we bring\n          max_connections in supported bounds.\n        */\n        max_connections= (ulong) min(files-10-TABLE_OPEN_CACHE_MIN*2,\n                                     max_connections);\n        /*\n          Decrease table_cache_size according to max_connections, but\n          not below TABLE_OPEN_CACHE_MIN.  Outer min() ensures that we\n          never increase table_cache_size automatically (that could\n          happen if max_connections is decreased above).\n        */\n        table_cache_size= (ulong) min(max((files-10-max_connections)/2,\n                                          TABLE_OPEN_CACHE_MIN),\n                                      table_cache_size);\n\tDBUG_PRINT(\"warning\",\n\t\t   (\"Changed limits: max_open_files: %u  max_connections: %ld  table_cache: %ld\",\n\t\t    files, max_connections, table_cache_size));\n\tif (global_system_variables.log_warnings)\n\t  sql_print_warning(\"Changed limits: max_open_files: %u  max_connections: %ld  table_cache: %ld\",\n\t\t\tfiles, max_connections, table_cache_size);\n      }\n      else if (global_system_variables.log_warnings)\n\tsql_print_warning(\"Could not increase number of max_open_files to more than %u (request: %u)\", files, wanted_files);\n    }\n    open_files_limit= files;\n  }\n  unireg_init(opt_specialflag); /* Set up extern variabels */\n  if (!(my_default_lc_messages=\n        my_locale_by_name(lc_messages)))\n  {\n    sql_print_error(\"Unknown locale: '%s'\", lc_messages);\n    return 1;\n  }\n  global_system_variables.lc_messages= my_default_lc_messages;\n  if (init_errmessage())\t/* Read error messages from file */\n    return 1;\n  init_client_errs();\n  mysql_library_init(unused,unused,unused); /* for replication */\n  lex_init();\n  if (item_create_init())\n    return 1;\n  item_init();\n#ifndef EMBEDDED_LIBRARY\n  my_regex_init(&my_charset_latin1, check_enough_stack_size);\n  my_string_stack_guard= check_enough_stack_size;\n#else\n  my_regex_init(&my_charset_latin1, NULL);\n#endif\n  /*\n    Process a comma-separated character set list and choose\n    the first available character set. This is mostly for\n    test purposes, to be able to start \"mysqld\" even if\n    the requested character set is not available (see bug#18743).\n  */\n  for (;;)\n  {\n    char *next_character_set_name= strchr(default_character_set_name, ',');\n    if (next_character_set_name)\n      *next_character_set_name++= '\\0';\n    if (!(default_charset_info=\n          get_charset_by_csname(default_character_set_name,\n                                MY_CS_PRIMARY, MYF(MY_WME))))\n    {\n      if (next_character_set_name)\n      {\n        default_character_set_name= next_character_set_name;\n        default_collation_name= 0;          // Ignore collation\n      }\n      else\n        return 1;                           // Eof of the list\n    }\n    else\n      break;\n  }\n\n  if (default_collation_name)\n  {\n    CHARSET_INFO *default_collation;\n    default_collation= get_charset_by_name(default_collation_name, MYF(0));\n    if (!default_collation)\n    {\n      sql_print_error(ER_DEFAULT(ER_UNKNOWN_COLLATION), default_collation_name);\n      return 1;\n    }\n    if (!my_charset_same(default_charset_info, default_collation))\n    {\n      sql_print_error(ER_DEFAULT(ER_COLLATION_CHARSET_MISMATCH),\n\t\t      default_collation_name,\n\t\t      default_charset_info->csname);\n      return 1;\n    }\n    default_charset_info= default_collation;\n  }\n  /* Set collactions that depends on the default collation */\n  global_system_variables.collation_server=\t default_charset_info;\n  global_system_variables.collation_database=\t default_charset_info;\n  global_system_variables.collation_connection=  default_charset_info;\n  global_system_variables.character_set_results= default_charset_info;\n  global_system_variables.character_set_client=  default_charset_info;\n\n  if (!(character_set_filesystem=\n        get_charset_by_csname(character_set_filesystem_name,\n                              MY_CS_PRIMARY, MYF(MY_WME))))\n    return 1;\n  global_system_variables.character_set_filesystem= character_set_filesystem;\n\n  if (!(my_default_lc_time_names=\n        my_locale_by_name(lc_time_names_name)))\n  {\n    sql_print_error(\"Unknown locale: '%s'\", lc_time_names_name);\n    return 1;\n  }\n  global_system_variables.lc_time_names= my_default_lc_time_names;\n\n  /* check log options and issue warnings if needed */\n  if (opt_log && opt_logname && *opt_logname &&\n      !(log_output_options & (LOG_FILE | LOG_NONE)))\n    sql_print_warning(\"Although a path was specified for the \"\n                      \"--log option, log tables are used. \"\n                      \"To enable logging to files use the --log-output option.\");\n\n  if (opt_slow_log && opt_slow_logname && *opt_slow_logname &&\n      !(log_output_options & (LOG_FILE | LOG_NONE)))\n    sql_print_warning(\"Although a path was specified for the \"\n                      \"--log-slow-queries option, log tables are used. \"\n                      \"To enable logging to files use the --log-output=file option.\");\n\n  if (!opt_logname || !*opt_logname)\n    make_default_log_name(&opt_logname, \".log\", false);\n  if (!opt_slow_logname || !*opt_slow_logname)\n    make_default_log_name(&opt_slow_logname, \"-slow.log\", false);\n\n#if defined(ENABLED_DEBUG_SYNC)\n  /* Initialize the debug sync facility. See debug_sync.cc. */\n  if (debug_sync_init())\n    return 1; /* purecov: tested */\n#endif /* defined(ENABLED_DEBUG_SYNC) */\n\n#if (ENABLE_TEMP_POOL)\n  if (use_temp_pool && bitmap_init(&temp_pool,0,1024,1))\n    return 1;\n#else\n  use_temp_pool= 0;\n#endif\n\n  if (my_dboptions_cache_init())\n    return 1;\n\n  /*\n    Ensure that lower_case_table_names is set on system where we have case\n    insensitive names.  If this is not done the users MyISAM tables will\n    get corrupted if accesses with names of different case.\n  */\n  DBUG_PRINT(\"info\", (\"lower_case_table_names: %d\", lower_case_table_names));\n  lower_case_file_system= test_if_case_insensitive(mysql_real_data_home);\n  if (!lower_case_table_names && lower_case_file_system == 1)\n  {\n    if (lower_case_table_names_used)\n    {\n#if MYSQL_VERSION_ID < 100100\n      if (global_system_variables.log_warnings)\n        sql_print_warning(\"You have forced lower_case_table_names to 0 through \"\n                          \"a command-line option, even though your file system \"\n                          \"'%s' is case insensitive.  This means that you can \"\n                          \"corrupt your tables if you access them using names \"\n                          \"with different letter case. You should consider \"\n                          \"changing lower_case_table_names to 1 or 2\",\n                          mysql_real_data_home);\n#else\n      sql_print_error(\"The server option 'lower_case_table_names' is \"\n                      \"configured to use case sensitive table names but the \"\n                      \"data directory resides on a case-insensitive file system. \"\n                      \"Please use a case sensitive file system for your data \"\n                      \"directory or switch to a case-insensitive table name \"\n                      \"mode.\");\n#endif\n      return 1;\n    }\n    else\n    {\n      if (global_system_variables.log_warnings)\n\tsql_print_warning(\"Setting lower_case_table_names=2 because file system for %s is case insensitive\", mysql_real_data_home);\n      lower_case_table_names= 2;\n    }\n  }\n  else if (lower_case_table_names == 2 &&\n           !(lower_case_file_system= (lower_case_file_system == 1)))\n  {\n    if (global_system_variables.log_warnings)\n      sql_print_warning(\"lower_case_table_names was set to 2, even though your \"\n                        \"the file system '%s' is case sensitive.  Now setting \"\n                        \"lower_case_table_names to 0 to avoid future problems.\",\n\t\t\tmysql_real_data_home);\n    lower_case_table_names= 0;\n  }\n  else\n  {\n    lower_case_file_system= (lower_case_file_system == 1);\n  }\n\n  /* Reset table_alias_charset, now that lower_case_table_names is set. */\n  table_alias_charset= (lower_case_table_names ?\n\t\t\tfiles_charset_info :\n\t\t\t&my_charset_bin);\n\n  if (ignore_db_dirs_process_additions())\n  {\n    sql_print_error(\"An error occurred while storing ignore_db_dirs to a hash.\");\n    return 1;\n  }\n\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,7 @@\n \n   max_system_variables.pseudo_thread_id= (ulong)~0;\n   server_start_time= flush_status_time= my_time(0);\n+  my_disable_copystat_in_redel= 1;\n \n   rpl_filter= new Rpl_filter;\n   binlog_filter= new Rpl_filter;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  my_disable_copystat_in_redel= 1;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-9794",
        "func_name": "torvalds/linux/snd_pcm_period_elapsed",
        "description": "Race condition in the snd_pcm_period_elapsed function in sound/core/pcm_lib.c in the ALSA subsystem in the Linux kernel before 4.7 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a crafted SNDRV_PCM_TRIGGER_START command.",
        "git_url": "https://github.com/torvalds/linux/commit/3aa02cb664c5fb1042958c8d1aa8c35055a2ebc4",
        "commit_title": "ALSA: pcm : Call kill_fasync() in stream lock",
        "commit_text": " Currently kill_fasync() is called outside the stream lock in snd_pcm_period_elapsed().  This is potentially racy, since the stream may get released even during the irq handler is running.  Although snd_pcm_release_substream() calls snd_pcm_drop(), this doesn't guarantee that the irq handler finishes, thus the kill_fasync() call outside the stream spin lock may be invoked after the substream is detached, as recently reported by KASAN.  As a quick workaround, move kill_fasync() call inside the stream lock.  The fasync is rarely used interface, so this shouldn't have a big impact from the performance POV.  Ideally, we should implement some sync mechanism for the proper finish of stream and irq handler.  But this oneliner should suffice for most cases, so far. ",
        "func_before": "void snd_pcm_period_elapsed(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tunsigned long flags;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\n\tsnd_pcm_stream_lock_irqsave(substream, flags);\n\tif (!snd_pcm_running(substream) ||\n\t    snd_pcm_update_hw_ptr0(substream, 1) < 0)\n\t\tgoto _end;\n\n#ifdef CONFIG_SND_PCM_TIMER\n\tif (substream->timer_running)\n\t\tsnd_timer_interrupt(substream->timer, 1);\n#endif\n _end:\n\tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n}",
        "func": "void snd_pcm_period_elapsed(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tunsigned long flags;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\n\tsnd_pcm_stream_lock_irqsave(substream, flags);\n\tif (!snd_pcm_running(substream) ||\n\t    snd_pcm_update_hw_ptr0(substream, 1) < 0)\n\t\tgoto _end;\n\n#ifdef CONFIG_SND_PCM_TIMER\n\tif (substream->timer_running)\n\t\tsnd_timer_interrupt(substream->timer, 1);\n#endif\n _end:\n\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n\tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,6 +17,6 @@\n \t\tsnd_timer_interrupt(substream->timer, 1);\n #endif\n  _end:\n+\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n \tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n-\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);"
            ],
            "added_lines": [
                "\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-9806",
        "func_name": "torvalds/linux/netlink_dump",
        "description": "Race condition in the netlink_dump function in net/netlink/af_netlink.c in the Linux kernel before 4.6.3 allows local users to cause a denial of service (double free) or possibly have unspecified other impact via a crafted application that makes sendmsg system calls, leading to a free operation associated with a new dump that started earlier than anticipated.",
        "git_url": "https://github.com/torvalds/linux/commit/92964c79b357efd980812c4de5c1fd2ec8bb5520",
        "commit_title": "netlink: Fix dump skb leak/double free",
        "commit_text": " When we free cb->skb after a dump, we do it after releasing the lock.  This means that a new dump could have started in the time being and we'll end up freeing their skb instead of ours.  This patch saves the skb and module before we unlock so we free the right memory. ",
        "func_before": "static int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tint len, err = -ENOBUFS;\n\tint alloc_min_size;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tcb = &nlk->cb;\n\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (alloc_min_size < nlk->max_recvmsg_len) {\n\t\talloc_size = nlk->max_recvmsg_len;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL |\n\t\t\t\t\t    __GFP_NOWARN | __GFP_NORETRY);\n\t}\n\tif (!skb) {\n\t\talloc_size = alloc_min_size;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL);\n\t}\n\tif (!skb)\n\t\tgoto errout_skb;\n\n\t/* Trim skb to allocated size. User is expected to provide buffer as\n\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at\n\t * netlink_recvmsg())). dump will pack as many smaller messages as\n\t * could fit within the allocated skb. skb is typically allocated\n\t * with larger space than required (could be as much as near 2x the\n\t * requested size with align to next power of 2 approach). Allowing\n\t * dump to use the excess space makes it difficult for a user to have a\n\t * reasonable static buffer based on the expected largest dump of a\n\t * single netdev. The outcome is MSG_TRUNC error.\n\t */\n\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(cb->module);\n\tconsume_skb(cb->skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}",
        "func": "static int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tstruct module *module;\n\tint len, err = -ENOBUFS;\n\tint alloc_min_size;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tcb = &nlk->cb;\n\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (alloc_min_size < nlk->max_recvmsg_len) {\n\t\talloc_size = nlk->max_recvmsg_len;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL |\n\t\t\t\t\t    __GFP_NOWARN | __GFP_NORETRY);\n\t}\n\tif (!skb) {\n\t\talloc_size = alloc_min_size;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL);\n\t}\n\tif (!skb)\n\t\tgoto errout_skb;\n\n\t/* Trim skb to allocated size. User is expected to provide buffer as\n\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at\n\t * netlink_recvmsg())). dump will pack as many smaller messages as\n\t * could fit within the allocated skb. skb is typically allocated\n\t * with larger space than required (could be as much as near 2x the\n\t * requested size with align to next power of 2 approach). Allowing\n\t * dump to use the excess space makes it difficult for a user to have a\n\t * reasonable static buffer based on the expected largest dump of a\n\t * single netdev. The outcome is MSG_TRUNC error.\n\t */\n\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmodule = cb->module;\n\tskb = cb->skb;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(module);\n\tconsume_skb(skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,7 @@\n \tstruct netlink_callback *cb;\n \tstruct sk_buff *skb = NULL;\n \tstruct nlmsghdr *nlh;\n+\tstruct module *module;\n \tint len, err = -ENOBUFS;\n \tint alloc_min_size;\n \tint alloc_size;\n@@ -79,9 +80,11 @@\n \t\tcb->done(cb);\n \n \tnlk->cb_running = false;\n+\tmodule = cb->module;\n+\tskb = cb->skb;\n \tmutex_unlock(nlk->cb_mutex);\n-\tmodule_put(cb->module);\n-\tconsume_skb(cb->skb);\n+\tmodule_put(module);\n+\tconsume_skb(skb);\n \treturn 0;\n \n errout_skb:",
        "diff_line_info": {
            "deleted_lines": [
                "\tmodule_put(cb->module);",
                "\tconsume_skb(cb->skb);"
            ],
            "added_lines": [
                "\tstruct module *module;",
                "\tmodule = cb->module;",
                "\tskb = cb->skb;",
                "\tmodule_put(module);",
                "\tconsume_skb(skb);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-3196",
        "func_name": "openssl/ssl3_send_client_key_exchange",
        "description": "ssl/s3_clnt.c in OpenSSL 1.0.0 before 1.0.0t, 1.0.1 before 1.0.1p, and 1.0.2 before 1.0.2d, when used for a multi-threaded client, writes the PSK identity hint to an incorrect data structure, which allows remote servers to cause a denial of service (race condition and double free) via a crafted ServerKeyExchange message.",
        "git_url": "https://git.openssl.org/?p=openssl.git;a=commit;h=3c66a669dfc7b3792f7af0758ea26fe8502ce70c",
        "commit_title": "",
        "commit_text": "Fix PSK handling.  The PSK identity hint should be stored in the SSL_SESSION structure and not in the parent context (which will overwrite values used by other SSL structures with the same SSL_CTX).  Use BUF_strndup when copying identity as it may not be null terminated.  ",
        "func_before": "int ssl3_send_client_key_exchange(SSL *s)\n{\n    unsigned char *p;\n    int n;\n    unsigned long alg_k;\n#ifndef OPENSSL_NO_RSA\n    unsigned char *q;\n    EVP_PKEY *pkey = NULL;\n#endif\n#ifndef OPENSSL_NO_KRB5\n    KSSL_ERR kssl_err;\n#endif                          /* OPENSSL_NO_KRB5 */\n#ifndef OPENSSL_NO_ECDH\n    EC_KEY *clnt_ecdh = NULL;\n    const EC_POINT *srvr_ecpoint = NULL;\n    EVP_PKEY *srvr_pub_pkey = NULL;\n    unsigned char *encodedPoint = NULL;\n    int encoded_pt_len = 0;\n    BN_CTX *bn_ctx = NULL;\n#endif\n\n    if (s->state == SSL3_ST_CW_KEY_EXCH_A) {\n        p = ssl_handshake_start(s);\n\n        alg_k = s->s3->tmp.new_cipher->algorithm_mkey;\n\n        /* Fool emacs indentation */\n        if (0) {\n        }\n#ifndef OPENSSL_NO_RSA\n        else if (alg_k & SSL_kRSA) {\n            RSA *rsa;\n            unsigned char tmp_buf[SSL_MAX_MASTER_KEY_LENGTH];\n\n            if (s->session->sess_cert == NULL) {\n                /*\n                 * We should always have a server certificate with SSL_kRSA.\n                 */\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n\n            if (s->session->sess_cert->peer_rsa_tmp != NULL)\n                rsa = s->session->sess_cert->peer_rsa_tmp;\n            else {\n                pkey =\n                    X509_get_pubkey(s->session->\n                                    sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].\n                                    x509);\n                if ((pkey == NULL) || (pkey->type != EVP_PKEY_RSA)\n                    || (pkey->pkey.rsa == NULL)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_INTERNAL_ERROR);\n                    goto err;\n                }\n                rsa = pkey->pkey.rsa;\n                EVP_PKEY_free(pkey);\n            }\n\n            tmp_buf[0] = s->client_version >> 8;\n            tmp_buf[1] = s->client_version & 0xff;\n            if (RAND_bytes(&(tmp_buf[2]), sizeof tmp_buf - 2) <= 0)\n                goto err;\n\n            s->session->master_key_length = sizeof tmp_buf;\n\n            q = p;\n            /* Fix buf for TLS and beyond */\n            if (s->version > SSL3_VERSION)\n                p += 2;\n            n = RSA_public_encrypt(sizeof tmp_buf,\n                                   tmp_buf, p, rsa, RSA_PKCS1_PADDING);\n# ifdef PKCS1_CHECK\n            if (s->options & SSL_OP_PKCS1_CHECK_1)\n                p[1]++;\n            if (s->options & SSL_OP_PKCS1_CHECK_2)\n                tmp_buf[0] = 0x70;\n# endif\n            if (n <= 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_BAD_RSA_ENCRYPT);\n                goto err;\n            }\n\n            /* Fix buf for TLS and beyond */\n            if (s->version > SSL3_VERSION) {\n                s2n(n, q);\n                n += 2;\n            }\n\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            tmp_buf,\n                                                            sizeof tmp_buf);\n            OPENSSL_cleanse(tmp_buf, sizeof tmp_buf);\n        }\n#endif\n#ifndef OPENSSL_NO_KRB5\n        else if (alg_k & SSL_kKRB5) {\n            krb5_error_code krb5rc;\n            KSSL_CTX *kssl_ctx = s->kssl_ctx;\n            /*  krb5_data   krb5_ap_req;  */\n            krb5_data *enc_ticket;\n            krb5_data authenticator, *authp = NULL;\n            EVP_CIPHER_CTX ciph_ctx;\n            const EVP_CIPHER *enc = NULL;\n            unsigned char iv[EVP_MAX_IV_LENGTH];\n            unsigned char tmp_buf[SSL_MAX_MASTER_KEY_LENGTH];\n            unsigned char epms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_IV_LENGTH];\n            int padl, outl = sizeof(epms);\n\n            EVP_CIPHER_CTX_init(&ciph_ctx);\n\n# ifdef KSSL_DEBUG\n            fprintf(stderr, \"ssl3_send_client_key_exchange(%lx & %lx)\\n\",\n                    alg_k, SSL_kKRB5);\n# endif                         /* KSSL_DEBUG */\n\n            authp = NULL;\n# ifdef KRB5SENDAUTH\n            if (KRB5SENDAUTH)\n                authp = &authenticator;\n# endif                         /* KRB5SENDAUTH */\n\n            krb5rc = kssl_cget_tkt(kssl_ctx, &enc_ticket, authp, &kssl_err);\n            enc = kssl_map_enc(kssl_ctx->enctype);\n            if (enc == NULL)\n                goto err;\n# ifdef KSSL_DEBUG\n            {\n                fprintf(stderr, \"kssl_cget_tkt rtn %d\\n\", krb5rc);\n                if (krb5rc && kssl_err.text)\n                    fprintf(stderr, \"kssl_cget_tkt kssl_err=%s\\n\",\n                            kssl_err.text);\n            }\n# endif                         /* KSSL_DEBUG */\n\n            if (krb5rc) {\n                ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_HANDSHAKE_FAILURE);\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, kssl_err.reason);\n                goto err;\n            }\n\n            /*-\n             * 20010406 VRS - Earlier versions used KRB5 AP_REQ\n             * in place of RFC 2712 KerberosWrapper, as in:\n             *\n             * Send ticket (copy to *p, set n = length)\n             * n = krb5_ap_req.length;\n             * memcpy(p, krb5_ap_req.data, krb5_ap_req.length);\n             * if (krb5_ap_req.data)\n             *   kssl_krb5_free_data_contents(NULL,&krb5_ap_req);\n             *\n             * Now using real RFC 2712 KerberosWrapper\n             * (Thanks to Simon Wilkinson <sxw@sxw.org.uk>)\n             * Note: 2712 \"opaque\" types are here replaced\n             * with a 2-byte length followed by the value.\n             * Example:\n             * KerberosWrapper= xx xx asn1ticket 0 0 xx xx encpms\n             * Where \"xx xx\" = length bytes.  Shown here with\n             * optional authenticator omitted.\n             */\n\n            /*  KerberosWrapper.Ticket              */\n            s2n(enc_ticket->length, p);\n            memcpy(p, enc_ticket->data, enc_ticket->length);\n            p += enc_ticket->length;\n            n = enc_ticket->length + 2;\n\n            /*  KerberosWrapper.Authenticator       */\n            if (authp && authp->length) {\n                s2n(authp->length, p);\n                memcpy(p, authp->data, authp->length);\n                p += authp->length;\n                n += authp->length + 2;\n\n                free(authp->data);\n                authp->data = NULL;\n                authp->length = 0;\n            } else {\n                s2n(0, p);      /* null authenticator length */\n                n += 2;\n            }\n\n            tmp_buf[0] = s->client_version >> 8;\n            tmp_buf[1] = s->client_version & 0xff;\n            if (RAND_bytes(&(tmp_buf[2]), sizeof tmp_buf - 2) <= 0)\n                goto err;\n\n            /*-\n             * 20010420 VRS.  Tried it this way; failed.\n             *      EVP_EncryptInit_ex(&ciph_ctx,enc, NULL,NULL);\n             *      EVP_CIPHER_CTX_set_key_length(&ciph_ctx,\n             *                              kssl_ctx->length);\n             *      EVP_EncryptInit_ex(&ciph_ctx,NULL, key,iv);\n             */\n\n            memset(iv, 0, sizeof iv); /* per RFC 1510 */\n            EVP_EncryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv);\n            EVP_EncryptUpdate(&ciph_ctx, epms, &outl, tmp_buf,\n                              sizeof tmp_buf);\n            EVP_EncryptFinal_ex(&ciph_ctx, &(epms[outl]), &padl);\n            outl += padl;\n            if (outl > (int)sizeof epms) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n            EVP_CIPHER_CTX_cleanup(&ciph_ctx);\n\n            /*  KerberosWrapper.EncryptedPreMasterSecret    */\n            s2n(outl, p);\n            memcpy(p, epms, outl);\n            p += outl;\n            n += outl + 2;\n\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            tmp_buf,\n                                                            sizeof tmp_buf);\n\n            OPENSSL_cleanse(tmp_buf, sizeof tmp_buf);\n            OPENSSL_cleanse(epms, outl);\n        }\n#endif\n#ifndef OPENSSL_NO_DH\n        else if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {\n            DH *dh_srvr, *dh_clnt;\n            SESS_CERT *scert = s->session->sess_cert;\n\n            if (scert == NULL) {\n                ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_UNEXPECTED_MESSAGE);\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_UNEXPECTED_MESSAGE);\n                goto err;\n            }\n\n            if (scert->peer_dh_tmp != NULL) {\n                dh_srvr = scert->peer_dh_tmp;\n            } else {\n                dh_srvr = get_server_static_dh_key(scert);\n                if (dh_srvr == NULL)\n                    goto err;\n            }\n\n            if (s->s3->flags & TLS1_FLAGS_SKIP_CERT_VERIFY) {\n                /* Use client certificate key */\n                EVP_PKEY *clkey = s->cert->key->privatekey;\n                dh_clnt = NULL;\n                if (clkey)\n                    dh_clnt = EVP_PKEY_get1_DH(clkey);\n                if (dh_clnt == NULL) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_INTERNAL_ERROR);\n                    goto err;\n                }\n            } else {\n                /* generate a new random key */\n                if ((dh_clnt = DHparams_dup(dh_srvr)) == NULL) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);\n                    goto err;\n                }\n                if (!DH_generate_key(dh_clnt)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);\n                    DH_free(dh_clnt);\n                    goto err;\n                }\n            }\n\n            /*\n             * use the 'p' output buffer for the DH key, but make sure to\n             * clear it out afterwards\n             */\n\n            n = DH_compute_key(p, dh_srvr->pub_key, dh_clnt);\n            if (scert->peer_dh_tmp == NULL)\n                DH_free(dh_srvr);\n\n            if (n <= 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);\n                DH_free(dh_clnt);\n                goto err;\n            }\n\n            /* generate master key from the result */\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            p, n);\n            /* clean up */\n            memset(p, 0, n);\n\n            if (s->s3->flags & TLS1_FLAGS_SKIP_CERT_VERIFY)\n                n = 0;\n            else {\n                /* send off the data */\n                n = BN_num_bytes(dh_clnt->pub_key);\n                s2n(n, p);\n                BN_bn2bin(dh_clnt->pub_key, p);\n                n += 2;\n            }\n\n            DH_free(dh_clnt);\n        }\n#endif\n\n#ifndef OPENSSL_NO_ECDH\n        else if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {\n            const EC_GROUP *srvr_group = NULL;\n            EC_KEY *tkey;\n            int ecdh_clnt_cert = 0;\n            int field_size = 0;\n\n            if (s->session->sess_cert == NULL) {\n                ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_UNEXPECTED_MESSAGE);\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_UNEXPECTED_MESSAGE);\n                goto err;\n            }\n\n            /*\n             * Did we send out the client's ECDH share for use in premaster\n             * computation as part of client certificate? If so, set\n             * ecdh_clnt_cert to 1.\n             */\n            if ((alg_k & (SSL_kECDHr | SSL_kECDHe)) && (s->cert != NULL)) {\n                /*-\n                 * XXX: For now, we do not support client\n                 * authentication using ECDH certificates.\n                 * To add such support, one needs to add\n                 * code that checks for appropriate\n                 * conditions and sets ecdh_clnt_cert to 1.\n                 * For example, the cert have an ECC\n                 * key on the same curve as the server's\n                 * and the key should be authorized for\n                 * key agreement.\n                 *\n                 * One also needs to add code in ssl3_connect\n                 * to skip sending the certificate verify\n                 * message.\n                 *\n                 * if ((s->cert->key->privatekey != NULL) &&\n                 *     (s->cert->key->privatekey->type ==\n                 *      EVP_PKEY_EC) && ...)\n                 * ecdh_clnt_cert = 1;\n                 */\n            }\n\n            if (s->session->sess_cert->peer_ecdh_tmp != NULL) {\n                tkey = s->session->sess_cert->peer_ecdh_tmp;\n            } else {\n                /* Get the Server Public Key from Cert */\n                srvr_pub_pkey =\n                    X509_get_pubkey(s->session->\n                                    sess_cert->peer_pkeys[SSL_PKEY_ECC].x509);\n                if ((srvr_pub_pkey == NULL)\n                    || (srvr_pub_pkey->type != EVP_PKEY_EC)\n                    || (srvr_pub_pkey->pkey.ec == NULL)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_INTERNAL_ERROR);\n                    goto err;\n                }\n\n                tkey = srvr_pub_pkey->pkey.ec;\n            }\n\n            srvr_group = EC_KEY_get0_group(tkey);\n            srvr_ecpoint = EC_KEY_get0_public_key(tkey);\n\n            if ((srvr_group == NULL) || (srvr_ecpoint == NULL)) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n\n            if ((clnt_ecdh = EC_KEY_new()) == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_MALLOC_FAILURE);\n                goto err;\n            }\n\n            if (!EC_KEY_set_group(clnt_ecdh, srvr_group)) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);\n                goto err;\n            }\n            if (ecdh_clnt_cert) {\n                /*\n                 * Reuse key info from our certificate We only need our\n                 * private key to perform the ECDH computation.\n                 */\n                const BIGNUM *priv_key;\n                tkey = s->cert->key->privatekey->pkey.ec;\n                priv_key = EC_KEY_get0_private_key(tkey);\n                if (priv_key == NULL) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_MALLOC_FAILURE);\n                    goto err;\n                }\n                if (!EC_KEY_set_private_key(clnt_ecdh, priv_key)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);\n                    goto err;\n                }\n            } else {\n                /* Generate a new ECDH key pair */\n                if (!(EC_KEY_generate_key(clnt_ecdh))) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_ECDH_LIB);\n                    goto err;\n                }\n            }\n\n            /*\n             * use the 'p' output buffer for the ECDH key, but make sure to\n             * clear it out afterwards\n             */\n\n            field_size = EC_GROUP_get_degree(srvr_group);\n            if (field_size <= 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);\n                goto err;\n            }\n            n = ECDH_compute_key(p, (field_size + 7) / 8, srvr_ecpoint,\n                                 clnt_ecdh, NULL);\n            if (n <= 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);\n                goto err;\n            }\n\n            /* generate master key from the result */\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            p, n);\n\n            memset(p, 0, n);    /* clean up */\n\n            if (ecdh_clnt_cert) {\n                /* Send empty client key exch message */\n                n = 0;\n            } else {\n                /*\n                 * First check the size of encoding and allocate memory\n                 * accordingly.\n                 */\n                encoded_pt_len =\n                    EC_POINT_point2oct(srvr_group,\n                                       EC_KEY_get0_public_key(clnt_ecdh),\n                                       POINT_CONVERSION_UNCOMPRESSED,\n                                       NULL, 0, NULL);\n\n                encodedPoint = (unsigned char *)\n                    OPENSSL_malloc(encoded_pt_len * sizeof(unsigned char));\n                bn_ctx = BN_CTX_new();\n                if ((encodedPoint == NULL) || (bn_ctx == NULL)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_MALLOC_FAILURE);\n                    goto err;\n                }\n\n                /* Encode the public key */\n                n = EC_POINT_point2oct(srvr_group,\n                                       EC_KEY_get0_public_key(clnt_ecdh),\n                                       POINT_CONVERSION_UNCOMPRESSED,\n                                       encodedPoint, encoded_pt_len, bn_ctx);\n\n                *p = n;         /* length of encoded point */\n                /* Encoded point will be copied here */\n                p += 1;\n                /* copy the point */\n                memcpy((unsigned char *)p, encodedPoint, n);\n                /* increment n to account for length field */\n                n += 1;\n            }\n\n            /* Free allocated memory */\n            BN_CTX_free(bn_ctx);\n            if (encodedPoint != NULL)\n                OPENSSL_free(encodedPoint);\n            if (clnt_ecdh != NULL)\n                EC_KEY_free(clnt_ecdh);\n            EVP_PKEY_free(srvr_pub_pkey);\n        }\n#endif                          /* !OPENSSL_NO_ECDH */\n        else if (alg_k & SSL_kGOST) {\n            /* GOST key exchange message creation */\n            EVP_PKEY_CTX *pkey_ctx;\n            X509 *peer_cert;\n            size_t msglen;\n            unsigned int md_len;\n            int keytype;\n            unsigned char premaster_secret[32], shared_ukm[32], tmp[256];\n            EVP_MD_CTX *ukm_hash;\n            EVP_PKEY *pub_key;\n\n            /*\n             * Get server sertificate PKEY and create ctx from it\n             */\n            peer_cert =\n                s->session->\n                sess_cert->peer_pkeys[(keytype = SSL_PKEY_GOST01)].x509;\n            if (!peer_cert)\n                peer_cert =\n                    s->session->\n                    sess_cert->peer_pkeys[(keytype = SSL_PKEY_GOST94)].x509;\n            if (!peer_cert) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_NO_GOST_CERTIFICATE_SENT_BY_PEER);\n                goto err;\n            }\n\n            pkey_ctx = EVP_PKEY_CTX_new(pub_key =\n                                        X509_get_pubkey(peer_cert), NULL);\n            /*\n             * If we have send a certificate, and certificate key\n             *\n             * * parameters match those of server certificate, use\n             * certificate key for key exchange\n             */\n\n            /* Otherwise, generate ephemeral key pair */\n\n            EVP_PKEY_encrypt_init(pkey_ctx);\n            /* Generate session key */\n            if (RAND_bytes(premaster_secret, 32) <= 0) {\n                EVP_PKEY_CTX_free(pkey_ctx);\n                goto err;\n            }\n            /*\n             * If we have client certificate, use its secret as peer key\n             */\n            if (s->s3->tmp.cert_req && s->cert->key->privatekey) {\n                if (EVP_PKEY_derive_set_peer\n                    (pkey_ctx, s->cert->key->privatekey) <= 0) {\n                    /*\n                     * If there was an error - just ignore it. Ephemeral key\n                     * * would be used\n                     */\n                    ERR_clear_error();\n                }\n            }\n            /*\n             * Compute shared IV and store it in algorithm-specific context\n             * data\n             */\n            ukm_hash = EVP_MD_CTX_create();\n            EVP_DigestInit(ukm_hash,\n                           EVP_get_digestbynid(NID_id_GostR3411_94));\n            EVP_DigestUpdate(ukm_hash, s->s3->client_random,\n                             SSL3_RANDOM_SIZE);\n            EVP_DigestUpdate(ukm_hash, s->s3->server_random,\n                             SSL3_RANDOM_SIZE);\n            EVP_DigestFinal_ex(ukm_hash, shared_ukm, &md_len);\n            EVP_MD_CTX_destroy(ukm_hash);\n            if (EVP_PKEY_CTX_ctrl\n                (pkey_ctx, -1, EVP_PKEY_OP_ENCRYPT, EVP_PKEY_CTRL_SET_IV, 8,\n                 shared_ukm) < 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_LIBRARY_BUG);\n                goto err;\n            }\n            /* Make GOST keytransport blob message */\n            /*\n             * Encapsulate it into sequence\n             */\n            *(p++) = V_ASN1_SEQUENCE | V_ASN1_CONSTRUCTED;\n            msglen = 255;\n            if (EVP_PKEY_encrypt(pkey_ctx, tmp, &msglen, premaster_secret, 32)\n                < 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_LIBRARY_BUG);\n                goto err;\n            }\n            if (msglen >= 0x80) {\n                *(p++) = 0x81;\n                *(p++) = msglen & 0xff;\n                n = msglen + 3;\n            } else {\n                *(p++) = msglen & 0xff;\n                n = msglen + 2;\n            }\n            memcpy(p, tmp, msglen);\n            /* Check if pubkey from client certificate was used */\n            if (EVP_PKEY_CTX_ctrl\n                (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0) {\n                /* Set flag \"skip certificate verify\" */\n                s->s3->flags |= TLS1_FLAGS_SKIP_CERT_VERIFY;\n            }\n            EVP_PKEY_CTX_free(pkey_ctx);\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            premaster_secret,\n                                                            32);\n            EVP_PKEY_free(pub_key);\n\n        }\n#ifndef OPENSSL_NO_SRP\n        else if (alg_k & SSL_kSRP) {\n            if (s->srp_ctx.A != NULL) {\n                /* send off the data */\n                n = BN_num_bytes(s->srp_ctx.A);\n                s2n(n, p);\n                BN_bn2bin(s->srp_ctx.A, p);\n                n += 2;\n            } else {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n            if (s->session->srp_username != NULL)\n                OPENSSL_free(s->session->srp_username);\n            s->session->srp_username = BUF_strdup(s->srp_ctx.login);\n            if (s->session->srp_username == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_MALLOC_FAILURE);\n                goto err;\n            }\n\n            if ((s->session->master_key_length =\n                 SRP_generate_client_master_secret(s,\n                                                   s->session->master_key)) <\n                0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n        }\n#endif\n#ifndef OPENSSL_NO_PSK\n        else if (alg_k & SSL_kPSK) {\n            /*\n             * The callback needs PSK_MAX_IDENTITY_LEN + 1 bytes to return a\n             * \\0-terminated identity. The last byte is for us for simulating\n             * strnlen.\n             */\n            char identity[PSK_MAX_IDENTITY_LEN + 2];\n            size_t identity_len;\n            unsigned char *t = NULL;\n            unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];\n            unsigned int pre_ms_len = 0, psk_len = 0;\n            int psk_err = 1;\n\n            n = 0;\n            if (s->psk_client_callback == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_PSK_NO_CLIENT_CB);\n                goto err;\n            }\n\n            memset(identity, 0, sizeof(identity));\n            psk_len = s->psk_client_callback(s, s->ctx->psk_identity_hint,\n                                             identity, sizeof(identity) - 1,\n                                             psk_or_pre_ms,\n                                             sizeof(psk_or_pre_ms));\n            if (psk_len > PSK_MAX_PSK_LEN) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto psk_err;\n            } else if (psk_len == 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_PSK_IDENTITY_NOT_FOUND);\n                goto psk_err;\n            }\n            identity[PSK_MAX_IDENTITY_LEN + 1] = '\\0';\n            identity_len = strlen(identity);\n            if (identity_len > PSK_MAX_IDENTITY_LEN) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto psk_err;\n            }\n            /* create PSK pre_master_secret */\n            pre_ms_len = 2 + psk_len + 2 + psk_len;\n            t = psk_or_pre_ms;\n            memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);\n            s2n(psk_len, t);\n            memset(t, 0, psk_len);\n            t += psk_len;\n            s2n(psk_len, t);\n\n            if (s->session->psk_identity_hint != NULL)\n                OPENSSL_free(s->session->psk_identity_hint);\n            s->session->psk_identity_hint =\n                BUF_strdup(s->ctx->psk_identity_hint);\n            if (s->ctx->psk_identity_hint != NULL\n                && s->session->psk_identity_hint == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_MALLOC_FAILURE);\n                goto psk_err;\n            }\n\n            if (s->session->psk_identity != NULL)\n                OPENSSL_free(s->session->psk_identity);\n            s->session->psk_identity = BUF_strdup(identity);\n            if (s->session->psk_identity == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_MALLOC_FAILURE);\n                goto psk_err;\n            }\n\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            psk_or_pre_ms,\n                                                            pre_ms_len);\n            s2n(identity_len, p);\n            memcpy(p, identity, identity_len);\n            n = 2 + identity_len;\n            psk_err = 0;\n psk_err:\n            OPENSSL_cleanse(identity, sizeof(identity));\n            OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));\n            if (psk_err != 0) {\n                ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_HANDSHAKE_FAILURE);\n                goto err;\n            }\n        }\n#endif\n        else {\n            ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_HANDSHAKE_FAILURE);\n            SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);\n            goto err;\n        }\n\n        ssl_set_handshake_header(s, SSL3_MT_CLIENT_KEY_EXCHANGE, n);\n        s->state = SSL3_ST_CW_KEY_EXCH_B;\n    }\n\n    /* SSL3_ST_CW_KEY_EXCH_B */\n    return ssl_do_write(s);\n err:\n#ifndef OPENSSL_NO_ECDH\n    BN_CTX_free(bn_ctx);\n    if (encodedPoint != NULL)\n        OPENSSL_free(encodedPoint);\n    if (clnt_ecdh != NULL)\n        EC_KEY_free(clnt_ecdh);\n    EVP_PKEY_free(srvr_pub_pkey);\n#endif\n    s->state = SSL_ST_ERR;\n    return (-1);\n}",
        "func": "int ssl3_send_client_key_exchange(SSL *s)\n{\n    unsigned char *p;\n    int n;\n    unsigned long alg_k;\n#ifndef OPENSSL_NO_RSA\n    unsigned char *q;\n    EVP_PKEY *pkey = NULL;\n#endif\n#ifndef OPENSSL_NO_KRB5\n    KSSL_ERR kssl_err;\n#endif                          /* OPENSSL_NO_KRB5 */\n#ifndef OPENSSL_NO_ECDH\n    EC_KEY *clnt_ecdh = NULL;\n    const EC_POINT *srvr_ecpoint = NULL;\n    EVP_PKEY *srvr_pub_pkey = NULL;\n    unsigned char *encodedPoint = NULL;\n    int encoded_pt_len = 0;\n    BN_CTX *bn_ctx = NULL;\n#endif\n\n    if (s->state == SSL3_ST_CW_KEY_EXCH_A) {\n        p = ssl_handshake_start(s);\n\n        alg_k = s->s3->tmp.new_cipher->algorithm_mkey;\n\n        /* Fool emacs indentation */\n        if (0) {\n        }\n#ifndef OPENSSL_NO_RSA\n        else if (alg_k & SSL_kRSA) {\n            RSA *rsa;\n            unsigned char tmp_buf[SSL_MAX_MASTER_KEY_LENGTH];\n\n            if (s->session->sess_cert == NULL) {\n                /*\n                 * We should always have a server certificate with SSL_kRSA.\n                 */\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n\n            if (s->session->sess_cert->peer_rsa_tmp != NULL)\n                rsa = s->session->sess_cert->peer_rsa_tmp;\n            else {\n                pkey =\n                    X509_get_pubkey(s->session->\n                                    sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].\n                                    x509);\n                if ((pkey == NULL) || (pkey->type != EVP_PKEY_RSA)\n                    || (pkey->pkey.rsa == NULL)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_INTERNAL_ERROR);\n                    goto err;\n                }\n                rsa = pkey->pkey.rsa;\n                EVP_PKEY_free(pkey);\n            }\n\n            tmp_buf[0] = s->client_version >> 8;\n            tmp_buf[1] = s->client_version & 0xff;\n            if (RAND_bytes(&(tmp_buf[2]), sizeof tmp_buf - 2) <= 0)\n                goto err;\n\n            s->session->master_key_length = sizeof tmp_buf;\n\n            q = p;\n            /* Fix buf for TLS and beyond */\n            if (s->version > SSL3_VERSION)\n                p += 2;\n            n = RSA_public_encrypt(sizeof tmp_buf,\n                                   tmp_buf, p, rsa, RSA_PKCS1_PADDING);\n# ifdef PKCS1_CHECK\n            if (s->options & SSL_OP_PKCS1_CHECK_1)\n                p[1]++;\n            if (s->options & SSL_OP_PKCS1_CHECK_2)\n                tmp_buf[0] = 0x70;\n# endif\n            if (n <= 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_BAD_RSA_ENCRYPT);\n                goto err;\n            }\n\n            /* Fix buf for TLS and beyond */\n            if (s->version > SSL3_VERSION) {\n                s2n(n, q);\n                n += 2;\n            }\n\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            tmp_buf,\n                                                            sizeof tmp_buf);\n            OPENSSL_cleanse(tmp_buf, sizeof tmp_buf);\n        }\n#endif\n#ifndef OPENSSL_NO_KRB5\n        else if (alg_k & SSL_kKRB5) {\n            krb5_error_code krb5rc;\n            KSSL_CTX *kssl_ctx = s->kssl_ctx;\n            /*  krb5_data   krb5_ap_req;  */\n            krb5_data *enc_ticket;\n            krb5_data authenticator, *authp = NULL;\n            EVP_CIPHER_CTX ciph_ctx;\n            const EVP_CIPHER *enc = NULL;\n            unsigned char iv[EVP_MAX_IV_LENGTH];\n            unsigned char tmp_buf[SSL_MAX_MASTER_KEY_LENGTH];\n            unsigned char epms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_IV_LENGTH];\n            int padl, outl = sizeof(epms);\n\n            EVP_CIPHER_CTX_init(&ciph_ctx);\n\n# ifdef KSSL_DEBUG\n            fprintf(stderr, \"ssl3_send_client_key_exchange(%lx & %lx)\\n\",\n                    alg_k, SSL_kKRB5);\n# endif                         /* KSSL_DEBUG */\n\n            authp = NULL;\n# ifdef KRB5SENDAUTH\n            if (KRB5SENDAUTH)\n                authp = &authenticator;\n# endif                         /* KRB5SENDAUTH */\n\n            krb5rc = kssl_cget_tkt(kssl_ctx, &enc_ticket, authp, &kssl_err);\n            enc = kssl_map_enc(kssl_ctx->enctype);\n            if (enc == NULL)\n                goto err;\n# ifdef KSSL_DEBUG\n            {\n                fprintf(stderr, \"kssl_cget_tkt rtn %d\\n\", krb5rc);\n                if (krb5rc && kssl_err.text)\n                    fprintf(stderr, \"kssl_cget_tkt kssl_err=%s\\n\",\n                            kssl_err.text);\n            }\n# endif                         /* KSSL_DEBUG */\n\n            if (krb5rc) {\n                ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_HANDSHAKE_FAILURE);\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, kssl_err.reason);\n                goto err;\n            }\n\n            /*-\n             * 20010406 VRS - Earlier versions used KRB5 AP_REQ\n             * in place of RFC 2712 KerberosWrapper, as in:\n             *\n             * Send ticket (copy to *p, set n = length)\n             * n = krb5_ap_req.length;\n             * memcpy(p, krb5_ap_req.data, krb5_ap_req.length);\n             * if (krb5_ap_req.data)\n             *   kssl_krb5_free_data_contents(NULL,&krb5_ap_req);\n             *\n             * Now using real RFC 2712 KerberosWrapper\n             * (Thanks to Simon Wilkinson <sxw@sxw.org.uk>)\n             * Note: 2712 \"opaque\" types are here replaced\n             * with a 2-byte length followed by the value.\n             * Example:\n             * KerberosWrapper= xx xx asn1ticket 0 0 xx xx encpms\n             * Where \"xx xx\" = length bytes.  Shown here with\n             * optional authenticator omitted.\n             */\n\n            /*  KerberosWrapper.Ticket              */\n            s2n(enc_ticket->length, p);\n            memcpy(p, enc_ticket->data, enc_ticket->length);\n            p += enc_ticket->length;\n            n = enc_ticket->length + 2;\n\n            /*  KerberosWrapper.Authenticator       */\n            if (authp && authp->length) {\n                s2n(authp->length, p);\n                memcpy(p, authp->data, authp->length);\n                p += authp->length;\n                n += authp->length + 2;\n\n                free(authp->data);\n                authp->data = NULL;\n                authp->length = 0;\n            } else {\n                s2n(0, p);      /* null authenticator length */\n                n += 2;\n            }\n\n            tmp_buf[0] = s->client_version >> 8;\n            tmp_buf[1] = s->client_version & 0xff;\n            if (RAND_bytes(&(tmp_buf[2]), sizeof tmp_buf - 2) <= 0)\n                goto err;\n\n            /*-\n             * 20010420 VRS.  Tried it this way; failed.\n             *      EVP_EncryptInit_ex(&ciph_ctx,enc, NULL,NULL);\n             *      EVP_CIPHER_CTX_set_key_length(&ciph_ctx,\n             *                              kssl_ctx->length);\n             *      EVP_EncryptInit_ex(&ciph_ctx,NULL, key,iv);\n             */\n\n            memset(iv, 0, sizeof iv); /* per RFC 1510 */\n            EVP_EncryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv);\n            EVP_EncryptUpdate(&ciph_ctx, epms, &outl, tmp_buf,\n                              sizeof tmp_buf);\n            EVP_EncryptFinal_ex(&ciph_ctx, &(epms[outl]), &padl);\n            outl += padl;\n            if (outl > (int)sizeof epms) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n            EVP_CIPHER_CTX_cleanup(&ciph_ctx);\n\n            /*  KerberosWrapper.EncryptedPreMasterSecret    */\n            s2n(outl, p);\n            memcpy(p, epms, outl);\n            p += outl;\n            n += outl + 2;\n\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            tmp_buf,\n                                                            sizeof tmp_buf);\n\n            OPENSSL_cleanse(tmp_buf, sizeof tmp_buf);\n            OPENSSL_cleanse(epms, outl);\n        }\n#endif\n#ifndef OPENSSL_NO_DH\n        else if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {\n            DH *dh_srvr, *dh_clnt;\n            SESS_CERT *scert = s->session->sess_cert;\n\n            if (scert == NULL) {\n                ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_UNEXPECTED_MESSAGE);\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_UNEXPECTED_MESSAGE);\n                goto err;\n            }\n\n            if (scert->peer_dh_tmp != NULL) {\n                dh_srvr = scert->peer_dh_tmp;\n            } else {\n                dh_srvr = get_server_static_dh_key(scert);\n                if (dh_srvr == NULL)\n                    goto err;\n            }\n\n            if (s->s3->flags & TLS1_FLAGS_SKIP_CERT_VERIFY) {\n                /* Use client certificate key */\n                EVP_PKEY *clkey = s->cert->key->privatekey;\n                dh_clnt = NULL;\n                if (clkey)\n                    dh_clnt = EVP_PKEY_get1_DH(clkey);\n                if (dh_clnt == NULL) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_INTERNAL_ERROR);\n                    goto err;\n                }\n            } else {\n                /* generate a new random key */\n                if ((dh_clnt = DHparams_dup(dh_srvr)) == NULL) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);\n                    goto err;\n                }\n                if (!DH_generate_key(dh_clnt)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);\n                    DH_free(dh_clnt);\n                    goto err;\n                }\n            }\n\n            /*\n             * use the 'p' output buffer for the DH key, but make sure to\n             * clear it out afterwards\n             */\n\n            n = DH_compute_key(p, dh_srvr->pub_key, dh_clnt);\n            if (scert->peer_dh_tmp == NULL)\n                DH_free(dh_srvr);\n\n            if (n <= 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);\n                DH_free(dh_clnt);\n                goto err;\n            }\n\n            /* generate master key from the result */\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            p, n);\n            /* clean up */\n            memset(p, 0, n);\n\n            if (s->s3->flags & TLS1_FLAGS_SKIP_CERT_VERIFY)\n                n = 0;\n            else {\n                /* send off the data */\n                n = BN_num_bytes(dh_clnt->pub_key);\n                s2n(n, p);\n                BN_bn2bin(dh_clnt->pub_key, p);\n                n += 2;\n            }\n\n            DH_free(dh_clnt);\n        }\n#endif\n\n#ifndef OPENSSL_NO_ECDH\n        else if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {\n            const EC_GROUP *srvr_group = NULL;\n            EC_KEY *tkey;\n            int ecdh_clnt_cert = 0;\n            int field_size = 0;\n\n            if (s->session->sess_cert == NULL) {\n                ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_UNEXPECTED_MESSAGE);\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_UNEXPECTED_MESSAGE);\n                goto err;\n            }\n\n            /*\n             * Did we send out the client's ECDH share for use in premaster\n             * computation as part of client certificate? If so, set\n             * ecdh_clnt_cert to 1.\n             */\n            if ((alg_k & (SSL_kECDHr | SSL_kECDHe)) && (s->cert != NULL)) {\n                /*-\n                 * XXX: For now, we do not support client\n                 * authentication using ECDH certificates.\n                 * To add such support, one needs to add\n                 * code that checks for appropriate\n                 * conditions and sets ecdh_clnt_cert to 1.\n                 * For example, the cert have an ECC\n                 * key on the same curve as the server's\n                 * and the key should be authorized for\n                 * key agreement.\n                 *\n                 * One also needs to add code in ssl3_connect\n                 * to skip sending the certificate verify\n                 * message.\n                 *\n                 * if ((s->cert->key->privatekey != NULL) &&\n                 *     (s->cert->key->privatekey->type ==\n                 *      EVP_PKEY_EC) && ...)\n                 * ecdh_clnt_cert = 1;\n                 */\n            }\n\n            if (s->session->sess_cert->peer_ecdh_tmp != NULL) {\n                tkey = s->session->sess_cert->peer_ecdh_tmp;\n            } else {\n                /* Get the Server Public Key from Cert */\n                srvr_pub_pkey =\n                    X509_get_pubkey(s->session->\n                                    sess_cert->peer_pkeys[SSL_PKEY_ECC].x509);\n                if ((srvr_pub_pkey == NULL)\n                    || (srvr_pub_pkey->type != EVP_PKEY_EC)\n                    || (srvr_pub_pkey->pkey.ec == NULL)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_INTERNAL_ERROR);\n                    goto err;\n                }\n\n                tkey = srvr_pub_pkey->pkey.ec;\n            }\n\n            srvr_group = EC_KEY_get0_group(tkey);\n            srvr_ecpoint = EC_KEY_get0_public_key(tkey);\n\n            if ((srvr_group == NULL) || (srvr_ecpoint == NULL)) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n\n            if ((clnt_ecdh = EC_KEY_new()) == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_MALLOC_FAILURE);\n                goto err;\n            }\n\n            if (!EC_KEY_set_group(clnt_ecdh, srvr_group)) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);\n                goto err;\n            }\n            if (ecdh_clnt_cert) {\n                /*\n                 * Reuse key info from our certificate We only need our\n                 * private key to perform the ECDH computation.\n                 */\n                const BIGNUM *priv_key;\n                tkey = s->cert->key->privatekey->pkey.ec;\n                priv_key = EC_KEY_get0_private_key(tkey);\n                if (priv_key == NULL) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_MALLOC_FAILURE);\n                    goto err;\n                }\n                if (!EC_KEY_set_private_key(clnt_ecdh, priv_key)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);\n                    goto err;\n                }\n            } else {\n                /* Generate a new ECDH key pair */\n                if (!(EC_KEY_generate_key(clnt_ecdh))) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_ECDH_LIB);\n                    goto err;\n                }\n            }\n\n            /*\n             * use the 'p' output buffer for the ECDH key, but make sure to\n             * clear it out afterwards\n             */\n\n            field_size = EC_GROUP_get_degree(srvr_group);\n            if (field_size <= 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);\n                goto err;\n            }\n            n = ECDH_compute_key(p, (field_size + 7) / 8, srvr_ecpoint,\n                                 clnt_ecdh, NULL);\n            if (n <= 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);\n                goto err;\n            }\n\n            /* generate master key from the result */\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            p, n);\n\n            memset(p, 0, n);    /* clean up */\n\n            if (ecdh_clnt_cert) {\n                /* Send empty client key exch message */\n                n = 0;\n            } else {\n                /*\n                 * First check the size of encoding and allocate memory\n                 * accordingly.\n                 */\n                encoded_pt_len =\n                    EC_POINT_point2oct(srvr_group,\n                                       EC_KEY_get0_public_key(clnt_ecdh),\n                                       POINT_CONVERSION_UNCOMPRESSED,\n                                       NULL, 0, NULL);\n\n                encodedPoint = (unsigned char *)\n                    OPENSSL_malloc(encoded_pt_len * sizeof(unsigned char));\n                bn_ctx = BN_CTX_new();\n                if ((encodedPoint == NULL) || (bn_ctx == NULL)) {\n                    SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                           ERR_R_MALLOC_FAILURE);\n                    goto err;\n                }\n\n                /* Encode the public key */\n                n = EC_POINT_point2oct(srvr_group,\n                                       EC_KEY_get0_public_key(clnt_ecdh),\n                                       POINT_CONVERSION_UNCOMPRESSED,\n                                       encodedPoint, encoded_pt_len, bn_ctx);\n\n                *p = n;         /* length of encoded point */\n                /* Encoded point will be copied here */\n                p += 1;\n                /* copy the point */\n                memcpy((unsigned char *)p, encodedPoint, n);\n                /* increment n to account for length field */\n                n += 1;\n            }\n\n            /* Free allocated memory */\n            BN_CTX_free(bn_ctx);\n            if (encodedPoint != NULL)\n                OPENSSL_free(encodedPoint);\n            if (clnt_ecdh != NULL)\n                EC_KEY_free(clnt_ecdh);\n            EVP_PKEY_free(srvr_pub_pkey);\n        }\n#endif                          /* !OPENSSL_NO_ECDH */\n        else if (alg_k & SSL_kGOST) {\n            /* GOST key exchange message creation */\n            EVP_PKEY_CTX *pkey_ctx;\n            X509 *peer_cert;\n            size_t msglen;\n            unsigned int md_len;\n            int keytype;\n            unsigned char premaster_secret[32], shared_ukm[32], tmp[256];\n            EVP_MD_CTX *ukm_hash;\n            EVP_PKEY *pub_key;\n\n            /*\n             * Get server sertificate PKEY and create ctx from it\n             */\n            peer_cert =\n                s->session->\n                sess_cert->peer_pkeys[(keytype = SSL_PKEY_GOST01)].x509;\n            if (!peer_cert)\n                peer_cert =\n                    s->session->\n                    sess_cert->peer_pkeys[(keytype = SSL_PKEY_GOST94)].x509;\n            if (!peer_cert) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_NO_GOST_CERTIFICATE_SENT_BY_PEER);\n                goto err;\n            }\n\n            pkey_ctx = EVP_PKEY_CTX_new(pub_key =\n                                        X509_get_pubkey(peer_cert), NULL);\n            /*\n             * If we have send a certificate, and certificate key\n             *\n             * * parameters match those of server certificate, use\n             * certificate key for key exchange\n             */\n\n            /* Otherwise, generate ephemeral key pair */\n\n            EVP_PKEY_encrypt_init(pkey_ctx);\n            /* Generate session key */\n            if (RAND_bytes(premaster_secret, 32) <= 0) {\n                EVP_PKEY_CTX_free(pkey_ctx);\n                goto err;\n            }\n            /*\n             * If we have client certificate, use its secret as peer key\n             */\n            if (s->s3->tmp.cert_req && s->cert->key->privatekey) {\n                if (EVP_PKEY_derive_set_peer\n                    (pkey_ctx, s->cert->key->privatekey) <= 0) {\n                    /*\n                     * If there was an error - just ignore it. Ephemeral key\n                     * * would be used\n                     */\n                    ERR_clear_error();\n                }\n            }\n            /*\n             * Compute shared IV and store it in algorithm-specific context\n             * data\n             */\n            ukm_hash = EVP_MD_CTX_create();\n            EVP_DigestInit(ukm_hash,\n                           EVP_get_digestbynid(NID_id_GostR3411_94));\n            EVP_DigestUpdate(ukm_hash, s->s3->client_random,\n                             SSL3_RANDOM_SIZE);\n            EVP_DigestUpdate(ukm_hash, s->s3->server_random,\n                             SSL3_RANDOM_SIZE);\n            EVP_DigestFinal_ex(ukm_hash, shared_ukm, &md_len);\n            EVP_MD_CTX_destroy(ukm_hash);\n            if (EVP_PKEY_CTX_ctrl\n                (pkey_ctx, -1, EVP_PKEY_OP_ENCRYPT, EVP_PKEY_CTRL_SET_IV, 8,\n                 shared_ukm) < 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_LIBRARY_BUG);\n                goto err;\n            }\n            /* Make GOST keytransport blob message */\n            /*\n             * Encapsulate it into sequence\n             */\n            *(p++) = V_ASN1_SEQUENCE | V_ASN1_CONSTRUCTED;\n            msglen = 255;\n            if (EVP_PKEY_encrypt(pkey_ctx, tmp, &msglen, premaster_secret, 32)\n                < 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_LIBRARY_BUG);\n                goto err;\n            }\n            if (msglen >= 0x80) {\n                *(p++) = 0x81;\n                *(p++) = msglen & 0xff;\n                n = msglen + 3;\n            } else {\n                *(p++) = msglen & 0xff;\n                n = msglen + 2;\n            }\n            memcpy(p, tmp, msglen);\n            /* Check if pubkey from client certificate was used */\n            if (EVP_PKEY_CTX_ctrl\n                (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0) {\n                /* Set flag \"skip certificate verify\" */\n                s->s3->flags |= TLS1_FLAGS_SKIP_CERT_VERIFY;\n            }\n            EVP_PKEY_CTX_free(pkey_ctx);\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            premaster_secret,\n                                                            32);\n            EVP_PKEY_free(pub_key);\n\n        }\n#ifndef OPENSSL_NO_SRP\n        else if (alg_k & SSL_kSRP) {\n            if (s->srp_ctx.A != NULL) {\n                /* send off the data */\n                n = BN_num_bytes(s->srp_ctx.A);\n                s2n(n, p);\n                BN_bn2bin(s->srp_ctx.A, p);\n                n += 2;\n            } else {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n            if (s->session->srp_username != NULL)\n                OPENSSL_free(s->session->srp_username);\n            s->session->srp_username = BUF_strdup(s->srp_ctx.login);\n            if (s->session->srp_username == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_MALLOC_FAILURE);\n                goto err;\n            }\n\n            if ((s->session->master_key_length =\n                 SRP_generate_client_master_secret(s,\n                                                   s->session->master_key)) <\n                0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto err;\n            }\n        }\n#endif\n#ifndef OPENSSL_NO_PSK\n        else if (alg_k & SSL_kPSK) {\n            /*\n             * The callback needs PSK_MAX_IDENTITY_LEN + 1 bytes to return a\n             * \\0-terminated identity. The last byte is for us for simulating\n             * strnlen.\n             */\n            char identity[PSK_MAX_IDENTITY_LEN + 2];\n            size_t identity_len;\n            unsigned char *t = NULL;\n            unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];\n            unsigned int pre_ms_len = 0, psk_len = 0;\n            int psk_err = 1;\n\n            n = 0;\n            if (s->psk_client_callback == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_PSK_NO_CLIENT_CB);\n                goto err;\n            }\n\n            memset(identity, 0, sizeof(identity));\n            psk_len = s->psk_client_callback(s, s->session->psk_identity_hint,\n                                             identity, sizeof(identity) - 1,\n                                             psk_or_pre_ms,\n                                             sizeof(psk_or_pre_ms));\n            if (psk_len > PSK_MAX_PSK_LEN) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto psk_err;\n            } else if (psk_len == 0) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       SSL_R_PSK_IDENTITY_NOT_FOUND);\n                goto psk_err;\n            }\n            identity[PSK_MAX_IDENTITY_LEN + 1] = '\\0';\n            identity_len = strlen(identity);\n            if (identity_len > PSK_MAX_IDENTITY_LEN) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_INTERNAL_ERROR);\n                goto psk_err;\n            }\n            /* create PSK pre_master_secret */\n            pre_ms_len = 2 + psk_len + 2 + psk_len;\n            t = psk_or_pre_ms;\n            memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);\n            s2n(psk_len, t);\n            memset(t, 0, psk_len);\n            t += psk_len;\n            s2n(psk_len, t);\n\n            if (s->session->psk_identity_hint != NULL)\n                OPENSSL_free(s->session->psk_identity_hint);\n            s->session->psk_identity_hint =\n                BUF_strdup(s->ctx->psk_identity_hint);\n            if (s->ctx->psk_identity_hint != NULL\n                && s->session->psk_identity_hint == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_MALLOC_FAILURE);\n                goto psk_err;\n            }\n\n            if (s->session->psk_identity != NULL)\n                OPENSSL_free(s->session->psk_identity);\n            s->session->psk_identity = BUF_strdup(identity);\n            if (s->session->psk_identity == NULL) {\n                SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,\n                       ERR_R_MALLOC_FAILURE);\n                goto psk_err;\n            }\n\n            s->session->master_key_length =\n                s->method->ssl3_enc->generate_master_secret(s,\n                                                            s->\n                                                            session->master_key,\n                                                            psk_or_pre_ms,\n                                                            pre_ms_len);\n            s2n(identity_len, p);\n            memcpy(p, identity, identity_len);\n            n = 2 + identity_len;\n            psk_err = 0;\n psk_err:\n            OPENSSL_cleanse(identity, sizeof(identity));\n            OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));\n            if (psk_err != 0) {\n                ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_HANDSHAKE_FAILURE);\n                goto err;\n            }\n        }\n#endif\n        else {\n            ssl3_send_alert(s, SSL3_AL_FATAL, SSL_AD_HANDSHAKE_FAILURE);\n            SSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);\n            goto err;\n        }\n\n        ssl_set_handshake_header(s, SSL3_MT_CLIENT_KEY_EXCHANGE, n);\n        s->state = SSL3_ST_CW_KEY_EXCH_B;\n    }\n\n    /* SSL3_ST_CW_KEY_EXCH_B */\n    return ssl_do_write(s);\n err:\n#ifndef OPENSSL_NO_ECDH\n    BN_CTX_free(bn_ctx);\n    if (encodedPoint != NULL)\n        OPENSSL_free(encodedPoint);\n    if (clnt_ecdh != NULL)\n        EC_KEY_free(clnt_ecdh);\n    EVP_PKEY_free(srvr_pub_pkey);\n#endif\n    s->state = SSL_ST_ERR;\n    return (-1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -656,7 +656,7 @@\n             }\n \n             memset(identity, 0, sizeof(identity));\n-            psk_len = s->psk_client_callback(s, s->ctx->psk_identity_hint,\n+            psk_len = s->psk_client_callback(s, s->session->psk_identity_hint,\n                                              identity, sizeof(identity) - 1,\n                                              psk_or_pre_ms,\n                                              sizeof(psk_or_pre_ms));",
        "diff_line_info": {
            "deleted_lines": [
                "            psk_len = s->psk_client_callback(s, s->ctx->psk_identity_hint,"
            ],
            "added_lines": [
                "            psk_len = s->psk_client_callback(s, s->session->psk_identity_hint,"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-3196",
        "func_name": "openssl/ssl3_get_key_exchange",
        "description": "ssl/s3_clnt.c in OpenSSL 1.0.0 before 1.0.0t, 1.0.1 before 1.0.1p, and 1.0.2 before 1.0.2d, when used for a multi-threaded client, writes the PSK identity hint to an incorrect data structure, which allows remote servers to cause a denial of service (race condition and double free) via a crafted ServerKeyExchange message.",
        "git_url": "https://git.openssl.org/?p=openssl.git;a=commit;h=3c66a669dfc7b3792f7af0758ea26fe8502ce70c",
        "commit_title": "",
        "commit_text": "Fix PSK handling.  The PSK identity hint should be stored in the SSL_SESSION structure and not in the parent context (which will overwrite values used by other SSL structures with the same SSL_CTX).  Use BUF_strndup when copying identity as it may not be null terminated.  ",
        "func_before": "int ssl3_get_key_exchange(SSL *s)\n{\n#ifndef OPENSSL_NO_RSA\n    unsigned char *q, md_buf[EVP_MAX_MD_SIZE * 2];\n#endif\n    EVP_MD_CTX md_ctx;\n    unsigned char *param, *p;\n    int al, j, ok;\n    long i, param_len, n, alg_k, alg_a;\n    EVP_PKEY *pkey = NULL;\n    const EVP_MD *md = NULL;\n#ifndef OPENSSL_NO_RSA\n    RSA *rsa = NULL;\n#endif\n#ifndef OPENSSL_NO_DH\n    DH *dh = NULL;\n#endif\n#ifndef OPENSSL_NO_ECDH\n    EC_KEY *ecdh = NULL;\n    BN_CTX *bn_ctx = NULL;\n    EC_POINT *srvr_ecpoint = NULL;\n    int curve_nid = 0;\n    int encoded_pt_len = 0;\n#endif\n\n    EVP_MD_CTX_init(&md_ctx);\n\n    /*\n     * use same message size as in ssl3_get_certificate_request() as\n     * ServerKeyExchange message may be skipped\n     */\n    n = s->method->ssl_get_message(s,\n                                   SSL3_ST_CR_KEY_EXCH_A,\n                                   SSL3_ST_CR_KEY_EXCH_B,\n                                   -1, s->max_cert_list, &ok);\n    if (!ok)\n        return ((int)n);\n\n    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;\n\n    if (s->s3->tmp.message_type != SSL3_MT_SERVER_KEY_EXCHANGE) {\n        /*\n         * Can't skip server key exchange if this is an ephemeral\n         * ciphersuite.\n         */\n        if (alg_k & (SSL_kDHE | SSL_kECDHE)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_UNEXPECTED_MESSAGE);\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            goto f_err;\n        }\n#ifndef OPENSSL_NO_PSK\n        /*\n         * In plain PSK ciphersuite, ServerKeyExchange can be omitted if no\n         * identity hint is sent. Set session->sess_cert anyway to avoid\n         * problems later.\n         */\n        if (alg_k & SSL_kPSK) {\n            s->session->sess_cert = ssl_sess_cert_new();\n            if (s->ctx->psk_identity_hint)\n                OPENSSL_free(s->ctx->psk_identity_hint);\n            s->ctx->psk_identity_hint = NULL;\n        }\n#endif\n        s->s3->tmp.reuse_message = 1;\n        return (1);\n    }\n\n    param = p = (unsigned char *)s->init_msg;\n    if (s->session->sess_cert != NULL) {\n#ifndef OPENSSL_NO_RSA\n        if (s->session->sess_cert->peer_rsa_tmp != NULL) {\n            RSA_free(s->session->sess_cert->peer_rsa_tmp);\n            s->session->sess_cert->peer_rsa_tmp = NULL;\n        }\n#endif\n#ifndef OPENSSL_NO_DH\n        if (s->session->sess_cert->peer_dh_tmp) {\n            DH_free(s->session->sess_cert->peer_dh_tmp);\n            s->session->sess_cert->peer_dh_tmp = NULL;\n        }\n#endif\n#ifndef OPENSSL_NO_ECDH\n        if (s->session->sess_cert->peer_ecdh_tmp) {\n            EC_KEY_free(s->session->sess_cert->peer_ecdh_tmp);\n            s->session->sess_cert->peer_ecdh_tmp = NULL;\n        }\n#endif\n    } else {\n        s->session->sess_cert = ssl_sess_cert_new();\n    }\n\n    /* Total length of the parameters including the length prefix */\n    param_len = 0;\n\n    alg_a = s->s3->tmp.new_cipher->algorithm_auth;\n\n    al = SSL_AD_DECODE_ERROR;\n\n#ifndef OPENSSL_NO_PSK\n    if (alg_k & SSL_kPSK) {\n        char tmp_id_hint[PSK_MAX_IDENTITY_LEN + 1];\n\n        param_len = 2;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n\n        /*\n         * Store PSK identity hint for later use, hint is used in\n         * ssl3_send_client_key_exchange.  Assume that the maximum length of\n         * a PSK identity hint can be as long as the maximum length of a PSK\n         * identity.\n         */\n        if (i > PSK_MAX_IDENTITY_LEN) {\n            al = SSL_AD_HANDSHAKE_FAILURE;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_DATA_LENGTH_TOO_LONG);\n            goto f_err;\n        }\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE,\n                   SSL_R_BAD_PSK_IDENTITY_HINT_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        /*\n         * If received PSK identity hint contains NULL characters, the hint\n         * is truncated from the first NULL. p may not be ending with NULL,\n         * so create a NULL-terminated string.\n         */\n        memcpy(tmp_id_hint, p, i);\n        memset(tmp_id_hint + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);\n        if (s->ctx->psk_identity_hint != NULL)\n            OPENSSL_free(s->ctx->psk_identity_hint);\n        s->ctx->psk_identity_hint = BUF_strdup(tmp_id_hint);\n        if (s->ctx->psk_identity_hint == NULL) {\n            al = SSL_AD_HANDSHAKE_FAILURE;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);\n            goto f_err;\n        }\n\n        p += i;\n        n -= param_len;\n    } else\n#endif                          /* !OPENSSL_NO_PSK */\n#ifndef OPENSSL_NO_SRP\n    if (alg_k & SSL_kSRP) {\n        param_len = 2;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_N_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(s->srp_ctx.N = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_G_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(s->srp_ctx.g = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (1 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 1;\n\n        i = (unsigned int)(p[0]);\n        p++;\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_S_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(s->srp_ctx.s = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_B_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(s->srp_ctx.B = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n        n -= param_len;\n\n        if (!srp_verify_server_param(s, &al)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_PARAMETERS);\n            goto f_err;\n        }\n\n/* We must check if there is a certificate */\n# ifndef OPENSSL_NO_RSA\n        if (alg_a & SSL_aRSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].x509);\n# else\n        if (0) ;\n# endif\n# ifndef OPENSSL_NO_DSA\n        else if (alg_a & SSL_aDSS)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_DSA_SIGN].\n                                x509);\n# endif\n    } else\n#endif                          /* !OPENSSL_NO_SRP */\n#ifndef OPENSSL_NO_RSA\n    if (alg_k & SSL_kRSA) {\n        /* Temporary RSA keys only allowed in export ciphersuites */\n        if (!SSL_C_IS_EXPORT(s->s3->tmp.new_cipher)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_UNEXPECTED_MESSAGE);\n            goto f_err;\n        }\n        if ((rsa = RSA_new()) == NULL) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);\n            goto err;\n        }\n\n        param_len = 2;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_RSA_MODULUS_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(rsa->n = BN_bin2bn(p, i, rsa->n))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_RSA_E_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(rsa->e = BN_bin2bn(p, i, rsa->e))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n        n -= param_len;\n\n        /* this should be because we are using an export cipher */\n        if (alg_a & SSL_aRSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].x509);\n        else {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);\n            goto err;\n        }\n\n        if (EVP_PKEY_bits(pkey) <= SSL_C_EXPORT_PKEYLENGTH(s->s3->tmp.new_cipher)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_UNEXPECTED_MESSAGE);\n            goto f_err;\n        }\n\n        s->session->sess_cert->peer_rsa_tmp = rsa;\n        rsa = NULL;\n    }\n#else                           /* OPENSSL_NO_RSA */\n    if (0) ;\n#endif\n#ifndef OPENSSL_NO_DH\n    else if (alg_k & SSL_kEDH) {\n        if ((dh = DH_new()) == NULL) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_DH_LIB);\n            goto err;\n        }\n\n        param_len = 2;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_DH_P_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(dh->p = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_DH_G_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(dh->g = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_DH_PUB_KEY_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(dh->pub_key = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n        n -= param_len;\n\n# ifndef OPENSSL_NO_RSA\n        if (alg_a & SSL_aRSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].x509);\n# else\n        if (0) ;\n# endif\n# ifndef OPENSSL_NO_DSA\n        else if (alg_a & SSL_aDSS)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_DSA_SIGN].\n                                x509);\n# endif\n        /* else anonymous DH, so no certificate or pkey. */\n\n        s->session->sess_cert->peer_dh_tmp = dh;\n        dh = NULL;\n    } else if ((alg_k & SSL_kDHr) || (alg_k & SSL_kDHd)) {\n        al = SSL_AD_ILLEGAL_PARAMETER;\n        SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE,\n               SSL_R_TRIED_TO_USE_UNSUPPORTED_CIPHER);\n        goto f_err;\n    }\n#endif                          /* !OPENSSL_NO_DH */\n\n#ifndef OPENSSL_NO_ECDH\n    else if (alg_k & SSL_kEECDH) {\n        EC_GROUP *ngroup;\n        const EC_GROUP *group;\n\n        if ((ecdh = EC_KEY_new()) == NULL) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);\n            goto err;\n        }\n\n        /*\n         * Extract elliptic curve parameters and the server's ephemeral ECDH\n         * public key. Keep accumulating lengths of various components in\n         * param_len and make sure it never exceeds n.\n         */\n\n        /*\n         * XXX: For now we only support named (not generic) curves and the\n         * ECParameters in this case is just three bytes. We also need one\n         * byte for the length of the encoded point\n         */\n        param_len = 4;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        /*\n         * Check curve is one of our preferences, if not server has sent an\n         * invalid curve. ECParameters is 3 bytes.\n         */\n        if (!tls1_check_curve(s, p, 3)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_WRONG_CURVE);\n            goto f_err;\n        }\n\n        if ((curve_nid = tls1_ec_curve_id2nid(*(p + 2))) == 0) {\n            al = SSL_AD_INTERNAL_ERROR;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE,\n                   SSL_R_UNABLE_TO_FIND_ECDH_PARAMETERS);\n            goto f_err;\n        }\n\n        ngroup = EC_GROUP_new_by_curve_name(curve_nid);\n        if (ngroup == NULL) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_EC_LIB);\n            goto err;\n        }\n        if (EC_KEY_set_group(ecdh, ngroup) == 0) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_EC_LIB);\n            goto err;\n        }\n        EC_GROUP_free(ngroup);\n\n        group = EC_KEY_get0_group(ecdh);\n\n        if (SSL_C_IS_EXPORT(s->s3->tmp.new_cipher) &&\n            (EC_GROUP_get_degree(group) > 163)) {\n            al = SSL_AD_EXPORT_RESTRICTION;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE,\n                   SSL_R_ECGROUP_TOO_LARGE_FOR_CIPHER);\n            goto f_err;\n        }\n\n        p += 3;\n\n        /* Next, get the encoded ECPoint */\n        if (((srvr_ecpoint = EC_POINT_new(group)) == NULL) ||\n            ((bn_ctx = BN_CTX_new()) == NULL)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);\n            goto err;\n        }\n\n        encoded_pt_len = *p;    /* length of encoded point */\n        p += 1;\n\n        if ((encoded_pt_len > n - param_len) ||\n            (EC_POINT_oct2point(group, srvr_ecpoint,\n                                p, encoded_pt_len, bn_ctx) == 0)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_ECPOINT);\n            goto f_err;\n        }\n        param_len += encoded_pt_len;\n\n        n -= param_len;\n        p += encoded_pt_len;\n\n        /*\n         * The ECC/TLS specification does not mention the use of DSA to sign\n         * ECParameters in the server key exchange message. We do support RSA\n         * and ECDSA.\n         */\n        if (0) ;\n# ifndef OPENSSL_NO_RSA\n        else if (alg_a & SSL_aRSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].x509);\n# endif\n# ifndef OPENSSL_NO_ECDSA\n        else if (alg_a & SSL_aECDSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_ECC].x509);\n# endif\n        /* else anonymous ECDH, so no certificate or pkey. */\n        EC_KEY_set_public_key(ecdh, srvr_ecpoint);\n        s->session->sess_cert->peer_ecdh_tmp = ecdh;\n        ecdh = NULL;\n        BN_CTX_free(bn_ctx);\n        bn_ctx = NULL;\n        EC_POINT_free(srvr_ecpoint);\n        srvr_ecpoint = NULL;\n    } else if (alg_k) {\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_UNEXPECTED_MESSAGE);\n        goto f_err;\n    }\n#endif                          /* !OPENSSL_NO_ECDH */\n\n    /* p points to the next byte, there are 'n' bytes left */\n\n    /* if it was signed, check the signature */\n    if (pkey != NULL) {\n        if (SSL_USE_SIGALGS(s)) {\n            int rv;\n            if (2 > n) {\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n                goto f_err;\n            }\n            rv = tls12_check_peer_sigalg(&md, s, p, pkey);\n            if (rv == -1)\n                goto err;\n            else if (rv == 0) {\n                goto f_err;\n            }\n#ifdef SSL_DEBUG\n            fprintf(stderr, \"USING TLSv1.2 HASH %s\\n\", EVP_MD_name(md));\n#endif\n            p += 2;\n            n -= 2;\n        } else\n            md = EVP_sha1();\n\n        if (2 > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n        n -= 2;\n        j = EVP_PKEY_size(pkey);\n\n        /*\n         * Check signature length. If n is 0 then signature is empty\n         */\n        if ((i != n) || (n > j) || (n <= 0)) {\n            /* wrong packet length */\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_WRONG_SIGNATURE_LENGTH);\n            goto f_err;\n        }\n#ifndef OPENSSL_NO_RSA\n        if (pkey->type == EVP_PKEY_RSA && !SSL_USE_SIGALGS(s)) {\n            int num;\n            unsigned int size;\n\n            j = 0;\n            q = md_buf;\n            for (num = 2; num > 0; num--) {\n                EVP_MD_CTX_set_flags(&md_ctx, EVP_MD_CTX_FLAG_NON_FIPS_ALLOW);\n                EVP_DigestInit_ex(&md_ctx, (num == 2)\n                                  ? s->ctx->md5 : s->ctx->sha1, NULL);\n                EVP_DigestUpdate(&md_ctx, &(s->s3->client_random[0]),\n                                 SSL3_RANDOM_SIZE);\n                EVP_DigestUpdate(&md_ctx, &(s->s3->server_random[0]),\n                                 SSL3_RANDOM_SIZE);\n                EVP_DigestUpdate(&md_ctx, param, param_len);\n                EVP_DigestFinal_ex(&md_ctx, q, &size);\n                q += size;\n                j += size;\n            }\n            i = RSA_verify(NID_md5_sha1, md_buf, j, p, n, pkey->pkey.rsa);\n            if (i < 0) {\n                al = SSL_AD_DECRYPT_ERROR;\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_RSA_DECRYPT);\n                goto f_err;\n            }\n            if (i == 0) {\n                /* bad signature */\n                al = SSL_AD_DECRYPT_ERROR;\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SIGNATURE);\n                goto f_err;\n            }\n        } else\n#endif\n        {\n            EVP_VerifyInit_ex(&md_ctx, md, NULL);\n            EVP_VerifyUpdate(&md_ctx, &(s->s3->client_random[0]),\n                             SSL3_RANDOM_SIZE);\n            EVP_VerifyUpdate(&md_ctx, &(s->s3->server_random[0]),\n                             SSL3_RANDOM_SIZE);\n            EVP_VerifyUpdate(&md_ctx, param, param_len);\n            if (EVP_VerifyFinal(&md_ctx, p, (int)n, pkey) <= 0) {\n                /* bad signature */\n                al = SSL_AD_DECRYPT_ERROR;\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SIGNATURE);\n                goto f_err;\n            }\n        }\n    } else {\n        /* aNULL, aSRP or kPSK do not need public keys */\n        if (!(alg_a & (SSL_aNULL | SSL_aSRP)) && !(alg_k & SSL_kPSK)) {\n            /* Might be wrong key type, check it */\n            if (ssl3_check_cert_and_algorithm(s))\n                /* Otherwise this shouldn't happen */\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);\n            goto err;\n        }\n        /* still data left over */\n        if (n != 0) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_EXTRA_DATA_IN_MESSAGE);\n            goto f_err;\n        }\n    }\n    EVP_PKEY_free(pkey);\n    EVP_MD_CTX_cleanup(&md_ctx);\n    return (1);\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n err:\n    EVP_PKEY_free(pkey);\n#ifndef OPENSSL_NO_RSA\n    if (rsa != NULL)\n        RSA_free(rsa);\n#endif\n#ifndef OPENSSL_NO_DH\n    if (dh != NULL)\n        DH_free(dh);\n#endif\n#ifndef OPENSSL_NO_ECDH\n    BN_CTX_free(bn_ctx);\n    EC_POINT_free(srvr_ecpoint);\n    if (ecdh != NULL)\n        EC_KEY_free(ecdh);\n#endif\n    EVP_MD_CTX_cleanup(&md_ctx);\n    s->state = SSL_ST_ERR;\n    return (-1);\n}",
        "func": "int ssl3_get_key_exchange(SSL *s)\n{\n#ifndef OPENSSL_NO_RSA\n    unsigned char *q, md_buf[EVP_MAX_MD_SIZE * 2];\n#endif\n    EVP_MD_CTX md_ctx;\n    unsigned char *param, *p;\n    int al, j, ok;\n    long i, param_len, n, alg_k, alg_a;\n    EVP_PKEY *pkey = NULL;\n    const EVP_MD *md = NULL;\n#ifndef OPENSSL_NO_RSA\n    RSA *rsa = NULL;\n#endif\n#ifndef OPENSSL_NO_DH\n    DH *dh = NULL;\n#endif\n#ifndef OPENSSL_NO_ECDH\n    EC_KEY *ecdh = NULL;\n    BN_CTX *bn_ctx = NULL;\n    EC_POINT *srvr_ecpoint = NULL;\n    int curve_nid = 0;\n    int encoded_pt_len = 0;\n#endif\n\n    EVP_MD_CTX_init(&md_ctx);\n\n    /*\n     * use same message size as in ssl3_get_certificate_request() as\n     * ServerKeyExchange message may be skipped\n     */\n    n = s->method->ssl_get_message(s,\n                                   SSL3_ST_CR_KEY_EXCH_A,\n                                   SSL3_ST_CR_KEY_EXCH_B,\n                                   -1, s->max_cert_list, &ok);\n    if (!ok)\n        return ((int)n);\n\n    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;\n\n    if (s->s3->tmp.message_type != SSL3_MT_SERVER_KEY_EXCHANGE) {\n        /*\n         * Can't skip server key exchange if this is an ephemeral\n         * ciphersuite.\n         */\n        if (alg_k & (SSL_kDHE | SSL_kECDHE)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_UNEXPECTED_MESSAGE);\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            goto f_err;\n        }\n#ifndef OPENSSL_NO_PSK\n        /*\n         * In plain PSK ciphersuite, ServerKeyExchange can be omitted if no\n         * identity hint is sent. Set session->sess_cert anyway to avoid\n         * problems later.\n         */\n        if (alg_k & SSL_kPSK) {\n            s->session->sess_cert = ssl_sess_cert_new();\n            if (s->ctx->psk_identity_hint)\n                OPENSSL_free(s->ctx->psk_identity_hint);\n            s->ctx->psk_identity_hint = NULL;\n        }\n#endif\n        s->s3->tmp.reuse_message = 1;\n        return (1);\n    }\n\n    param = p = (unsigned char *)s->init_msg;\n    if (s->session->sess_cert != NULL) {\n#ifndef OPENSSL_NO_RSA\n        if (s->session->sess_cert->peer_rsa_tmp != NULL) {\n            RSA_free(s->session->sess_cert->peer_rsa_tmp);\n            s->session->sess_cert->peer_rsa_tmp = NULL;\n        }\n#endif\n#ifndef OPENSSL_NO_DH\n        if (s->session->sess_cert->peer_dh_tmp) {\n            DH_free(s->session->sess_cert->peer_dh_tmp);\n            s->session->sess_cert->peer_dh_tmp = NULL;\n        }\n#endif\n#ifndef OPENSSL_NO_ECDH\n        if (s->session->sess_cert->peer_ecdh_tmp) {\n            EC_KEY_free(s->session->sess_cert->peer_ecdh_tmp);\n            s->session->sess_cert->peer_ecdh_tmp = NULL;\n        }\n#endif\n    } else {\n        s->session->sess_cert = ssl_sess_cert_new();\n    }\n\n    /* Total length of the parameters including the length prefix */\n    param_len = 0;\n\n    alg_a = s->s3->tmp.new_cipher->algorithm_auth;\n\n    al = SSL_AD_DECODE_ERROR;\n\n#ifndef OPENSSL_NO_PSK\n    if (alg_k & SSL_kPSK) {\n        param_len = 2;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n\n        /*\n         * Store PSK identity hint for later use, hint is used in\n         * ssl3_send_client_key_exchange.  Assume that the maximum length of\n         * a PSK identity hint can be as long as the maximum length of a PSK\n         * identity.\n         */\n        if (i > PSK_MAX_IDENTITY_LEN) {\n            al = SSL_AD_HANDSHAKE_FAILURE;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_DATA_LENGTH_TOO_LONG);\n            goto f_err;\n        }\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE,\n                   SSL_R_BAD_PSK_IDENTITY_HINT_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        s->session->psk_identity_hint = BUF_strndup((char *)p, i);\n        if (s->session->psk_identity_hint == NULL) {\n            al = SSL_AD_HANDSHAKE_FAILURE;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);\n            goto f_err;\n        }\n\n        p += i;\n        n -= param_len;\n    } else\n#endif                          /* !OPENSSL_NO_PSK */\n#ifndef OPENSSL_NO_SRP\n    if (alg_k & SSL_kSRP) {\n        param_len = 2;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_N_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(s->srp_ctx.N = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_G_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(s->srp_ctx.g = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (1 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 1;\n\n        i = (unsigned int)(p[0]);\n        p++;\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_S_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(s->srp_ctx.s = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_B_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(s->srp_ctx.B = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n        n -= param_len;\n\n        if (!srp_verify_server_param(s, &al)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SRP_PARAMETERS);\n            goto f_err;\n        }\n\n/* We must check if there is a certificate */\n# ifndef OPENSSL_NO_RSA\n        if (alg_a & SSL_aRSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].x509);\n# else\n        if (0) ;\n# endif\n# ifndef OPENSSL_NO_DSA\n        else if (alg_a & SSL_aDSS)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_DSA_SIGN].\n                                x509);\n# endif\n    } else\n#endif                          /* !OPENSSL_NO_SRP */\n#ifndef OPENSSL_NO_RSA\n    if (alg_k & SSL_kRSA) {\n        /* Temporary RSA keys only allowed in export ciphersuites */\n        if (!SSL_C_IS_EXPORT(s->s3->tmp.new_cipher)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_UNEXPECTED_MESSAGE);\n            goto f_err;\n        }\n        if ((rsa = RSA_new()) == NULL) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);\n            goto err;\n        }\n\n        param_len = 2;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_RSA_MODULUS_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(rsa->n = BN_bin2bn(p, i, rsa->n))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_RSA_E_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(rsa->e = BN_bin2bn(p, i, rsa->e))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n        n -= param_len;\n\n        /* this should be because we are using an export cipher */\n        if (alg_a & SSL_aRSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].x509);\n        else {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);\n            goto err;\n        }\n\n        if (EVP_PKEY_bits(pkey) <= SSL_C_EXPORT_PKEYLENGTH(s->s3->tmp.new_cipher)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_UNEXPECTED_MESSAGE);\n            goto f_err;\n        }\n\n        s->session->sess_cert->peer_rsa_tmp = rsa;\n        rsa = NULL;\n    }\n#else                           /* OPENSSL_NO_RSA */\n    if (0) ;\n#endif\n#ifndef OPENSSL_NO_DH\n    else if (alg_k & SSL_kEDH) {\n        if ((dh = DH_new()) == NULL) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_DH_LIB);\n            goto err;\n        }\n\n        param_len = 2;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_DH_P_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(dh->p = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_DH_G_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(dh->g = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n\n        if (2 > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        param_len += 2;\n\n        n2s(p, i);\n\n        if (i > n - param_len) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_DH_PUB_KEY_LENGTH);\n            goto f_err;\n        }\n        param_len += i;\n\n        if (!(dh->pub_key = BN_bin2bn(p, i, NULL))) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_BN_LIB);\n            goto err;\n        }\n        p += i;\n        n -= param_len;\n\n# ifndef OPENSSL_NO_RSA\n        if (alg_a & SSL_aRSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].x509);\n# else\n        if (0) ;\n# endif\n# ifndef OPENSSL_NO_DSA\n        else if (alg_a & SSL_aDSS)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_DSA_SIGN].\n                                x509);\n# endif\n        /* else anonymous DH, so no certificate or pkey. */\n\n        s->session->sess_cert->peer_dh_tmp = dh;\n        dh = NULL;\n    } else if ((alg_k & SSL_kDHr) || (alg_k & SSL_kDHd)) {\n        al = SSL_AD_ILLEGAL_PARAMETER;\n        SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE,\n               SSL_R_TRIED_TO_USE_UNSUPPORTED_CIPHER);\n        goto f_err;\n    }\n#endif                          /* !OPENSSL_NO_DH */\n\n#ifndef OPENSSL_NO_ECDH\n    else if (alg_k & SSL_kEECDH) {\n        EC_GROUP *ngroup;\n        const EC_GROUP *group;\n\n        if ((ecdh = EC_KEY_new()) == NULL) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);\n            goto err;\n        }\n\n        /*\n         * Extract elliptic curve parameters and the server's ephemeral ECDH\n         * public key. Keep accumulating lengths of various components in\n         * param_len and make sure it never exceeds n.\n         */\n\n        /*\n         * XXX: For now we only support named (not generic) curves and the\n         * ECParameters in this case is just three bytes. We also need one\n         * byte for the length of the encoded point\n         */\n        param_len = 4;\n        if (param_len > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        /*\n         * Check curve is one of our preferences, if not server has sent an\n         * invalid curve. ECParameters is 3 bytes.\n         */\n        if (!tls1_check_curve(s, p, 3)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_WRONG_CURVE);\n            goto f_err;\n        }\n\n        if ((curve_nid = tls1_ec_curve_id2nid(*(p + 2))) == 0) {\n            al = SSL_AD_INTERNAL_ERROR;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE,\n                   SSL_R_UNABLE_TO_FIND_ECDH_PARAMETERS);\n            goto f_err;\n        }\n\n        ngroup = EC_GROUP_new_by_curve_name(curve_nid);\n        if (ngroup == NULL) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_EC_LIB);\n            goto err;\n        }\n        if (EC_KEY_set_group(ecdh, ngroup) == 0) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_EC_LIB);\n            goto err;\n        }\n        EC_GROUP_free(ngroup);\n\n        group = EC_KEY_get0_group(ecdh);\n\n        if (SSL_C_IS_EXPORT(s->s3->tmp.new_cipher) &&\n            (EC_GROUP_get_degree(group) > 163)) {\n            al = SSL_AD_EXPORT_RESTRICTION;\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE,\n                   SSL_R_ECGROUP_TOO_LARGE_FOR_CIPHER);\n            goto f_err;\n        }\n\n        p += 3;\n\n        /* Next, get the encoded ECPoint */\n        if (((srvr_ecpoint = EC_POINT_new(group)) == NULL) ||\n            ((bn_ctx = BN_CTX_new()) == NULL)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);\n            goto err;\n        }\n\n        encoded_pt_len = *p;    /* length of encoded point */\n        p += 1;\n\n        if ((encoded_pt_len > n - param_len) ||\n            (EC_POINT_oct2point(group, srvr_ecpoint,\n                                p, encoded_pt_len, bn_ctx) == 0)) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_ECPOINT);\n            goto f_err;\n        }\n        param_len += encoded_pt_len;\n\n        n -= param_len;\n        p += encoded_pt_len;\n\n        /*\n         * The ECC/TLS specification does not mention the use of DSA to sign\n         * ECParameters in the server key exchange message. We do support RSA\n         * and ECDSA.\n         */\n        if (0) ;\n# ifndef OPENSSL_NO_RSA\n        else if (alg_a & SSL_aRSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_RSA_ENC].x509);\n# endif\n# ifndef OPENSSL_NO_ECDSA\n        else if (alg_a & SSL_aECDSA)\n            pkey =\n                X509_get_pubkey(s->session->\n                                sess_cert->peer_pkeys[SSL_PKEY_ECC].x509);\n# endif\n        /* else anonymous ECDH, so no certificate or pkey. */\n        EC_KEY_set_public_key(ecdh, srvr_ecpoint);\n        s->session->sess_cert->peer_ecdh_tmp = ecdh;\n        ecdh = NULL;\n        BN_CTX_free(bn_ctx);\n        bn_ctx = NULL;\n        EC_POINT_free(srvr_ecpoint);\n        srvr_ecpoint = NULL;\n    } else if (alg_k) {\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_UNEXPECTED_MESSAGE);\n        goto f_err;\n    }\n#endif                          /* !OPENSSL_NO_ECDH */\n\n    /* p points to the next byte, there are 'n' bytes left */\n\n    /* if it was signed, check the signature */\n    if (pkey != NULL) {\n        if (SSL_USE_SIGALGS(s)) {\n            int rv;\n            if (2 > n) {\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n                goto f_err;\n            }\n            rv = tls12_check_peer_sigalg(&md, s, p, pkey);\n            if (rv == -1)\n                goto err;\n            else if (rv == 0) {\n                goto f_err;\n            }\n#ifdef SSL_DEBUG\n            fprintf(stderr, \"USING TLSv1.2 HASH %s\\n\", EVP_MD_name(md));\n#endif\n            p += 2;\n            n -= 2;\n        } else\n            md = EVP_sha1();\n\n        if (2 > n) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n            goto f_err;\n        }\n        n2s(p, i);\n        n -= 2;\n        j = EVP_PKEY_size(pkey);\n\n        /*\n         * Check signature length. If n is 0 then signature is empty\n         */\n        if ((i != n) || (n > j) || (n <= 0)) {\n            /* wrong packet length */\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_WRONG_SIGNATURE_LENGTH);\n            goto f_err;\n        }\n#ifndef OPENSSL_NO_RSA\n        if (pkey->type == EVP_PKEY_RSA && !SSL_USE_SIGALGS(s)) {\n            int num;\n            unsigned int size;\n\n            j = 0;\n            q = md_buf;\n            for (num = 2; num > 0; num--) {\n                EVP_MD_CTX_set_flags(&md_ctx, EVP_MD_CTX_FLAG_NON_FIPS_ALLOW);\n                EVP_DigestInit_ex(&md_ctx, (num == 2)\n                                  ? s->ctx->md5 : s->ctx->sha1, NULL);\n                EVP_DigestUpdate(&md_ctx, &(s->s3->client_random[0]),\n                                 SSL3_RANDOM_SIZE);\n                EVP_DigestUpdate(&md_ctx, &(s->s3->server_random[0]),\n                                 SSL3_RANDOM_SIZE);\n                EVP_DigestUpdate(&md_ctx, param, param_len);\n                EVP_DigestFinal_ex(&md_ctx, q, &size);\n                q += size;\n                j += size;\n            }\n            i = RSA_verify(NID_md5_sha1, md_buf, j, p, n, pkey->pkey.rsa);\n            if (i < 0) {\n                al = SSL_AD_DECRYPT_ERROR;\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_RSA_DECRYPT);\n                goto f_err;\n            }\n            if (i == 0) {\n                /* bad signature */\n                al = SSL_AD_DECRYPT_ERROR;\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SIGNATURE);\n                goto f_err;\n            }\n        } else\n#endif\n        {\n            EVP_VerifyInit_ex(&md_ctx, md, NULL);\n            EVP_VerifyUpdate(&md_ctx, &(s->s3->client_random[0]),\n                             SSL3_RANDOM_SIZE);\n            EVP_VerifyUpdate(&md_ctx, &(s->s3->server_random[0]),\n                             SSL3_RANDOM_SIZE);\n            EVP_VerifyUpdate(&md_ctx, param, param_len);\n            if (EVP_VerifyFinal(&md_ctx, p, (int)n, pkey) <= 0) {\n                /* bad signature */\n                al = SSL_AD_DECRYPT_ERROR;\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_BAD_SIGNATURE);\n                goto f_err;\n            }\n        }\n    } else {\n        /* aNULL, aSRP or kPSK do not need public keys */\n        if (!(alg_a & (SSL_aNULL | SSL_aSRP)) && !(alg_k & SSL_kPSK)) {\n            /* Might be wrong key type, check it */\n            if (ssl3_check_cert_and_algorithm(s))\n                /* Otherwise this shouldn't happen */\n                SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);\n            goto err;\n        }\n        /* still data left over */\n        if (n != 0) {\n            SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_EXTRA_DATA_IN_MESSAGE);\n            goto f_err;\n        }\n    }\n    EVP_PKEY_free(pkey);\n    EVP_MD_CTX_cleanup(&md_ctx);\n    return (1);\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n err:\n    EVP_PKEY_free(pkey);\n#ifndef OPENSSL_NO_RSA\n    if (rsa != NULL)\n        RSA_free(rsa);\n#endif\n#ifndef OPENSSL_NO_DH\n    if (dh != NULL)\n        DH_free(dh);\n#endif\n#ifndef OPENSSL_NO_ECDH\n    BN_CTX_free(bn_ctx);\n    EC_POINT_free(srvr_ecpoint);\n    if (ecdh != NULL)\n        EC_KEY_free(ecdh);\n#endif\n    EVP_MD_CTX_cleanup(&md_ctx);\n    s->state = SSL_ST_ERR;\n    return (-1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -98,8 +98,6 @@\n \n #ifndef OPENSSL_NO_PSK\n     if (alg_k & SSL_kPSK) {\n-        char tmp_id_hint[PSK_MAX_IDENTITY_LEN + 1];\n-\n         param_len = 2;\n         if (param_len > n) {\n             SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, SSL_R_LENGTH_TOO_SHORT);\n@@ -125,17 +123,8 @@\n         }\n         param_len += i;\n \n-        /*\n-         * If received PSK identity hint contains NULL characters, the hint\n-         * is truncated from the first NULL. p may not be ending with NULL,\n-         * so create a NULL-terminated string.\n-         */\n-        memcpy(tmp_id_hint, p, i);\n-        memset(tmp_id_hint + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);\n-        if (s->ctx->psk_identity_hint != NULL)\n-            OPENSSL_free(s->ctx->psk_identity_hint);\n-        s->ctx->psk_identity_hint = BUF_strdup(tmp_id_hint);\n-        if (s->ctx->psk_identity_hint == NULL) {\n+        s->session->psk_identity_hint = BUF_strndup((char *)p, i);\n+        if (s->session->psk_identity_hint == NULL) {\n             al = SSL_AD_HANDSHAKE_FAILURE;\n             SSLerr(SSL_F_SSL3_GET_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);\n             goto f_err;",
        "diff_line_info": {
            "deleted_lines": [
                "        char tmp_id_hint[PSK_MAX_IDENTITY_LEN + 1];",
                "",
                "        /*",
                "         * If received PSK identity hint contains NULL characters, the hint",
                "         * is truncated from the first NULL. p may not be ending with NULL,",
                "         * so create a NULL-terminated string.",
                "         */",
                "        memcpy(tmp_id_hint, p, i);",
                "        memset(tmp_id_hint + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);",
                "        if (s->ctx->psk_identity_hint != NULL)",
                "            OPENSSL_free(s->ctx->psk_identity_hint);",
                "        s->ctx->psk_identity_hint = BUF_strdup(tmp_id_hint);",
                "        if (s->ctx->psk_identity_hint == NULL) {"
            ],
            "added_lines": [
                "        s->session->psk_identity_hint = BUF_strndup((char *)p, i);",
                "        if (s->session->psk_identity_hint == NULL) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-6789",
        "func_name": "chromium/MutationObserverInterestGroup::MutationObserverInterestGroup",
        "description": "Race condition in the MutationObserver implementation in Blink, as used in Google Chrome before 47.0.2526.80, allows remote attackers to cause a denial of service (use-after-free) or possibly have unspecified other impact by leveraging unanticipated object deletion.",
        "git_url": "https://chromium.googlesource.com/chromium/src/+/a17c2c87065be2c4dcb586583b1d69a5c85dae20",
        "commit_title": "Use RefPtr for MutationObserver in MutationObserverInterestGroup.",
        "commit_text": " In MutaionObserverInterestGroup, MutationObservers were held in HashSet as raw pointers.  In case a MutationObserver is gone while mutation events are collected (and garbage collector collects the object), it causes use-after-free while the code tries to enqueue the recorded mutation events.  Use RefPtr<> to hold the pointer so that the object will be kept until it goes out of scope.    ",
        "func_before": "MutationObserverInterestGroup::MutationObserverInterestGroup(WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationRecordDeliveryOptions oldValueFlag)\n    : m_oldValueFlag(oldValueFlag)\n{\n    ASSERT(!observers.isEmpty());\n    m_observers.swap(observers);\n}",
        "func": "MutationObserverInterestGroup::MutationObserverInterestGroup(WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationRecordDeliveryOptions oldValueFlag)\n    : m_oldValueFlag(oldValueFlag)\n{\n    ASSERT(!observers.isEmpty());\n    m_observers.swap(observers);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-MutationObserverInterestGroup::MutationObserverInterestGroup(WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationRecordDeliveryOptions oldValueFlag)\n+MutationObserverInterestGroup::MutationObserverInterestGroup(WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationRecordDeliveryOptions oldValueFlag)\n     : m_oldValueFlag(oldValueFlag)\n {\n     ASSERT(!observers.isEmpty());",
        "diff_line_info": {
            "deleted_lines": [
                "MutationObserverInterestGroup::MutationObserverInterestGroup(WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationRecordDeliveryOptions oldValueFlag)"
            ],
            "added_lines": [
                "MutationObserverInterestGroup::MutationObserverInterestGroup(WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationRecordDeliveryOptions oldValueFlag)"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-6789",
        "func_name": "chromium/MutationObserverInterestGroup::enqueueMutationRecord",
        "description": "Race condition in the MutationObserver implementation in Blink, as used in Google Chrome before 47.0.2526.80, allows remote attackers to cause a denial of service (use-after-free) or possibly have unspecified other impact by leveraging unanticipated object deletion.",
        "git_url": "https://chromium.googlesource.com/chromium/src/+/a17c2c87065be2c4dcb586583b1d69a5c85dae20",
        "commit_title": "Use RefPtr for MutationObserver in MutationObserverInterestGroup.",
        "commit_text": " In MutaionObserverInterestGroup, MutationObservers were held in HashSet as raw pointers.  In case a MutationObserver is gone while mutation events are collected (and garbage collector collects the object), it causes use-after-free while the code tries to enqueue the recorded mutation events.  Use RefPtr<> to hold the pointer so that the object will be kept until it goes out of scope.    ",
        "func_before": "void MutationObserverInterestGroup::enqueueMutationRecord(PassRefPtrWillBeRawPtr<MutationRecord> prpMutation)\n{\n    RefPtrWillBeRawPtr<MutationRecord> mutation = prpMutation;\n    RefPtrWillBeRawPtr<MutationRecord> mutationWithNullOldValue = nullptr;\n    for (auto& iter : m_observers) {\n        MutationObserver* observer = iter.key;\n        if (hasOldValue(iter.value)) {\n            observer->enqueueMutationRecord(mutation);\n            continue;\n        }\n        if (!mutationWithNullOldValue) {\n            if (mutation->oldValue().isNull())\n                mutationWithNullOldValue = mutation;\n            else\n                mutationWithNullOldValue = MutationRecord::createWithNullOldValue(mutation).get();\n        }\n        observer->enqueueMutationRecord(mutationWithNullOldValue);\n    }\n}",
        "func": "void MutationObserverInterestGroup::enqueueMutationRecord(PassRefPtrWillBeRawPtr<MutationRecord> prpMutation)\n{\n    RefPtrWillBeRawPtr<MutationRecord> mutation = prpMutation;\n    RefPtrWillBeRawPtr<MutationRecord> mutationWithNullOldValue = nullptr;\n    for (auto& iter : m_observers) {\n        MutationObserver* observer = iter.key.get();\n        if (hasOldValue(iter.value)) {\n            observer->enqueueMutationRecord(mutation);\n            continue;\n        }\n        if (!mutationWithNullOldValue) {\n            if (mutation->oldValue().isNull())\n                mutationWithNullOldValue = mutation;\n            else\n                mutationWithNullOldValue = MutationRecord::createWithNullOldValue(mutation).get();\n        }\n        observer->enqueueMutationRecord(mutationWithNullOldValue);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n     RefPtrWillBeRawPtr<MutationRecord> mutation = prpMutation;\n     RefPtrWillBeRawPtr<MutationRecord> mutationWithNullOldValue = nullptr;\n     for (auto& iter : m_observers) {\n-        MutationObserver* observer = iter.key;\n+        MutationObserver* observer = iter.key.get();\n         if (hasOldValue(iter.value)) {\n             observer->enqueueMutationRecord(mutation);\n             continue;",
        "diff_line_info": {
            "deleted_lines": [
                "        MutationObserver* observer = iter.key;"
            ],
            "added_lines": [
                "        MutationObserver* observer = iter.key.get();"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-6789",
        "func_name": "chromium/MutationObserverInterestGroup::createIfNeeded",
        "description": "Race condition in the MutationObserver implementation in Blink, as used in Google Chrome before 47.0.2526.80, allows remote attackers to cause a denial of service (use-after-free) or possibly have unspecified other impact by leveraging unanticipated object deletion.",
        "git_url": "https://chromium.googlesource.com/chromium/src/+/a17c2c87065be2c4dcb586583b1d69a5c85dae20",
        "commit_title": "Use RefPtr for MutationObserver in MutationObserverInterestGroup.",
        "commit_text": " In MutaionObserverInterestGroup, MutationObservers were held in HashSet as raw pointers.  In case a MutationObserver is gone while mutation events are collected (and garbage collector collects the object), it causes use-after-free while the code tries to enqueue the recorded mutation events.  Use RefPtr<> to hold the pointer so that the object will be kept until it goes out of scope.    ",
        "func_before": "PassOwnPtrWillBeRawPtr<MutationObserverInterestGroup> MutationObserverInterestGroup::createIfNeeded(Node& target, MutationObserver::MutationType type, MutationRecordDeliveryOptions oldValueFlag, const QualifiedName* attributeName)\n{\n    ASSERT((type == MutationObserver::Attributes && attributeName) || !attributeName);\n    WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions> observers;\n    target.getRegisteredMutationObserversOfType(observers, type, attributeName);\n    if (observers.isEmpty())\n        return nullptr;\n\n    return adoptPtrWillBeNoop(new MutationObserverInterestGroup(observers, oldValueFlag));\n}",
        "func": "PassOwnPtrWillBeRawPtr<MutationObserverInterestGroup> MutationObserverInterestGroup::createIfNeeded(Node& target, MutationObserver::MutationType type, MutationRecordDeliveryOptions oldValueFlag, const QualifiedName* attributeName)\n{\n    ASSERT((type == MutationObserver::Attributes && attributeName) || !attributeName);\n    WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions> observers;\n    target.getRegisteredMutationObserversOfType(observers, type, attributeName);\n    if (observers.isEmpty())\n        return nullptr;\n\n    return adoptPtrWillBeNoop(new MutationObserverInterestGroup(observers, oldValueFlag));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n PassOwnPtrWillBeRawPtr<MutationObserverInterestGroup> MutationObserverInterestGroup::createIfNeeded(Node& target, MutationObserver::MutationType type, MutationRecordDeliveryOptions oldValueFlag, const QualifiedName* attributeName)\n {\n     ASSERT((type == MutationObserver::Attributes && attributeName) || !attributeName);\n-    WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions> observers;\n+    WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions> observers;\n     target.getRegisteredMutationObserversOfType(observers, type, attributeName);\n     if (observers.isEmpty())\n         return nullptr;",
        "diff_line_info": {
            "deleted_lines": [
                "    WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions> observers;"
            ],
            "added_lines": [
                "    WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions> observers;"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-6789",
        "func_name": "chromium/collectMatchingObserversForMutation",
        "description": "Race condition in the MutationObserver implementation in Blink, as used in Google Chrome before 47.0.2526.80, allows remote attackers to cause a denial of service (use-after-free) or possibly have unspecified other impact by leveraging unanticipated object deletion.",
        "git_url": "https://chromium.googlesource.com/chromium/src/+/a17c2c87065be2c4dcb586583b1d69a5c85dae20",
        "commit_title": "Use RefPtr for MutationObserver in MutationObserverInterestGroup.",
        "commit_text": " In MutaionObserverInterestGroup, MutationObservers were held in HashSet as raw pointers.  In case a MutationObserver is gone while mutation events are collected (and garbage collector collects the object), it causes use-after-free while the code tries to enqueue the recorded mutation events.  Use RefPtr<> to hold the pointer so that the object will be kept until it goes out of scope.    ",
        "func_before": "static inline void collectMatchingObserversForMutation(WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, Registry* registry, Node& target, MutationObserver::MutationType type, const QualifiedName* attributeName)\n{\n    if (!registry)\n        return;\n\n    for (const auto& registration : *registry) {\n        if (registration->shouldReceiveMutationFrom(target, type, attributeName)) {\n            MutationRecordDeliveryOptions deliveryOptions = registration->deliveryOptions();\n            WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>::AddResult result = observers.add(&registration->observer(), deliveryOptions);\n            if (!result.isNewEntry)\n                result.storedValue->value |= deliveryOptions;\n        }\n    }\n}",
        "func": "static inline void collectMatchingObserversForMutation(WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, Registry* registry, Node& target, MutationObserver::MutationType type, const QualifiedName* attributeName)\n{\n    if (!registry)\n        return;\n\n    for (const auto& registration : *registry) {\n        if (registration->shouldReceiveMutationFrom(target, type, attributeName)) {\n            MutationRecordDeliveryOptions deliveryOptions = registration->deliveryOptions();\n            WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>::AddResult result = observers.add(&registration->observer(), deliveryOptions);\n            if (!result.isNewEntry)\n                result.storedValue->value |= deliveryOptions;\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-static inline void collectMatchingObserversForMutation(WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, Registry* registry, Node& target, MutationObserver::MutationType type, const QualifiedName* attributeName)\n+static inline void collectMatchingObserversForMutation(WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, Registry* registry, Node& target, MutationObserver::MutationType type, const QualifiedName* attributeName)\n {\n     if (!registry)\n         return;\n@@ -6,7 +6,7 @@\n     for (const auto& registration : *registry) {\n         if (registration->shouldReceiveMutationFrom(target, type, attributeName)) {\n             MutationRecordDeliveryOptions deliveryOptions = registration->deliveryOptions();\n-            WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>::AddResult result = observers.add(&registration->observer(), deliveryOptions);\n+            WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>::AddResult result = observers.add(&registration->observer(), deliveryOptions);\n             if (!result.isNewEntry)\n                 result.storedValue->value |= deliveryOptions;\n         }",
        "diff_line_info": {
            "deleted_lines": [
                "static inline void collectMatchingObserversForMutation(WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, Registry* registry, Node& target, MutationObserver::MutationType type, const QualifiedName* attributeName)",
                "            WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>::AddResult result = observers.add(&registration->observer(), deliveryOptions);"
            ],
            "added_lines": [
                "static inline void collectMatchingObserversForMutation(WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, Registry* registry, Node& target, MutationObserver::MutationType type, const QualifiedName* attributeName)",
                "            WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>::AddResult result = observers.add(&registration->observer(), deliveryOptions);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-6789",
        "func_name": "chromium/Node::getRegisteredMutationObserversOfType",
        "description": "Race condition in the MutationObserver implementation in Blink, as used in Google Chrome before 47.0.2526.80, allows remote attackers to cause a denial of service (use-after-free) or possibly have unspecified other impact by leveraging unanticipated object deletion.",
        "git_url": "https://chromium.googlesource.com/chromium/src/+/a17c2c87065be2c4dcb586583b1d69a5c85dae20",
        "commit_title": "Use RefPtr for MutationObserver in MutationObserverInterestGroup.",
        "commit_text": " In MutaionObserverInterestGroup, MutationObservers were held in HashSet as raw pointers.  In case a MutationObserver is gone while mutation events are collected (and garbage collector collects the object), it causes use-after-free while the code tries to enqueue the recorded mutation events.  Use RefPtr<> to hold the pointer so that the object will be kept until it goes out of scope.    ",
        "func_before": "void Node::getRegisteredMutationObserversOfType(WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationObserver::MutationType type, const QualifiedName* attributeName)\n{\n    ASSERT((type == MutationObserver::Attributes && attributeName) || !attributeName);\n    collectMatchingObserversForMutation(observers, mutationObserverRegistry(), *this, type, attributeName);\n    collectMatchingObserversForMutation(observers, transientMutationObserverRegistry(), *this, type, attributeName);\n    ScriptForbiddenScope forbidScriptDuringRawIteration;\n    for (Node* node = parentNode(); node; node = node->parentNode()) {\n        collectMatchingObserversForMutation(observers, node->mutationObserverRegistry(), *this, type, attributeName);\n        collectMatchingObserversForMutation(observers, node->transientMutationObserverRegistry(), *this, type, attributeName);\n    }\n}",
        "func": "void Node::getRegisteredMutationObserversOfType(WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationObserver::MutationType type, const QualifiedName* attributeName)\n{\n    ASSERT((type == MutationObserver::Attributes && attributeName) || !attributeName);\n    collectMatchingObserversForMutation(observers, mutationObserverRegistry(), *this, type, attributeName);\n    collectMatchingObserversForMutation(observers, transientMutationObserverRegistry(), *this, type, attributeName);\n    ScriptForbiddenScope forbidScriptDuringRawIteration;\n    for (Node* node = parentNode(); node; node = node->parentNode()) {\n        collectMatchingObserversForMutation(observers, node->mutationObserverRegistry(), *this, type, attributeName);\n        collectMatchingObserversForMutation(observers, node->transientMutationObserverRegistry(), *this, type, attributeName);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-void Node::getRegisteredMutationObserversOfType(WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationObserver::MutationType type, const QualifiedName* attributeName)\n+void Node::getRegisteredMutationObserversOfType(WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationObserver::MutationType type, const QualifiedName* attributeName)\n {\n     ASSERT((type == MutationObserver::Attributes && attributeName) || !attributeName);\n     collectMatchingObserversForMutation(observers, mutationObserverRegistry(), *this, type, attributeName);",
        "diff_line_info": {
            "deleted_lines": [
                "void Node::getRegisteredMutationObserversOfType(WillBeHeapHashMap<RawPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationObserver::MutationType type, const QualifiedName* attributeName)"
            ],
            "added_lines": [
                "void Node::getRegisteredMutationObserversOfType(WillBeHeapHashMap<RefPtrWillBeMember<MutationObserver>, MutationRecordDeliveryOptions>& observers, MutationObserver::MutationType type, const QualifiedName* attributeName)"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-7990",
        "func_name": "torvalds/linux/__rds_conn_create",
        "description": "Race condition in the rds_sendmsg function in net/rds/sendmsg.c in the Linux kernel before 4.3.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by using a socket that was not properly bound.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2015-6937.",
        "git_url": "https://github.com/torvalds/linux/commit/8c7188b23474cca017b3ef354c4a58456f68303a",
        "commit_title": "RDS: fix race condition when sending a message on unbound socket",
        "commit_text": " Sasha's found a NULL pointer dereference in the RDS connection code when sending a message to an apparently unbound socket.  The problem is caused by the code checking if the socket is bound in rds_sendmsg(), which checks the rs_bound_addr field without taking a lock on the socket.  This opens a race where rs_bound_addr is temporarily set but where the transport is not in rds_bind(), leading to a NULL pointer dereference when trying to dereference 'trans' in __rds_conn_create().  Vegard wrote a reproducer for this issue, so kindly ask him to share if you're interested.  I cannot reproduce the NULL pointer dereference using Vegard's reproducer with this patch, whereas I could without.  Complete earlier incomplete fix to CVE-2015-6937:    74e98eb08588 (\"RDS: verify the underlying transport exists before creating a connection\")  Cc: David S. Miller <davem@davemloft.net> Cc: stable@vger.kernel.org ",
        "func_before": "static struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\t__be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp,\n\t\t\t\t       int is_outgoing)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret;\n\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans);\n\tif (conn && conn->c_loopback && conn->c_trans != &rds_loop_transport &&\n\t    laddr == faddr && !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = laddr;\n\tconn->c_faddr = faddr;\n\tspin_lock_init(&conn->c_lock);\n\tconn->c_next_tx_seq = 1;\n\trds_conn_net_set(conn, net);\n\n\tinit_waitqueue_head(&conn->c_waitq);\n\tINIT_LIST_HEAD(&conn->c_send_queue);\n\tINIT_LIST_HEAD(&conn->c_retrans);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (is_outgoing && trans->t_prefer_loopback) {\n\t\t\t/* \"outgoing\" connection - and the transport\n\t\t\t * says it wants the connection handled by the\n\t\t\t * loopback transport. This is what TCP does.\n\t\t\t */\n\t\t\ttrans = &rds_loop_transport;\n\t\t}\n\t}\n\n\tif (trans == NULL) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(-ENODEV);\n\t\tgoto out;\n\t}\n\n\tconn->c_trans = trans;\n\n\tret = trans->conn_alloc(conn, gfp);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tatomic_set(&conn->c_state, RDS_CONN_DOWN);\n\tconn->c_send_gen = 0;\n\tconn->c_outgoing = (is_outgoing ? 1 : 0);\n\tconn->c_reconnect_jiffies = 0;\n\tINIT_DELAYED_WORK(&conn->c_send_w, rds_send_worker);\n\tINIT_DELAYED_WORK(&conn->c_recv_w, rds_recv_worker);\n\tINIT_DELAYED_WORK(&conn->c_conn_w, rds_connect_worker);\n\tINIT_WORK(&conn->c_down_w, rds_shutdown_worker);\n\tmutex_init(&conn->c_cm_lock);\n\tconn->c_flags = 0;\n\n\trdsdebug(\"allocated conn %p for %pI4 -> %pI4 over %s %s\\n\",\n\t  conn, &laddr, &faddr,\n\t  trans->t_name ? trans->t_name : \"[unknown]\",\n\t  is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans);\n\t\tif (found) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\nout:\n\treturn conn;\n}",
        "func": "static struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\t__be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp,\n\t\t\t\t       int is_outgoing)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret;\n\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans);\n\tif (conn && conn->c_loopback && conn->c_trans != &rds_loop_transport &&\n\t    laddr == faddr && !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = laddr;\n\tconn->c_faddr = faddr;\n\tspin_lock_init(&conn->c_lock);\n\tconn->c_next_tx_seq = 1;\n\trds_conn_net_set(conn, net);\n\n\tinit_waitqueue_head(&conn->c_waitq);\n\tINIT_LIST_HEAD(&conn->c_send_queue);\n\tINIT_LIST_HEAD(&conn->c_retrans);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (is_outgoing && trans->t_prefer_loopback) {\n\t\t\t/* \"outgoing\" connection - and the transport\n\t\t\t * says it wants the connection handled by the\n\t\t\t * loopback transport. This is what TCP does.\n\t\t\t */\n\t\t\ttrans = &rds_loop_transport;\n\t\t}\n\t}\n\n\tconn->c_trans = trans;\n\n\tret = trans->conn_alloc(conn, gfp);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tatomic_set(&conn->c_state, RDS_CONN_DOWN);\n\tconn->c_send_gen = 0;\n\tconn->c_outgoing = (is_outgoing ? 1 : 0);\n\tconn->c_reconnect_jiffies = 0;\n\tINIT_DELAYED_WORK(&conn->c_send_w, rds_send_worker);\n\tINIT_DELAYED_WORK(&conn->c_recv_w, rds_recv_worker);\n\tINIT_DELAYED_WORK(&conn->c_conn_w, rds_connect_worker);\n\tINIT_WORK(&conn->c_down_w, rds_shutdown_worker);\n\tmutex_init(&conn->c_cm_lock);\n\tconn->c_flags = 0;\n\n\trdsdebug(\"allocated conn %p for %pI4 -> %pI4 over %s %s\\n\",\n\t  conn, &laddr, &faddr,\n\t  trans->t_name ? trans->t_name : \"[unknown]\",\n\t  is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans);\n\t\tif (found) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\nout:\n\treturn conn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -66,12 +66,6 @@\n \t\t}\n \t}\n \n-\tif (trans == NULL) {\n-\t\tkmem_cache_free(rds_conn_slab, conn);\n-\t\tconn = ERR_PTR(-ENODEV);\n-\t\tgoto out;\n-\t}\n-\n \tconn->c_trans = trans;\n \n \tret = trans->conn_alloc(conn, gfp);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (trans == NULL) {",
                "\t\tkmem_cache_free(rds_conn_slab, conn);",
                "\t\tconn = ERR_PTR(-ENODEV);",
                "\t\tgoto out;",
                "\t}",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2015-7990",
        "func_name": "torvalds/linux/rds_sendmsg",
        "description": "Race condition in the rds_sendmsg function in net/rds/sendmsg.c in the Linux kernel before 4.3.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by using a socket that was not properly bound.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2015-6937.",
        "git_url": "https://github.com/torvalds/linux/commit/8c7188b23474cca017b3ef354c4a58456f68303a",
        "commit_title": "RDS: fix race condition when sending a message on unbound socket",
        "commit_text": " Sasha's found a NULL pointer dereference in the RDS connection code when sending a message to an apparently unbound socket.  The problem is caused by the code checking if the socket is bound in rds_sendmsg(), which checks the rs_bound_addr field without taking a lock on the socket.  This opens a race where rs_bound_addr is temporarily set but where the transport is not in rds_bind(), leading to a NULL pointer dereference when trying to dereference 'trans' in __rds_conn_create().  Vegard wrote a reproducer for this issue, so kindly ask him to share if you're interested.  I cannot reproduce the NULL pointer dereference using Vegard's reproducer with this patch, whereas I could without.  Complete earlier incomplete fix to CVE-2015-6937:    74e98eb08588 (\"RDS: verify the underlying transport exists before creating a connection\")  Cc: David S. Miller <davem@davemloft.net> Cc: stable@vger.kernel.org ",
        "func_before": "int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rds_sock *rs = rds_sk_to_rs(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t__be32 daddr;\n\t__be16 dport;\n\tstruct rds_message *rm = NULL;\n\tstruct rds_connection *conn;\n\tint ret = 0;\n\tint queued = 0, allocated_mr = 0;\n\tint nonblock = msg->msg_flags & MSG_DONTWAIT;\n\tlong timeo = sock_sndtimeo(sk, nonblock);\n\n\t/* Mirror Linux UDP mirror of BSD error message compatibility */\n\t/* XXX: Perhaps MSG_MORE someday */\n\tif (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (msg->msg_namelen) {\n\t\t/* XXX fail non-unicast destination IPs? */\n\t\tif (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t} else {\n\t\t/* We only care about consistency with ->connect() */\n\t\tlock_sock(sk);\n\t\tdaddr = rs->rs_conn_addr;\n\t\tdport = rs->rs_conn_port;\n\t\trelease_sock(sk);\n\t}\n\n\t/* racing with another thread binding seems ok here */\n\tif (daddr == 0 || rs->rs_bound_addr == 0) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (payload_len > rds_sk_sndbuf(rs)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\t/* size of rm including all sgs */\n\tret = rds_rm_size(msg, payload_len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\trm = rds_message_alloc(ret, GFP_KERNEL);\n\tif (!rm) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Attach data to the rm */\n\tif (payload_len) {\n\t\trm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));\n\t\tif (!rm->data.op_sg) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = rds_message_copy_from_user(rm, &msg->msg_iter);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\trm->data.op_active = 1;\n\n\trm->m_daddr = daddr;\n\n\t/* rds_conn_create has a spinlock that runs with IRQ off.\n\t * Caching the conn in the socket helps a lot. */\n\tif (rs->rs_conn && rs->rs_conn->c_faddr == daddr)\n\t\tconn = rs->rs_conn;\n\telse {\n\t\tconn = rds_conn_create_outgoing(sock_net(sock->sk),\n\t\t\t\t\t\trs->rs_bound_addr, daddr,\n\t\t\t\t\trs->rs_transport,\n\t\t\t\t\tsock->sk->sk_allocation);\n\t\tif (IS_ERR(conn)) {\n\t\t\tret = PTR_ERR(conn);\n\t\t\tgoto out;\n\t\t}\n\t\trs->rs_conn = conn;\n\t}\n\n\t/* Parse any control messages the user may have included. */\n\tret = rds_cmsg_send(rs, rm, msg, &allocated_mr);\n\tif (ret)\n\t\tgoto out;\n\n\tif (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {\n\t\tprintk_ratelimited(KERN_NOTICE \"rdma_op %p conn xmit_rdma %p\\n\",\n\t\t\t       &rm->rdma, conn->c_trans->xmit_rdma);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {\n\t\tprintk_ratelimited(KERN_NOTICE \"atomic_op %p conn xmit_atomic %p\\n\",\n\t\t\t       &rm->atomic, conn->c_trans->xmit_atomic);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\trds_conn_connect_if_down(conn);\n\n\tret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);\n\tif (ret) {\n\t\trs->rs_seen_congestion = 1;\n\t\tgoto out;\n\t}\n\n\twhile (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,\n\t\t\t\t  dport, &queued)) {\n\t\trds_stats_inc(s_send_queue_full);\n\n\t\tif (nonblock) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\trds_send_queue_rm(rs, conn, rm,\n\t\t\t\t\t\t\t  rs->rs_bound_port,\n\t\t\t\t\t\t\t  dport,\n\t\t\t\t\t\t\t  &queued),\n\t\t\t\t\ttimeo);\n\t\trdsdebug(\"sendmsg woke queued %d timeo %ld\\n\", queued, timeo);\n\t\tif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\n\t\t\tcontinue;\n\n\t\tret = timeo;\n\t\tif (ret == 0)\n\t\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * By now we've committed to the send.  We reuse rds_send_worker()\n\t * to retry sends in the rds thread if the transport asks us to.\n\t */\n\trds_stats_inc(s_send_queued);\n\n\tret = rds_send_xmit(conn);\n\tif (ret == -ENOMEM || ret == -EAGAIN)\n\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 1);\n\n\trds_message_put(rm);\n\treturn payload_len;\n\nout:\n\t/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.\n\t * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN\n\t * or in any other way, we need to destroy the MR again */\n\tif (allocated_mr)\n\t\trds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);\n\n\tif (rm)\n\t\trds_message_put(rm);\n\treturn ret;\n}",
        "func": "int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rds_sock *rs = rds_sk_to_rs(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t__be32 daddr;\n\t__be16 dport;\n\tstruct rds_message *rm = NULL;\n\tstruct rds_connection *conn;\n\tint ret = 0;\n\tint queued = 0, allocated_mr = 0;\n\tint nonblock = msg->msg_flags & MSG_DONTWAIT;\n\tlong timeo = sock_sndtimeo(sk, nonblock);\n\n\t/* Mirror Linux UDP mirror of BSD error message compatibility */\n\t/* XXX: Perhaps MSG_MORE someday */\n\tif (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (msg->msg_namelen) {\n\t\t/* XXX fail non-unicast destination IPs? */\n\t\tif (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t} else {\n\t\t/* We only care about consistency with ->connect() */\n\t\tlock_sock(sk);\n\t\tdaddr = rs->rs_conn_addr;\n\t\tdport = rs->rs_conn_port;\n\t\trelease_sock(sk);\n\t}\n\n\tlock_sock(sk);\n\tif (daddr == 0 || rs->rs_bound_addr == 0) {\n\t\trelease_sock(sk);\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\trelease_sock(sk);\n\n\tif (payload_len > rds_sk_sndbuf(rs)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\t/* size of rm including all sgs */\n\tret = rds_rm_size(msg, payload_len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\trm = rds_message_alloc(ret, GFP_KERNEL);\n\tif (!rm) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Attach data to the rm */\n\tif (payload_len) {\n\t\trm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));\n\t\tif (!rm->data.op_sg) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = rds_message_copy_from_user(rm, &msg->msg_iter);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\trm->data.op_active = 1;\n\n\trm->m_daddr = daddr;\n\n\t/* rds_conn_create has a spinlock that runs with IRQ off.\n\t * Caching the conn in the socket helps a lot. */\n\tif (rs->rs_conn && rs->rs_conn->c_faddr == daddr)\n\t\tconn = rs->rs_conn;\n\telse {\n\t\tconn = rds_conn_create_outgoing(sock_net(sock->sk),\n\t\t\t\t\t\trs->rs_bound_addr, daddr,\n\t\t\t\t\trs->rs_transport,\n\t\t\t\t\tsock->sk->sk_allocation);\n\t\tif (IS_ERR(conn)) {\n\t\t\tret = PTR_ERR(conn);\n\t\t\tgoto out;\n\t\t}\n\t\trs->rs_conn = conn;\n\t}\n\n\t/* Parse any control messages the user may have included. */\n\tret = rds_cmsg_send(rs, rm, msg, &allocated_mr);\n\tif (ret)\n\t\tgoto out;\n\n\tif (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {\n\t\tprintk_ratelimited(KERN_NOTICE \"rdma_op %p conn xmit_rdma %p\\n\",\n\t\t\t       &rm->rdma, conn->c_trans->xmit_rdma);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {\n\t\tprintk_ratelimited(KERN_NOTICE \"atomic_op %p conn xmit_atomic %p\\n\",\n\t\t\t       &rm->atomic, conn->c_trans->xmit_atomic);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\trds_conn_connect_if_down(conn);\n\n\tret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);\n\tif (ret) {\n\t\trs->rs_seen_congestion = 1;\n\t\tgoto out;\n\t}\n\n\twhile (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,\n\t\t\t\t  dport, &queued)) {\n\t\trds_stats_inc(s_send_queue_full);\n\n\t\tif (nonblock) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\trds_send_queue_rm(rs, conn, rm,\n\t\t\t\t\t\t\t  rs->rs_bound_port,\n\t\t\t\t\t\t\t  dport,\n\t\t\t\t\t\t\t  &queued),\n\t\t\t\t\ttimeo);\n\t\trdsdebug(\"sendmsg woke queued %d timeo %ld\\n\", queued, timeo);\n\t\tif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\n\t\t\tcontinue;\n\n\t\tret = timeo;\n\t\tif (ret == 0)\n\t\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * By now we've committed to the send.  We reuse rds_send_worker()\n\t * to retry sends in the rds thread if the transport asks us to.\n\t */\n\trds_stats_inc(s_send_queued);\n\n\tret = rds_send_xmit(conn);\n\tif (ret == -ENOMEM || ret == -EAGAIN)\n\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 1);\n\n\trds_message_put(rm);\n\treturn payload_len;\n\nout:\n\t/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.\n\t * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN\n\t * or in any other way, we need to destroy the MR again */\n\tif (allocated_mr)\n\t\trds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);\n\n\tif (rm)\n\t\trds_message_put(rm);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -35,11 +35,13 @@\n \t\trelease_sock(sk);\n \t}\n \n-\t/* racing with another thread binding seems ok here */\n+\tlock_sock(sk);\n \tif (daddr == 0 || rs->rs_bound_addr == 0) {\n+\t\trelease_sock(sk);\n \t\tret = -ENOTCONN; /* XXX not a great errno */\n \t\tgoto out;\n \t}\n+\trelease_sock(sk);\n \n \tif (payload_len > rds_sk_sndbuf(rs)) {\n \t\tret = -EMSGSIZE;",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* racing with another thread binding seems ok here */"
            ],
            "added_lines": [
                "\tlock_sock(sk);",
                "\t\trelease_sock(sk);",
                "\trelease_sock(sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-7550",
        "func_name": "torvalds/linux/keyctl_read_key",
        "description": "The keyctl_read_key function in security/keys/keyctl.c in the Linux kernel before 4.3.4 does not properly use a semaphore, which allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact via a crafted application that leverages a race condition between keyctl_revoke and keyctl_read calls.",
        "git_url": "https://github.com/torvalds/linux/commit/b4a1b4f5047e4f54e194681125c74c0aa64d637d",
        "commit_title": "KEYS: Fix race between read and revoke",
        "commit_text": " This fixes CVE-2015-7550.  There's a race between keyctl_read() and keyctl_revoke().  If the revoke happens between keyctl_read() checking the validity of a key and the key's semaphore being taken, then the key type read method will see a revoked key.  This causes a problem for the user-defined key type because it assumes in its read method that there will always be a payload in a non-revoked key and doesn't check for a NULL pointer.  Fix this by making keyctl_read() check the validity of a key after taking semaphore instead of before.  I think the bug was introduced with the original keyrings code.  This was discovered by a multithreaded test program generated by syzkaller (http://github.com/google/syzkaller).  Here's a cleaned up version:  \t#include <sys/types.h> \t#include <keyutils.h> \t#include <pthread.h> \tvoid *thr0(void *arg) \t{ \t\tkey_serial_t key = (unsigned long)arg; \t\tkeyctl_revoke(key); \t\treturn 0; \t} \tvoid *thr1(void *arg) \t{ \t\tkey_serial_t key = (unsigned long)arg; \t\tchar buffer[16]; \t\tkeyctl_read(key, buffer, 16); \t\treturn 0; \t} \tint main() \t{ \t\tkey_serial_t key = add_key(\"user\", \"%\", \"foo\", 3, KEY_SPEC_USER_KEYRING); \t\tpthread_t th[5]; \t\tpthread_create(&th[0], 0, thr0, (void *)(unsigned long)key); \t\tpthread_create(&th[1], 0, thr1, (void *)(unsigned long)key); \t\tpthread_create(&th[2], 0, thr0, (void *)(unsigned long)key); \t\tpthread_create(&th[3], 0, thr1, (void *)(unsigned long)key); \t\tpthread_join(th[0], 0); \t\tpthread_join(th[1], 0); \t\tpthread_join(th[2], 0); \t\tpthread_join(th[3], 0); \t\treturn 0; \t}  Build as:  \tcc -o keyctl-race keyctl-race.c -lkeyutils -lpthread  Run as:  \twhile keyctl-race; do :; done  as it may need several iterations to crash the kernel.  The crash can be summarised as:  \tBUG: unable to handle kernel NULL pointer dereference at 0000000000000010 \tIP: [<ffffffff81279b08>] user_read+0x56/0xa3 \t... \tCall Trace: \t [<ffffffff81276aa9>] keyctl_read_key+0xb6/0xd7 \t [<ffffffff81277815>] SyS_keyctl+0x83/0xe0 \t [<ffffffff815dbb97>] entry_SYSCALL_64_fastpath+0x12/0x6f  Cc: stable@vger.kernel.org",
        "func_before": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = key_validate(key);\n\tif (ret == 0) {\n\t\tret = -EOPNOTSUPP;\n\t\tif (key->type->read) {\n\t\t\t/* read the data with the semaphore held (since we\n\t\t\t * might sleep) */\n\t\t\tdown_read(&key->sem);\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\t\tup_read(&key->sem);\n\t\t}\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}",
        "func": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = -EOPNOTSUPP;\n\tif (key->type->read) {\n\t\t/* Read the data with the semaphore held (since we might sleep)\n\t\t * to protect against the key being updated or revoked.\n\t\t */\n\t\tdown_read(&key->sem);\n\t\tret = key_validate(key);\n\t\tif (ret == 0)\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\tup_read(&key->sem);\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -31,16 +31,16 @@\n \n \t/* the key is probably readable - now try to read it */\n can_read_key:\n-\tret = key_validate(key);\n-\tif (ret == 0) {\n-\t\tret = -EOPNOTSUPP;\n-\t\tif (key->type->read) {\n-\t\t\t/* read the data with the semaphore held (since we\n-\t\t\t * might sleep) */\n-\t\t\tdown_read(&key->sem);\n+\tret = -EOPNOTSUPP;\n+\tif (key->type->read) {\n+\t\t/* Read the data with the semaphore held (since we might sleep)\n+\t\t * to protect against the key being updated or revoked.\n+\t\t */\n+\t\tdown_read(&key->sem);\n+\t\tret = key_validate(key);\n+\t\tif (ret == 0)\n \t\t\tret = key->type->read(key, buffer, buflen);\n-\t\t\tup_read(&key->sem);\n-\t\t}\n+\t\tup_read(&key->sem);\n \t}\n \n error2:",
        "diff_line_info": {
            "deleted_lines": [
                "\tret = key_validate(key);",
                "\tif (ret == 0) {",
                "\t\tret = -EOPNOTSUPP;",
                "\t\tif (key->type->read) {",
                "\t\t\t/* read the data with the semaphore held (since we",
                "\t\t\t * might sleep) */",
                "\t\t\tdown_read(&key->sem);",
                "\t\t\tup_read(&key->sem);",
                "\t\t}"
            ],
            "added_lines": [
                "\tret = -EOPNOTSUPP;",
                "\tif (key->type->read) {",
                "\t\t/* Read the data with the semaphore held (since we might sleep)",
                "\t\t * to protect against the key being updated or revoked.",
                "\t\t */",
                "\t\tdown_read(&key->sem);",
                "\t\tret = key_validate(key);",
                "\t\tif (ret == 0)",
                "\t\tup_read(&key->sem);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-8767",
        "func_name": "torvalds/linux/sctp_generate_heartbeat_event",
        "description": "net/sctp/sm_sideeffect.c in the Linux kernel before 4.3 does not properly manage the relationship between a lock and a socket, which allows local users to cause a denial of service (deadlock) via a crafted sctp_accept call.",
        "git_url": "https://github.com/torvalds/linux/commit/635682a14427d241bab7bbdeebb48a7d7b91638e",
        "commit_title": "sctp: Prevent soft lockup when sctp_accept() is called during a timeout event",
        "commit_text": " A case can occur when sctp_accept() is called by the user during a heartbeat timeout event after the 4-way handshake.  Since sctp_assoc_migrate() changes both assoc->base.sk and assoc->ep, the bh_sock_lock in sctp_generate_heartbeat_event() will be taken with the listening socket but released with the new association socket. The result is a deadlock on any future attempts to take the listening socket lock.  Note that this race can occur with other SCTP timeouts that take the bh_lock_sock() in the event sctp_accept() is called.   BUG: soft lockup - CPU#9 stuck for 67s! [swapper:0]  ...  RIP: 0010:[<ffffffff8152d48e>]  [<ffffffff8152d48e>] _spin_lock+0x1e/0x30  RSP: 0018:ffff880028323b20  EFLAGS: 00000206  RAX: 0000000000000002 RBX: ffff880028323b20 RCX: 0000000000000000  RDX: 0000000000000000 RSI: ffff880028323be0 RDI: ffff8804632c4b48  RBP: ffffffff8100bb93 R08: 0000000000000000 R09: 0000000000000000  R10: ffff880610662280 R11: 0000000000000100 R12: ffff880028323aa0  R13: ffff8804383c3880 R14: ffff880028323a90 R15: ffffffff81534225  FS:  0000000000000000(0000) GS:ffff880028320000(0000) knlGS:0000000000000000  CS:  0010 DS: 0018 ES: 0018 CR0: 000000008005003b  CR2: 00000000006df528 CR3: 0000000001a85000 CR4: 00000000000006e0  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000  DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400  Process swapper (pid: 0, threadinfo ffff880616b70000, task ffff880616b6cab0)  Stack:  ffff880028323c40 ffffffffa01c2582 ffff880614cfb020 0000000000000000  <d> 0100000000000000 00000014383a6c44 ffff8804383c3880 ffff880614e93c00  <d> ffff880614e93c00 0000000000000000 ffff8804632c4b00 ffff8804383c38b8  Call Trace:  <IRQ>  [<ffffffffa01c2582>] ? sctp_rcv+0x492/0xa10 [sctp]  [<ffffffff8148c559>] ? nf_iterate+0x69/0xb0  [<ffffffff814974a0>] ? ip_local_deliver_finish+0x0/0x2d0  [<ffffffff8148c716>] ? nf_hook_slow+0x76/0x120  [<ffffffff814974a0>] ? ip_local_deliver_finish+0x0/0x2d0  [<ffffffff8149757d>] ? ip_local_deliver_finish+0xdd/0x2d0  [<ffffffff81497808>] ? ip_local_deliver+0x98/0xa0  [<ffffffff81496ccd>] ? ip_rcv_finish+0x12d/0x440  [<ffffffff81497255>] ? ip_rcv+0x275/0x350  [<ffffffff8145cfeb>] ? __netif_receive_skb+0x4ab/0x750  ...  With lockdep debugging:   =====================================  [ BUG: bad unlock balance detected! ]  -------------------------------------  CslRx/12087 is trying to release lock (slock-AF_INET) at:  [<ffffffffa01bcae0>] sctp_generate_timeout_event+0x40/0xe0 [sctp]  but there are no more locks to release!   other info that might help us debug this:  2 locks held by CslRx/12087:  #0:  (&asoc->timers[i]){+.-...}, at: [<ffffffff8108ce1f>] run_timer_softirq+0x16f/0x3e0  #1:  (slock-AF_INET){+.-...}, at: [<ffffffffa01bcac3>] sctp_generate_timeout_event+0x23/0xe0 [sctp]  Ensure the socket taken is also the same one that is released by saving a copy of the socket before entering the timeout event critical section. ",
        "func_before": "void sctp_generate_heartbeat_event(unsigned long data)\n{\n\tint error = 0;\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct net *net = sock_net(asoc->base.sk);\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->hb_timer, jiffies + (HZ/20)))\n\t\t\tsctp_transport_hold(transport);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (transport->dead)\n\t\tgoto out_unlock;\n\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(SCTP_EVENT_TIMEOUT_HEARTBEAT),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   transport, GFP_ATOMIC);\n\n\tif (error)\n\t\tasoc->base.sk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_transport_put(transport);\n}",
        "func": "void sctp_generate_heartbeat_event(unsigned long data)\n{\n\tint error = 0;\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->hb_timer, jiffies + (HZ/20)))\n\t\t\tsctp_transport_hold(transport);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (transport->dead)\n\t\tgoto out_unlock;\n\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(SCTP_EVENT_TIMEOUT_HEARTBEAT),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   transport, GFP_ATOMIC);\n\n\tif (error)\n\t\tsk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_transport_put(transport);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,10 +3,11 @@\n \tint error = 0;\n \tstruct sctp_transport *transport = (struct sctp_transport *) data;\n \tstruct sctp_association *asoc = transport->asoc;\n-\tstruct net *net = sock_net(asoc->base.sk);\n+\tstruct sock *sk = asoc->base.sk;\n+\tstruct net *net = sock_net(sk);\n \n-\tbh_lock_sock(asoc->base.sk);\n-\tif (sock_owned_by_user(asoc->base.sk)) {\n+\tbh_lock_sock(sk);\n+\tif (sock_owned_by_user(sk)) {\n \t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n \n \t\t/* Try again later.  */\n@@ -27,9 +28,9 @@\n \t\t\t   transport, GFP_ATOMIC);\n \n \tif (error)\n-\t\tasoc->base.sk->sk_err = -error;\n+\t\tsk->sk_err = -error;\n \n out_unlock:\n-\tbh_unlock_sock(asoc->base.sk);\n+\tbh_unlock_sock(sk);\n \tsctp_transport_put(transport);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct net *net = sock_net(asoc->base.sk);",
                "\tbh_lock_sock(asoc->base.sk);",
                "\tif (sock_owned_by_user(asoc->base.sk)) {",
                "\t\tasoc->base.sk->sk_err = -error;",
                "\tbh_unlock_sock(asoc->base.sk);"
            ],
            "added_lines": [
                "\tstruct sock *sk = asoc->base.sk;",
                "\tstruct net *net = sock_net(sk);",
                "\tbh_lock_sock(sk);",
                "\tif (sock_owned_by_user(sk)) {",
                "\t\tsk->sk_err = -error;",
                "\tbh_unlock_sock(sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-8767",
        "func_name": "torvalds/linux/sctp_generate_timeout_event",
        "description": "net/sctp/sm_sideeffect.c in the Linux kernel before 4.3 does not properly manage the relationship between a lock and a socket, which allows local users to cause a denial of service (deadlock) via a crafted sctp_accept call.",
        "git_url": "https://github.com/torvalds/linux/commit/635682a14427d241bab7bbdeebb48a7d7b91638e",
        "commit_title": "sctp: Prevent soft lockup when sctp_accept() is called during a timeout event",
        "commit_text": " A case can occur when sctp_accept() is called by the user during a heartbeat timeout event after the 4-way handshake.  Since sctp_assoc_migrate() changes both assoc->base.sk and assoc->ep, the bh_sock_lock in sctp_generate_heartbeat_event() will be taken with the listening socket but released with the new association socket. The result is a deadlock on any future attempts to take the listening socket lock.  Note that this race can occur with other SCTP timeouts that take the bh_lock_sock() in the event sctp_accept() is called.   BUG: soft lockup - CPU#9 stuck for 67s! [swapper:0]  ...  RIP: 0010:[<ffffffff8152d48e>]  [<ffffffff8152d48e>] _spin_lock+0x1e/0x30  RSP: 0018:ffff880028323b20  EFLAGS: 00000206  RAX: 0000000000000002 RBX: ffff880028323b20 RCX: 0000000000000000  RDX: 0000000000000000 RSI: ffff880028323be0 RDI: ffff8804632c4b48  RBP: ffffffff8100bb93 R08: 0000000000000000 R09: 0000000000000000  R10: ffff880610662280 R11: 0000000000000100 R12: ffff880028323aa0  R13: ffff8804383c3880 R14: ffff880028323a90 R15: ffffffff81534225  FS:  0000000000000000(0000) GS:ffff880028320000(0000) knlGS:0000000000000000  CS:  0010 DS: 0018 ES: 0018 CR0: 000000008005003b  CR2: 00000000006df528 CR3: 0000000001a85000 CR4: 00000000000006e0  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000  DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400  Process swapper (pid: 0, threadinfo ffff880616b70000, task ffff880616b6cab0)  Stack:  ffff880028323c40 ffffffffa01c2582 ffff880614cfb020 0000000000000000  <d> 0100000000000000 00000014383a6c44 ffff8804383c3880 ffff880614e93c00  <d> ffff880614e93c00 0000000000000000 ffff8804632c4b00 ffff8804383c38b8  Call Trace:  <IRQ>  [<ffffffffa01c2582>] ? sctp_rcv+0x492/0xa10 [sctp]  [<ffffffff8148c559>] ? nf_iterate+0x69/0xb0  [<ffffffff814974a0>] ? ip_local_deliver_finish+0x0/0x2d0  [<ffffffff8148c716>] ? nf_hook_slow+0x76/0x120  [<ffffffff814974a0>] ? ip_local_deliver_finish+0x0/0x2d0  [<ffffffff8149757d>] ? ip_local_deliver_finish+0xdd/0x2d0  [<ffffffff81497808>] ? ip_local_deliver+0x98/0xa0  [<ffffffff81496ccd>] ? ip_rcv_finish+0x12d/0x440  [<ffffffff81497255>] ? ip_rcv+0x275/0x350  [<ffffffff8145cfeb>] ? __netif_receive_skb+0x4ab/0x750  ...  With lockdep debugging:   =====================================  [ BUG: bad unlock balance detected! ]  -------------------------------------  CslRx/12087 is trying to release lock (slock-AF_INET) at:  [<ffffffffa01bcae0>] sctp_generate_timeout_event+0x40/0xe0 [sctp]  but there are no more locks to release!   other info that might help us debug this:  2 locks held by CslRx/12087:  #0:  (&asoc->timers[i]){+.-...}, at: [<ffffffff8108ce1f>] run_timer_softirq+0x16f/0x3e0  #1:  (slock-AF_INET){+.-...}, at: [<ffffffffa01bcac3>] sctp_generate_timeout_event+0x23/0xe0 [sctp]  Ensure the socket taken is also the same one that is released by saving a copy of the socket before entering the timeout event critical section. ",
        "func_before": "static void sctp_generate_timeout_event(struct sctp_association *asoc,\n\t\t\t\t\tsctp_event_timeout_t timeout_type)\n{\n\tstruct net *net = sock_net(asoc->base.sk);\n\tint error = 0;\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy: timer %d\\n\", __func__,\n\t\t\t timeout_type);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&asoc->timers[timeout_type], jiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this association really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(timeout_type),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   (void *)timeout_type, GFP_ATOMIC);\n\n\tif (error)\n\t\tasoc->base.sk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_association_put(asoc);\n}",
        "func": "static void sctp_generate_timeout_event(struct sctp_association *asoc,\n\t\t\t\t\tsctp_event_timeout_t timeout_type)\n{\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\tint error = 0;\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy: timer %d\\n\", __func__,\n\t\t\t timeout_type);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&asoc->timers[timeout_type], jiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this association really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(timeout_type),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   (void *)timeout_type, GFP_ATOMIC);\n\n\tif (error)\n\t\tsk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_association_put(asoc);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,12 @@\n static void sctp_generate_timeout_event(struct sctp_association *asoc,\n \t\t\t\t\tsctp_event_timeout_t timeout_type)\n {\n-\tstruct net *net = sock_net(asoc->base.sk);\n+\tstruct sock *sk = asoc->base.sk;\n+\tstruct net *net = sock_net(sk);\n \tint error = 0;\n \n-\tbh_lock_sock(asoc->base.sk);\n-\tif (sock_owned_by_user(asoc->base.sk)) {\n+\tbh_lock_sock(sk);\n+\tif (sock_owned_by_user(sk)) {\n \t\tpr_debug(\"%s: sock is busy: timer %d\\n\", __func__,\n \t\t\t timeout_type);\n \n@@ -28,9 +29,9 @@\n \t\t\t   (void *)timeout_type, GFP_ATOMIC);\n \n \tif (error)\n-\t\tasoc->base.sk->sk_err = -error;\n+\t\tsk->sk_err = -error;\n \n out_unlock:\n-\tbh_unlock_sock(asoc->base.sk);\n+\tbh_unlock_sock(sk);\n \tsctp_association_put(asoc);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct net *net = sock_net(asoc->base.sk);",
                "\tbh_lock_sock(asoc->base.sk);",
                "\tif (sock_owned_by_user(asoc->base.sk)) {",
                "\t\tasoc->base.sk->sk_err = -error;",
                "\tbh_unlock_sock(asoc->base.sk);"
            ],
            "added_lines": [
                "\tstruct sock *sk = asoc->base.sk;",
                "\tstruct net *net = sock_net(sk);",
                "\tbh_lock_sock(sk);",
                "\tif (sock_owned_by_user(sk)) {",
                "\t\tsk->sk_err = -error;",
                "\tbh_unlock_sock(sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-8767",
        "func_name": "torvalds/linux/sctp_generate_t3_rtx_event",
        "description": "net/sctp/sm_sideeffect.c in the Linux kernel before 4.3 does not properly manage the relationship between a lock and a socket, which allows local users to cause a denial of service (deadlock) via a crafted sctp_accept call.",
        "git_url": "https://github.com/torvalds/linux/commit/635682a14427d241bab7bbdeebb48a7d7b91638e",
        "commit_title": "sctp: Prevent soft lockup when sctp_accept() is called during a timeout event",
        "commit_text": " A case can occur when sctp_accept() is called by the user during a heartbeat timeout event after the 4-way handshake.  Since sctp_assoc_migrate() changes both assoc->base.sk and assoc->ep, the bh_sock_lock in sctp_generate_heartbeat_event() will be taken with the listening socket but released with the new association socket. The result is a deadlock on any future attempts to take the listening socket lock.  Note that this race can occur with other SCTP timeouts that take the bh_lock_sock() in the event sctp_accept() is called.   BUG: soft lockup - CPU#9 stuck for 67s! [swapper:0]  ...  RIP: 0010:[<ffffffff8152d48e>]  [<ffffffff8152d48e>] _spin_lock+0x1e/0x30  RSP: 0018:ffff880028323b20  EFLAGS: 00000206  RAX: 0000000000000002 RBX: ffff880028323b20 RCX: 0000000000000000  RDX: 0000000000000000 RSI: ffff880028323be0 RDI: ffff8804632c4b48  RBP: ffffffff8100bb93 R08: 0000000000000000 R09: 0000000000000000  R10: ffff880610662280 R11: 0000000000000100 R12: ffff880028323aa0  R13: ffff8804383c3880 R14: ffff880028323a90 R15: ffffffff81534225  FS:  0000000000000000(0000) GS:ffff880028320000(0000) knlGS:0000000000000000  CS:  0010 DS: 0018 ES: 0018 CR0: 000000008005003b  CR2: 00000000006df528 CR3: 0000000001a85000 CR4: 00000000000006e0  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000  DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400  Process swapper (pid: 0, threadinfo ffff880616b70000, task ffff880616b6cab0)  Stack:  ffff880028323c40 ffffffffa01c2582 ffff880614cfb020 0000000000000000  <d> 0100000000000000 00000014383a6c44 ffff8804383c3880 ffff880614e93c00  <d> ffff880614e93c00 0000000000000000 ffff8804632c4b00 ffff8804383c38b8  Call Trace:  <IRQ>  [<ffffffffa01c2582>] ? sctp_rcv+0x492/0xa10 [sctp]  [<ffffffff8148c559>] ? nf_iterate+0x69/0xb0  [<ffffffff814974a0>] ? ip_local_deliver_finish+0x0/0x2d0  [<ffffffff8148c716>] ? nf_hook_slow+0x76/0x120  [<ffffffff814974a0>] ? ip_local_deliver_finish+0x0/0x2d0  [<ffffffff8149757d>] ? ip_local_deliver_finish+0xdd/0x2d0  [<ffffffff81497808>] ? ip_local_deliver+0x98/0xa0  [<ffffffff81496ccd>] ? ip_rcv_finish+0x12d/0x440  [<ffffffff81497255>] ? ip_rcv+0x275/0x350  [<ffffffff8145cfeb>] ? __netif_receive_skb+0x4ab/0x750  ...  With lockdep debugging:   =====================================  [ BUG: bad unlock balance detected! ]  -------------------------------------  CslRx/12087 is trying to release lock (slock-AF_INET) at:  [<ffffffffa01bcae0>] sctp_generate_timeout_event+0x40/0xe0 [sctp]  but there are no more locks to release!   other info that might help us debug this:  2 locks held by CslRx/12087:  #0:  (&asoc->timers[i]){+.-...}, at: [<ffffffff8108ce1f>] run_timer_softirq+0x16f/0x3e0  #1:  (slock-AF_INET){+.-...}, at: [<ffffffffa01bcac3>] sctp_generate_timeout_event+0x23/0xe0 [sctp]  Ensure the socket taken is also the same one that is released by saving a copy of the socket before entering the timeout event critical section. ",
        "func_before": "void sctp_generate_t3_rtx_event(unsigned long peer)\n{\n\tint error;\n\tstruct sctp_transport *transport = (struct sctp_transport *) peer;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct net *net = sock_net(asoc->base.sk);\n\n\t/* Check whether a task is in the sock.  */\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->T3_rtx_timer, jiffies + (HZ/20)))\n\t\t\tsctp_transport_hold(transport);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this transport really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (transport->dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(SCTP_EVENT_TIMEOUT_T3_RTX),\n\t\t\t   asoc->state,\n\t\t\t   asoc->ep, asoc,\n\t\t\t   transport, GFP_ATOMIC);\n\n\tif (error)\n\t\tasoc->base.sk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_transport_put(transport);\n}",
        "func": "void sctp_generate_t3_rtx_event(unsigned long peer)\n{\n\tint error;\n\tstruct sctp_transport *transport = (struct sctp_transport *) peer;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\n\t/* Check whether a task is in the sock.  */\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->T3_rtx_timer, jiffies + (HZ/20)))\n\t\t\tsctp_transport_hold(transport);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this transport really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (transport->dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(SCTP_EVENT_TIMEOUT_T3_RTX),\n\t\t\t   asoc->state,\n\t\t\t   asoc->ep, asoc,\n\t\t\t   transport, GFP_ATOMIC);\n\n\tif (error)\n\t\tsk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_transport_put(transport);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,12 +3,13 @@\n \tint error;\n \tstruct sctp_transport *transport = (struct sctp_transport *) peer;\n \tstruct sctp_association *asoc = transport->asoc;\n-\tstruct net *net = sock_net(asoc->base.sk);\n+\tstruct sock *sk = asoc->base.sk;\n+\tstruct net *net = sock_net(sk);\n \n \t/* Check whether a task is in the sock.  */\n \n-\tbh_lock_sock(asoc->base.sk);\n-\tif (sock_owned_by_user(asoc->base.sk)) {\n+\tbh_lock_sock(sk);\n+\tif (sock_owned_by_user(sk)) {\n \t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n \n \t\t/* Try again later.  */\n@@ -31,9 +32,9 @@\n \t\t\t   transport, GFP_ATOMIC);\n \n \tif (error)\n-\t\tasoc->base.sk->sk_err = -error;\n+\t\tsk->sk_err = -error;\n \n out_unlock:\n-\tbh_unlock_sock(asoc->base.sk);\n+\tbh_unlock_sock(sk);\n \tsctp_transport_put(transport);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct net *net = sock_net(asoc->base.sk);",
                "\tbh_lock_sock(asoc->base.sk);",
                "\tif (sock_owned_by_user(asoc->base.sk)) {",
                "\t\tasoc->base.sk->sk_err = -error;",
                "\tbh_unlock_sock(asoc->base.sk);"
            ],
            "added_lines": [
                "\tstruct sock *sk = asoc->base.sk;",
                "\tstruct net *net = sock_net(sk);",
                "\tbh_lock_sock(sk);",
                "\tif (sock_owned_by_user(sk)) {",
                "\t\tsk->sk_err = -error;",
                "\tbh_unlock_sock(sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-8767",
        "func_name": "torvalds/linux/sctp_generate_proto_unreach_event",
        "description": "net/sctp/sm_sideeffect.c in the Linux kernel before 4.3 does not properly manage the relationship between a lock and a socket, which allows local users to cause a denial of service (deadlock) via a crafted sctp_accept call.",
        "git_url": "https://github.com/torvalds/linux/commit/635682a14427d241bab7bbdeebb48a7d7b91638e",
        "commit_title": "sctp: Prevent soft lockup when sctp_accept() is called during a timeout event",
        "commit_text": " A case can occur when sctp_accept() is called by the user during a heartbeat timeout event after the 4-way handshake.  Since sctp_assoc_migrate() changes both assoc->base.sk and assoc->ep, the bh_sock_lock in sctp_generate_heartbeat_event() will be taken with the listening socket but released with the new association socket. The result is a deadlock on any future attempts to take the listening socket lock.  Note that this race can occur with other SCTP timeouts that take the bh_lock_sock() in the event sctp_accept() is called.   BUG: soft lockup - CPU#9 stuck for 67s! [swapper:0]  ...  RIP: 0010:[<ffffffff8152d48e>]  [<ffffffff8152d48e>] _spin_lock+0x1e/0x30  RSP: 0018:ffff880028323b20  EFLAGS: 00000206  RAX: 0000000000000002 RBX: ffff880028323b20 RCX: 0000000000000000  RDX: 0000000000000000 RSI: ffff880028323be0 RDI: ffff8804632c4b48  RBP: ffffffff8100bb93 R08: 0000000000000000 R09: 0000000000000000  R10: ffff880610662280 R11: 0000000000000100 R12: ffff880028323aa0  R13: ffff8804383c3880 R14: ffff880028323a90 R15: ffffffff81534225  FS:  0000000000000000(0000) GS:ffff880028320000(0000) knlGS:0000000000000000  CS:  0010 DS: 0018 ES: 0018 CR0: 000000008005003b  CR2: 00000000006df528 CR3: 0000000001a85000 CR4: 00000000000006e0  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000  DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400  Process swapper (pid: 0, threadinfo ffff880616b70000, task ffff880616b6cab0)  Stack:  ffff880028323c40 ffffffffa01c2582 ffff880614cfb020 0000000000000000  <d> 0100000000000000 00000014383a6c44 ffff8804383c3880 ffff880614e93c00  <d> ffff880614e93c00 0000000000000000 ffff8804632c4b00 ffff8804383c38b8  Call Trace:  <IRQ>  [<ffffffffa01c2582>] ? sctp_rcv+0x492/0xa10 [sctp]  [<ffffffff8148c559>] ? nf_iterate+0x69/0xb0  [<ffffffff814974a0>] ? ip_local_deliver_finish+0x0/0x2d0  [<ffffffff8148c716>] ? nf_hook_slow+0x76/0x120  [<ffffffff814974a0>] ? ip_local_deliver_finish+0x0/0x2d0  [<ffffffff8149757d>] ? ip_local_deliver_finish+0xdd/0x2d0  [<ffffffff81497808>] ? ip_local_deliver+0x98/0xa0  [<ffffffff81496ccd>] ? ip_rcv_finish+0x12d/0x440  [<ffffffff81497255>] ? ip_rcv+0x275/0x350  [<ffffffff8145cfeb>] ? __netif_receive_skb+0x4ab/0x750  ...  With lockdep debugging:   =====================================  [ BUG: bad unlock balance detected! ]  -------------------------------------  CslRx/12087 is trying to release lock (slock-AF_INET) at:  [<ffffffffa01bcae0>] sctp_generate_timeout_event+0x40/0xe0 [sctp]  but there are no more locks to release!   other info that might help us debug this:  2 locks held by CslRx/12087:  #0:  (&asoc->timers[i]){+.-...}, at: [<ffffffff8108ce1f>] run_timer_softirq+0x16f/0x3e0  #1:  (slock-AF_INET){+.-...}, at: [<ffffffffa01bcac3>] sctp_generate_timeout_event+0x23/0xe0 [sctp]  Ensure the socket taken is also the same one that is released by saving a copy of the socket before entering the timeout event critical section. ",
        "func_before": "void sctp_generate_proto_unreach_event(unsigned long data)\n{\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct net *net = sock_net(asoc->base.sk);\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->proto_unreach_timer,\n\t\t\t\tjiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\tsctp_do_sm(net, SCTP_EVENT_T_OTHER,\n\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n\t\t   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_association_put(asoc);\n}",
        "func": "void sctp_generate_proto_unreach_event(unsigned long data)\n{\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->proto_unreach_timer,\n\t\t\t\tjiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\tsctp_do_sm(net, SCTP_EVENT_T_OTHER,\n\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n\t\t   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_association_put(asoc);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,10 +2,11 @@\n {\n \tstruct sctp_transport *transport = (struct sctp_transport *) data;\n \tstruct sctp_association *asoc = transport->asoc;\n-\tstruct net *net = sock_net(asoc->base.sk);\n+\tstruct sock *sk = asoc->base.sk;\n+\tstruct net *net = sock_net(sk);\n \n-\tbh_lock_sock(asoc->base.sk);\n-\tif (sock_owned_by_user(asoc->base.sk)) {\n+\tbh_lock_sock(sk);\n+\tif (sock_owned_by_user(sk)) {\n \t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n \n \t\t/* Try again later.  */\n@@ -26,6 +27,6 @@\n \t\t   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);\n \n out_unlock:\n-\tbh_unlock_sock(asoc->base.sk);\n+\tbh_unlock_sock(sk);\n \tsctp_association_put(asoc);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct net *net = sock_net(asoc->base.sk);",
                "\tbh_lock_sock(asoc->base.sk);",
                "\tif (sock_owned_by_user(asoc->base.sk)) {",
                "\tbh_unlock_sock(asoc->base.sk);"
            ],
            "added_lines": [
                "\tstruct sock *sk = asoc->base.sk;",
                "\tstruct net *net = sock_net(sk);",
                "\tbh_lock_sock(sk);",
                "\tif (sock_owned_by_user(sk)) {",
                "\tbh_unlock_sock(sk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-0723",
        "func_name": "torvalds/linux/tty_ioctl",
        "description": "Race condition in the tty_ioctl function in drivers/tty/tty_io.c in the Linux kernel through 4.4.1 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free and system crash) by making a TIOCGETD ioctl call during processing of a TIOCSETD ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/5c17c861a357e9458001f021a7afa7aab9937439",
        "commit_title": "tty: Fix unsafe ldisc reference via ioctl(TIOCGETD)",
        "commit_text": " ioctl(TIOCGETD) retrieves the line discipline id directly from the ldisc because the line discipline id (c_line) in termios is untrustworthy; userspace may have set termios via ioctl(TCSETS*) without actually changing the line discipline via ioctl(TIOCSETD).  However, directly accessing the current ldisc via tty->ldisc is unsafe; the ldisc ptr dereferenced may be stale if the line discipline is changing via ioctl(TIOCSETD) or hangup.  Wait for the line discipline reference (just like read() or write()) to retrieve the \"current\" line discipline id.  Cc: <stable@vger.kernel.org>",
        "func_before": "long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct tty_struct *tty = file_tty(file);\n\tstruct tty_struct *real_tty;\n\tvoid __user *p = (void __user *)arg;\n\tint retval;\n\tstruct tty_ldisc *ld;\n\n\tif (tty_paranoia_check(tty, file_inode(file), \"tty_ioctl\"))\n\t\treturn -EINVAL;\n\n\treal_tty = tty_pair_get_tty(tty);\n\n\t/*\n\t * Factor out some common prep work\n\t */\n\tswitch (cmd) {\n\tcase TIOCSETD:\n\tcase TIOCSBRK:\n\tcase TIOCCBRK:\n\tcase TCSBRK:\n\tcase TCSBRKP:\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t\tif (cmd != TIOCCBRK) {\n\t\t\ttty_wait_until_sent(tty, 0);\n\t\t\tif (signal_pending(current))\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t *\tNow do the stuff.\n\t */\n\tswitch (cmd) {\n\tcase TIOCSTI:\n\t\treturn tiocsti(tty, p);\n\tcase TIOCGWINSZ:\n\t\treturn tiocgwinsz(real_tty, p);\n\tcase TIOCSWINSZ:\n\t\treturn tiocswinsz(real_tty, p);\n\tcase TIOCCONS:\n\t\treturn real_tty != tty ? -EINVAL : tioccons(file);\n\tcase FIONBIO:\n\t\treturn fionbio(file, p);\n\tcase TIOCEXCL:\n\t\tset_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCNXCL:\n\t\tclear_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCGEXCL:\n\t{\n\t\tint excl = test_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn put_user(excl, (int __user *)p);\n\t}\n\tcase TIOCNOTTY:\n\t\tif (current->signal->tty != tty)\n\t\t\treturn -ENOTTY;\n\t\tno_tty();\n\t\treturn 0;\n\tcase TIOCSCTTY:\n\t\treturn tiocsctty(real_tty, file, arg);\n\tcase TIOCGPGRP:\n\t\treturn tiocgpgrp(tty, real_tty, p);\n\tcase TIOCSPGRP:\n\t\treturn tiocspgrp(tty, real_tty, p);\n\tcase TIOCGSID:\n\t\treturn tiocgsid(tty, real_tty, p);\n\tcase TIOCGETD:\n\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);\n\tcase TIOCSETD:\n\t\treturn tiocsetd(tty, p);\n\tcase TIOCVHANGUP:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\ttty_vhangup(tty);\n\t\treturn 0;\n\tcase TIOCGDEV:\n\t{\n\t\tunsigned int ret = new_encode_dev(tty_devnum(real_tty));\n\t\treturn put_user(ret, (unsigned int __user *)p);\n\t}\n\t/*\n\t * Break handling\n\t */\n\tcase TIOCSBRK:\t/* Turn break on, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, -1);\n\t\treturn 0;\n\tcase TIOCCBRK:\t/* Turn break off, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, 0);\n\t\treturn 0;\n\tcase TCSBRK:   /* SVID version: non-zero arg --> no break */\n\t\t/* non-zero arg means wait for all output data\n\t\t * to be sent (performed above) but don't send break.\n\t\t * This is used by the tcdrain() termios function.\n\t\t */\n\t\tif (!arg)\n\t\t\treturn send_break(tty, 250);\n\t\treturn 0;\n\tcase TCSBRKP:\t/* support for POSIX tcsendbreak() */\n\t\treturn send_break(tty, arg ? arg*100 : 250);\n\n\tcase TIOCMGET:\n\t\treturn tty_tiocmget(tty, p);\n\tcase TIOCMSET:\n\tcase TIOCMBIC:\n\tcase TIOCMBIS:\n\t\treturn tty_tiocmset(tty, cmd, p);\n\tcase TIOCGICOUNT:\n\t\tretval = tty_tiocgicount(tty, p);\n\t\t/* For the moment allow fall through to the old method */\n        \tif (retval != -EINVAL)\n\t\t\treturn retval;\n\t\tbreak;\n\tcase TCFLSH:\n\t\tswitch (arg) {\n\t\tcase TCIFLUSH:\n\t\tcase TCIOFLUSH:\n\t\t/* flush tty buffer and allow ldisc to process ioctl */\n\t\t\ttty_buffer_flush(tty, NULL);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase TIOCSSERIAL:\n\t\ttty_warn_deprecated_flags(p);\n\t\tbreak;\n\t}\n\tif (tty->ops->ioctl) {\n\t\tretval = tty->ops->ioctl(tty, cmd, arg);\n\t\tif (retval != -ENOIOCTLCMD)\n\t\t\treturn retval;\n\t}\n\tld = tty_ldisc_ref_wait(tty);\n\tretval = -EINVAL;\n\tif (ld->ops->ioctl) {\n\t\tretval = ld->ops->ioctl(tty, file, cmd, arg);\n\t\tif (retval == -ENOIOCTLCMD)\n\t\t\tretval = -ENOTTY;\n\t}\n\ttty_ldisc_deref(ld);\n\treturn retval;\n}",
        "func": "long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct tty_struct *tty = file_tty(file);\n\tstruct tty_struct *real_tty;\n\tvoid __user *p = (void __user *)arg;\n\tint retval;\n\tstruct tty_ldisc *ld;\n\n\tif (tty_paranoia_check(tty, file_inode(file), \"tty_ioctl\"))\n\t\treturn -EINVAL;\n\n\treal_tty = tty_pair_get_tty(tty);\n\n\t/*\n\t * Factor out some common prep work\n\t */\n\tswitch (cmd) {\n\tcase TIOCSETD:\n\tcase TIOCSBRK:\n\tcase TIOCCBRK:\n\tcase TCSBRK:\n\tcase TCSBRKP:\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t\tif (cmd != TIOCCBRK) {\n\t\t\ttty_wait_until_sent(tty, 0);\n\t\t\tif (signal_pending(current))\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t *\tNow do the stuff.\n\t */\n\tswitch (cmd) {\n\tcase TIOCSTI:\n\t\treturn tiocsti(tty, p);\n\tcase TIOCGWINSZ:\n\t\treturn tiocgwinsz(real_tty, p);\n\tcase TIOCSWINSZ:\n\t\treturn tiocswinsz(real_tty, p);\n\tcase TIOCCONS:\n\t\treturn real_tty != tty ? -EINVAL : tioccons(file);\n\tcase FIONBIO:\n\t\treturn fionbio(file, p);\n\tcase TIOCEXCL:\n\t\tset_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCNXCL:\n\t\tclear_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCGEXCL:\n\t{\n\t\tint excl = test_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn put_user(excl, (int __user *)p);\n\t}\n\tcase TIOCNOTTY:\n\t\tif (current->signal->tty != tty)\n\t\t\treturn -ENOTTY;\n\t\tno_tty();\n\t\treturn 0;\n\tcase TIOCSCTTY:\n\t\treturn tiocsctty(real_tty, file, arg);\n\tcase TIOCGPGRP:\n\t\treturn tiocgpgrp(tty, real_tty, p);\n\tcase TIOCSPGRP:\n\t\treturn tiocspgrp(tty, real_tty, p);\n\tcase TIOCGSID:\n\t\treturn tiocgsid(tty, real_tty, p);\n\tcase TIOCGETD:\n\t\treturn tiocgetd(tty, p);\n\tcase TIOCSETD:\n\t\treturn tiocsetd(tty, p);\n\tcase TIOCVHANGUP:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\ttty_vhangup(tty);\n\t\treturn 0;\n\tcase TIOCGDEV:\n\t{\n\t\tunsigned int ret = new_encode_dev(tty_devnum(real_tty));\n\t\treturn put_user(ret, (unsigned int __user *)p);\n\t}\n\t/*\n\t * Break handling\n\t */\n\tcase TIOCSBRK:\t/* Turn break on, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, -1);\n\t\treturn 0;\n\tcase TIOCCBRK:\t/* Turn break off, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, 0);\n\t\treturn 0;\n\tcase TCSBRK:   /* SVID version: non-zero arg --> no break */\n\t\t/* non-zero arg means wait for all output data\n\t\t * to be sent (performed above) but don't send break.\n\t\t * This is used by the tcdrain() termios function.\n\t\t */\n\t\tif (!arg)\n\t\t\treturn send_break(tty, 250);\n\t\treturn 0;\n\tcase TCSBRKP:\t/* support for POSIX tcsendbreak() */\n\t\treturn send_break(tty, arg ? arg*100 : 250);\n\n\tcase TIOCMGET:\n\t\treturn tty_tiocmget(tty, p);\n\tcase TIOCMSET:\n\tcase TIOCMBIC:\n\tcase TIOCMBIS:\n\t\treturn tty_tiocmset(tty, cmd, p);\n\tcase TIOCGICOUNT:\n\t\tretval = tty_tiocgicount(tty, p);\n\t\t/* For the moment allow fall through to the old method */\n        \tif (retval != -EINVAL)\n\t\t\treturn retval;\n\t\tbreak;\n\tcase TCFLSH:\n\t\tswitch (arg) {\n\t\tcase TCIFLUSH:\n\t\tcase TCIOFLUSH:\n\t\t/* flush tty buffer and allow ldisc to process ioctl */\n\t\t\ttty_buffer_flush(tty, NULL);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase TIOCSSERIAL:\n\t\ttty_warn_deprecated_flags(p);\n\t\tbreak;\n\t}\n\tif (tty->ops->ioctl) {\n\t\tretval = tty->ops->ioctl(tty, cmd, arg);\n\t\tif (retval != -ENOIOCTLCMD)\n\t\t\treturn retval;\n\t}\n\tld = tty_ldisc_ref_wait(tty);\n\tretval = -EINVAL;\n\tif (ld->ops->ioctl) {\n\t\tretval = ld->ops->ioctl(tty, file, cmd, arg);\n\t\tif (retval == -ENOIOCTLCMD)\n\t\t\tretval = -ENOTTY;\n\t}\n\ttty_ldisc_deref(ld);\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -70,7 +70,7 @@\n \tcase TIOCGSID:\n \t\treturn tiocgsid(tty, real_tty, p);\n \tcase TIOCGETD:\n-\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);\n+\t\treturn tiocgetd(tty, p);\n \tcase TIOCSETD:\n \t\treturn tiocsetd(tty, p);\n \tcase TIOCVHANGUP:",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);"
            ],
            "added_lines": [
                "\t\treturn tiocgetd(tty, p);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-3611",
        "func_name": "torvalds/linux/__kvm_migrate_pit_timer",
        "description": "Race condition in the __kvm_migrate_pit_timer function in arch/x86/kvm/i8254.c in the KVM subsystem in the Linux kernel through 3.17.2 allows guest OS users to cause a denial of service (host OS crash) by leveraging incorrect PIT emulation.",
        "git_url": "https://github.com/torvalds/linux/commit/2febc839133280d5a5e8e1179c94ea674489dae2",
        "commit_title": "KVM: x86: Improve thread safety in pit",
        "commit_text": " There's a race condition in the PIT emulation code in KVM.  In __kvm_migrate_pit_timer the pit_timer object is accessed without synchronization.  If the race condition occurs at the wrong time this can crash the host kernel.  This fixes CVE-2014-3611.  Cc: stable@vger.kernel.org",
        "func_before": "void __kvm_migrate_pit_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_pit *pit = vcpu->kvm->arch.vpit;\n\tstruct hrtimer *timer;\n\n\tif (!kvm_vcpu_is_bsp(vcpu) || !pit)\n\t\treturn;\n\n\ttimer = &pit->pit_state.timer;\n\tif (hrtimer_cancel(timer))\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n}",
        "func": "void __kvm_migrate_pit_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_pit *pit = vcpu->kvm->arch.vpit;\n\tstruct hrtimer *timer;\n\n\tif (!kvm_vcpu_is_bsp(vcpu) || !pit)\n\t\treturn;\n\n\ttimer = &pit->pit_state.timer;\n\tmutex_lock(&pit->pit_state.lock);\n\tif (hrtimer_cancel(timer))\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n\tmutex_unlock(&pit->pit_state.lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,8 @@\n \t\treturn;\n \n \ttimer = &pit->pit_state.timer;\n+\tmutex_lock(&pit->pit_state.lock);\n \tif (hrtimer_cancel(timer))\n \t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n+\tmutex_unlock(&pit->pit_state.lock);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_lock(&pit->pit_state.lock);",
                "\tmutex_unlock(&pit->pit_state.lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-5313",
        "func_name": "torvalds/linux/handle_emulation_failure",
        "description": "Race condition in arch/x86/kvm/x86.c in the Linux kernel before 2.6.38 allows L2 guest OS users to cause a denial of service (L1 guest OS crash) via a crafted instruction that triggers an L2 emulation failure report, a similar issue to CVE-2014-7842.",
        "git_url": "https://github.com/torvalds/linux/commit/fc3a9157d3148ab91039c75423da8ef97be3e105",
        "commit_title": "KVM: X86: Don't report L2 emulation failures to user-space",
        "commit_text": " This patch prevents that emulation failures which result from emulating an instruction for an L2-Guest results in being reported to userspace. Without this patch a malicious L2-Guest would be able to kill the L1 by triggering a race-condition between an vmexit and the instruction emulator. With this patch the L2 will most likely only kill itself in this situation. ",
        "func_before": "static int handle_emulation_failure(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.insn_emulation_fail;\n\ttrace_kvm_emulate_insn_failed(vcpu);\n\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\tvcpu->run->internal.ndata = 0;\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn EMULATE_FAIL;\n}",
        "func": "static int handle_emulation_failure(struct kvm_vcpu *vcpu)\n{\n\tint r = EMULATE_DONE;\n\n\t++vcpu->stat.insn_emulation_fail;\n\ttrace_kvm_emulate_insn_failed(vcpu);\n\tif (!is_guest_mode(vcpu)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\tr = EMULATE_FAIL;\n\t}\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\n\treturn r;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,16 @@\n static int handle_emulation_failure(struct kvm_vcpu *vcpu)\n {\n+\tint r = EMULATE_DONE;\n+\n \t++vcpu->stat.insn_emulation_fail;\n \ttrace_kvm_emulate_insn_failed(vcpu);\n-\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n-\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n-\tvcpu->run->internal.ndata = 0;\n+\tif (!is_guest_mode(vcpu)) {\n+\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n+\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n+\t\tvcpu->run->internal.ndata = 0;\n+\t\tr = EMULATE_FAIL;\n+\t}\n \tkvm_queue_exception(vcpu, UD_VECTOR);\n-\treturn EMULATE_FAIL;\n+\n+\treturn r;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;",
                "\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;",
                "\tvcpu->run->internal.ndata = 0;",
                "\treturn EMULATE_FAIL;"
            ],
            "added_lines": [
                "\tint r = EMULATE_DONE;",
                "",
                "\tif (!is_guest_mode(vcpu)) {",
                "\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;",
                "\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;",
                "\t\tvcpu->run->internal.ndata = 0;",
                "\t\tr = EMULATE_FAIL;",
                "\t}",
                "",
                "\treturn r;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-7842",
        "func_name": "torvalds/linux/handle_emulation_failure",
        "description": "Race condition in arch/x86/kvm/x86.c in the Linux kernel before 3.17.4 allows guest OS users to cause a denial of service (guest OS crash) via a crafted application that performs an MMIO transaction or a PIO transaction to trigger a guest userspace emulation error report, a similar issue to CVE-2010-5313.",
        "git_url": "https://github.com/torvalds/linux/commit/a2b9e6c1a35afcc0973acb72e591c714e78885ff",
        "commit_title": "KVM: x86: Don't report guest userspace emulation error to userspace",
        "commit_text": " Commit fc3a9157d314 (\"KVM: X86: Don't report L2 emulation failures to user-space\") disabled the reporting of L2 (nested guest) emulation failures to userspace due to race-condition between a vmexit and the instruction emulator. The same rational applies also to userspace applications that are permitted by the guest OS to access MMIO area or perform PIO.  This patch extends the current behavior - of injecting a #UD instead of reporting it to userspace - also for guest userspace code. ",
        "func_before": "static int handle_emulation_failure(struct kvm_vcpu *vcpu)\n{\n\tint r = EMULATE_DONE;\n\n\t++vcpu->stat.insn_emulation_fail;\n\ttrace_kvm_emulate_insn_failed(vcpu);\n\tif (!is_guest_mode(vcpu)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\tr = EMULATE_FAIL;\n\t}\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\n\treturn r;\n}",
        "func": "static int handle_emulation_failure(struct kvm_vcpu *vcpu)\n{\n\tint r = EMULATE_DONE;\n\n\t++vcpu->stat.insn_emulation_fail;\n\ttrace_kvm_emulate_insn_failed(vcpu);\n\tif (!is_guest_mode(vcpu) && kvm_x86_ops->get_cpl(vcpu) == 0) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\tr = EMULATE_FAIL;\n\t}\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\n\treturn r;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n \n \t++vcpu->stat.insn_emulation_fail;\n \ttrace_kvm_emulate_insn_failed(vcpu);\n-\tif (!is_guest_mode(vcpu)) {\n+\tif (!is_guest_mode(vcpu) && kvm_x86_ops->get_cpl(vcpu) == 0) {\n \t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n \t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n \t\tvcpu->run->internal.ndata = 0;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!is_guest_mode(vcpu)) {"
            ],
            "added_lines": [
                "\tif (!is_guest_mode(vcpu) && kvm_x86_ops->get_cpl(vcpu) == 0) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9529",
        "func_name": "torvalds/linux/key_gc_unused_keys",
        "description": "Race condition in the key_gc_unused_keys function in security/keys/gc.c in the Linux kernel through 3.18.2 allows local users to cause a denial of service (memory corruption or panic) or possibly have unspecified other impact via keyctl commands that trigger access to a key structure member during garbage collection of a key.",
        "git_url": "https://github.com/torvalds/linux/commit/a3a8784454692dd72e5d5d34dcdab17b4420e74c",
        "commit_title": "KEYS: close race between key lookup and freeing",
        "commit_text": " When a key is being garbage collected, it's key->user would get put before the ->destroy() callback is called, where the key is removed from it's respective tracking structures.  This leaves a key hanging in a semi-invalid state which leaves a window open for a different task to try an access key->user. An example is find_keyring_by_name() which would dereference key->user for a key that is in the process of being garbage collected (where key->user was freed but ->destroy() wasn't called yet - so it's still present in the linked list).  This would cause either a panic, or corrupt memory.  Fixes CVE-2014-9529. ",
        "func_before": "static noinline void key_gc_unused_keys(struct list_head *keys)\n{\n\twhile (!list_empty(keys)) {\n\t\tstruct key *key =\n\t\t\tlist_entry(keys->next, struct key, graveyard_link);\n\t\tlist_del(&key->graveyard_link);\n\n\t\tkdebug(\"- %u\", key->serial);\n\t\tkey_check(key);\n\n\t\tsecurity_key_free(key);\n\n\t\t/* deal with the user's key tracking and quota */\n\t\tif (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {\n\t\t\tspin_lock(&key->user->lock);\n\t\t\tkey->user->qnkeys--;\n\t\t\tkey->user->qnbytes -= key->quotalen;\n\t\t\tspin_unlock(&key->user->lock);\n\t\t}\n\n\t\tatomic_dec(&key->user->nkeys);\n\t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n\t\t\tatomic_dec(&key->user->nikeys);\n\n\t\tkey_user_put(key->user);\n\n\t\t/* now throw away the key memory */\n\t\tif (key->type->destroy)\n\t\t\tkey->type->destroy(key);\n\n\t\tkfree(key->description);\n\n#ifdef KEY_DEBUGGING\n\t\tkey->magic = KEY_DEBUG_MAGIC_X;\n#endif\n\t\tkmem_cache_free(key_jar, key);\n\t}\n}",
        "func": "static noinline void key_gc_unused_keys(struct list_head *keys)\n{\n\twhile (!list_empty(keys)) {\n\t\tstruct key *key =\n\t\t\tlist_entry(keys->next, struct key, graveyard_link);\n\t\tlist_del(&key->graveyard_link);\n\n\t\tkdebug(\"- %u\", key->serial);\n\t\tkey_check(key);\n\n\t\tsecurity_key_free(key);\n\n\t\t/* deal with the user's key tracking and quota */\n\t\tif (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {\n\t\t\tspin_lock(&key->user->lock);\n\t\t\tkey->user->qnkeys--;\n\t\t\tkey->user->qnbytes -= key->quotalen;\n\t\t\tspin_unlock(&key->user->lock);\n\t\t}\n\n\t\tatomic_dec(&key->user->nkeys);\n\t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n\t\t\tatomic_dec(&key->user->nikeys);\n\n\t\t/* now throw away the key memory */\n\t\tif (key->type->destroy)\n\t\t\tkey->type->destroy(key);\n\n\t\tkey_user_put(key->user);\n\n\t\tkfree(key->description);\n\n#ifdef KEY_DEBUGGING\n\t\tkey->magic = KEY_DEBUG_MAGIC_X;\n#endif\n\t\tkmem_cache_free(key_jar, key);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,11 +22,11 @@\n \t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n \t\t\tatomic_dec(&key->user->nikeys);\n \n-\t\tkey_user_put(key->user);\n-\n \t\t/* now throw away the key memory */\n \t\tif (key->type->destroy)\n \t\t\tkey->type->destroy(key);\n+\n+\t\tkey_user_put(key->user);\n \n \t\tkfree(key->description);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tkey_user_put(key->user);",
                ""
            ],
            "added_lines": [
                "",
                "\t\tkey_user_put(key->user);"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-4895",
        "func_name": "torvalds/linux/tty_fasync",
        "description": "Race condition in the tty_fasync function in drivers/char/tty_io.c in the Linux kernel before 2.6.32.6 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact via unknown vectors, related to the put_tty_queue and __f_setown functions.  NOTE: the vulnerability was addressed in a different way in 2.6.32.9.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=703625118069f9f8960d356676662d3db5a9d116",
        "commit_title": "We need to keep the lock held over the call to __f_setown() to",
        "commit_text": "prevent a PID race.  Thanks to Al Viro for pointing out the problem, and to Travis for making us look here in the first place.  Cc: Eric W. Biederman <ebiederm@xmission.com> Cc: Al Viro <viro@ZenIV.linux.org.uk> Cc: Alan Cox <alan@lxorguk.ukuu.org.uk> Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Tavis Ormandy <taviso@google.com> Cc: Jeff Dike <jdike@addtoit.com> Cc: Julien Tinnes <jln@google.com> Cc: Matt Mackall <mpm@selenic.com> Cc: stable <stable@kernel.org> ",
        "func_before": "static int tty_fasync(int fd, struct file *filp, int on)\n{\n\tstruct tty_struct *tty;\n\tunsigned long flags;\n\tint retval = 0;\n\n\tlock_kernel();\n\ttty = (struct tty_struct *)filp->private_data;\n\tif (tty_paranoia_check(tty, filp->f_path.dentry->d_inode, \"tty_fasync\"))\n\t\tgoto out;\n\n\tretval = fasync_helper(fd, filp, on, &tty->fasync);\n\tif (retval <= 0)\n\t\tgoto out;\n\n\tif (on) {\n\t\tenum pid_type type;\n\t\tstruct pid *pid;\n\t\tif (!waitqueue_active(&tty->read_wait))\n\t\t\ttty->minimum_to_wake = 1;\n\t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t\tif (tty->pgrp) {\n\t\t\tpid = tty->pgrp;\n\t\t\ttype = PIDTYPE_PGID;\n\t\t} else {\n\t\t\tpid = task_pid(current);\n\t\t\ttype = PIDTYPE_PID;\n\t\t}\n\t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\t\tretval = __f_setown(filp, pid, type, 0);\n\t\tif (retval)\n\t\t\tgoto out;\n\t} else {\n\t\tif (!tty->fasync && !waitqueue_active(&tty->read_wait))\n\t\t\ttty->minimum_to_wake = N_TTY_BUF_SIZE;\n\t}\n\tretval = 0;\nout:\n\tunlock_kernel();\n\treturn retval;\n}",
        "func": "static int tty_fasync(int fd, struct file *filp, int on)\n{\n\tstruct tty_struct *tty;\n\tunsigned long flags;\n\tint retval = 0;\n\n\tlock_kernel();\n\ttty = (struct tty_struct *)filp->private_data;\n\tif (tty_paranoia_check(tty, filp->f_path.dentry->d_inode, \"tty_fasync\"))\n\t\tgoto out;\n\n\tretval = fasync_helper(fd, filp, on, &tty->fasync);\n\tif (retval <= 0)\n\t\tgoto out;\n\n\tif (on) {\n\t\tenum pid_type type;\n\t\tstruct pid *pid;\n\t\tif (!waitqueue_active(&tty->read_wait))\n\t\t\ttty->minimum_to_wake = 1;\n\t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t\tif (tty->pgrp) {\n\t\t\tpid = tty->pgrp;\n\t\t\ttype = PIDTYPE_PGID;\n\t\t} else {\n\t\t\tpid = task_pid(current);\n\t\t\ttype = PIDTYPE_PID;\n\t\t}\n\t\tretval = __f_setown(filp, pid, type, 0);\n\t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\t\tif (retval)\n\t\t\tgoto out;\n\t} else {\n\t\tif (!tty->fasync && !waitqueue_active(&tty->read_wait))\n\t\t\ttty->minimum_to_wake = N_TTY_BUF_SIZE;\n\t}\n\tretval = 0;\nout:\n\tunlock_kernel();\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,8 +26,8 @@\n \t\t\tpid = task_pid(current);\n \t\t\ttype = PIDTYPE_PID;\n \t\t}\n+\t\tretval = __f_setown(filp, pid, type, 0);\n \t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n-\t\tretval = __f_setown(filp, pid, type, 0);\n \t\tif (retval)\n \t\t\tgoto out;\n \t} else {",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tretval = __f_setown(filp, pid, type, 0);"
            ],
            "added_lines": [
                "\t\tretval = __f_setown(filp, pid, type, 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2653",
        "func_name": "torvalds/linux/hvc_close",
        "description": "Race condition in the hvc_close function in drivers/char/hvc_console.c in the Linux kernel before 2.6.34 allows local users to cause a denial of service or possibly have unspecified other impact by closing a Hypervisor Virtual Console device, related to the hvc_open and hvc_remove functions.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=320718ee074acce5ffced6506cb51af1388942aa",
        "commit_title": "I don't claim to understand the tty layer, but it seems like hvc_open and",
        "commit_text": "hvc_close should be balanced in their kref reference counting.  Right now we get a kref every call to hvc_open:          if (hp->count++ > 0) {                 tty_kref_get(tty); <----- here                 spin_unlock_irqrestore(&hp->lock, flags);                 hvc_kick();                 return 0;         } /* else count == 0 */          tty->driver_data = hp;          hp->tty = tty_kref_get(tty); <------ or here if hp->count was 0  But hvc_close has:          tty_kref_get(tty);          if (--hp->count == 0) { ...                 /* Put the ref obtained in hvc_open() */                 tty_kref_put(tty); ...         }          tty_kref_put(tty);  Since the outside kref get/put balance we only do a single kref_put when count reaches 0.  The patch below changes things to call tty_kref_put once for every hvc_close call, and with that my machine boots fine.  ",
        "func_before": "static void hvc_close(struct tty_struct *tty, struct file * filp)\n{\n\tstruct hvc_struct *hp;\n\tunsigned long flags;\n\n\tif (tty_hung_up_p(filp))\n\t\treturn;\n\n\t/*\n\t * No driver_data means that this close was issued after a failed\n\t * hvc_open by the tty layer's release_dev() function and we can just\n\t * exit cleanly because the kref reference wasn't made.\n\t */\n\tif (!tty->driver_data)\n\t\treturn;\n\n\thp = tty->driver_data;\n\n\tspin_lock_irqsave(&hp->lock, flags);\n\ttty_kref_get(tty);\n\n\tif (--hp->count == 0) {\n\t\t/* We are done with the tty pointer now. */\n\t\thp->tty = NULL;\n\t\tspin_unlock_irqrestore(&hp->lock, flags);\n\n\t\t/* Put the ref obtained in hvc_open() */\n\t\ttty_kref_put(tty);\n\n\t\tif (hp->ops->notifier_del)\n\t\t\thp->ops->notifier_del(hp, hp->data);\n\n\t\t/* cancel pending tty resize work */\n\t\tcancel_work_sync(&hp->tty_resize);\n\n\t\t/*\n\t\t * Chain calls chars_in_buffer() and returns immediately if\n\t\t * there is no buffered data otherwise sleeps on a wait queue\n\t\t * waking periodically to check chars_in_buffer().\n\t\t */\n\t\ttty_wait_until_sent(tty, HVC_CLOSE_WAIT);\n\t} else {\n\t\tif (hp->count < 0)\n\t\t\tprintk(KERN_ERR \"hvc_close %X: oops, count is %d\\n\",\n\t\t\t\thp->vtermno, hp->count);\n\t\tspin_unlock_irqrestore(&hp->lock, flags);\n\t}\n\n\ttty_kref_put(tty);\n\tkref_put(&hp->kref, destroy_hvc_struct);\n}",
        "func": "static void hvc_close(struct tty_struct *tty, struct file * filp)\n{\n\tstruct hvc_struct *hp;\n\tunsigned long flags;\n\n\tif (tty_hung_up_p(filp))\n\t\treturn;\n\n\t/*\n\t * No driver_data means that this close was issued after a failed\n\t * hvc_open by the tty layer's release_dev() function and we can just\n\t * exit cleanly because the kref reference wasn't made.\n\t */\n\tif (!tty->driver_data)\n\t\treturn;\n\n\thp = tty->driver_data;\n\n\tspin_lock_irqsave(&hp->lock, flags);\n\n\tif (--hp->count == 0) {\n\t\t/* We are done with the tty pointer now. */\n\t\thp->tty = NULL;\n\t\tspin_unlock_irqrestore(&hp->lock, flags);\n\n\t\tif (hp->ops->notifier_del)\n\t\t\thp->ops->notifier_del(hp, hp->data);\n\n\t\t/* cancel pending tty resize work */\n\t\tcancel_work_sync(&hp->tty_resize);\n\n\t\t/*\n\t\t * Chain calls chars_in_buffer() and returns immediately if\n\t\t * there is no buffered data otherwise sleeps on a wait queue\n\t\t * waking periodically to check chars_in_buffer().\n\t\t */\n\t\ttty_wait_until_sent(tty, HVC_CLOSE_WAIT);\n\t} else {\n\t\tif (hp->count < 0)\n\t\t\tprintk(KERN_ERR \"hvc_close %X: oops, count is %d\\n\",\n\t\t\t\thp->vtermno, hp->count);\n\t\tspin_unlock_irqrestore(&hp->lock, flags);\n\t}\n\n\ttty_kref_put(tty);\n\tkref_put(&hp->kref, destroy_hvc_struct);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,15 +17,11 @@\n \thp = tty->driver_data;\n \n \tspin_lock_irqsave(&hp->lock, flags);\n-\ttty_kref_get(tty);\n \n \tif (--hp->count == 0) {\n \t\t/* We are done with the tty pointer now. */\n \t\thp->tty = NULL;\n \t\tspin_unlock_irqrestore(&hp->lock, flags);\n-\n-\t\t/* Put the ref obtained in hvc_open() */\n-\t\ttty_kref_put(tty);\n \n \t\tif (hp->ops->notifier_del)\n \t\t\thp->ops->notifier_del(hp, hp->data);",
        "diff_line_info": {
            "deleted_lines": [
                "\ttty_kref_get(tty);",
                "",
                "\t\t/* Put the ref obtained in hvc_open() */",
                "\t\ttty_kref_put(tty);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2010-4248",
        "func_name": "torvalds/linux/__exit_signal",
        "description": "Race condition in the __exit_signal function in kernel/exit.c in the Linux kernel before 2.6.37-rc2 allows local users to cause a denial of service via vectors related to multithreaded exec, the use of a thread group leader in kernel/posix-cpu-timers.c, and the selection of a new thread group leader in the de_thread function in fs/exec.c.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=e0a70217107e6f9844628120412cb27bb4cea194",
        "commit_title": "posix-cpu-timers.c correctly assumes that the dying process does",
        "commit_text": "posix_cpu_timers_exit_group() and removes all !CPUCLOCK_PERTHREAD timers from signal->cpu_timers list.  But, it also assumes that timer->it.cpu.task is always the group leader, and thus the dead ->task means the dead thread group.  This is obviously not true after de_thread() changes the leader. After that almost every posix_cpu_timer_ method has problems.  It is not simple to fix this bug correctly. First of all, I think that timer->it.cpu should use struct pid instead of task_struct. Also, the locking should be reworked completely. In particular, tasklist_lock should not be used at all. This all needs a lot of nontrivial and hard-to-test changes.  Change __exit_signal() to do posix_cpu_timers_exit_group() when the old leader dies during exec. This is not the fix, just the temporary hack to hide the problem for 2.6.37 and stable. IOW, this is obviously wrong but this is what we currently have anyway: cpu timers do not work after mt exec.  In theory this change adds another race. The exiting leader can detach the timers which were attached to the new leader. However, the window between de_thread() and release_task() is small, we can pretend that sys_timer_create() was called before de_thread().  ",
        "func_before": "static void __exit_signal(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tbool group_dead = thread_group_leader(tsk);\n\tstruct sighand_struct *sighand;\n\tstruct tty_struct *uninitialized_var(tty);\n\n\tsighand = rcu_dereference_check(tsk->sighand,\n\t\t\t\t\trcu_read_lock_held() ||\n\t\t\t\t\tlockdep_tasklist_lock_is_held());\n\tspin_lock(&sighand->siglock);\n\n\tposix_cpu_timers_exit(tsk);\n\tif (group_dead) {\n\t\tposix_cpu_timers_exit_group(tsk);\n\t\ttty = sig->tty;\n\t\tsig->tty = NULL;\n\t} else {\n\t\t/*\n\t\t * If there is any task waiting for the group exit\n\t\t * then notify it:\n\t\t */\n\t\tif (sig->notify_count > 0 && !--sig->notify_count)\n\t\t\twake_up_process(sig->group_exit_task);\n\n\t\tif (tsk == sig->curr_target)\n\t\t\tsig->curr_target = next_thread(tsk);\n\t\t/*\n\t\t * Accumulate here the counters for all threads but the\n\t\t * group leader as they die, so they can be added into\n\t\t * the process-wide totals when those are taken.\n\t\t * The group leader stays around as a zombie as long\n\t\t * as there are other threads.  When it gets reaped,\n\t\t * the exit.c code will add its counts into these totals.\n\t\t * We won't ever get here for the group leader, since it\n\t\t * will have been the last reference on the signal_struct.\n\t\t */\n\t\tsig->utime = cputime_add(sig->utime, tsk->utime);\n\t\tsig->stime = cputime_add(sig->stime, tsk->stime);\n\t\tsig->gtime = cputime_add(sig->gtime, tsk->gtime);\n\t\tsig->min_flt += tsk->min_flt;\n\t\tsig->maj_flt += tsk->maj_flt;\n\t\tsig->nvcsw += tsk->nvcsw;\n\t\tsig->nivcsw += tsk->nivcsw;\n\t\tsig->inblock += task_io_get_inblock(tsk);\n\t\tsig->oublock += task_io_get_oublock(tsk);\n\t\ttask_io_accounting_add(&sig->ioac, &tsk->ioac);\n\t\tsig->sum_sched_runtime += tsk->se.sum_exec_runtime;\n\t}\n\n\tsig->nr_threads--;\n\t__unhash_process(tsk, group_dead);\n\n\t/*\n\t * Do this under ->siglock, we can race with another thread\n\t * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.\n\t */\n\tflush_sigqueue(&tsk->pending);\n\ttsk->sighand = NULL;\n\tspin_unlock(&sighand->siglock);\n\n\t__cleanup_sighand(sighand);\n\tclear_tsk_thread_flag(tsk,TIF_SIGPENDING);\n\tif (group_dead) {\n\t\tflush_sigqueue(&sig->shared_pending);\n\t\ttty_kref_put(tty);\n\t}\n}",
        "func": "static void __exit_signal(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tbool group_dead = thread_group_leader(tsk);\n\tstruct sighand_struct *sighand;\n\tstruct tty_struct *uninitialized_var(tty);\n\n\tsighand = rcu_dereference_check(tsk->sighand,\n\t\t\t\t\trcu_read_lock_held() ||\n\t\t\t\t\tlockdep_tasklist_lock_is_held());\n\tspin_lock(&sighand->siglock);\n\n\tposix_cpu_timers_exit(tsk);\n\tif (group_dead) {\n\t\tposix_cpu_timers_exit_group(tsk);\n\t\ttty = sig->tty;\n\t\tsig->tty = NULL;\n\t} else {\n\t\t/*\n\t\t * This can only happen if the caller is de_thread().\n\t\t * FIXME: this is the temporary hack, we should teach\n\t\t * posix-cpu-timers to handle this case correctly.\n\t\t */\n\t\tif (unlikely(has_group_leader_pid(tsk)))\n\t\t\tposix_cpu_timers_exit_group(tsk);\n\n\t\t/*\n\t\t * If there is any task waiting for the group exit\n\t\t * then notify it:\n\t\t */\n\t\tif (sig->notify_count > 0 && !--sig->notify_count)\n\t\t\twake_up_process(sig->group_exit_task);\n\n\t\tif (tsk == sig->curr_target)\n\t\t\tsig->curr_target = next_thread(tsk);\n\t\t/*\n\t\t * Accumulate here the counters for all threads but the\n\t\t * group leader as they die, so they can be added into\n\t\t * the process-wide totals when those are taken.\n\t\t * The group leader stays around as a zombie as long\n\t\t * as there are other threads.  When it gets reaped,\n\t\t * the exit.c code will add its counts into these totals.\n\t\t * We won't ever get here for the group leader, since it\n\t\t * will have been the last reference on the signal_struct.\n\t\t */\n\t\tsig->utime = cputime_add(sig->utime, tsk->utime);\n\t\tsig->stime = cputime_add(sig->stime, tsk->stime);\n\t\tsig->gtime = cputime_add(sig->gtime, tsk->gtime);\n\t\tsig->min_flt += tsk->min_flt;\n\t\tsig->maj_flt += tsk->maj_flt;\n\t\tsig->nvcsw += tsk->nvcsw;\n\t\tsig->nivcsw += tsk->nivcsw;\n\t\tsig->inblock += task_io_get_inblock(tsk);\n\t\tsig->oublock += task_io_get_oublock(tsk);\n\t\ttask_io_accounting_add(&sig->ioac, &tsk->ioac);\n\t\tsig->sum_sched_runtime += tsk->se.sum_exec_runtime;\n\t}\n\n\tsig->nr_threads--;\n\t__unhash_process(tsk, group_dead);\n\n\t/*\n\t * Do this under ->siglock, we can race with another thread\n\t * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.\n\t */\n\tflush_sigqueue(&tsk->pending);\n\ttsk->sighand = NULL;\n\tspin_unlock(&sighand->siglock);\n\n\t__cleanup_sighand(sighand);\n\tclear_tsk_thread_flag(tsk,TIF_SIGPENDING);\n\tif (group_dead) {\n\t\tflush_sigqueue(&sig->shared_pending);\n\t\ttty_kref_put(tty);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,6 +16,14 @@\n \t\ttty = sig->tty;\n \t\tsig->tty = NULL;\n \t} else {\n+\t\t/*\n+\t\t * This can only happen if the caller is de_thread().\n+\t\t * FIXME: this is the temporary hack, we should teach\n+\t\t * posix-cpu-timers to handle this case correctly.\n+\t\t */\n+\t\tif (unlikely(has_group_leader_pid(tsk)))\n+\t\t\tposix_cpu_timers_exit_group(tsk);\n+\n \t\t/*\n \t\t * If there is any task waiting for the group exit\n \t\t * then notify it:",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t/*",
                "\t\t * This can only happen if the caller is de_thread().",
                "\t\t * FIXME: this is the temporary hack, we should teach",
                "\t\t * posix-cpu-timers to handle this case correctly.",
                "\t\t */",
                "\t\tif (unlikely(has_group_leader_pid(tsk)))",
                "\t\t\tposix_cpu_timers_exit_group(tsk);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4526",
        "func_name": "torvalds/linux/sctp_transport_init",
        "description": "Race condition in the sctp_icmp_proto_unreachable function in net/sctp/input.c in Linux kernel 2.6.11-rc2 through 2.6.33 allows remote attackers to cause a denial of service (panic) via an ICMP unreachable message to a socket that is already locked by a user, which causes the socket to be freed and triggers list corruption, related to the sctp_wait_for_connect function.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff;h=50b5d6ad63821cea324a5a7a19854d4de1a0a819",
        "commit_title": "ICMP protocol unreachable handling completely disregarded",
        "commit_text": "the fact that the user may have locked the socket.  It proceeded to destroy the association, even though the user may have held the lock and had a ref on the association.  This resulted in the following:  Attempt to release alive inet socket f6afcc00  ========================= [ BUG: held lock freed! ] ------------------------- somenu/2672 is freeing memory f6afcc00-f6afcfff, with a lock still held there!  (sk_lock-AF_INET){+.+.+.}, at: [<c122098a>] sctp_connect+0x13/0x4c 1 lock held by somenu/2672:  #0:  (sk_lock-AF_INET){+.+.+.}, at: [<c122098a>] sctp_connect+0x13/0x4c  stack backtrace: Pid: 2672, comm: somenu Not tainted 2.6.32-telco #55 Call Trace:  [<c1232266>] ? printk+0xf/0x11  [<c1038553>] debug_check_no_locks_freed+0xce/0xff  [<c10620b4>] kmem_cache_free+0x21/0x66  [<c1185f25>] __sk_free+0x9d/0xab  [<c1185f9c>] sk_free+0x1c/0x1e  [<c1216e38>] sctp_association_put+0x32/0x89  [<c1220865>] __sctp_connect+0x36d/0x3f4  [<c122098a>] ? sctp_connect+0x13/0x4c  [<c102d073>] ? autoremove_wake_function+0x0/0x33  [<c12209a8>] sctp_connect+0x31/0x4c  [<c11d1e80>] inet_dgram_connect+0x4b/0x55  [<c11834fa>] sys_connect+0x54/0x71  [<c103a3a2>] ? lock_release_non_nested+0x88/0x239  [<c1054026>] ? might_fault+0x42/0x7c  [<c1054026>] ? might_fault+0x42/0x7c  [<c11847ab>] sys_socketcall+0x6d/0x178  [<c10da994>] ? trace_hardirqs_on_thunk+0xc/0x10  [<c1002959>] syscall_call+0x7/0xb  This was because the sctp_wait_for_connect() would aqcure the socket lock and then proceed to release the last reference count on the association, thus cause the fully destruction path to finish freeing the socket.  The simplest solution is to start a very short timer in case the socket is owned by user.  When the timer expires, we can do some verification and be able to do the release properly.  ",
        "func_before": "static struct sctp_transport *sctp_transport_init(struct sctp_transport *peer,\n\t\t\t\t\t\t  const union sctp_addr *addr,\n\t\t\t\t\t\t  gfp_t gfp)\n{\n\t/* Copy in the address.  */\n\tpeer->ipaddr = *addr;\n\tpeer->af_specific = sctp_get_af_specific(addr->sa.sa_family);\n\tpeer->asoc = NULL;\n\n\tpeer->dst = NULL;\n\tmemset(&peer->saddr, 0, sizeof(union sctp_addr));\n\n\t/* From 6.3.1 RTO Calculation:\n\t *\n\t * C1) Until an RTT measurement has been made for a packet sent to the\n\t * given destination transport address, set RTO to the protocol\n\t * parameter 'RTO.Initial'.\n\t */\n\tpeer->rto = msecs_to_jiffies(sctp_rto_initial);\n\tpeer->rtt = 0;\n\tpeer->rttvar = 0;\n\tpeer->srtt = 0;\n\tpeer->rto_pending = 0;\n\tpeer->hb_sent = 0;\n\tpeer->fast_recovery = 0;\n\n\tpeer->last_time_heard = jiffies;\n\tpeer->last_time_ecne_reduced = jiffies;\n\n\tpeer->init_sent_count = 0;\n\n\tpeer->param_flags = SPP_HB_DISABLE |\n\t\t\t    SPP_PMTUD_ENABLE |\n\t\t\t    SPP_SACKDELAY_ENABLE;\n\tpeer->hbinterval  = 0;\n\n\t/* Initialize the default path max_retrans.  */\n\tpeer->pathmaxrxt  = sctp_max_retrans_path;\n\tpeer->error_count = 0;\n\n\tINIT_LIST_HEAD(&peer->transmitted);\n\tINIT_LIST_HEAD(&peer->send_ready);\n\tINIT_LIST_HEAD(&peer->transports);\n\n\tpeer->T3_rtx_timer.expires = 0;\n\tpeer->hb_timer.expires = 0;\n\n\tsetup_timer(&peer->T3_rtx_timer, sctp_generate_t3_rtx_event,\n\t\t\t(unsigned long)peer);\n\tsetup_timer(&peer->hb_timer, sctp_generate_heartbeat_event,\n\t\t\t(unsigned long)peer);\n\n\t/* Initialize the 64-bit random nonce sent with heartbeat. */\n\tget_random_bytes(&peer->hb_nonce, sizeof(peer->hb_nonce));\n\n\tatomic_set(&peer->refcnt, 1);\n\tpeer->dead = 0;\n\n\tpeer->malloced = 0;\n\n\t/* Initialize the state information for SFR-CACC */\n\tpeer->cacc.changeover_active = 0;\n\tpeer->cacc.cycling_changeover = 0;\n\tpeer->cacc.next_tsn_at_change = 0;\n\tpeer->cacc.cacc_saw_newack = 0;\n\n\treturn peer;\n}",
        "func": "static struct sctp_transport *sctp_transport_init(struct sctp_transport *peer,\n\t\t\t\t\t\t  const union sctp_addr *addr,\n\t\t\t\t\t\t  gfp_t gfp)\n{\n\t/* Copy in the address.  */\n\tpeer->ipaddr = *addr;\n\tpeer->af_specific = sctp_get_af_specific(addr->sa.sa_family);\n\tpeer->asoc = NULL;\n\n\tpeer->dst = NULL;\n\tmemset(&peer->saddr, 0, sizeof(union sctp_addr));\n\n\t/* From 6.3.1 RTO Calculation:\n\t *\n\t * C1) Until an RTT measurement has been made for a packet sent to the\n\t * given destination transport address, set RTO to the protocol\n\t * parameter 'RTO.Initial'.\n\t */\n\tpeer->rto = msecs_to_jiffies(sctp_rto_initial);\n\tpeer->rtt = 0;\n\tpeer->rttvar = 0;\n\tpeer->srtt = 0;\n\tpeer->rto_pending = 0;\n\tpeer->hb_sent = 0;\n\tpeer->fast_recovery = 0;\n\n\tpeer->last_time_heard = jiffies;\n\tpeer->last_time_ecne_reduced = jiffies;\n\n\tpeer->init_sent_count = 0;\n\n\tpeer->param_flags = SPP_HB_DISABLE |\n\t\t\t    SPP_PMTUD_ENABLE |\n\t\t\t    SPP_SACKDELAY_ENABLE;\n\tpeer->hbinterval  = 0;\n\n\t/* Initialize the default path max_retrans.  */\n\tpeer->pathmaxrxt  = sctp_max_retrans_path;\n\tpeer->error_count = 0;\n\n\tINIT_LIST_HEAD(&peer->transmitted);\n\tINIT_LIST_HEAD(&peer->send_ready);\n\tINIT_LIST_HEAD(&peer->transports);\n\n\tpeer->T3_rtx_timer.expires = 0;\n\tpeer->hb_timer.expires = 0;\n\n\tsetup_timer(&peer->T3_rtx_timer, sctp_generate_t3_rtx_event,\n\t\t\t(unsigned long)peer);\n\tsetup_timer(&peer->hb_timer, sctp_generate_heartbeat_event,\n\t\t\t(unsigned long)peer);\n\tsetup_timer(&peer->proto_unreach_timer,\n\t\t    sctp_generate_proto_unreach_event, (unsigned long)peer);\n\n\t/* Initialize the 64-bit random nonce sent with heartbeat. */\n\tget_random_bytes(&peer->hb_nonce, sizeof(peer->hb_nonce));\n\n\tatomic_set(&peer->refcnt, 1);\n\tpeer->dead = 0;\n\n\tpeer->malloced = 0;\n\n\t/* Initialize the state information for SFR-CACC */\n\tpeer->cacc.changeover_active = 0;\n\tpeer->cacc.cycling_changeover = 0;\n\tpeer->cacc.next_tsn_at_change = 0;\n\tpeer->cacc.cacc_saw_newack = 0;\n\n\treturn peer;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -49,6 +49,8 @@\n \t\t\t(unsigned long)peer);\n \tsetup_timer(&peer->hb_timer, sctp_generate_heartbeat_event,\n \t\t\t(unsigned long)peer);\n+\tsetup_timer(&peer->proto_unreach_timer,\n+\t\t    sctp_generate_proto_unreach_event, (unsigned long)peer);\n \n \t/* Initialize the 64-bit random nonce sent with heartbeat. */\n \tget_random_bytes(&peer->hb_nonce, sizeof(peer->hb_nonce));",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tsetup_timer(&peer->proto_unreach_timer,",
                "\t\t    sctp_generate_proto_unreach_event, (unsigned long)peer);"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4526",
        "func_name": "torvalds/linux/sctp_icmp_proto_unreachable",
        "description": "Race condition in the sctp_icmp_proto_unreachable function in net/sctp/input.c in Linux kernel 2.6.11-rc2 through 2.6.33 allows remote attackers to cause a denial of service (panic) via an ICMP unreachable message to a socket that is already locked by a user, which causes the socket to be freed and triggers list corruption, related to the sctp_wait_for_connect function.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff;h=50b5d6ad63821cea324a5a7a19854d4de1a0a819",
        "commit_title": "ICMP protocol unreachable handling completely disregarded",
        "commit_text": "the fact that the user may have locked the socket.  It proceeded to destroy the association, even though the user may have held the lock and had a ref on the association.  This resulted in the following:  Attempt to release alive inet socket f6afcc00  ========================= [ BUG: held lock freed! ] ------------------------- somenu/2672 is freeing memory f6afcc00-f6afcfff, with a lock still held there!  (sk_lock-AF_INET){+.+.+.}, at: [<c122098a>] sctp_connect+0x13/0x4c 1 lock held by somenu/2672:  #0:  (sk_lock-AF_INET){+.+.+.}, at: [<c122098a>] sctp_connect+0x13/0x4c  stack backtrace: Pid: 2672, comm: somenu Not tainted 2.6.32-telco #55 Call Trace:  [<c1232266>] ? printk+0xf/0x11  [<c1038553>] debug_check_no_locks_freed+0xce/0xff  [<c10620b4>] kmem_cache_free+0x21/0x66  [<c1185f25>] __sk_free+0x9d/0xab  [<c1185f9c>] sk_free+0x1c/0x1e  [<c1216e38>] sctp_association_put+0x32/0x89  [<c1220865>] __sctp_connect+0x36d/0x3f4  [<c122098a>] ? sctp_connect+0x13/0x4c  [<c102d073>] ? autoremove_wake_function+0x0/0x33  [<c12209a8>] sctp_connect+0x31/0x4c  [<c11d1e80>] inet_dgram_connect+0x4b/0x55  [<c11834fa>] sys_connect+0x54/0x71  [<c103a3a2>] ? lock_release_non_nested+0x88/0x239  [<c1054026>] ? might_fault+0x42/0x7c  [<c1054026>] ? might_fault+0x42/0x7c  [<c11847ab>] sys_socketcall+0x6d/0x178  [<c10da994>] ? trace_hardirqs_on_thunk+0xc/0x10  [<c1002959>] syscall_call+0x7/0xb  This was because the sctp_wait_for_connect() would aqcure the socket lock and then proceed to release the last reference count on the association, thus cause the fully destruction path to finish freeing the socket.  The simplest solution is to start a very short timer in case the socket is owned by user.  When the timer expires, we can do some verification and be able to do the release properly.  ",
        "func_before": "void sctp_icmp_proto_unreachable(struct sock *sk,\n\t\t\t   struct sctp_association *asoc,\n\t\t\t   struct sctp_transport *t)\n{\n\tSCTP_DEBUG_PRINTK(\"%s\\n\",  __func__);\n\n\tsctp_do_sm(SCTP_EVENT_T_OTHER,\n\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n\t\t   asoc->state, asoc->ep, asoc, t,\n\t\t   GFP_ATOMIC);\n\n}",
        "func": "void sctp_icmp_proto_unreachable(struct sock *sk,\n\t\t\t   struct sctp_association *asoc,\n\t\t\t   struct sctp_transport *t)\n{\n\tSCTP_DEBUG_PRINTK(\"%s\\n\",  __func__);\n\n\tif (sock_owned_by_user(sk)) {\n\t\tif (timer_pending(&t->proto_unreach_timer))\n\t\t\treturn;\n\t\telse {\n\t\t\tif (!mod_timer(&t->proto_unreach_timer,\n\t\t\t\t\t\tjiffies + (HZ/20)))\n\t\t\t\tsctp_association_hold(asoc);\n\t\t}\n\t\t\t\n\t} else {\n\t\tif (timer_pending(&t->proto_unreach_timer) &&\n\t\t    del_timer(&t->proto_unreach_timer))\n\t\t\tsctp_association_put(asoc);\n\n\t\tsctp_do_sm(SCTP_EVENT_T_OTHER,\n\t\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n\t\t\t   asoc->state, asoc->ep, asoc, t,\n\t\t\t   GFP_ATOMIC);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,9 +4,23 @@\n {\n \tSCTP_DEBUG_PRINTK(\"%s\\n\",  __func__);\n \n-\tsctp_do_sm(SCTP_EVENT_T_OTHER,\n-\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n-\t\t   asoc->state, asoc->ep, asoc, t,\n-\t\t   GFP_ATOMIC);\n+\tif (sock_owned_by_user(sk)) {\n+\t\tif (timer_pending(&t->proto_unreach_timer))\n+\t\t\treturn;\n+\t\telse {\n+\t\t\tif (!mod_timer(&t->proto_unreach_timer,\n+\t\t\t\t\t\tjiffies + (HZ/20)))\n+\t\t\t\tsctp_association_hold(asoc);\n+\t\t}\n+\t\t\t\n+\t} else {\n+\t\tif (timer_pending(&t->proto_unreach_timer) &&\n+\t\t    del_timer(&t->proto_unreach_timer))\n+\t\t\tsctp_association_put(asoc);\n \n+\t\tsctp_do_sm(SCTP_EVENT_T_OTHER,\n+\t\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n+\t\t\t   asoc->state, asoc->ep, asoc, t,\n+\t\t\t   GFP_ATOMIC);\n+\t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tsctp_do_sm(SCTP_EVENT_T_OTHER,",
                "\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),",
                "\t\t   asoc->state, asoc->ep, asoc, t,",
                "\t\t   GFP_ATOMIC);"
            ],
            "added_lines": [
                "\tif (sock_owned_by_user(sk)) {",
                "\t\tif (timer_pending(&t->proto_unreach_timer))",
                "\t\t\treturn;",
                "\t\telse {",
                "\t\t\tif (!mod_timer(&t->proto_unreach_timer,",
                "\t\t\t\t\t\tjiffies + (HZ/20)))",
                "\t\t\t\tsctp_association_hold(asoc);",
                "\t\t}",
                "\t\t\t",
                "\t} else {",
                "\t\tif (timer_pending(&t->proto_unreach_timer) &&",
                "\t\t    del_timer(&t->proto_unreach_timer))",
                "\t\t\tsctp_association_put(asoc);",
                "\t\tsctp_do_sm(SCTP_EVENT_T_OTHER,",
                "\t\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),",
                "\t\t\t   asoc->state, asoc->ep, asoc, t,",
                "\t\t\t   GFP_ATOMIC);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-0990",
        "func_name": "mono/ves_icall_System_Array_FastCopy",
        "description": "Race condition in the FastCopy optimization in the Array.Copy method in metadata/icall.c in Mono, when Moonlight 2.x before 2.4.1 or 3.x before 3.99.3 is used, allows remote attackers to trigger a buffer overflow and modify internal data structures, and cause a denial of service (plugin crash) or corrupt the internal state of the security manager, via a crafted media file in which a thread makes a change after a type check but before a copy action.",
        "git_url": "https://github.com/mono/mono/commit/2f00e4bbb2137130845afb1b2a1e678552fc8e5c",
        "commit_title": "Disable some of the FastCopy fast paths since they are racy.",
        "commit_text": "",
        "func_before": "static gboolean\nves_icall_System_Array_FastCopy (MonoArray *source, int source_idx, MonoArray* dest, int dest_idx, int length)\n{\n\tint element_size;\n\tvoid * dest_addr;\n\tvoid * source_addr;\n\tMonoClass *src_class;\n\tMonoClass *dest_class;\n\tint i;\n\n\tMONO_ARCH_SAVE_REGS;\n\n\tif (source->obj.vtable->klass->rank != dest->obj.vtable->klass->rank)\n\t\treturn FALSE;\n\n\tif (source->bounds || dest->bounds)\n\t\treturn FALSE;\n\n\t/* there's no integer overflow since mono_array_length returns an unsigned integer */\n\tif ((dest_idx + length > mono_array_length (dest)) ||\n\t\t(source_idx + length > mono_array_length (source)))\n\t\treturn FALSE;\n\n\tsrc_class = source->obj.vtable->klass->element_class;\n\tdest_class = dest->obj.vtable->klass->element_class;\n\n\t/*\n\t * Handle common cases.\n\t */\n\n\t/* Case1: object[] -> valuetype[] (ArrayList::ToArray) */\n\tif (src_class == mono_defaults.object_class && dest_class->valuetype) {\n\t\tint has_refs = dest_class->has_references;\n\t\tfor (i = source_idx; i < source_idx + length; ++i) {\n\t\t\tMonoObject *elem = mono_array_get (source, MonoObject*, i);\n\t\t\tif (elem && !mono_object_isinst (elem, dest_class))\n\t\t\t\treturn FALSE;\n\t\t}\n\n\t\telement_size = mono_array_element_size (dest->obj.vtable->klass);\n\t\tmemset (mono_array_addr_with_size (dest, element_size, dest_idx), 0, element_size * length);\n\t\tfor (i = 0; i < length; ++i) {\n\t\t\tMonoObject *elem = mono_array_get (source, MonoObject*, source_idx + i);\n\t\t\tvoid *addr = mono_array_addr_with_size (dest, element_size, dest_idx + i);\n\t\t\tif (!elem)\n\t\t\t\tcontinue;\n\t\t\tif (has_refs)\n\t\t\t\tmono_value_copy (addr, (char *)elem + sizeof (MonoObject), dest_class);\n\t\t\telse\n\t\t\t\tmemcpy (addr, (char *)elem + sizeof (MonoObject), element_size);\n\t\t}\n\t\treturn TRUE;\n\t}\n\n\t/* Check if we're copying a char[] <==> (u)short[] */\n\tif (src_class != dest_class) {\n\t\tif (dest_class->valuetype || dest_class->enumtype || src_class->valuetype || src_class->enumtype)\n\t\t\treturn FALSE;\n\n\t\tif (mono_class_is_subclass_of (src_class, dest_class, FALSE))\n\t\t\t;\n\t\t/* Case2: object[] -> reftype[] (ArrayList::ToArray) */\n\t\telse if (mono_class_is_subclass_of (dest_class, src_class, FALSE))\n\t\t\tfor (i = source_idx; i < source_idx + length; ++i) {\n\t\t\t\tMonoObject *elem = mono_array_get (source, MonoObject*, i);\n\t\t\t\tif (elem && !mono_object_isinst (elem, dest_class))\n\t\t\t\t\treturn FALSE;\n\t\t\t}\n\t\telse\n\t\t\treturn FALSE;\n\t}\n\n\tif (dest_class->valuetype) {\n\t\telement_size = mono_array_element_size (source->obj.vtable->klass);\n\t\tsource_addr = mono_array_addr_with_size (source, element_size, source_idx);\n\t\tif (dest_class->has_references) {\n\t\t\tmono_value_copy_array (dest, dest_idx, source_addr, length);\n\t\t} else {\n\t\t\tdest_addr = mono_array_addr_with_size (dest, element_size, dest_idx);\n\t\t\tmemmove (dest_addr, source_addr, element_size * length);\n\t\t}\n\t} else {\n\t\tmono_array_memcpy_refs (dest, dest_idx, source, source_idx, length);\n\t}\n\n\treturn TRUE;\n}",
        "func": "static gboolean\nves_icall_System_Array_FastCopy (MonoArray *source, int source_idx, MonoArray* dest, int dest_idx, int length)\n{\n\tint element_size;\n\tvoid * dest_addr;\n\tvoid * source_addr;\n\tMonoClass *src_class;\n\tMonoClass *dest_class;\n\n\tMONO_ARCH_SAVE_REGS;\n\n\tif (source->obj.vtable->klass->rank != dest->obj.vtable->klass->rank)\n\t\treturn FALSE;\n\n\tif (source->bounds || dest->bounds)\n\t\treturn FALSE;\n\n\t/* there's no integer overflow since mono_array_length returns an unsigned integer */\n\tif ((dest_idx + length > mono_array_length (dest)) ||\n\t\t(source_idx + length > mono_array_length (source)))\n\t\treturn FALSE;\n\n\tsrc_class = source->obj.vtable->klass->element_class;\n\tdest_class = dest->obj.vtable->klass->element_class;\n\n\t/*\n\t * Handle common cases.\n\t */\n\n\t/* Case1: object[] -> valuetype[] (ArrayList::ToArray) */\n\tif (src_class == mono_defaults.object_class && dest_class->valuetype) {\n\t\t// FIXME: This is racy\n\t\treturn FALSE;\n\t\t/*\n\t\t  int i;\n\t\tint has_refs = dest_class->has_references;\n\t\tfor (i = source_idx; i < source_idx + length; ++i) {\n\t\t\tMonoObject *elem = mono_array_get (source, MonoObject*, i);\n\t\t\tif (elem && !mono_object_isinst (elem, dest_class))\n\t\t\t\treturn FALSE;\n\t\t}\n\n\t\telement_size = mono_array_element_size (dest->obj.vtable->klass);\n\t\tmemset (mono_array_addr_with_size (dest, element_size, dest_idx), 0, element_size * length);\n\t\tfor (i = 0; i < length; ++i) {\n\t\t\tMonoObject *elem = mono_array_get (source, MonoObject*, source_idx + i);\n\t\t\tvoid *addr = mono_array_addr_with_size (dest, element_size, dest_idx + i);\n\t\t\tif (!elem)\n\t\t\t\tcontinue;\n\t\t\tif (has_refs)\n\t\t\t\tmono_value_copy (addr, (char *)elem + sizeof (MonoObject), dest_class);\n\t\t\telse\n\t\t\t\tmemcpy (addr, (char *)elem + sizeof (MonoObject), element_size);\n\t\t}\n\t\treturn TRUE;\n\t\t*/\n\t}\n\n\t/* Check if we're copying a char[] <==> (u)short[] */\n\tif (src_class != dest_class) {\n\t\tif (dest_class->valuetype || dest_class->enumtype || src_class->valuetype || src_class->enumtype)\n\t\t\treturn FALSE;\n\n\t\tif (mono_class_is_subclass_of (src_class, dest_class, FALSE))\n\t\t\t;\n\t\t/* Case2: object[] -> reftype[] (ArrayList::ToArray) */\n\t\telse if (mono_class_is_subclass_of (dest_class, src_class, FALSE)) {\n\t\t\t// FIXME: This is racy\n\t\t\treturn FALSE;\n\t\t\t/*\n\t\t\t  int i;\n\t\t\tfor (i = source_idx; i < source_idx + length; ++i) {\n\t\t\t\tMonoObject *elem = mono_array_get (source, MonoObject*, i);\n\t\t\t\tif (elem && !mono_object_isinst (elem, dest_class))\n\t\t\t\t\treturn FALSE;\n\t\t\t}\n\t\t\t*/\n\t\t} else\n\t\t\treturn FALSE;\n\t}\n\n\tif (dest_class->valuetype) {\n\t\telement_size = mono_array_element_size (source->obj.vtable->klass);\n\t\tsource_addr = mono_array_addr_with_size (source, element_size, source_idx);\n\t\tif (dest_class->has_references) {\n\t\t\tmono_value_copy_array (dest, dest_idx, source_addr, length);\n\t\t} else {\n\t\t\tdest_addr = mono_array_addr_with_size (dest, element_size, dest_idx);\n\t\t\tmemmove (dest_addr, source_addr, element_size * length);\n\t\t}\n\t} else {\n\t\tmono_array_memcpy_refs (dest, dest_idx, source, source_idx, length);\n\t}\n\n\treturn TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,6 @@\n \tvoid * source_addr;\n \tMonoClass *src_class;\n \tMonoClass *dest_class;\n-\tint i;\n \n \tMONO_ARCH_SAVE_REGS;\n \n@@ -30,6 +29,10 @@\n \n \t/* Case1: object[] -> valuetype[] (ArrayList::ToArray) */\n \tif (src_class == mono_defaults.object_class && dest_class->valuetype) {\n+\t\t// FIXME: This is racy\n+\t\treturn FALSE;\n+\t\t/*\n+\t\t  int i;\n \t\tint has_refs = dest_class->has_references;\n \t\tfor (i = source_idx; i < source_idx + length; ++i) {\n \t\t\tMonoObject *elem = mono_array_get (source, MonoObject*, i);\n@@ -50,6 +53,7 @@\n \t\t\t\tmemcpy (addr, (char *)elem + sizeof (MonoObject), element_size);\n \t\t}\n \t\treturn TRUE;\n+\t\t*/\n \t}\n \n \t/* Check if we're copying a char[] <==> (u)short[] */\n@@ -60,13 +64,18 @@\n \t\tif (mono_class_is_subclass_of (src_class, dest_class, FALSE))\n \t\t\t;\n \t\t/* Case2: object[] -> reftype[] (ArrayList::ToArray) */\n-\t\telse if (mono_class_is_subclass_of (dest_class, src_class, FALSE))\n+\t\telse if (mono_class_is_subclass_of (dest_class, src_class, FALSE)) {\n+\t\t\t// FIXME: This is racy\n+\t\t\treturn FALSE;\n+\t\t\t/*\n+\t\t\t  int i;\n \t\t\tfor (i = source_idx; i < source_idx + length; ++i) {\n \t\t\t\tMonoObject *elem = mono_array_get (source, MonoObject*, i);\n \t\t\t\tif (elem && !mono_object_isinst (elem, dest_class))\n \t\t\t\t\treturn FALSE;\n \t\t\t}\n-\t\telse\n+\t\t\t*/\n+\t\t} else\n \t\t\treturn FALSE;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tint i;",
                "\t\telse if (mono_class_is_subclass_of (dest_class, src_class, FALSE))",
                "\t\telse"
            ],
            "added_lines": [
                "\t\t// FIXME: This is racy",
                "\t\treturn FALSE;",
                "\t\t/*",
                "\t\t  int i;",
                "\t\t*/",
                "\t\telse if (mono_class_is_subclass_of (dest_class, src_class, FALSE)) {",
                "\t\t\t// FIXME: This is racy",
                "\t\t\treturn FALSE;",
                "\t\t\t/*",
                "\t\t\t  int i;",
                "\t\t\t*/",
                "\t\t} else"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-10200",
        "func_name": "torvalds/linux/l2tp_ip6_bind",
        "description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c.",
        "git_url": "https://github.com/torvalds/linux/commit/32c231164b762dddefa13af5a0101032c70b50ef",
        "commit_title": "l2tp: fix racy SOCK_ZAPPED flag check in l2tp_ip{,6}_bind()",
        "commit_text": " Lock socket before checking the SOCK_ZAPPED flag in l2tp_ip6_bind(). Without lock, a concurrent call could modify the socket flags between the sock_flag(sk, SOCK_ZAPPED) test and the lock_sock() call. This way, a socket could be inserted twice in l2tp_ip6_bind_table. Releasing it would then leave a stale pointer there, generating use-after-free errors when walking through the list or modifying adjacent entries.  BUG: KASAN: use-after-free in l2tp_ip6_close+0x22e/0x290 at addr ffff8800081b0ed8 Write of size 8 by task syz-executor/10987 CPU: 0 PID: 10987 Comm: syz-executor Not tainted 4.8.0+ #39 Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.8.2-0-g33fbe13 by qemu-project.org 04/01/2014  ffff880031d97838 ffffffff829f835b ffff88001b5a1640 ffff8800081b0ec0  ffff8800081b15a0 ffff8800081b6d20 ffff880031d97860 ffffffff8174d3cc  ffff880031d978f0 ffff8800081b0e80 ffff88001b5a1640 ffff880031d978e0 Call Trace:  [<ffffffff829f835b>] dump_stack+0xb3/0x118 lib/dump_stack.c:15  [<ffffffff8174d3cc>] kasan_object_err+0x1c/0x70 mm/kasan/report.c:156  [<     inline     >] print_address_description mm/kasan/report.c:194  [<ffffffff8174d666>] kasan_report_error+0x1f6/0x4d0 mm/kasan/report.c:283  [<     inline     >] kasan_report mm/kasan/report.c:303  [<ffffffff8174db7e>] __asan_report_store8_noabort+0x3e/0x40 mm/kasan/report.c:329  [<     inline     >] __write_once_size ./include/linux/compiler.h:249  [<     inline     >] __hlist_del ./include/linux/list.h:622  [<     inline     >] hlist_del_init ./include/linux/list.h:637  [<ffffffff8579047e>] l2tp_ip6_close+0x22e/0x290 net/l2tp/l2tp_ip6.c:239  [<ffffffff850b2dfd>] inet_release+0xed/0x1c0 net/ipv4/af_inet.c:415  [<ffffffff851dc5a0>] inet6_release+0x50/0x70 net/ipv6/af_inet6.c:422  [<ffffffff84c4581d>] sock_release+0x8d/0x1d0 net/socket.c:570  [<ffffffff84c45976>] sock_close+0x16/0x20 net/socket.c:1017  [<ffffffff817a108c>] __fput+0x28c/0x780 fs/file_table.c:208  [<ffffffff817a1605>] ____fput+0x15/0x20 fs/file_table.c:244  [<ffffffff813774f9>] task_work_run+0xf9/0x170  [<ffffffff81324aae>] do_exit+0x85e/0x2a00  [<ffffffff81326dc8>] do_group_exit+0x108/0x330  [<ffffffff81348cf7>] get_signal+0x617/0x17a0 kernel/signal.c:2307  [<ffffffff811b49af>] do_signal+0x7f/0x18f0  [<ffffffff810039bf>] exit_to_usermode_loop+0xbf/0x150 arch/x86/entry/common.c:156  [<     inline     >] prepare_exit_to_usermode arch/x86/entry/common.c:190  [<ffffffff81006060>] syscall_return_slowpath+0x1a0/0x1e0 arch/x86/entry/common.c:259  [<ffffffff85e4d726>] entry_SYSCALL_64_fastpath+0xc4/0xc6 Object at ffff8800081b0ec0, in cache L2TP/IPv6 size: 1448 Allocated: PID = 10987  [ 1116.897025] [<ffffffff811ddcb6>] save_stack_trace+0x16/0x20  [ 1116.897025] [<ffffffff8174c736>] save_stack+0x46/0xd0  [ 1116.897025] [<ffffffff8174c9ad>] kasan_kmalloc+0xad/0xe0  [ 1116.897025] [<ffffffff8174cee2>] kasan_slab_alloc+0x12/0x20  [ 1116.897025] [<     inline     >] slab_post_alloc_hook mm/slab.h:417  [ 1116.897025] [<     inline     >] slab_alloc_node mm/slub.c:2708  [ 1116.897025] [<     inline     >] slab_alloc mm/slub.c:2716  [ 1116.897025] [<ffffffff817476a8>] kmem_cache_alloc+0xc8/0x2b0 mm/slub.c:2721  [ 1116.897025] [<ffffffff84c4f6a9>] sk_prot_alloc+0x69/0x2b0 net/core/sock.c:1326  [ 1116.897025] [<ffffffff84c58ac8>] sk_alloc+0x38/0xae0 net/core/sock.c:1388  [ 1116.897025] [<ffffffff851ddf67>] inet6_create+0x2d7/0x1000 net/ipv6/af_inet6.c:182  [ 1116.897025] [<ffffffff84c4af7b>] __sock_create+0x37b/0x640 net/socket.c:1153  [ 1116.897025] [<     inline     >] sock_create net/socket.c:1193  [ 1116.897025] [<     inline     >] SYSC_socket net/socket.c:1223  [ 1116.897025] [<ffffffff84c4b46f>] SyS_socket+0xef/0x1b0 net/socket.c:1203  [ 1116.897025] [<ffffffff85e4d685>] entry_SYSCALL_64_fastpath+0x23/0xc6 Freed: PID = 10987  [ 1116.897025] [<ffffffff811ddcb6>] save_stack_trace+0x16/0x20  [ 1116.897025] [<ffffffff8174c736>] save_stack+0x46/0xd0  [ 1116.897025] [<ffffffff8174cf61>] kasan_slab_free+0x71/0xb0  [ 1116.897025] [<     inline     >] slab_free_hook mm/slub.c:1352  [ 1116.897025] [<     inline     >] slab_free_freelist_hook mm/slub.c:1374  [ 1116.897025] [<     inline     >] slab_free mm/slub.c:2951  [ 1116.897025] [<ffffffff81748b28>] kmem_cache_free+0xc8/0x330 mm/slub.c:2973  [ 1116.897025] [<     inline     >] sk_prot_free net/core/sock.c:1369  [ 1116.897025] [<ffffffff84c541eb>] __sk_destruct+0x32b/0x4f0 net/core/sock.c:1444  [ 1116.897025] [<ffffffff84c5aca4>] sk_destruct+0x44/0x80 net/core/sock.c:1452  [ 1116.897025] [<ffffffff84c5ad33>] __sk_free+0x53/0x220 net/core/sock.c:1460  [ 1116.897025] [<ffffffff84c5af23>] sk_free+0x23/0x30 net/core/sock.c:1471  [ 1116.897025] [<ffffffff84c5cb6c>] sk_common_release+0x28c/0x3e0 ./include/net/sock.h:1589  [ 1116.897025] [<ffffffff8579044e>] l2tp_ip6_close+0x1fe/0x290 net/l2tp/l2tp_ip6.c:243  [ 1116.897025] [<ffffffff850b2dfd>] inet_release+0xed/0x1c0 net/ipv4/af_inet.c:415  [ 1116.897025] [<ffffffff851dc5a0>] inet6_release+0x50/0x70 net/ipv6/af_inet6.c:422  [ 1116.897025] [<ffffffff84c4581d>] sock_release+0x8d/0x1d0 net/socket.c:570  [ 1116.897025] [<ffffffff84c45976>] sock_close+0x16/0x20 net/socket.c:1017  [ 1116.897025] [<ffffffff817a108c>] __fput+0x28c/0x780 fs/file_table.c:208  [ 1116.897025] [<ffffffff817a1605>] ____fput+0x15/0x20 fs/file_table.c:244  [ 1116.897025] [<ffffffff813774f9>] task_work_run+0xf9/0x170  [ 1116.897025] [<ffffffff81324aae>] do_exit+0x85e/0x2a00  [ 1116.897025] [<ffffffff81326dc8>] do_group_exit+0x108/0x330  [ 1116.897025] [<ffffffff81348cf7>] get_signal+0x617/0x17a0 kernel/signal.c:2307  [ 1116.897025] [<ffffffff811b49af>] do_signal+0x7f/0x18f0  [ 1116.897025] [<ffffffff810039bf>] exit_to_usermode_loop+0xbf/0x150 arch/x86/entry/common.c:156  [ 1116.897025] [<     inline     >] prepare_exit_to_usermode arch/x86/entry/common.c:190  [ 1116.897025] [<ffffffff81006060>] syscall_return_slowpath+0x1a0/0x1e0 arch/x86/entry/common.c:259  [ 1116.897025] [<ffffffff85e4d726>] entry_SYSCALL_64_fastpath+0xc4/0xc6 Memory state around the buggy address:  ffff8800081b0d80: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc  ffff8800081b0e00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc >ffff8800081b0e80: fc fc fc fc fc fc fc fc fb fb fb fb fb fb fb fb                                                     ^  ffff8800081b0f00: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb  ffff8800081b0f80: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb  ==================================================================  The same issue exists with l2tp_ip_bind() and l2tp_ip_bind_table. ",
        "func_before": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "func": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out_unlock;\n\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,8 +8,6 @@\n \tint addr_type;\n \tint err;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET6)\n \t\treturn -EINVAL;\n \tif (addr_len < sizeof(*addr))\n@@ -35,6 +33,9 @@\n \tlock_sock(sk);\n \n \terr = -EINVAL;\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out_unlock;\n+\n \tif (sk->sk_state != TCP_CLOSE)\n \t\tgoto out_unlock;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ],
            "added_lines": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out_unlock;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2016-10200",
        "func_name": "torvalds/linux/l2tp_ip_bind",
        "description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c.",
        "git_url": "https://github.com/torvalds/linux/commit/32c231164b762dddefa13af5a0101032c70b50ef",
        "commit_title": "l2tp: fix racy SOCK_ZAPPED flag check in l2tp_ip{,6}_bind()",
        "commit_text": " Lock socket before checking the SOCK_ZAPPED flag in l2tp_ip6_bind(). Without lock, a concurrent call could modify the socket flags between the sock_flag(sk, SOCK_ZAPPED) test and the lock_sock() call. This way, a socket could be inserted twice in l2tp_ip6_bind_table. Releasing it would then leave a stale pointer there, generating use-after-free errors when walking through the list or modifying adjacent entries.  BUG: KASAN: use-after-free in l2tp_ip6_close+0x22e/0x290 at addr ffff8800081b0ed8 Write of size 8 by task syz-executor/10987 CPU: 0 PID: 10987 Comm: syz-executor Not tainted 4.8.0+ #39 Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.8.2-0-g33fbe13 by qemu-project.org 04/01/2014  ffff880031d97838 ffffffff829f835b ffff88001b5a1640 ffff8800081b0ec0  ffff8800081b15a0 ffff8800081b6d20 ffff880031d97860 ffffffff8174d3cc  ffff880031d978f0 ffff8800081b0e80 ffff88001b5a1640 ffff880031d978e0 Call Trace:  [<ffffffff829f835b>] dump_stack+0xb3/0x118 lib/dump_stack.c:15  [<ffffffff8174d3cc>] kasan_object_err+0x1c/0x70 mm/kasan/report.c:156  [<     inline     >] print_address_description mm/kasan/report.c:194  [<ffffffff8174d666>] kasan_report_error+0x1f6/0x4d0 mm/kasan/report.c:283  [<     inline     >] kasan_report mm/kasan/report.c:303  [<ffffffff8174db7e>] __asan_report_store8_noabort+0x3e/0x40 mm/kasan/report.c:329  [<     inline     >] __write_once_size ./include/linux/compiler.h:249  [<     inline     >] __hlist_del ./include/linux/list.h:622  [<     inline     >] hlist_del_init ./include/linux/list.h:637  [<ffffffff8579047e>] l2tp_ip6_close+0x22e/0x290 net/l2tp/l2tp_ip6.c:239  [<ffffffff850b2dfd>] inet_release+0xed/0x1c0 net/ipv4/af_inet.c:415  [<ffffffff851dc5a0>] inet6_release+0x50/0x70 net/ipv6/af_inet6.c:422  [<ffffffff84c4581d>] sock_release+0x8d/0x1d0 net/socket.c:570  [<ffffffff84c45976>] sock_close+0x16/0x20 net/socket.c:1017  [<ffffffff817a108c>] __fput+0x28c/0x780 fs/file_table.c:208  [<ffffffff817a1605>] ____fput+0x15/0x20 fs/file_table.c:244  [<ffffffff813774f9>] task_work_run+0xf9/0x170  [<ffffffff81324aae>] do_exit+0x85e/0x2a00  [<ffffffff81326dc8>] do_group_exit+0x108/0x330  [<ffffffff81348cf7>] get_signal+0x617/0x17a0 kernel/signal.c:2307  [<ffffffff811b49af>] do_signal+0x7f/0x18f0  [<ffffffff810039bf>] exit_to_usermode_loop+0xbf/0x150 arch/x86/entry/common.c:156  [<     inline     >] prepare_exit_to_usermode arch/x86/entry/common.c:190  [<ffffffff81006060>] syscall_return_slowpath+0x1a0/0x1e0 arch/x86/entry/common.c:259  [<ffffffff85e4d726>] entry_SYSCALL_64_fastpath+0xc4/0xc6 Object at ffff8800081b0ec0, in cache L2TP/IPv6 size: 1448 Allocated: PID = 10987  [ 1116.897025] [<ffffffff811ddcb6>] save_stack_trace+0x16/0x20  [ 1116.897025] [<ffffffff8174c736>] save_stack+0x46/0xd0  [ 1116.897025] [<ffffffff8174c9ad>] kasan_kmalloc+0xad/0xe0  [ 1116.897025] [<ffffffff8174cee2>] kasan_slab_alloc+0x12/0x20  [ 1116.897025] [<     inline     >] slab_post_alloc_hook mm/slab.h:417  [ 1116.897025] [<     inline     >] slab_alloc_node mm/slub.c:2708  [ 1116.897025] [<     inline     >] slab_alloc mm/slub.c:2716  [ 1116.897025] [<ffffffff817476a8>] kmem_cache_alloc+0xc8/0x2b0 mm/slub.c:2721  [ 1116.897025] [<ffffffff84c4f6a9>] sk_prot_alloc+0x69/0x2b0 net/core/sock.c:1326  [ 1116.897025] [<ffffffff84c58ac8>] sk_alloc+0x38/0xae0 net/core/sock.c:1388  [ 1116.897025] [<ffffffff851ddf67>] inet6_create+0x2d7/0x1000 net/ipv6/af_inet6.c:182  [ 1116.897025] [<ffffffff84c4af7b>] __sock_create+0x37b/0x640 net/socket.c:1153  [ 1116.897025] [<     inline     >] sock_create net/socket.c:1193  [ 1116.897025] [<     inline     >] SYSC_socket net/socket.c:1223  [ 1116.897025] [<ffffffff84c4b46f>] SyS_socket+0xef/0x1b0 net/socket.c:1203  [ 1116.897025] [<ffffffff85e4d685>] entry_SYSCALL_64_fastpath+0x23/0xc6 Freed: PID = 10987  [ 1116.897025] [<ffffffff811ddcb6>] save_stack_trace+0x16/0x20  [ 1116.897025] [<ffffffff8174c736>] save_stack+0x46/0xd0  [ 1116.897025] [<ffffffff8174cf61>] kasan_slab_free+0x71/0xb0  [ 1116.897025] [<     inline     >] slab_free_hook mm/slub.c:1352  [ 1116.897025] [<     inline     >] slab_free_freelist_hook mm/slub.c:1374  [ 1116.897025] [<     inline     >] slab_free mm/slub.c:2951  [ 1116.897025] [<ffffffff81748b28>] kmem_cache_free+0xc8/0x330 mm/slub.c:2973  [ 1116.897025] [<     inline     >] sk_prot_free net/core/sock.c:1369  [ 1116.897025] [<ffffffff84c541eb>] __sk_destruct+0x32b/0x4f0 net/core/sock.c:1444  [ 1116.897025] [<ffffffff84c5aca4>] sk_destruct+0x44/0x80 net/core/sock.c:1452  [ 1116.897025] [<ffffffff84c5ad33>] __sk_free+0x53/0x220 net/core/sock.c:1460  [ 1116.897025] [<ffffffff84c5af23>] sk_free+0x23/0x30 net/core/sock.c:1471  [ 1116.897025] [<ffffffff84c5cb6c>] sk_common_release+0x28c/0x3e0 ./include/net/sock.h:1589  [ 1116.897025] [<ffffffff8579044e>] l2tp_ip6_close+0x1fe/0x290 net/l2tp/l2tp_ip6.c:243  [ 1116.897025] [<ffffffff850b2dfd>] inet_release+0xed/0x1c0 net/ipv4/af_inet.c:415  [ 1116.897025] [<ffffffff851dc5a0>] inet6_release+0x50/0x70 net/ipv6/af_inet6.c:422  [ 1116.897025] [<ffffffff84c4581d>] sock_release+0x8d/0x1d0 net/socket.c:570  [ 1116.897025] [<ffffffff84c45976>] sock_close+0x16/0x20 net/socket.c:1017  [ 1116.897025] [<ffffffff817a108c>] __fput+0x28c/0x780 fs/file_table.c:208  [ 1116.897025] [<ffffffff817a1605>] ____fput+0x15/0x20 fs/file_table.c:244  [ 1116.897025] [<ffffffff813774f9>] task_work_run+0xf9/0x170  [ 1116.897025] [<ffffffff81324aae>] do_exit+0x85e/0x2a00  [ 1116.897025] [<ffffffff81326dc8>] do_group_exit+0x108/0x330  [ 1116.897025] [<ffffffff81348cf7>] get_signal+0x617/0x17a0 kernel/signal.c:2307  [ 1116.897025] [<ffffffff811b49af>] do_signal+0x7f/0x18f0  [ 1116.897025] [<ffffffff810039bf>] exit_to_usermode_loop+0xbf/0x150 arch/x86/entry/common.c:156  [ 1116.897025] [<     inline     >] prepare_exit_to_usermode arch/x86/entry/common.c:190  [ 1116.897025] [<ffffffff81006060>] syscall_return_slowpath+0x1a0/0x1e0 arch/x86/entry/common.c:259  [ 1116.897025] [<ffffffff85e4d726>] entry_SYSCALL_64_fastpath+0xc4/0xc6 Memory state around the buggy address:  ffff8800081b0d80: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc  ffff8800081b0e00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc >ffff8800081b0e80: fc fc fc fc fc fc fc fc fb fb fb fb fb fb fb fb                                                     ^  ffff8800081b0f00: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb  ffff8800081b0f80: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb  ==================================================================  The same issue exists with l2tp_ip_bind() and l2tp_ip_bind_table. ",
        "func_before": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "func": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,8 +6,6 @@\n \tint ret;\n \tint chk_addr_ret;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr_len < sizeof(struct sockaddr_l2tpip))\n \t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET)\n@@ -22,6 +20,9 @@\n \tread_unlock_bh(&l2tp_ip_lock);\n \n \tlock_sock(sk);\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out;\n+\n \tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n \t\tgoto out;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ],
            "added_lines": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-6874",
        "func_name": "torvalds/linux/get_ucounts",
        "description": "Race condition in kernel/ucount.c in the Linux kernel through 4.10.2 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via crafted system calls that leverage certain decrement behavior that causes incorrect interaction between put_ucounts and get_ucounts.",
        "git_url": "https://github.com/torvalds/linux/commit/040757f738e13caaa9c5078bca79aa97e11dde88",
        "commit_title": "ucount: Remove the atomicity from ucount->count",
        "commit_text": " Always increment/decrement ucount->count under the ucounts_lock.  The increments are there already and moving the decrements there means the locking logic of the code is simpler.  This simplification in the locking logic fixes a race between put_ucounts and get_ucounts that could result in a use-after-free because the count could go zero then be found by get_ucounts and then be freed by put_ucounts.  A bug presumably this one was found by a combination of syzkaller and KASAN.  JongWhan Kim reported the syzkaller failure and Dmitry Vyukov spotted the race in the code.  Cc: stable@vger.kernel.org",
        "func_before": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 0);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n\t\tucounts = NULL;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "func": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tnew->count = 0;\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (ucounts->count == INT_MAX)\n\t\tucounts = NULL;\n\telse\n\t\tucounts->count += 1;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,7 @@\n \n \t\tnew->ns = ns;\n \t\tnew->uid = uid;\n-\t\tatomic_set(&new->count, 0);\n+\t\tnew->count = 0;\n \n \t\tspin_lock_irq(&ucounts_lock);\n \t\tucounts = find_ucounts(ns, uid, hashent);\n@@ -25,8 +25,10 @@\n \t\t\tucounts = new;\n \t\t}\n \t}\n-\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n+\tif (ucounts->count == INT_MAX)\n \t\tucounts = NULL;\n+\telse\n+\t\tucounts->count += 1;\n \tspin_unlock_irq(&ucounts_lock);\n \treturn ucounts;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tatomic_set(&new->count, 0);",
                "\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))"
            ],
            "added_lines": [
                "\t\tnew->count = 0;",
                "\tif (ucounts->count == INT_MAX)",
                "\telse",
                "\t\tucounts->count += 1;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-6874",
        "func_name": "torvalds/linux/put_ucounts",
        "description": "Race condition in kernel/ucount.c in the Linux kernel through 4.10.2 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via crafted system calls that leverage certain decrement behavior that causes incorrect interaction between put_ucounts and get_ucounts.",
        "git_url": "https://github.com/torvalds/linux/commit/040757f738e13caaa9c5078bca79aa97e11dde88",
        "commit_title": "ucount: Remove the atomicity from ucount->count",
        "commit_text": " Always increment/decrement ucount->count under the ucounts_lock.  The increments are there already and moving the decrements there means the locking logic of the code is simpler.  This simplification in the locking logic fixes a race between put_ucounts and get_ucounts that could result in a use-after-free because the count could go zero then be found by get_ucounts and then be freed by put_ucounts.  A bug presumably this one was found by a combination of syzkaller and KASAN.  JongWhan Kim reported the syzkaller failure and Dmitry Vyukov spotted the race in the code.  Cc: stable@vger.kernel.org",
        "func_before": "static void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\n\tif (atomic_dec_and_test(&ucounts->count)) {\n\t\tspin_lock_irqsave(&ucounts_lock, flags);\n\t\thlist_del_init(&ucounts->node);\n\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\n\t\tkfree(ucounts);\n\t}\n}",
        "func": "static void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ucounts_lock, flags);\n\tucounts->count -= 1;\n\tif (!ucounts->count)\n\t\thlist_del_init(&ucounts->node);\n\telse\n\t\tucounts = NULL;\n\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\n\tkfree(ucounts);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,11 +2,13 @@\n {\n \tunsigned long flags;\n \n-\tif (atomic_dec_and_test(&ucounts->count)) {\n-\t\tspin_lock_irqsave(&ucounts_lock, flags);\n+\tspin_lock_irqsave(&ucounts_lock, flags);\n+\tucounts->count -= 1;\n+\tif (!ucounts->count)\n \t\thlist_del_init(&ucounts->node);\n-\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n+\telse\n+\t\tucounts = NULL;\n+\tspin_unlock_irqrestore(&ucounts_lock, flags);\n \n-\t\tkfree(ucounts);\n-\t}\n+\tkfree(ucounts);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (atomic_dec_and_test(&ucounts->count)) {",
                "\t\tspin_lock_irqsave(&ucounts_lock, flags);",
                "\t\tspin_unlock_irqrestore(&ucounts_lock, flags);",
                "\t\tkfree(ucounts);",
                "\t}"
            ],
            "added_lines": [
                "\tspin_lock_irqsave(&ucounts_lock, flags);",
                "\tucounts->count -= 1;",
                "\tif (!ucounts->count)",
                "\telse",
                "\t\tucounts = NULL;",
                "\tspin_unlock_irqrestore(&ucounts_lock, flags);",
                "\tkfree(ucounts);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-5232",
        "func_name": "cornelisnetworks/opa-fm/unix_client_connect",
        "description": "Race conditions in opa-fm before 10.4.0.0.196 and opa-ff before 10.4.0.0.197.",
        "git_url": "https://github.com/cornelisnetworks/opa-fm/commit/c5759e7b76f5bf844be6c6641cc1b356bbc83869",
        "commit_title": "Fix scripts and code that use well-known tmp files.",
        "commit_text": "",
        "func_before": "hsm_com_errno_t\nunix_client_connect(hsm_com_client_hdl_t *hdl)\n{\n\tint\t\t\t\t\tfd, len;\n\tstruct sockaddr_un\tunix_addr;\n\n\tif ((fd = socket(AF_UNIX, SOCK_STREAM, 0)) < 0) \n\t{\n\t\treturn HSM_COM_ERROR;\n\t}\n\n\tmemset(&unix_addr,0,sizeof(unix_addr));\n\n\tunix_addr.sun_family = AF_UNIX;\n\t\n\tif(strlen(hdl->c_path) >= sizeof(unix_addr.sun_path))\n\t{\n\t\tclose(fd);\n\t\treturn HSM_COM_PATH_ERR;\n\t}\n\n\tsnprintf(unix_addr.sun_path, sizeof(unix_addr.sun_path), \"%s\", hdl->c_path);\n\n\tlen = SUN_LEN(&unix_addr);\n\n\tunlink(unix_addr.sun_path);\n\n\tif(bind(fd, (struct sockaddr *)&unix_addr, len) < 0)\n\t{\n\t\tunlink(hdl->c_path);\n\t\tclose(fd);\n\n\t\treturn HSM_COM_BIND_ERR;\n\t}\n\n\tif(chmod(unix_addr.sun_path, S_IRWXU) < 0)\n\t{\n\t\tunlink(hdl->c_path);\n\t\tclose(fd);\n\t\treturn HSM_COM_CHMOD_ERR;\n\t}\n\n\tmemset(&unix_addr,0,sizeof(unix_addr));\n\n\tunix_addr.sun_family = AF_UNIX;\n\tstrncpy(unix_addr.sun_path, hdl->s_path, sizeof(unix_addr.sun_path));\n\tunix_addr.sun_path[sizeof(unix_addr.sun_path)-1] = 0;\n\n\tlen = SUN_LEN(&unix_addr);\n\n\tif (connect(fd, (struct sockaddr *) &unix_addr, len) < 0) \n\t{\n\t\tunlink(hdl->c_path);\n\t\tclose(fd);\n\t\treturn HSM_COM_CONX_ERR;\n\t}\n\n\thdl->client_fd = fd;\n\thdl->client_state = HSM_COM_C_STATE_CT;\n\n\t// Send connection data packet\n\tif(unix_sck_send_conn(hdl, 2) != HSM_COM_OK)\n\t{\n\t\thdl->client_state = HSM_COM_C_STATE_IN;\n\t\treturn HSM_COM_SEND_ERR;\n\t}\n\n\n\n\treturn HSM_COM_OK;\n\n}",
        "func": "hsm_com_errno_t\nunix_client_connect(hsm_com_client_hdl_t *hdl)\n{\n\tint\t\t\t\t\tfd, len;\n\tstruct sockaddr_un\tunix_addr;\n\thsm_com_errno_t\t\tres = HSM_COM_OK;\n\n\tif ((fd = socket(AF_UNIX, SOCK_STREAM, 0)) < 0) \n\t{\n\t\treturn HSM_COM_ERROR;\n\t}\n\n\tmemset(&unix_addr,0,sizeof(unix_addr));\n\n\tunix_addr.sun_family = AF_UNIX;\n\t\n\tif(strlen(hdl->c_path) >= sizeof(unix_addr.sun_path))\n\t{\n\t\tres = HSM_COM_PATH_ERR;\n\t\tgoto cleanup;\n\t}\n\n\tsnprintf(unix_addr.sun_path, sizeof(unix_addr.sun_path), \"%s\", hdl->c_path);\n\n\tlen = SUN_LEN(&unix_addr);\n\n\tunlink(unix_addr.sun_path);\n\n\tif(bind(fd, (struct sockaddr *)&unix_addr, len) < 0)\n\t{\n\t\tres = HSM_COM_BIND_ERR;\n\t\tgoto cleanup;\n\t}\n\n\tif(chmod(unix_addr.sun_path, S_IRWXU) < 0)\n\t{\n\t\tres = HSM_COM_CHMOD_ERR;\n\t\tgoto cleanup;\n\t}\n\n\tmemset(&unix_addr,0,sizeof(unix_addr));\n\n\tunix_addr.sun_family = AF_UNIX;\n\tstrncpy(unix_addr.sun_path, hdl->s_path, sizeof(unix_addr.sun_path));\n\tunix_addr.sun_path[sizeof(unix_addr.sun_path)-1] = 0;\n\n\tlen = SUN_LEN(&unix_addr);\n\n\tif (connect(fd, (struct sockaddr *) &unix_addr, len) < 0) \n\t{\n\t\tres = HSM_COM_CONX_ERR;\n\t\tgoto cleanup;\n\t}\n\n\thdl->client_fd = fd;\n\thdl->client_state = HSM_COM_C_STATE_CT;\n\n\t// Send connection data packet\n\tif(unix_sck_send_conn(hdl, 2) != HSM_COM_OK)\n\t{\n\t\thdl->client_state = HSM_COM_C_STATE_IN;\n\t\tres = HSM_COM_SEND_ERR;\n\t}\n\n\treturn res;\n\ncleanup:\n\tclose(fd);\n\treturn res;\n\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,7 @@\n {\n \tint\t\t\t\t\tfd, len;\n \tstruct sockaddr_un\tunix_addr;\n+\thsm_com_errno_t\t\tres = HSM_COM_OK;\n \n \tif ((fd = socket(AF_UNIX, SOCK_STREAM, 0)) < 0) \n \t{\n@@ -15,8 +16,8 @@\n \t\n \tif(strlen(hdl->c_path) >= sizeof(unix_addr.sun_path))\n \t{\n-\t\tclose(fd);\n-\t\treturn HSM_COM_PATH_ERR;\n+\t\tres = HSM_COM_PATH_ERR;\n+\t\tgoto cleanup;\n \t}\n \n \tsnprintf(unix_addr.sun_path, sizeof(unix_addr.sun_path), \"%s\", hdl->c_path);\n@@ -27,17 +28,14 @@\n \n \tif(bind(fd, (struct sockaddr *)&unix_addr, len) < 0)\n \t{\n-\t\tunlink(hdl->c_path);\n-\t\tclose(fd);\n-\n-\t\treturn HSM_COM_BIND_ERR;\n+\t\tres = HSM_COM_BIND_ERR;\n+\t\tgoto cleanup;\n \t}\n \n \tif(chmod(unix_addr.sun_path, S_IRWXU) < 0)\n \t{\n-\t\tunlink(hdl->c_path);\n-\t\tclose(fd);\n-\t\treturn HSM_COM_CHMOD_ERR;\n+\t\tres = HSM_COM_CHMOD_ERR;\n+\t\tgoto cleanup;\n \t}\n \n \tmemset(&unix_addr,0,sizeof(unix_addr));\n@@ -50,9 +48,8 @@\n \n \tif (connect(fd, (struct sockaddr *) &unix_addr, len) < 0) \n \t{\n-\t\tunlink(hdl->c_path);\n-\t\tclose(fd);\n-\t\treturn HSM_COM_CONX_ERR;\n+\t\tres = HSM_COM_CONX_ERR;\n+\t\tgoto cleanup;\n \t}\n \n \thdl->client_fd = fd;\n@@ -62,11 +59,13 @@\n \tif(unix_sck_send_conn(hdl, 2) != HSM_COM_OK)\n \t{\n \t\thdl->client_state = HSM_COM_C_STATE_IN;\n-\t\treturn HSM_COM_SEND_ERR;\n+\t\tres = HSM_COM_SEND_ERR;\n \t}\n \n+\treturn res;\n \n-\n-\treturn HSM_COM_OK;\n+cleanup:\n+\tclose(fd);\n+\treturn res;\n \n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tclose(fd);",
                "\t\treturn HSM_COM_PATH_ERR;",
                "\t\tunlink(hdl->c_path);",
                "\t\tclose(fd);",
                "",
                "\t\treturn HSM_COM_BIND_ERR;",
                "\t\tunlink(hdl->c_path);",
                "\t\tclose(fd);",
                "\t\treturn HSM_COM_CHMOD_ERR;",
                "\t\tunlink(hdl->c_path);",
                "\t\tclose(fd);",
                "\t\treturn HSM_COM_CONX_ERR;",
                "\t\treturn HSM_COM_SEND_ERR;",
                "",
                "\treturn HSM_COM_OK;"
            ],
            "added_lines": [
                "\thsm_com_errno_t\t\tres = HSM_COM_OK;",
                "\t\tres = HSM_COM_PATH_ERR;",
                "\t\tgoto cleanup;",
                "\t\tres = HSM_COM_BIND_ERR;",
                "\t\tgoto cleanup;",
                "\t\tres = HSM_COM_CHMOD_ERR;",
                "\t\tgoto cleanup;",
                "\t\tres = HSM_COM_CONX_ERR;",
                "\t\tgoto cleanup;",
                "\t\tres = HSM_COM_SEND_ERR;",
                "\treturn res;",
                "cleanup:",
                "\tclose(fd);",
                "\treturn res;"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-5232",
        "func_name": "cornelisnetworks/opa-fm/hcom_client_init",
        "description": "Race conditions in opa-fm before 10.4.0.0.196 and opa-ff before 10.4.0.0.197.",
        "git_url": "https://github.com/cornelisnetworks/opa-fm/commit/c5759e7b76f5bf844be6c6641cc1b356bbc83869",
        "commit_title": "Fix scripts and code that use well-known tmp files.",
        "commit_text": "",
        "func_before": "hsm_com_errno_t\nhcom_client_init\n(\n\t\tOUT\tp_hsm_com_client_hdl_t\t*p_hdl,\n\tIN\t\tchar\t\t\t\t\t*server_path,\n\tIN\t\tchar\t\t\t\t\t*client_path,\n\tIN\t\tint\t\t\t\t\t\tmax_data_len\n)\n{\n\thsm_com_client_hdl_t\t*hdl = NULL;\n\thsm_com_errno_t\t\t\tres = HSM_COM_OK;\n\t\n\n\tif((strlen(server_path) > (HSM_COM_SVR_MAX_PATH - 1)) ||\n\t   (strlen(server_path) == 0)){\n\t\tres = HSM_COM_PATH_ERR;\n\t\tgoto cleanup;\n\t}\n\n\tif((strlen(client_path) > (HSM_COM_SVR_MAX_PATH - 1)) ||\n\t   (strlen(client_path) == 0)){\n\t\tres = HSM_COM_PATH_ERR;\n\t\tgoto cleanup;\n\t}\n\n\n\tif((hdl = calloc(1,sizeof(hsm_com_client_hdl_t))) == NULL)\n\t{\n\t\tres = HSM_COM_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\tif((hdl->scr.scratch = malloc(max_data_len)) == NULL) \n\t{\n\t\tres = HSM_COM_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\tif((hdl->recv_buf = malloc(max_data_len)) == NULL) \n\t{\n\t\tres = HSM_COM_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\tif((hdl->send_buf = malloc(max_data_len)) == NULL) \n\t{\n\t\tres = HSM_COM_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\thdl->scr.scratch_fill = 0;\n\thdl->scr.scratch_len = max_data_len;\n\thdl->buf_len = max_data_len;\n\thdl->trans_id = 1;\n\n\n\tstrcpy(hdl->s_path,server_path);\n\tstrcpy(hdl->c_path,client_path);\n\n\n\thdl->client_state = HSM_COM_C_STATE_IN;\n\n\t*p_hdl = hdl;\n\n\treturn res;\n\ncleanup:\n\tif(hdl)\n\t{\n\t\tif (hdl->scr.scratch) {\n\t\t\tfree(hdl->scr.scratch);\n\t\t}\n\t\tif (hdl->recv_buf) {\n\t\t\tfree(hdl->recv_buf);\n\t\t}\n\t\tfree(hdl);\n\t}\n\n\treturn res;\n\n}",
        "func": "hsm_com_errno_t\nhcom_client_init\n(\n\t\tOUT\tp_hsm_com_client_hdl_t\t*p_hdl,\n\tIN\t\tchar\t\t\t\t\t*server_path,\n\tIN\t\tchar\t\t\t\t\t*client_path,\n\tIN\t\tint\t\t\t\t\t\tmax_data_len\n)\n{\n\thsm_com_client_hdl_t\t*hdl = NULL;\n\thsm_com_errno_t\t\t\tres = HSM_COM_OK;\n\t\n\n\tif((strlen(server_path) > (HSM_COM_SVR_MAX_PATH - 1)) ||\n\t   (strlen(server_path) == 0)){\n\t\tres = HSM_COM_PATH_ERR;\n\t\tgoto cleanup;\n\t}\n\n\tif((strlen(client_path) > (HSM_COM_SVR_MAX_PATH - 1)) ||\n\t   (strlen(client_path) == 0)){\n\t\tres = HSM_COM_PATH_ERR;\n\t\tgoto cleanup;\n\t}\n\n\n\tif((hdl = calloc(1,sizeof(hsm_com_client_hdl_t))) == NULL)\n\t{\n\t\tres = HSM_COM_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\tif((hdl->scr.scratch = malloc(max_data_len)) == NULL) \n\t{\n\t\tres = HSM_COM_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\tif((hdl->recv_buf = malloc(max_data_len)) == NULL) \n\t{\n\t\tres = HSM_COM_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\tif((hdl->send_buf = malloc(max_data_len)) == NULL) \n\t{\n\t\tres = HSM_COM_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\thdl->scr.scratch_fill = 0;\n\thdl->scr.scratch_len = max_data_len;\n\thdl->buf_len = max_data_len;\n\thdl->trans_id = 1;\n\n\n\tstrcpy(hdl->s_path,server_path);\n\tstrcpy(hdl->c_path,client_path);\n\n\tif (mkstemp(hdl->c_path) == -1)\n\t{\n\t\tres = HSM_COM_PATH_ERR;\n\t\tgoto cleanup;\n\t}\n\n\thdl->client_state = HSM_COM_C_STATE_IN;\n\n\t*p_hdl = hdl;\n\n\treturn res;\n\ncleanup:\n\tif(hdl)\n\t{\n\t\tif (hdl->scr.scratch) {\n\t\t\tfree(hdl->scr.scratch);\n\t\t}\n\t\tif (hdl->recv_buf) {\n\t\t\tfree(hdl->recv_buf);\n\t\t}\n\t\tfree(hdl);\n\t}\n\n\treturn res;\n\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -57,6 +57,11 @@\n \tstrcpy(hdl->s_path,server_path);\n \tstrcpy(hdl->c_path,client_path);\n \n+\tif (mkstemp(hdl->c_path) == -1)\n+\t{\n+\t\tres = HSM_COM_PATH_ERR;\n+\t\tgoto cleanup;\n+\t}\n \n \thdl->client_state = HSM_COM_C_STATE_IN;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (mkstemp(hdl->c_path) == -1)",
                "\t{",
                "\t\tres = HSM_COM_PATH_ERR;",
                "\t\tgoto cleanup;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-5232",
        "func_name": "cornelisnetworks/opa-fm/main",
        "description": "Race conditions in opa-fm before 10.4.0.0.196 and opa-ff before 10.4.0.0.197.",
        "git_url": "https://github.com/cornelisnetworks/opa-fm/commit/c5759e7b76f5bf844be6c6641cc1b356bbc83869",
        "commit_title": "Fix scripts and code that use well-known tmp files.",
        "commit_text": "",
        "func_before": "int main(int argc, char *argv[]) {\n\tp_fm_config_conx_hdlt\thdl;\n\tint\t\t\t\t\t\tinstance = 0;\n\tfm_mgr_config_errno_t\tres;\n\tchar\t\t\t\t\t*rem_addr = NULL;\n\tchar\t\t\t\t\t*community = \"public\";\n\tchar            \t\tOpts[256];\n    int             \t\targ;\n\tchar \t\t\t\t\t*command;\n\tint\t\t\t\t\t\ti;\n\n\t/* Get options at the command line (overide default values) */\n    strcpy(Opts, \"i:d:h-\");\n\n    while ((arg = getopt(argc, argv, Opts)) != EOF) {\n        switch (arg) {\n\t\tcase 'h':\n\t\tcase '-':\n\t\t\tusage(argv[0]);\n\t\t\treturn(0);\n\t\tcase 'i':\n\t\t\tinstance = atol(optarg);\n\t\t\tbreak;\n\t\tcase 'd':\n\t\t\trem_addr = optarg;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage(argv[0]);\n\t\t\treturn(-1);\n\t\t}\n\t}\n\n\tif(optind >= argc){\n        fprintf(stderr, \"Command required\\n\");\n\t\tusage(argv[0]);\n\t\treturn -1;\n\t}\n\n\tcommand = argv[optind++];\n\tprintf(\"Connecting to %s FM instance %d\\n\", (rem_addr==NULL) ? \"LOCAL\":rem_addr, instance);\n\tif((res = fm_mgr_config_init(&hdl,instance, rem_addr, community)) != FM_CONF_OK)\n\t{\n\t\tfprintf(stderr, \"Failed to initialize the client handle: %d\\n\", res);\n\t\tgoto die_clean;\n\t}\n\n\tif((res = fm_mgr_config_connect(hdl)) != FM_CONF_OK)\n\t{\n\t\tfprintf(stderr, \"Failed to connect: (%d) %s\\n\",res,fm_mgr_get_error_str(res));\n\t\tgoto die_clean;\n\t}\n\n\tfor(i=0;i<commandListLen;i++){\n\t\tif(strcmp(command,commandList[i].name) == 0){\n\t\t\treturn commandList[i].cmdPtr(hdl, commandList[i].mgr, (argc - optind), &argv[optind]);\n\t\t}\n\t}\n\n\tfprintf(stderr, \"Command (%s) is not valid\\n\",command);\n\tusage(argv[0]);\n\tres = -1;\n\ndie_clean:\n\tif (hdl) free(hdl);\n\treturn res;\n}",
        "func": "int main(int argc, char *argv[]) {\n\tp_fm_config_conx_hdlt\thdl = NULL;\n\tint\t\t\t\t\t\tinstance = 0;\n\tfm_mgr_config_errno_t\tres;\n\tchar\t\t\t\t\t*rem_addr = NULL;\n\tchar\t\t\t\t\t*community = \"public\";\n\tchar            \t\tOpts[256];\n    int             \t\targ;\n\tchar \t\t\t\t\t*command;\n\tint\t\t\t\t\t\ti;\n\n\t/* Get options at the command line (overide default values) */\n    strcpy(Opts, \"i:d:h-\");\n\n    while ((arg = getopt(argc, argv, Opts)) != EOF) {\n        switch (arg) {\n\t\tcase 'h':\n\t\tcase '-':\n\t\t\tusage(argv[0]);\n\t\t\treturn(0);\n\t\tcase 'i':\n\t\t\tinstance = atol(optarg);\n\t\t\tbreak;\n\t\tcase 'd':\n\t\t\trem_addr = optarg;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage(argv[0]);\n\t\t\treturn(-1);\n\t\t}\n\t}\n\n\tif(optind >= argc){\n        fprintf(stderr, \"Command required\\n\");\n\t\tusage(argv[0]);\n\t\treturn -1;\n\t}\n\n\tcommand = argv[optind++];\n\tprintf(\"Connecting to %s FM instance %d\\n\", (rem_addr==NULL) ? \"LOCAL\":rem_addr, instance);\n\tif((res = fm_mgr_config_init(&hdl,instance, rem_addr, community)) != FM_CONF_OK)\n\t{\n\t\tfprintf(stderr, \"Failed to initialize the client handle: %d\\n\", res);\n\t\tgoto cleanup;\n\t}\n\n\tif((res = fm_mgr_config_connect(hdl)) != FM_CONF_OK)\n\t{\n\t\tfprintf(stderr, \"Failed to connect: (%d) %s\\n\",res,fm_mgr_get_error_str(res));\n\t\tgoto cleanup;\n\t}\n\n\tfor(i=0;i<commandListLen;i++){\n\t\tif(strcmp(command,commandList[i].name) == 0){\n\t\t\tres = commandList[i].cmdPtr(hdl, commandList[i].mgr, (argc - optind), &argv[optind]);\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\tfprintf(stderr, \"Command (%s) is not valid\\n\",command);\n\tusage(argv[0]);\n\tres = -1;\n\ncleanup:\n\tif (hdl)\n\t{\n\t\tif (hdl->sm_hdl)\n\t\t{\n\t\t\tif (hdl->sm_hdl->c_path[0])\n\t\t\t\tunlink(hdl->sm_hdl->c_path);\n\t\t}\n\t\tif (hdl->pm_hdl)\n\t\t{\n\t\t\tif (hdl->pm_hdl->c_path[0])\n\t\t\t\tunlink(hdl->pm_hdl->c_path);\n\t\t}\n\t\tif (hdl->fe_hdl)\n\t\t{\n\t\t\tif (hdl->fe_hdl->c_path[0])\n\t\t\t\tunlink(hdl->fe_hdl->c_path);\n\t\t}\n\t\tfree(hdl);\n\t}\n\n\treturn res;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n int main(int argc, char *argv[]) {\n-\tp_fm_config_conx_hdlt\thdl;\n+\tp_fm_config_conx_hdlt\thdl = NULL;\n \tint\t\t\t\t\t\tinstance = 0;\n \tfm_mgr_config_errno_t\tres;\n \tchar\t\t\t\t\t*rem_addr = NULL;\n@@ -41,18 +41,19 @@\n \tif((res = fm_mgr_config_init(&hdl,instance, rem_addr, community)) != FM_CONF_OK)\n \t{\n \t\tfprintf(stderr, \"Failed to initialize the client handle: %d\\n\", res);\n-\t\tgoto die_clean;\n+\t\tgoto cleanup;\n \t}\n \n \tif((res = fm_mgr_config_connect(hdl)) != FM_CONF_OK)\n \t{\n \t\tfprintf(stderr, \"Failed to connect: (%d) %s\\n\",res,fm_mgr_get_error_str(res));\n-\t\tgoto die_clean;\n+\t\tgoto cleanup;\n \t}\n \n \tfor(i=0;i<commandListLen;i++){\n \t\tif(strcmp(command,commandList[i].name) == 0){\n-\t\t\treturn commandList[i].cmdPtr(hdl, commandList[i].mgr, (argc - optind), &argv[optind]);\n+\t\t\tres = commandList[i].cmdPtr(hdl, commandList[i].mgr, (argc - optind), &argv[optind]);\n+\t\t\tgoto cleanup;\n \t\t}\n \t}\n \n@@ -60,7 +61,26 @@\n \tusage(argv[0]);\n \tres = -1;\n \n-die_clean:\n-\tif (hdl) free(hdl);\n+cleanup:\n+\tif (hdl)\n+\t{\n+\t\tif (hdl->sm_hdl)\n+\t\t{\n+\t\t\tif (hdl->sm_hdl->c_path[0])\n+\t\t\t\tunlink(hdl->sm_hdl->c_path);\n+\t\t}\n+\t\tif (hdl->pm_hdl)\n+\t\t{\n+\t\t\tif (hdl->pm_hdl->c_path[0])\n+\t\t\t\tunlink(hdl->pm_hdl->c_path);\n+\t\t}\n+\t\tif (hdl->fe_hdl)\n+\t\t{\n+\t\t\tif (hdl->fe_hdl->c_path[0])\n+\t\t\t\tunlink(hdl->fe_hdl->c_path);\n+\t\t}\n+\t\tfree(hdl);\n+\t}\n+\n \treturn res;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tp_fm_config_conx_hdlt\thdl;",
                "\t\tgoto die_clean;",
                "\t\tgoto die_clean;",
                "\t\t\treturn commandList[i].cmdPtr(hdl, commandList[i].mgr, (argc - optind), &argv[optind]);",
                "die_clean:",
                "\tif (hdl) free(hdl);"
            ],
            "added_lines": [
                "\tp_fm_config_conx_hdlt\thdl = NULL;",
                "\t\tgoto cleanup;",
                "\t\tgoto cleanup;",
                "\t\t\tres = commandList[i].cmdPtr(hdl, commandList[i].mgr, (argc - optind), &argv[optind]);",
                "\t\t\tgoto cleanup;",
                "cleanup:",
                "\tif (hdl)",
                "\t{",
                "\t\tif (hdl->sm_hdl)",
                "\t\t{",
                "\t\t\tif (hdl->sm_hdl->c_path[0])",
                "\t\t\t\tunlink(hdl->sm_hdl->c_path);",
                "\t\t}",
                "\t\tif (hdl->pm_hdl)",
                "\t\t{",
                "\t\t\tif (hdl->pm_hdl->c_path[0])",
                "\t\t\t\tunlink(hdl->pm_hdl->c_path);",
                "\t\t}",
                "\t\tif (hdl->fe_hdl)",
                "\t\t{",
                "\t\t\tif (hdl->fe_hdl->c_path[0])",
                "\t\t\t\tunlink(hdl->fe_hdl->c_path);",
                "\t\t}",
                "\t\tfree(hdl);",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2015-5232",
        "func_name": "cornelisnetworks/opa-fm/fm_mgr_config_init",
        "description": "Race conditions in opa-fm before 10.4.0.0.196 and opa-ff before 10.4.0.0.197.",
        "git_url": "https://github.com/cornelisnetworks/opa-fm/commit/c5759e7b76f5bf844be6c6641cc1b356bbc83869",
        "commit_title": "Fix scripts and code that use well-known tmp files.",
        "commit_text": "",
        "func_before": "fm_mgr_config_errno_t\nfm_mgr_config_init\n(\n\t\t\t\t\tOUT\tp_fm_config_conx_hdlt\t\t*p_hdl,\n\t\t\t\tIN\t\tint\t\t\t\t\t\t\tinstance,\n\tOPTIONAL\tIN\t\tchar\t\t\t\t\t\t*rem_address,\n\tOPTIONAL\tIN\t\tchar\t\t\t\t\t\t*community\n)\n{\n\tfm_config_conx_hdl      *hdl;\n\tfm_mgr_config_errno_t   res = FM_CONF_OK;\n\n\n\tif ( (hdl = calloc(1,sizeof(fm_config_conx_hdl))) == NULL )\n\t{\n\t\tres = FM_CONF_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\thdl->instance = instance;\n\n\t*p_hdl = hdl;\n\n\t\t// connect to the snmp agent via localhost?\n\tif(!rem_address || (strcmp(rem_address,\"localhost\") == 0))\n\t{\n\t\tif ( fm_mgr_config_mgr_connect(hdl, FM_MGR_SM) == FM_CONF_INIT_ERR )\n\t\t{\n\t\t\tres = FM_CONF_INIT_ERR;\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tif ( fm_mgr_config_mgr_connect(hdl, FM_MGR_PM) == FM_CONF_INIT_ERR )\n\t\t{\n\t\t\tres = FM_CONF_INIT_ERR;\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tif ( fm_mgr_config_mgr_connect(hdl, FM_MGR_FE) == FM_CONF_INIT_ERR )\n\t\t{\n\t\t\tres = FM_CONF_INIT_ERR;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\n\treturn res;\n\n\n\tcleanup:\n\n\tif ( hdl ) {\n\t\tfree(hdl);\n\t\thdl = NULL;\n\t}\n\n\treturn res;\n}",
        "func": "fm_mgr_config_errno_t\nfm_mgr_config_init\n(\n\t\t\t\t\tOUT\tp_fm_config_conx_hdlt\t\t*p_hdl,\n\t\t\t\tIN\t\tint\t\t\t\t\t\t\tinstance,\n\tOPTIONAL\tIN\t\tchar\t\t\t\t\t\t*rem_address,\n\tOPTIONAL\tIN\t\tchar\t\t\t\t\t\t*community\n)\n{\n\tfm_config_conx_hdl      *hdl;\n\tfm_mgr_config_errno_t   res = FM_CONF_OK;\n\n\n\tif ( (hdl = calloc(1,sizeof(fm_config_conx_hdl))) == NULL )\n\t{\n\t\tres = FM_CONF_NO_MEM;\n\t\tgoto cleanup;\n\t}\n\n\thdl->instance = instance;\n\n\t*p_hdl = hdl;\n\n\t\t// connect to the snmp agent via localhost?\n\tif(!rem_address || (strcmp(rem_address,\"localhost\") == 0))\n\t{\n\t\tif ( fm_mgr_config_mgr_connect(hdl, FM_MGR_SM) == FM_CONF_INIT_ERR )\n\t\t{\n\t\t\tres = FM_CONF_INIT_ERR;\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tif ( fm_mgr_config_mgr_connect(hdl, FM_MGR_PM) == FM_CONF_INIT_ERR )\n\t\t{\n\t\t\tres = FM_CONF_INIT_ERR;\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tif ( fm_mgr_config_mgr_connect(hdl, FM_MGR_FE) == FM_CONF_INIT_ERR )\n\t\t{\n\t\t\tres = FM_CONF_INIT_ERR;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\ncleanup:\n\treturn res;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -43,16 +43,6 @@\n \t\t}\n \t}\n \n-\n-\treturn res;\n-\n-\n-\tcleanup:\n-\n-\tif ( hdl ) {\n-\t\tfree(hdl);\n-\t\thdl = NULL;\n-\t}\n-\n+cleanup:\n \treturn res;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\treturn res;",
                "",
                "",
                "\tcleanup:",
                "",
                "\tif ( hdl ) {",
                "\t\tfree(hdl);",
                "\t\thdl = NULL;",
                "\t}",
                ""
            ],
            "added_lines": [
                "cleanup:"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-5232",
        "func_name": "cornelisnetworks/opa-fm/fm_mgr_config_mgr_connect",
        "description": "Race conditions in opa-fm before 10.4.0.0.196 and opa-ff before 10.4.0.0.197.",
        "git_url": "https://github.com/cornelisnetworks/opa-fm/commit/c5759e7b76f5bf844be6c6641cc1b356bbc83869",
        "commit_title": "Fix scripts and code that use well-known tmp files.",
        "commit_text": "",
        "func_before": "fm_mgr_config_errno_t\nfm_mgr_config_mgr_connect\n(\n\tfm_config_conx_hdl\t*hdl, \n\tfm_mgr_type_t \t\tmgr\n)\n{\n\tchar                    s_path[256];\n\tchar                    c_path[256];\n\tchar                    *mgr_prefix;\n\tp_hsm_com_client_hdl_t  *mgr_hdl;\n\tpid_t                   pid;\n\n\tmemset(s_path,0,sizeof(s_path));\n\tmemset(c_path,0,sizeof(c_path));\n\n\tpid = getpid();\n\n\tswitch ( mgr )\n\t{\n\t\tcase FM_MGR_SM:\n\t\t\tmgr_prefix  = HSM_FM_SCK_SM;\n\t\t\tmgr_hdl     = &hdl->sm_hdl;\n\t\t\tbreak;\n\t\tcase FM_MGR_PM:\n\t\t\tmgr_prefix  = HSM_FM_SCK_PM;\n\t\t\tmgr_hdl     = &hdl->pm_hdl;\n\t\t\tbreak;\n\t\tcase FM_MGR_FE:\n\t\t\tmgr_prefix  = HSM_FM_SCK_FE;\n\t\t\tmgr_hdl     = &hdl->fe_hdl;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn FM_CONF_INIT_ERR;\n\t}\n\n\t// Fill in the paths for the server and client sockets.\n\tsprintf(s_path,\"%s%s%d\",HSM_FM_SCK_PREFIX,mgr_prefix,hdl->instance);\n\n\tsprintf(c_path,\"%s%s%d_C_%lu\",HSM_FM_SCK_PREFIX,mgr_prefix,\n\t\t\thdl->instance, (long unsigned)pid);\n\n\tif ( *mgr_hdl == NULL )\n\t{\n\t\tif ( hcom_client_init(mgr_hdl,s_path,c_path,32768) != HSM_COM_OK )\n\t\t{\n\t\t\treturn FM_CONF_INIT_ERR;\n\t\t}\n\t}\n\n\tif ( hcom_client_connect(*mgr_hdl) == HSM_COM_OK )\n\t{\n\t\thdl->conx_mask |= mgr;\n\t\treturn FM_CONF_OK;\n\t}\n\n\treturn FM_CONF_CONX_ERR;\n\n}",
        "func": "fm_mgr_config_errno_t\nfm_mgr_config_mgr_connect\n(\n\tfm_config_conx_hdl\t*hdl, \n\tfm_mgr_type_t \t\tmgr\n)\n{\n\tchar                    s_path[256];\n\tchar                    c_path[256];\n\tchar                    *mgr_prefix;\n\tp_hsm_com_client_hdl_t  *mgr_hdl;\n\n\tmemset(s_path,0,sizeof(s_path));\n\tmemset(c_path,0,sizeof(c_path));\n\n\tswitch ( mgr )\n\t{\n\t\tcase FM_MGR_SM:\n\t\t\tmgr_prefix  = HSM_FM_SCK_SM;\n\t\t\tmgr_hdl     = &hdl->sm_hdl;\n\t\t\tbreak;\n\t\tcase FM_MGR_PM:\n\t\t\tmgr_prefix  = HSM_FM_SCK_PM;\n\t\t\tmgr_hdl     = &hdl->pm_hdl;\n\t\t\tbreak;\n\t\tcase FM_MGR_FE:\n\t\t\tmgr_prefix  = HSM_FM_SCK_FE;\n\t\t\tmgr_hdl     = &hdl->fe_hdl;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn FM_CONF_INIT_ERR;\n\t}\n\n\t// Fill in the paths for the server and client sockets.\n\tsprintf(s_path,\"%s%s%d\",HSM_FM_SCK_PREFIX,mgr_prefix,hdl->instance);\n\n\tsprintf(c_path,\"%s%s%d_C_XXXXXX\",HSM_FM_SCK_PREFIX,mgr_prefix,hdl->instance);\n\n\tif ( *mgr_hdl == NULL )\n\t{\n\t\tif ( hcom_client_init(mgr_hdl,s_path,c_path,32768) != HSM_COM_OK )\n\t\t{\n\t\t\treturn FM_CONF_INIT_ERR;\n\t\t}\n\t}\n\n\tif ( hcom_client_connect(*mgr_hdl) == HSM_COM_OK )\n\t{\n\t\thdl->conx_mask |= mgr;\n\t\treturn FM_CONF_OK;\n\t}\n\n\treturn FM_CONF_CONX_ERR;\n\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,12 +9,9 @@\n \tchar                    c_path[256];\n \tchar                    *mgr_prefix;\n \tp_hsm_com_client_hdl_t  *mgr_hdl;\n-\tpid_t                   pid;\n \n \tmemset(s_path,0,sizeof(s_path));\n \tmemset(c_path,0,sizeof(c_path));\n-\n-\tpid = getpid();\n \n \tswitch ( mgr )\n \t{\n@@ -37,8 +34,7 @@\n \t// Fill in the paths for the server and client sockets.\n \tsprintf(s_path,\"%s%s%d\",HSM_FM_SCK_PREFIX,mgr_prefix,hdl->instance);\n \n-\tsprintf(c_path,\"%s%s%d_C_%lu\",HSM_FM_SCK_PREFIX,mgr_prefix,\n-\t\t\thdl->instance, (long unsigned)pid);\n+\tsprintf(c_path,\"%s%s%d_C_XXXXXX\",HSM_FM_SCK_PREFIX,mgr_prefix,hdl->instance);\n \n \tif ( *mgr_hdl == NULL )\n \t{",
        "diff_line_info": {
            "deleted_lines": [
                "\tpid_t                   pid;",
                "",
                "\tpid = getpid();",
                "\tsprintf(c_path,\"%s%s%d_C_%lu\",HSM_FM_SCK_PREFIX,mgr_prefix,",
                "\t\t\thdl->instance, (long unsigned)pid);"
            ],
            "added_lines": [
                "\tsprintf(c_path,\"%s%s%d_C_XXXXXX\",HSM_FM_SCK_PREFIX,mgr_prefix,hdl->instance);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-10913",
        "func_name": "xen-project/xen/__gnttab_unmap_common_complete",
        "description": "The grant-table feature in Xen through 4.8.x provides false mapping information in certain cases of concurrent unmap calls, which allows backend attackers to obtain sensitive information or gain privileges, aka XSA-218 bug 1.",
        "git_url": "https://github.com/xen-project/xen/commit/b7f6cbb9d43f7384e1f38f8764b9a48216c8a525",
        "commit_title": "gnttab: Avoid potential double-put of maptrack entry",
        "commit_text": " Each grant mapping for a particular domain is tracked by an in-Xen \"maptrack\" entry.  This entry is is referenced by a \"handle\", which is given to the guest when it calls gnttab_map_grant_ref().  There are two types of mapping a particular handle can refer to: GNTMAP_host_map and GNTMAP_device_map.  A given gnttab_unmap_grant_ref() call can remove either only one or both of these entries.  When a particular handle has no entries left, it must be freed.  gnttab_unmap_grant_ref() loops through its grant unmap request list twice.  It first removes entries from any host pagetables and (if appropraite) iommus; then it does a single domain TLB flush; then it does the clean-up, including telling the granter that entries are no longer being used (if appropriate).  At the moment, it's during the first pass that the maptrack flags are cleared, but the second pass that the maptrack entry is freed.  Unfortunately this allows the following race, which results in a double-free:   A: (pass 1) clear host_map  B: (pass 1) clear device_map  A: (pass 2) See that maptrack entry has no mappings, free it  B: (pass 2) See that maptrack entry has no mappings, free it #  Unfortunately, unlike the active entry pinning update, we can't simply move the maptrack flag changes to the second half, because the maptrack flags are used to determine if iommu entries need to be added: a domain's iommu must never have fewer permissions than the maptrack flags indicate, or a subsequent map_grant_ref() might fail to add the necessary iommu entries.  Instead, free the maptrack entry in the first pass if there are no further mappings.  This is part of XSA-218. ",
        "func_before": "static void\n__gnttab_unmap_common_complete(struct gnttab_unmap_common *op)\n{\n    struct domain *ld, *rd = op->rd;\n    struct grant_table *rgt;\n    struct active_grant_entry *act;\n    grant_entry_header_t *sha;\n    struct page_info *pg;\n    uint16_t *status;\n    bool_t put_handle = 0;\n\n    if ( rd == NULL )\n    { \n        /*\n         * Suggests that __gntab_unmap_common failed in\n         * rcu_lock_domain_by_id() or earlier, and so we have nothing\n         * to complete\n         */\n        return;\n    }\n\n    ld = current->domain;\n\n    rcu_lock_domain(rd);\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n    if ( rgt->gt_version == 0 )\n        goto unlock_out;\n\n    act = active_entry_acquire(rgt, op->map->ref);\n    sha = shared_entry_header(rgt, op->map->ref);\n\n    if ( rgt->gt_version == 1 )\n        status = &sha->flags;\n    else\n        status = &status_entry(rgt, op->map->ref);\n\n    if ( unlikely(op->frame != act->frame) ) \n    {\n        /*\n         * Suggests that __gntab_unmap_common failed early and so\n         * nothing further to do\n         */\n        goto act_release_out;\n    }\n\n    pg = mfn_to_page(op->frame);\n\n    if ( op->flags & GNTMAP_device_map ) \n    {\n        if ( !is_iomem_page(_mfn(act->frame)) )\n        {\n            if ( op->flags & GNTMAP_readonly )\n                put_page(pg);\n            else\n                put_page_and_type(pg);\n        }\n\n        ASSERT(act->pin & (GNTPIN_devw_mask | GNTPIN_devr_mask));\n        if ( op->flags & GNTMAP_readonly )\n            act->pin -= GNTPIN_devr_inc;\n        else\n            act->pin -= GNTPIN_devw_inc;\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( op->status != 0 ) \n        {\n            /*\n             * Suggests that __gntab_unmap_common failed in\n             * replace_grant_host_mapping() or IOMMU handling, so nothing\n             * further to do (short of re-establishing the mapping in the\n             * latter case).\n             */\n            goto act_release_out;\n        }\n\n        if ( !is_iomem_page(_mfn(op->frame)) )\n        {\n            if ( gnttab_host_mapping_get_page_type(op, ld, rd) )\n                put_page_type(pg);\n            put_page(pg);\n        }\n\n        ASSERT(act->pin & (GNTPIN_hstw_mask | GNTPIN_hstr_mask));\n        if ( op->flags & GNTMAP_readonly )\n            act->pin -= GNTPIN_hstr_inc;\n        else\n            act->pin -= GNTPIN_hstw_inc;\n    }\n\n    if ( (op->map->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0 )\n        put_handle = 1;\n\n    if ( ((act->pin & (GNTPIN_devw_mask|GNTPIN_hstw_mask)) == 0) &&\n         !(op->flags & GNTMAP_readonly) )\n        gnttab_clear_flag(_GTF_writing, status);\n\n    if ( act->pin == 0 )\n        gnttab_clear_flag(_GTF_reading, status);\n\n act_release_out:\n    active_entry_release(act);\n unlock_out:\n    grant_read_unlock(rgt);\n\n    if ( put_handle )\n    {\n        op->map->flags = 0;\n        put_maptrack_handle(ld->grant_table, op->handle);\n    }\n    rcu_unlock_domain(rd);\n}",
        "func": "static void\n__gnttab_unmap_common_complete(struct gnttab_unmap_common *op)\n{\n    struct domain *ld, *rd = op->rd;\n    struct grant_table *rgt;\n    struct active_grant_entry *act;\n    grant_entry_header_t *sha;\n    struct page_info *pg;\n    uint16_t *status;\n\n    if ( rd == NULL )\n    { \n        /*\n         * Suggests that __gntab_unmap_common failed in\n         * rcu_lock_domain_by_id() or earlier, and so we have nothing\n         * to complete\n         */\n        return;\n    }\n\n    ld = current->domain;\n\n    rcu_lock_domain(rd);\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n    if ( rgt->gt_version == 0 )\n        goto unlock_out;\n\n    act = active_entry_acquire(rgt, op->ref);\n    sha = shared_entry_header(rgt, op->ref);\n\n    if ( rgt->gt_version == 1 )\n        status = &sha->flags;\n    else\n        status = &status_entry(rgt, op->ref);\n\n    if ( unlikely(op->frame != act->frame) ) \n    {\n        /*\n         * Suggests that __gntab_unmap_common failed early and so\n         * nothing further to do\n         */\n        goto act_release_out;\n    }\n\n    pg = mfn_to_page(op->frame);\n\n    if ( op->flags & GNTMAP_device_map ) \n    {\n        if ( !is_iomem_page(_mfn(act->frame)) )\n        {\n            if ( op->flags & GNTMAP_readonly )\n                put_page(pg);\n            else\n                put_page_and_type(pg);\n        }\n\n        ASSERT(act->pin & (GNTPIN_devw_mask | GNTPIN_devr_mask));\n        if ( op->flags & GNTMAP_readonly )\n            act->pin -= GNTPIN_devr_inc;\n        else\n            act->pin -= GNTPIN_devw_inc;\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( op->status != 0 ) \n        {\n            /*\n             * Suggests that __gntab_unmap_common failed in\n             * replace_grant_host_mapping() or IOMMU handling, so nothing\n             * further to do (short of re-establishing the mapping in the\n             * latter case).\n             */\n            goto act_release_out;\n        }\n\n        if ( !is_iomem_page(_mfn(op->frame)) )\n        {\n            if ( gnttab_host_mapping_get_page_type(op, ld, rd) )\n                put_page_type(pg);\n            put_page(pg);\n        }\n\n        ASSERT(act->pin & (GNTPIN_hstw_mask | GNTPIN_hstr_mask));\n        if ( op->flags & GNTMAP_readonly )\n            act->pin -= GNTPIN_hstr_inc;\n        else\n            act->pin -= GNTPIN_hstw_inc;\n    }\n\n    if ( ((act->pin & (GNTPIN_devw_mask|GNTPIN_hstw_mask)) == 0) &&\n         !(op->flags & GNTMAP_readonly) )\n        gnttab_clear_flag(_GTF_writing, status);\n\n    if ( act->pin == 0 )\n        gnttab_clear_flag(_GTF_reading, status);\n\n act_release_out:\n    active_entry_release(act);\n unlock_out:\n    grant_read_unlock(rgt);\n\n    rcu_unlock_domain(rd);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,6 @@\n     grant_entry_header_t *sha;\n     struct page_info *pg;\n     uint16_t *status;\n-    bool_t put_handle = 0;\n \n     if ( rd == NULL )\n     { \n@@ -28,13 +27,13 @@\n     if ( rgt->gt_version == 0 )\n         goto unlock_out;\n \n-    act = active_entry_acquire(rgt, op->map->ref);\n-    sha = shared_entry_header(rgt, op->map->ref);\n+    act = active_entry_acquire(rgt, op->ref);\n+    sha = shared_entry_header(rgt, op->ref);\n \n     if ( rgt->gt_version == 1 )\n         status = &sha->flags;\n     else\n-        status = &status_entry(rgt, op->map->ref);\n+        status = &status_entry(rgt, op->ref);\n \n     if ( unlikely(op->frame != act->frame) ) \n     {\n@@ -91,9 +90,6 @@\n             act->pin -= GNTPIN_hstw_inc;\n     }\n \n-    if ( (op->map->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0 )\n-        put_handle = 1;\n-\n     if ( ((act->pin & (GNTPIN_devw_mask|GNTPIN_hstw_mask)) == 0) &&\n          !(op->flags & GNTMAP_readonly) )\n         gnttab_clear_flag(_GTF_writing, status);\n@@ -106,10 +102,5 @@\n  unlock_out:\n     grant_read_unlock(rgt);\n \n-    if ( put_handle )\n-    {\n-        op->map->flags = 0;\n-        put_maptrack_handle(ld->grant_table, op->handle);\n-    }\n     rcu_unlock_domain(rd);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    bool_t put_handle = 0;",
                "    act = active_entry_acquire(rgt, op->map->ref);",
                "    sha = shared_entry_header(rgt, op->map->ref);",
                "        status = &status_entry(rgt, op->map->ref);",
                "    if ( (op->map->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0 )",
                "        put_handle = 1;",
                "",
                "    if ( put_handle )",
                "    {",
                "        op->map->flags = 0;",
                "        put_maptrack_handle(ld->grant_table, op->handle);",
                "    }"
            ],
            "added_lines": [
                "    act = active_entry_acquire(rgt, op->ref);",
                "    sha = shared_entry_header(rgt, op->ref);",
                "        status = &status_entry(rgt, op->ref);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-10913",
        "func_name": "xen-project/xen/__gnttab_unmap_common",
        "description": "The grant-table feature in Xen through 4.8.x provides false mapping information in certain cases of concurrent unmap calls, which allows backend attackers to obtain sensitive information or gain privileges, aka XSA-218 bug 1.",
        "git_url": "https://github.com/xen-project/xen/commit/b7f6cbb9d43f7384e1f38f8764b9a48216c8a525",
        "commit_title": "gnttab: Avoid potential double-put of maptrack entry",
        "commit_text": " Each grant mapping for a particular domain is tracked by an in-Xen \"maptrack\" entry.  This entry is is referenced by a \"handle\", which is given to the guest when it calls gnttab_map_grant_ref().  There are two types of mapping a particular handle can refer to: GNTMAP_host_map and GNTMAP_device_map.  A given gnttab_unmap_grant_ref() call can remove either only one or both of these entries.  When a particular handle has no entries left, it must be freed.  gnttab_unmap_grant_ref() loops through its grant unmap request list twice.  It first removes entries from any host pagetables and (if appropraite) iommus; then it does a single domain TLB flush; then it does the clean-up, including telling the granter that entries are no longer being used (if appropriate).  At the moment, it's during the first pass that the maptrack flags are cleared, but the second pass that the maptrack entry is freed.  Unfortunately this allows the following race, which results in a double-free:   A: (pass 1) clear host_map  B: (pass 1) clear device_map  A: (pass 2) See that maptrack entry has no mappings, free it  B: (pass 2) See that maptrack entry has no mappings, free it #  Unfortunately, unlike the active entry pinning update, we can't simply move the maptrack flag changes to the second half, because the maptrack flags are used to determine if iommu entries need to be added: a domain's iommu must never have fewer permissions than the maptrack flags indicate, or a subsequent map_grant_ref() might fail to add the necessary iommu entries.  Instead, free the maptrack entry in the first pass if there are no further mappings.  This is part of XSA-218. ",
        "func_before": "static void\n__gnttab_unmap_common(\n    struct gnttab_unmap_common *op)\n{\n    domid_t          dom;\n    struct domain   *ld, *rd;\n    struct grant_table *lgt, *rgt;\n    struct active_grant_entry *act;\n    s16              rc = 0;\n\n    ld = current->domain;\n    lgt = ld->grant_table;\n\n    op->frame = (unsigned long)(op->dev_bus_addr >> PAGE_SHIFT);\n\n    if ( unlikely(op->handle >= lgt->maptrack_limit) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    op->map = &maptrack_entry(lgt, op->handle);\n\n    grant_read_lock(lgt);\n\n    if ( unlikely(!read_atomic(&op->map->flags)) )\n    {\n        grant_read_unlock(lgt);\n        gdprintk(XENLOG_INFO, \"Zero flags for handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    dom = op->map->domid;\n    grant_read_unlock(lgt);\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(dom)) == NULL) )\n    {\n        /* This can happen when a grant is implicitly unmapped. */\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", dom);\n        domain_crash(ld); /* naughty... */\n        return;\n    }\n\n    rc = xsm_grant_unmapref(XSM_HOOK, ld, rd);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_UNMAP, dom);\n\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n\n    op->flags = read_atomic(&op->map->flags);\n    if ( unlikely(!op->flags) || unlikely(op->map->domid != dom) )\n    {\n        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n        rc = GNTST_bad_handle;\n        goto unmap_out;\n    }\n\n    op->rd = rd;\n    act = active_entry_acquire(rgt, op->map->ref);\n\n    if ( op->frame == 0 )\n    {\n        op->frame = act->frame;\n    }\n    else\n    {\n        if ( unlikely(op->frame != act->frame) )\n            PIN_FAIL(act_release_out, GNTST_general_error,\n                     \"Bad frame number doesn't match gntref. (%lx != %lx)\\n\",\n                     op->frame, act->frame);\n\n        op->map->flags &= ~GNTMAP_device_map;\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( (rc = replace_grant_host_mapping(op->host_addr,\n                                              op->frame, op->new_addr, \n                                              op->flags)) < 0 )\n            goto act_release_out;\n\n        op->map->flags &= ~GNTMAP_host_map;\n    }\n\n act_release_out:\n    active_entry_release(act);\n unmap_out:\n    grant_read_unlock(rgt);\n\n    if ( rc == GNTST_okay && gnttab_need_iommu_mapping(ld) )\n    {\n        unsigned int kind;\n        int err = 0;\n\n        double_gt_lock(lgt, rgt);\n\n        kind = mapkind(lgt, rd, op->frame);\n        if ( !kind )\n            err = iommu_unmap_page(ld, op->frame);\n        else if ( !(kind & MAPKIND_WRITE) )\n            err = iommu_map_page(ld, op->frame, op->frame, IOMMUF_readable);\n\n        double_gt_unlock(lgt, rgt);\n\n        if ( err )\n            rc = GNTST_general_error;\n    }\n\n    /* If just unmapped a writable mapping, mark as dirtied */\n    if ( rc == GNTST_okay && !(op->flags & GNTMAP_readonly) )\n         gnttab_mark_dirty(rd, op->frame);\n\n    op->status = rc;\n    rcu_unlock_domain(rd);\n}",
        "func": "static void\n__gnttab_unmap_common(\n    struct gnttab_unmap_common *op)\n{\n    domid_t          dom;\n    struct domain   *ld, *rd;\n    struct grant_table *lgt, *rgt;\n    struct active_grant_entry *act;\n    s16              rc = 0;\n    struct grant_mapping *map;\n    bool put_handle = false;\n\n    ld = current->domain;\n    lgt = ld->grant_table;\n\n    op->frame = (unsigned long)(op->dev_bus_addr >> PAGE_SHIFT);\n\n    if ( unlikely(op->handle >= lgt->maptrack_limit) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    map = &maptrack_entry(lgt, op->handle);\n\n    grant_read_lock(lgt);\n\n    if ( unlikely(!read_atomic(&map->flags)) )\n    {\n        grant_read_unlock(lgt);\n        gdprintk(XENLOG_INFO, \"Zero flags for handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    dom = map->domid;\n    grant_read_unlock(lgt);\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(dom)) == NULL) )\n    {\n        /* This can happen when a grant is implicitly unmapped. */\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", dom);\n        domain_crash(ld); /* naughty... */\n        return;\n    }\n\n    rc = xsm_grant_unmapref(XSM_HOOK, ld, rd);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_UNMAP, dom);\n\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n\n    op->rd = rd;\n    op->ref = map->ref;\n\n    /*\n     * We can't assume there was no racing unmap for this maptrack entry,\n     * and hence we can't assume map->ref is valid for rd. While the checks\n     * below (with the active entry lock held) will reject any such racing\n     * requests, we still need to make sure we don't attempt to acquire an\n     * invalid lock.\n     */\n    smp_rmb();\n    if ( unlikely(op->ref >= nr_grant_entries(rgt)) )\n    {\n        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n        rc = GNTST_bad_handle;\n        goto unlock_out;\n    }\n\n    act = active_entry_acquire(rgt, op->ref);\n\n    /*\n     * Note that we (ab)use the active entry lock here to protect against\n     * multiple unmaps of the same mapping here. We don't want to hold lgt's\n     * lock, and we only hold rgt's lock for reading (but the latter wouldn't\n     * be the right one anyway). Hence the easiest is to rely on a lock we\n     * hold anyway; see docs/misc/grant-tables.txt's \"Locking\" section.\n     */\n\n    op->flags = read_atomic(&map->flags);\n    smp_rmb();\n    if ( unlikely(!op->flags) || unlikely(map->domid != dom) ||\n         unlikely(map->ref != op->ref) )\n    {\n        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n        rc = GNTST_bad_handle;\n        goto act_release_out;\n    }\n\n    if ( op->frame == 0 )\n    {\n        op->frame = act->frame;\n    }\n    else\n    {\n        if ( unlikely(op->frame != act->frame) )\n            PIN_FAIL(act_release_out, GNTST_general_error,\n                     \"Bad frame number doesn't match gntref. (%lx != %lx)\\n\",\n                     op->frame, act->frame);\n\n        map->flags &= ~GNTMAP_device_map;\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( (rc = replace_grant_host_mapping(op->host_addr,\n                                              op->frame, op->new_addr, \n                                              op->flags)) < 0 )\n            goto act_release_out;\n\n        map->flags &= ~GNTMAP_host_map;\n    }\n\n    if ( !(map->flags & (GNTMAP_device_map|GNTMAP_host_map)) )\n    {\n        map->flags = 0;\n        put_handle = true;\n    }\n\n act_release_out:\n    active_entry_release(act);\n unlock_out:\n    grant_read_unlock(rgt);\n\n    if ( put_handle )\n        put_maptrack_handle(lgt, op->handle);\n\n    if ( rc == GNTST_okay && gnttab_need_iommu_mapping(ld) )\n    {\n        unsigned int kind;\n        int err = 0;\n\n        double_gt_lock(lgt, rgt);\n\n        kind = mapkind(lgt, rd, op->frame);\n        if ( !kind )\n            err = iommu_unmap_page(ld, op->frame);\n        else if ( !(kind & MAPKIND_WRITE) )\n            err = iommu_map_page(ld, op->frame, op->frame, IOMMUF_readable);\n\n        double_gt_unlock(lgt, rgt);\n\n        if ( err )\n            rc = GNTST_general_error;\n    }\n\n    /* If just unmapped a writable mapping, mark as dirtied */\n    if ( rc == GNTST_okay && !(op->flags & GNTMAP_readonly) )\n         gnttab_mark_dirty(rd, op->frame);\n\n    op->status = rc;\n    rcu_unlock_domain(rd);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,8 @@\n     struct grant_table *lgt, *rgt;\n     struct active_grant_entry *act;\n     s16              rc = 0;\n+    struct grant_mapping *map;\n+    bool put_handle = false;\n \n     ld = current->domain;\n     lgt = ld->grant_table;\n@@ -20,11 +22,11 @@\n         return;\n     }\n \n-    op->map = &maptrack_entry(lgt, op->handle);\n+    map = &maptrack_entry(lgt, op->handle);\n \n     grant_read_lock(lgt);\n \n-    if ( unlikely(!read_atomic(&op->map->flags)) )\n+    if ( unlikely(!read_atomic(&map->flags)) )\n     {\n         grant_read_unlock(lgt);\n         gdprintk(XENLOG_INFO, \"Zero flags for handle %#x\\n\", op->handle);\n@@ -32,7 +34,7 @@\n         return;\n     }\n \n-    dom = op->map->domid;\n+    dom = map->domid;\n     grant_read_unlock(lgt);\n \n     if ( unlikely((rd = rcu_lock_domain_by_id(dom)) == NULL) )\n@@ -57,16 +59,43 @@\n \n     grant_read_lock(rgt);\n \n-    op->flags = read_atomic(&op->map->flags);\n-    if ( unlikely(!op->flags) || unlikely(op->map->domid != dom) )\n+    op->rd = rd;\n+    op->ref = map->ref;\n+\n+    /*\n+     * We can't assume there was no racing unmap for this maptrack entry,\n+     * and hence we can't assume map->ref is valid for rd. While the checks\n+     * below (with the active entry lock held) will reject any such racing\n+     * requests, we still need to make sure we don't attempt to acquire an\n+     * invalid lock.\n+     */\n+    smp_rmb();\n+    if ( unlikely(op->ref >= nr_grant_entries(rgt)) )\n     {\n         gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n         rc = GNTST_bad_handle;\n-        goto unmap_out;\n+        goto unlock_out;\n     }\n \n-    op->rd = rd;\n-    act = active_entry_acquire(rgt, op->map->ref);\n+    act = active_entry_acquire(rgt, op->ref);\n+\n+    /*\n+     * Note that we (ab)use the active entry lock here to protect against\n+     * multiple unmaps of the same mapping here. We don't want to hold lgt's\n+     * lock, and we only hold rgt's lock for reading (but the latter wouldn't\n+     * be the right one anyway). Hence the easiest is to rely on a lock we\n+     * hold anyway; see docs/misc/grant-tables.txt's \"Locking\" section.\n+     */\n+\n+    op->flags = read_atomic(&map->flags);\n+    smp_rmb();\n+    if ( unlikely(!op->flags) || unlikely(map->domid != dom) ||\n+         unlikely(map->ref != op->ref) )\n+    {\n+        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n+        rc = GNTST_bad_handle;\n+        goto act_release_out;\n+    }\n \n     if ( op->frame == 0 )\n     {\n@@ -79,7 +108,7 @@\n                      \"Bad frame number doesn't match gntref. (%lx != %lx)\\n\",\n                      op->frame, act->frame);\n \n-        op->map->flags &= ~GNTMAP_device_map;\n+        map->flags &= ~GNTMAP_device_map;\n     }\n \n     if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n@@ -89,13 +118,22 @@\n                                               op->flags)) < 0 )\n             goto act_release_out;\n \n-        op->map->flags &= ~GNTMAP_host_map;\n+        map->flags &= ~GNTMAP_host_map;\n+    }\n+\n+    if ( !(map->flags & (GNTMAP_device_map|GNTMAP_host_map)) )\n+    {\n+        map->flags = 0;\n+        put_handle = true;\n     }\n \n  act_release_out:\n     active_entry_release(act);\n- unmap_out:\n+ unlock_out:\n     grant_read_unlock(rgt);\n+\n+    if ( put_handle )\n+        put_maptrack_handle(lgt, op->handle);\n \n     if ( rc == GNTST_okay && gnttab_need_iommu_mapping(ld) )\n     {",
        "diff_line_info": {
            "deleted_lines": [
                "    op->map = &maptrack_entry(lgt, op->handle);",
                "    if ( unlikely(!read_atomic(&op->map->flags)) )",
                "    dom = op->map->domid;",
                "    op->flags = read_atomic(&op->map->flags);",
                "    if ( unlikely(!op->flags) || unlikely(op->map->domid != dom) )",
                "        goto unmap_out;",
                "    op->rd = rd;",
                "    act = active_entry_acquire(rgt, op->map->ref);",
                "        op->map->flags &= ~GNTMAP_device_map;",
                "        op->map->flags &= ~GNTMAP_host_map;",
                " unmap_out:"
            ],
            "added_lines": [
                "    struct grant_mapping *map;",
                "    bool put_handle = false;",
                "    map = &maptrack_entry(lgt, op->handle);",
                "    if ( unlikely(!read_atomic(&map->flags)) )",
                "    dom = map->domid;",
                "    op->rd = rd;",
                "    op->ref = map->ref;",
                "",
                "    /*",
                "     * We can't assume there was no racing unmap for this maptrack entry,",
                "     * and hence we can't assume map->ref is valid for rd. While the checks",
                "     * below (with the active entry lock held) will reject any such racing",
                "     * requests, we still need to make sure we don't attempt to acquire an",
                "     * invalid lock.",
                "     */",
                "    smp_rmb();",
                "    if ( unlikely(op->ref >= nr_grant_entries(rgt)) )",
                "        goto unlock_out;",
                "    act = active_entry_acquire(rgt, op->ref);",
                "",
                "    /*",
                "     * Note that we (ab)use the active entry lock here to protect against",
                "     * multiple unmaps of the same mapping here. We don't want to hold lgt's",
                "     * lock, and we only hold rgt's lock for reading (but the latter wouldn't",
                "     * be the right one anyway). Hence the easiest is to rely on a lock we",
                "     * hold anyway; see docs/misc/grant-tables.txt's \"Locking\" section.",
                "     */",
                "",
                "    op->flags = read_atomic(&map->flags);",
                "    smp_rmb();",
                "    if ( unlikely(!op->flags) || unlikely(map->domid != dom) ||",
                "         unlikely(map->ref != op->ref) )",
                "    {",
                "        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);",
                "        rc = GNTST_bad_handle;",
                "        goto act_release_out;",
                "    }",
                "        map->flags &= ~GNTMAP_device_map;",
                "        map->flags &= ~GNTMAP_host_map;",
                "    }",
                "",
                "    if ( !(map->flags & (GNTMAP_device_map|GNTMAP_host_map)) )",
                "    {",
                "        map->flags = 0;",
                "        put_handle = true;",
                " unlock_out:",
                "",
                "    if ( put_handle )",
                "        put_maptrack_handle(lgt, op->handle);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-10913",
        "func_name": "xen-project/xen/mapkind",
        "description": "The grant-table feature in Xen through 4.8.x provides false mapping information in certain cases of concurrent unmap calls, which allows backend attackers to obtain sensitive information or gain privileges, aka XSA-218 bug 1.",
        "git_url": "https://github.com/xen-project/xen/commit/4b78efa91c8ae3c42e14b8eaeaad773c5eb3b71a",
        "commit_title": "gnttab: correct maptrack table accesses",
        "commit_text": " In order to observe a consistent (limit,pointer-table) pair, the reader needs to either hold the maptrack lock (in line with documentation) or both sides need to order their accesses suitably (the writer side barrier was removed by commit dff515dfea [\"gnttab: use per-VCPU maptrack free lists\"], and a read side barrier has never been there).  Make the writer publish a new table page before limit (for bounds checks to work), and new list head last (for racing maptrack_entry() invocations to work). At the same time add read barriers to lockless readers.  Additionally get_maptrack_handle() must not assume ->maptrack_head to not change behind its back: Another handle may be put (updating only ->maptrack_tail) and then got or stolen (updating ->maptrack_head).  This is part of XSA-218. ",
        "func_before": "static unsigned int mapkind(\n    struct grant_table *lgt, const struct domain *rd, unsigned long mfn)\n{\n    struct grant_mapping *map;\n    grant_handle_t handle;\n    unsigned int kind = 0;\n\n    /*\n     * Must have the local domain's grant table write lock when\n     * iterating over its maptrack entries.\n     */\n    ASSERT(percpu_rw_is_write_locked(&lgt->lock));\n    /*\n     * Must have the remote domain's grant table write lock while\n     * counting its active entries.\n     */\n    ASSERT(percpu_rw_is_write_locked(&rd->grant_table->lock));\n\n    for ( handle = 0; !(kind & MAPKIND_WRITE) &&\n                      handle < lgt->maptrack_limit; handle++ )\n    {\n        map = &maptrack_entry(lgt, handle);\n        if ( !(map->flags & (GNTMAP_device_map|GNTMAP_host_map)) ||\n             map->domid != rd->domain_id )\n            continue;\n        if ( _active_entry(rd->grant_table, map->ref).frame == mfn )\n            kind |= map->flags & GNTMAP_readonly ?\n                    MAPKIND_READ : MAPKIND_WRITE;\n    }\n\n    return kind;\n}",
        "func": "static unsigned int mapkind(\n    struct grant_table *lgt, const struct domain *rd, unsigned long mfn)\n{\n    struct grant_mapping *map;\n    grant_handle_t handle;\n    unsigned int kind = 0;\n\n    /*\n     * Must have the local domain's grant table write lock when\n     * iterating over its maptrack entries.\n     */\n    ASSERT(percpu_rw_is_write_locked(&lgt->lock));\n    /*\n     * Must have the remote domain's grant table write lock while\n     * counting its active entries.\n     */\n    ASSERT(percpu_rw_is_write_locked(&rd->grant_table->lock));\n\n    for ( handle = 0; !(kind & MAPKIND_WRITE) &&\n                      handle < lgt->maptrack_limit; handle++ )\n    {\n        smp_rmb();\n        map = &maptrack_entry(lgt, handle);\n        if ( !(map->flags & (GNTMAP_device_map|GNTMAP_host_map)) ||\n             map->domid != rd->domain_id )\n            continue;\n        if ( _active_entry(rd->grant_table, map->ref).frame == mfn )\n            kind |= map->flags & GNTMAP_readonly ?\n                    MAPKIND_READ : MAPKIND_WRITE;\n    }\n\n    return kind;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,6 +19,7 @@\n     for ( handle = 0; !(kind & MAPKIND_WRITE) &&\n                       handle < lgt->maptrack_limit; handle++ )\n     {\n+        smp_rmb();\n         map = &maptrack_entry(lgt, handle);\n         if ( !(map->flags & (GNTMAP_device_map|GNTMAP_host_map)) ||\n              map->domid != rd->domain_id )",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        smp_rmb();"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-10913",
        "func_name": "xen-project/xen/__gnttab_unmap_common",
        "description": "The grant-table feature in Xen through 4.8.x provides false mapping information in certain cases of concurrent unmap calls, which allows backend attackers to obtain sensitive information or gain privileges, aka XSA-218 bug 1.",
        "git_url": "https://github.com/xen-project/xen/commit/4b78efa91c8ae3c42e14b8eaeaad773c5eb3b71a",
        "commit_title": "gnttab: correct maptrack table accesses",
        "commit_text": " In order to observe a consistent (limit,pointer-table) pair, the reader needs to either hold the maptrack lock (in line with documentation) or both sides need to order their accesses suitably (the writer side barrier was removed by commit dff515dfea [\"gnttab: use per-VCPU maptrack free lists\"], and a read side barrier has never been there).  Make the writer publish a new table page before limit (for bounds checks to work), and new list head last (for racing maptrack_entry() invocations to work). At the same time add read barriers to lockless readers.  Additionally get_maptrack_handle() must not assume ->maptrack_head to not change behind its back: Another handle may be put (updating only ->maptrack_tail) and then got or stolen (updating ->maptrack_head).  This is part of XSA-218. ",
        "func_before": "static void\n__gnttab_unmap_common(\n    struct gnttab_unmap_common *op)\n{\n    domid_t          dom;\n    struct domain   *ld, *rd;\n    struct grant_table *lgt, *rgt;\n    struct active_grant_entry *act;\n    s16              rc = 0;\n    struct grant_mapping *map;\n    bool put_handle = false;\n\n    ld = current->domain;\n    lgt = ld->grant_table;\n\n    op->frame = (unsigned long)(op->dev_bus_addr >> PAGE_SHIFT);\n\n    if ( unlikely(op->handle >= lgt->maptrack_limit) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    map = &maptrack_entry(lgt, op->handle);\n\n    grant_read_lock(lgt);\n\n    if ( unlikely(!read_atomic(&map->flags)) )\n    {\n        grant_read_unlock(lgt);\n        gdprintk(XENLOG_INFO, \"Zero flags for handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    dom = map->domid;\n    grant_read_unlock(lgt);\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(dom)) == NULL) )\n    {\n        /* This can happen when a grant is implicitly unmapped. */\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", dom);\n        domain_crash(ld); /* naughty... */\n        return;\n    }\n\n    rc = xsm_grant_unmapref(XSM_HOOK, ld, rd);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_UNMAP, dom);\n\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n\n    op->rd = rd;\n    op->ref = map->ref;\n\n    /*\n     * We can't assume there was no racing unmap for this maptrack entry,\n     * and hence we can't assume map->ref is valid for rd. While the checks\n     * below (with the active entry lock held) will reject any such racing\n     * requests, we still need to make sure we don't attempt to acquire an\n     * invalid lock.\n     */\n    smp_rmb();\n    if ( unlikely(op->ref >= nr_grant_entries(rgt)) )\n    {\n        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n        rc = GNTST_bad_handle;\n        goto unlock_out;\n    }\n\n    act = active_entry_acquire(rgt, op->ref);\n\n    /*\n     * Note that we (ab)use the active entry lock here to protect against\n     * multiple unmaps of the same mapping here. We don't want to hold lgt's\n     * lock, and we only hold rgt's lock for reading (but the latter wouldn't\n     * be the right one anyway). Hence the easiest is to rely on a lock we\n     * hold anyway; see docs/misc/grant-tables.txt's \"Locking\" section.\n     */\n\n    op->flags = read_atomic(&map->flags);\n    smp_rmb();\n    if ( unlikely(!op->flags) || unlikely(map->domid != dom) ||\n         unlikely(map->ref != op->ref) )\n    {\n        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n        rc = GNTST_bad_handle;\n        goto act_release_out;\n    }\n\n    if ( op->frame == 0 )\n    {\n        op->frame = act->frame;\n    }\n    else\n    {\n        if ( unlikely(op->frame != act->frame) )\n            PIN_FAIL(act_release_out, GNTST_general_error,\n                     \"Bad frame number doesn't match gntref. (%lx != %lx)\\n\",\n                     op->frame, act->frame);\n\n        map->flags &= ~GNTMAP_device_map;\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( (rc = replace_grant_host_mapping(op->host_addr,\n                                              op->frame, op->new_addr, \n                                              op->flags)) < 0 )\n            goto act_release_out;\n\n        map->flags &= ~GNTMAP_host_map;\n    }\n\n    if ( !(map->flags & (GNTMAP_device_map|GNTMAP_host_map)) )\n    {\n        map->flags = 0;\n        put_handle = true;\n    }\n\n act_release_out:\n    active_entry_release(act);\n unlock_out:\n    grant_read_unlock(rgt);\n\n    if ( put_handle )\n        put_maptrack_handle(lgt, op->handle);\n\n    if ( rc == GNTST_okay && gnttab_need_iommu_mapping(ld) )\n    {\n        unsigned int kind;\n        int err = 0;\n\n        double_gt_lock(lgt, rgt);\n\n        kind = mapkind(lgt, rd, op->frame);\n        if ( !kind )\n            err = iommu_unmap_page(ld, op->frame);\n        else if ( !(kind & MAPKIND_WRITE) )\n            err = iommu_map_page(ld, op->frame, op->frame, IOMMUF_readable);\n\n        double_gt_unlock(lgt, rgt);\n\n        if ( err )\n            rc = GNTST_general_error;\n    }\n\n    /* If just unmapped a writable mapping, mark as dirtied */\n    if ( rc == GNTST_okay && !(op->flags & GNTMAP_readonly) )\n         gnttab_mark_dirty(rd, op->frame);\n\n    op->status = rc;\n    rcu_unlock_domain(rd);\n}",
        "func": "static void\n__gnttab_unmap_common(\n    struct gnttab_unmap_common *op)\n{\n    domid_t          dom;\n    struct domain   *ld, *rd;\n    struct grant_table *lgt, *rgt;\n    struct active_grant_entry *act;\n    s16              rc = 0;\n    struct grant_mapping *map;\n    bool put_handle = false;\n\n    ld = current->domain;\n    lgt = ld->grant_table;\n\n    op->frame = (unsigned long)(op->dev_bus_addr >> PAGE_SHIFT);\n\n    if ( unlikely(op->handle >= lgt->maptrack_limit) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    smp_rmb();\n    map = &maptrack_entry(lgt, op->handle);\n\n    grant_read_lock(lgt);\n\n    if ( unlikely(!read_atomic(&map->flags)) )\n    {\n        grant_read_unlock(lgt);\n        gdprintk(XENLOG_INFO, \"Zero flags for handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    dom = map->domid;\n    grant_read_unlock(lgt);\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(dom)) == NULL) )\n    {\n        /* This can happen when a grant is implicitly unmapped. */\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", dom);\n        domain_crash(ld); /* naughty... */\n        return;\n    }\n\n    rc = xsm_grant_unmapref(XSM_HOOK, ld, rd);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_UNMAP, dom);\n\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n\n    op->rd = rd;\n    op->ref = map->ref;\n\n    /*\n     * We can't assume there was no racing unmap for this maptrack entry,\n     * and hence we can't assume map->ref is valid for rd. While the checks\n     * below (with the active entry lock held) will reject any such racing\n     * requests, we still need to make sure we don't attempt to acquire an\n     * invalid lock.\n     */\n    smp_rmb();\n    if ( unlikely(op->ref >= nr_grant_entries(rgt)) )\n    {\n        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n        rc = GNTST_bad_handle;\n        goto unlock_out;\n    }\n\n    act = active_entry_acquire(rgt, op->ref);\n\n    /*\n     * Note that we (ab)use the active entry lock here to protect against\n     * multiple unmaps of the same mapping here. We don't want to hold lgt's\n     * lock, and we only hold rgt's lock for reading (but the latter wouldn't\n     * be the right one anyway). Hence the easiest is to rely on a lock we\n     * hold anyway; see docs/misc/grant-tables.txt's \"Locking\" section.\n     */\n\n    op->flags = read_atomic(&map->flags);\n    smp_rmb();\n    if ( unlikely(!op->flags) || unlikely(map->domid != dom) ||\n         unlikely(map->ref != op->ref) )\n    {\n        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n        rc = GNTST_bad_handle;\n        goto act_release_out;\n    }\n\n    if ( op->frame == 0 )\n    {\n        op->frame = act->frame;\n    }\n    else\n    {\n        if ( unlikely(op->frame != act->frame) )\n            PIN_FAIL(act_release_out, GNTST_general_error,\n                     \"Bad frame number doesn't match gntref. (%lx != %lx)\\n\",\n                     op->frame, act->frame);\n\n        map->flags &= ~GNTMAP_device_map;\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( (rc = replace_grant_host_mapping(op->host_addr,\n                                              op->frame, op->new_addr, \n                                              op->flags)) < 0 )\n            goto act_release_out;\n\n        map->flags &= ~GNTMAP_host_map;\n    }\n\n    if ( !(map->flags & (GNTMAP_device_map|GNTMAP_host_map)) )\n    {\n        map->flags = 0;\n        put_handle = true;\n    }\n\n act_release_out:\n    active_entry_release(act);\n unlock_out:\n    grant_read_unlock(rgt);\n\n    if ( put_handle )\n        put_maptrack_handle(lgt, op->handle);\n\n    if ( rc == GNTST_okay && gnttab_need_iommu_mapping(ld) )\n    {\n        unsigned int kind;\n        int err = 0;\n\n        double_gt_lock(lgt, rgt);\n\n        kind = mapkind(lgt, rd, op->frame);\n        if ( !kind )\n            err = iommu_unmap_page(ld, op->frame);\n        else if ( !(kind & MAPKIND_WRITE) )\n            err = iommu_map_page(ld, op->frame, op->frame, IOMMUF_readable);\n\n        double_gt_unlock(lgt, rgt);\n\n        if ( err )\n            rc = GNTST_general_error;\n    }\n\n    /* If just unmapped a writable mapping, mark as dirtied */\n    if ( rc == GNTST_okay && !(op->flags & GNTMAP_readonly) )\n         gnttab_mark_dirty(rd, op->frame);\n\n    op->status = rc;\n    rcu_unlock_domain(rd);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,6 +22,7 @@\n         return;\n     }\n \n+    smp_rmb();\n     map = &maptrack_entry(lgt, op->handle);\n \n     grant_read_lock(lgt);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    smp_rmb();"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-10913",
        "func_name": "xen-project/xen/get_maptrack_handle",
        "description": "The grant-table feature in Xen through 4.8.x provides false mapping information in certain cases of concurrent unmap calls, which allows backend attackers to obtain sensitive information or gain privileges, aka XSA-218 bug 1.",
        "git_url": "https://github.com/xen-project/xen/commit/4b78efa91c8ae3c42e14b8eaeaad773c5eb3b71a",
        "commit_title": "gnttab: correct maptrack table accesses",
        "commit_text": " In order to observe a consistent (limit,pointer-table) pair, the reader needs to either hold the maptrack lock (in line with documentation) or both sides need to order their accesses suitably (the writer side barrier was removed by commit dff515dfea [\"gnttab: use per-VCPU maptrack free lists\"], and a read side barrier has never been there).  Make the writer publish a new table page before limit (for bounds checks to work), and new list head last (for racing maptrack_entry() invocations to work). At the same time add read barriers to lockless readers.  Additionally get_maptrack_handle() must not assume ->maptrack_head to not change behind its back: Another handle may be put (updating only ->maptrack_tail) and then got or stolen (updating ->maptrack_head).  This is part of XSA-218. ",
        "func_before": "static inline int\nget_maptrack_handle(\n    struct grant_table *lgt)\n{\n    struct vcpu          *curr = current;\n    int                   i;\n    grant_handle_t        handle;\n    struct grant_mapping *new_mt;\n\n    handle = __get_maptrack_handle(lgt, curr);\n    if ( likely(handle != -1) )\n        return handle;\n\n    spin_lock(&lgt->maptrack_lock);\n\n    /*\n     * If we've run out of frames, try stealing an entry from another\n     * VCPU (in case the guest isn't mapping across its VCPUs evenly).\n     */\n    if ( nr_maptrack_frames(lgt) >= max_maptrack_frames )\n    {\n        /*\n         * Can drop the lock since no other VCPU can be adding a new\n         * frame once they've run out.\n         */\n        spin_unlock(&lgt->maptrack_lock);\n\n        /*\n         * Uninitialized free list? Steal an extra entry for the tail\n         * sentinel.\n         */\n        if ( curr->maptrack_tail == MAPTRACK_TAIL )\n        {\n            handle = steal_maptrack_handle(lgt, curr);\n            if ( handle == -1 )\n                return -1;\n            curr->maptrack_tail = handle;\n            write_atomic(&curr->maptrack_head, handle);\n        }\n        return steal_maptrack_handle(lgt, curr);\n    }\n\n    new_mt = alloc_xenheap_page();\n    if ( !new_mt )\n    {\n        spin_unlock(&lgt->maptrack_lock);\n        return -1;\n    }\n    clear_page(new_mt);\n\n    /*\n     * Use the first new entry and add the remaining entries to the\n     * head of the free list.\n     */\n    handle = lgt->maptrack_limit;\n\n    for ( i = 0; i < MAPTRACK_PER_PAGE; i++ )\n    {\n        new_mt[i].ref = handle + i + 1;\n        new_mt[i].vcpu = curr->vcpu_id;\n    }\n    new_mt[i - 1].ref = curr->maptrack_head;\n\n    /* Set tail directly if this is the first page for this VCPU. */\n    if ( curr->maptrack_tail == MAPTRACK_TAIL )\n        curr->maptrack_tail = handle + MAPTRACK_PER_PAGE - 1;\n\n    write_atomic(&curr->maptrack_head, handle + 1);\n\n    lgt->maptrack[nr_maptrack_frames(lgt)] = new_mt;\n    lgt->maptrack_limit += MAPTRACK_PER_PAGE;\n\n    spin_unlock(&lgt->maptrack_lock);\n\n    return handle;\n}",
        "func": "static inline int\nget_maptrack_handle(\n    struct grant_table *lgt)\n{\n    struct vcpu          *curr = current;\n    unsigned int          i, head;\n    grant_handle_t        handle;\n    struct grant_mapping *new_mt;\n\n    handle = __get_maptrack_handle(lgt, curr);\n    if ( likely(handle != -1) )\n        return handle;\n\n    spin_lock(&lgt->maptrack_lock);\n\n    /*\n     * If we've run out of frames, try stealing an entry from another\n     * VCPU (in case the guest isn't mapping across its VCPUs evenly).\n     */\n    if ( nr_maptrack_frames(lgt) >= max_maptrack_frames )\n    {\n        /*\n         * Can drop the lock since no other VCPU can be adding a new\n         * frame once they've run out.\n         */\n        spin_unlock(&lgt->maptrack_lock);\n\n        /*\n         * Uninitialized free list? Steal an extra entry for the tail\n         * sentinel.\n         */\n        if ( curr->maptrack_tail == MAPTRACK_TAIL )\n        {\n            handle = steal_maptrack_handle(lgt, curr);\n            if ( handle == -1 )\n                return -1;\n            curr->maptrack_tail = handle;\n            write_atomic(&curr->maptrack_head, handle);\n        }\n        return steal_maptrack_handle(lgt, curr);\n    }\n\n    new_mt = alloc_xenheap_page();\n    if ( !new_mt )\n    {\n        spin_unlock(&lgt->maptrack_lock);\n        return -1;\n    }\n    clear_page(new_mt);\n\n    /*\n     * Use the first new entry and add the remaining entries to the\n     * head of the free list.\n     */\n    handle = lgt->maptrack_limit;\n\n    for ( i = 0; i < MAPTRACK_PER_PAGE; i++ )\n    {\n        new_mt[i].ref = handle + i + 1;\n        new_mt[i].vcpu = curr->vcpu_id;\n    }\n\n    /* Set tail directly if this is the first page for this VCPU. */\n    if ( curr->maptrack_tail == MAPTRACK_TAIL )\n        curr->maptrack_tail = handle + MAPTRACK_PER_PAGE - 1;\n\n    lgt->maptrack[nr_maptrack_frames(lgt)] = new_mt;\n    smp_wmb();\n    lgt->maptrack_limit += MAPTRACK_PER_PAGE;\n\n    do {\n        new_mt[i - 1].ref = read_atomic(&curr->maptrack_head);\n        head = cmpxchg(&curr->maptrack_head, new_mt[i - 1].ref, handle + 1);\n    } while ( head != new_mt[i - 1].ref );\n\n    spin_unlock(&lgt->maptrack_lock);\n\n    return handle;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n     struct grant_table *lgt)\n {\n     struct vcpu          *curr = current;\n-    int                   i;\n+    unsigned int          i, head;\n     grant_handle_t        handle;\n     struct grant_mapping *new_mt;\n \n@@ -59,16 +59,19 @@\n         new_mt[i].ref = handle + i + 1;\n         new_mt[i].vcpu = curr->vcpu_id;\n     }\n-    new_mt[i - 1].ref = curr->maptrack_head;\n \n     /* Set tail directly if this is the first page for this VCPU. */\n     if ( curr->maptrack_tail == MAPTRACK_TAIL )\n         curr->maptrack_tail = handle + MAPTRACK_PER_PAGE - 1;\n \n-    write_atomic(&curr->maptrack_head, handle + 1);\n+    lgt->maptrack[nr_maptrack_frames(lgt)] = new_mt;\n+    smp_wmb();\n+    lgt->maptrack_limit += MAPTRACK_PER_PAGE;\n \n-    lgt->maptrack[nr_maptrack_frames(lgt)] = new_mt;\n-    lgt->maptrack_limit += MAPTRACK_PER_PAGE;\n+    do {\n+        new_mt[i - 1].ref = read_atomic(&curr->maptrack_head);\n+        head = cmpxchg(&curr->maptrack_head, new_mt[i - 1].ref, handle + 1);\n+    } while ( head != new_mt[i - 1].ref );\n \n     spin_unlock(&lgt->maptrack_lock);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    int                   i;",
                "    new_mt[i - 1].ref = curr->maptrack_head;",
                "    write_atomic(&curr->maptrack_head, handle + 1);",
                "    lgt->maptrack[nr_maptrack_frames(lgt)] = new_mt;",
                "    lgt->maptrack_limit += MAPTRACK_PER_PAGE;"
            ],
            "added_lines": [
                "    unsigned int          i, head;",
                "    lgt->maptrack[nr_maptrack_frames(lgt)] = new_mt;",
                "    smp_wmb();",
                "    lgt->maptrack_limit += MAPTRACK_PER_PAGE;",
                "    do {",
                "        new_mt[i - 1].ref = read_atomic(&curr->maptrack_head);",
                "        head = cmpxchg(&curr->maptrack_head, new_mt[i - 1].ref, handle + 1);",
                "    } while ( head != new_mt[i - 1].ref );"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-10913",
        "func_name": "xen-project/xen/__gnttab_unmap_common_complete",
        "description": "The grant-table feature in Xen through 4.8.x provides false mapping information in certain cases of concurrent unmap calls, which allows backend attackers to obtain sensitive information or gain privileges, aka XSA-218 bug 1.",
        "git_url": "https://github.com/xen-project/xen/commit/9a0bd460cfc28564d39fa23541bb872b13e7f7ea",
        "commit_title": "gnttab: fix unmap pin accounting race",
        "commit_text": " Once all {writable} mappings of a grant entry have been unmapped, the hypervisor informs the guest that the grant entry has been released by clearing the _GTF_{reading,writing} usage flags in the guest's grant table as appropriate.  Unfortunately, at the moment, the code that updates the accounting happens in a different critical section than the one which updates the usage flags; this means that under the right circumstances, there may be a window in time after the hypervisor reported the grant as being free during which the grant referee still had access to the page.  Move the grant accounting code into the same critical section as the reporting code to make sure this kind of race can't happen.  This is part of XSA-218. ",
        "func_before": "static void\n__gnttab_unmap_common_complete(struct gnttab_unmap_common *op)\n{\n    struct domain *ld, *rd = op->rd;\n    struct grant_table *rgt;\n    struct active_grant_entry *act;\n    grant_entry_header_t *sha;\n    struct page_info *pg;\n    uint16_t *status;\n    bool_t put_handle = 0;\n\n    if ( rd == NULL )\n    { \n        /*\n         * Suggests that __gntab_unmap_common failed in\n         * rcu_lock_domain_by_id() or earlier, and so we have nothing\n         * to complete\n         */\n        return;\n    }\n\n    ld = current->domain;\n\n    rcu_lock_domain(rd);\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n    if ( rgt->gt_version == 0 )\n        goto unlock_out;\n\n    act = active_entry_acquire(rgt, op->map->ref);\n    sha = shared_entry_header(rgt, op->map->ref);\n\n    if ( rgt->gt_version == 1 )\n        status = &sha->flags;\n    else\n        status = &status_entry(rgt, op->map->ref);\n\n    if ( unlikely(op->frame != act->frame) ) \n    {\n        /*\n         * Suggests that __gntab_unmap_common failed early and so\n         * nothing further to do\n         */\n        goto act_release_out;\n    }\n\n    pg = mfn_to_page(op->frame);\n\n    if ( op->flags & GNTMAP_device_map ) \n    {\n        if ( !is_iomem_page(_mfn(act->frame)) )\n        {\n            if ( op->flags & GNTMAP_readonly )\n                put_page(pg);\n            else\n                put_page_and_type(pg);\n        }\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( op->status != 0 ) \n        {\n            /*\n             * Suggests that __gntab_unmap_common failed in\n             * replace_grant_host_mapping() so nothing further to do\n             */\n            goto act_release_out;\n        }\n\n        if ( !is_iomem_page(_mfn(op->frame)) )\n        {\n            if ( gnttab_host_mapping_get_page_type(op, ld, rd) )\n                put_page_type(pg);\n            put_page(pg);\n        }\n    }\n\n    if ( (op->map->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0 )\n        put_handle = 1;\n\n    if ( ((act->pin & (GNTPIN_devw_mask|GNTPIN_hstw_mask)) == 0) &&\n         !(op->flags & GNTMAP_readonly) )\n        gnttab_clear_flag(_GTF_writing, status);\n\n    if ( act->pin == 0 )\n        gnttab_clear_flag(_GTF_reading, status);\n\n act_release_out:\n    active_entry_release(act);\n unlock_out:\n    grant_read_unlock(rgt);\n\n    if ( put_handle )\n    {\n        op->map->flags = 0;\n        put_maptrack_handle(ld->grant_table, op->handle);\n    }\n    rcu_unlock_domain(rd);\n}",
        "func": "static void\n__gnttab_unmap_common_complete(struct gnttab_unmap_common *op)\n{\n    struct domain *ld, *rd = op->rd;\n    struct grant_table *rgt;\n    struct active_grant_entry *act;\n    grant_entry_header_t *sha;\n    struct page_info *pg;\n    uint16_t *status;\n    bool_t put_handle = 0;\n\n    if ( rd == NULL )\n    { \n        /*\n         * Suggests that __gntab_unmap_common failed in\n         * rcu_lock_domain_by_id() or earlier, and so we have nothing\n         * to complete\n         */\n        return;\n    }\n\n    ld = current->domain;\n\n    rcu_lock_domain(rd);\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n    if ( rgt->gt_version == 0 )\n        goto unlock_out;\n\n    act = active_entry_acquire(rgt, op->map->ref);\n    sha = shared_entry_header(rgt, op->map->ref);\n\n    if ( rgt->gt_version == 1 )\n        status = &sha->flags;\n    else\n        status = &status_entry(rgt, op->map->ref);\n\n    if ( unlikely(op->frame != act->frame) ) \n    {\n        /*\n         * Suggests that __gntab_unmap_common failed early and so\n         * nothing further to do\n         */\n        goto act_release_out;\n    }\n\n    pg = mfn_to_page(op->frame);\n\n    if ( op->flags & GNTMAP_device_map ) \n    {\n        if ( !is_iomem_page(_mfn(act->frame)) )\n        {\n            if ( op->flags & GNTMAP_readonly )\n                put_page(pg);\n            else\n                put_page_and_type(pg);\n        }\n\n        ASSERT(act->pin & (GNTPIN_devw_mask | GNTPIN_devr_mask));\n        if ( op->flags & GNTMAP_readonly )\n            act->pin -= GNTPIN_devr_inc;\n        else\n            act->pin -= GNTPIN_devw_inc;\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( op->status != 0 ) \n        {\n            /*\n             * Suggests that __gntab_unmap_common failed in\n             * replace_grant_host_mapping() or IOMMU handling, so nothing\n             * further to do (short of re-establishing the mapping in the\n             * latter case).\n             */\n            goto act_release_out;\n        }\n\n        if ( !is_iomem_page(_mfn(op->frame)) )\n        {\n            if ( gnttab_host_mapping_get_page_type(op, ld, rd) )\n                put_page_type(pg);\n            put_page(pg);\n        }\n\n        ASSERT(act->pin & (GNTPIN_hstw_mask | GNTPIN_hstr_mask));\n        if ( op->flags & GNTMAP_readonly )\n            act->pin -= GNTPIN_hstr_inc;\n        else\n            act->pin -= GNTPIN_hstw_inc;\n    }\n\n    if ( (op->map->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0 )\n        put_handle = 1;\n\n    if ( ((act->pin & (GNTPIN_devw_mask|GNTPIN_hstw_mask)) == 0) &&\n         !(op->flags & GNTMAP_readonly) )\n        gnttab_clear_flag(_GTF_writing, status);\n\n    if ( act->pin == 0 )\n        gnttab_clear_flag(_GTF_reading, status);\n\n act_release_out:\n    active_entry_release(act);\n unlock_out:\n    grant_read_unlock(rgt);\n\n    if ( put_handle )\n    {\n        op->map->flags = 0;\n        put_maptrack_handle(ld->grant_table, op->handle);\n    }\n    rcu_unlock_domain(rd);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,6 +56,12 @@\n             else\n                 put_page_and_type(pg);\n         }\n+\n+        ASSERT(act->pin & (GNTPIN_devw_mask | GNTPIN_devr_mask));\n+        if ( op->flags & GNTMAP_readonly )\n+            act->pin -= GNTPIN_devr_inc;\n+        else\n+            act->pin -= GNTPIN_devw_inc;\n     }\n \n     if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n@@ -64,7 +70,9 @@\n         {\n             /*\n              * Suggests that __gntab_unmap_common failed in\n-             * replace_grant_host_mapping() so nothing further to do\n+             * replace_grant_host_mapping() or IOMMU handling, so nothing\n+             * further to do (short of re-establishing the mapping in the\n+             * latter case).\n              */\n             goto act_release_out;\n         }\n@@ -75,6 +83,12 @@\n                 put_page_type(pg);\n             put_page(pg);\n         }\n+\n+        ASSERT(act->pin & (GNTPIN_hstw_mask | GNTPIN_hstr_mask));\n+        if ( op->flags & GNTMAP_readonly )\n+            act->pin -= GNTPIN_hstr_inc;\n+        else\n+            act->pin -= GNTPIN_hstw_inc;\n     }\n \n     if ( (op->map->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0 )",
        "diff_line_info": {
            "deleted_lines": [
                "             * replace_grant_host_mapping() so nothing further to do"
            ],
            "added_lines": [
                "",
                "        ASSERT(act->pin & (GNTPIN_devw_mask | GNTPIN_devr_mask));",
                "        if ( op->flags & GNTMAP_readonly )",
                "            act->pin -= GNTPIN_devr_inc;",
                "        else",
                "            act->pin -= GNTPIN_devw_inc;",
                "             * replace_grant_host_mapping() or IOMMU handling, so nothing",
                "             * further to do (short of re-establishing the mapping in the",
                "             * latter case).",
                "",
                "        ASSERT(act->pin & (GNTPIN_hstw_mask | GNTPIN_hstr_mask));",
                "        if ( op->flags & GNTMAP_readonly )",
                "            act->pin -= GNTPIN_hstr_inc;",
                "        else",
                "            act->pin -= GNTPIN_hstw_inc;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-10913",
        "func_name": "xen-project/xen/__gnttab_unmap_common",
        "description": "The grant-table feature in Xen through 4.8.x provides false mapping information in certain cases of concurrent unmap calls, which allows backend attackers to obtain sensitive information or gain privileges, aka XSA-218 bug 1.",
        "git_url": "https://github.com/xen-project/xen/commit/9a0bd460cfc28564d39fa23541bb872b13e7f7ea",
        "commit_title": "gnttab: fix unmap pin accounting race",
        "commit_text": " Once all {writable} mappings of a grant entry have been unmapped, the hypervisor informs the guest that the grant entry has been released by clearing the _GTF_{reading,writing} usage flags in the guest's grant table as appropriate.  Unfortunately, at the moment, the code that updates the accounting happens in a different critical section than the one which updates the usage flags; this means that under the right circumstances, there may be a window in time after the hypervisor reported the grant as being free during which the grant referee still had access to the page.  Move the grant accounting code into the same critical section as the reporting code to make sure this kind of race can't happen.  This is part of XSA-218. ",
        "func_before": "static void\n__gnttab_unmap_common(\n    struct gnttab_unmap_common *op)\n{\n    domid_t          dom;\n    struct domain   *ld, *rd;\n    struct grant_table *lgt, *rgt;\n    struct active_grant_entry *act;\n    s16              rc = 0;\n\n    ld = current->domain;\n    lgt = ld->grant_table;\n\n    op->frame = (unsigned long)(op->dev_bus_addr >> PAGE_SHIFT);\n\n    if ( unlikely(op->handle >= lgt->maptrack_limit) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    op->map = &maptrack_entry(lgt, op->handle);\n\n    grant_read_lock(lgt);\n\n    if ( unlikely(!read_atomic(&op->map->flags)) )\n    {\n        grant_read_unlock(lgt);\n        gdprintk(XENLOG_INFO, \"Zero flags for handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    dom = op->map->domid;\n    grant_read_unlock(lgt);\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(dom)) == NULL) )\n    {\n        /* This can happen when a grant is implicitly unmapped. */\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", dom);\n        domain_crash(ld); /* naughty... */\n        return;\n    }\n\n    rc = xsm_grant_unmapref(XSM_HOOK, ld, rd);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_UNMAP, dom);\n\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n\n    op->flags = read_atomic(&op->map->flags);\n    if ( unlikely(!op->flags) || unlikely(op->map->domid != dom) )\n    {\n        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n        rc = GNTST_bad_handle;\n        goto unmap_out;\n    }\n\n    op->rd = rd;\n    act = active_entry_acquire(rgt, op->map->ref);\n\n    if ( op->frame == 0 )\n    {\n        op->frame = act->frame;\n    }\n    else\n    {\n        if ( unlikely(op->frame != act->frame) )\n            PIN_FAIL(act_release_out, GNTST_general_error,\n                     \"Bad frame number doesn't match gntref. (%lx != %lx)\\n\",\n                     op->frame, act->frame);\n        if ( op->flags & GNTMAP_device_map )\n        {\n            ASSERT(act->pin & (GNTPIN_devw_mask | GNTPIN_devr_mask));\n            op->map->flags &= ~GNTMAP_device_map;\n            if ( op->flags & GNTMAP_readonly )\n                act->pin -= GNTPIN_devr_inc;\n            else\n                act->pin -= GNTPIN_devw_inc;\n        }\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( (rc = replace_grant_host_mapping(op->host_addr,\n                                              op->frame, op->new_addr, \n                                              op->flags)) < 0 )\n            goto act_release_out;\n\n        ASSERT(act->pin & (GNTPIN_hstw_mask | GNTPIN_hstr_mask));\n        op->map->flags &= ~GNTMAP_host_map;\n        if ( op->flags & GNTMAP_readonly )\n            act->pin -= GNTPIN_hstr_inc;\n        else\n            act->pin -= GNTPIN_hstw_inc;\n    }\n\n act_release_out:\n    active_entry_release(act);\n unmap_out:\n    grant_read_unlock(rgt);\n\n    if ( rc == GNTST_okay && gnttab_need_iommu_mapping(ld) )\n    {\n        unsigned int kind;\n        int err = 0;\n\n        double_gt_lock(lgt, rgt);\n\n        kind = mapkind(lgt, rd, op->frame);\n        if ( !kind )\n            err = iommu_unmap_page(ld, op->frame);\n        else if ( !(kind & MAPKIND_WRITE) )\n            err = iommu_map_page(ld, op->frame, op->frame, IOMMUF_readable);\n\n        double_gt_unlock(lgt, rgt);\n\n        if ( err )\n            rc = GNTST_general_error;\n    }\n\n    /* If just unmapped a writable mapping, mark as dirtied */\n    if ( rc == GNTST_okay && !(op->flags & GNTMAP_readonly) )\n         gnttab_mark_dirty(rd, op->frame);\n\n    op->status = rc;\n    rcu_unlock_domain(rd);\n}",
        "func": "static void\n__gnttab_unmap_common(\n    struct gnttab_unmap_common *op)\n{\n    domid_t          dom;\n    struct domain   *ld, *rd;\n    struct grant_table *lgt, *rgt;\n    struct active_grant_entry *act;\n    s16              rc = 0;\n\n    ld = current->domain;\n    lgt = ld->grant_table;\n\n    op->frame = (unsigned long)(op->dev_bus_addr >> PAGE_SHIFT);\n\n    if ( unlikely(op->handle >= lgt->maptrack_limit) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    op->map = &maptrack_entry(lgt, op->handle);\n\n    grant_read_lock(lgt);\n\n    if ( unlikely(!read_atomic(&op->map->flags)) )\n    {\n        grant_read_unlock(lgt);\n        gdprintk(XENLOG_INFO, \"Zero flags for handle %#x\\n\", op->handle);\n        op->status = GNTST_bad_handle;\n        return;\n    }\n\n    dom = op->map->domid;\n    grant_read_unlock(lgt);\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(dom)) == NULL) )\n    {\n        /* This can happen when a grant is implicitly unmapped. */\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", dom);\n        domain_crash(ld); /* naughty... */\n        return;\n    }\n\n    rc = xsm_grant_unmapref(XSM_HOOK, ld, rd);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_UNMAP, dom);\n\n    rgt = rd->grant_table;\n\n    grant_read_lock(rgt);\n\n    op->flags = read_atomic(&op->map->flags);\n    if ( unlikely(!op->flags) || unlikely(op->map->domid != dom) )\n    {\n        gdprintk(XENLOG_WARNING, \"Unstable handle %#x\\n\", op->handle);\n        rc = GNTST_bad_handle;\n        goto unmap_out;\n    }\n\n    op->rd = rd;\n    act = active_entry_acquire(rgt, op->map->ref);\n\n    if ( op->frame == 0 )\n    {\n        op->frame = act->frame;\n    }\n    else\n    {\n        if ( unlikely(op->frame != act->frame) )\n            PIN_FAIL(act_release_out, GNTST_general_error,\n                     \"Bad frame number doesn't match gntref. (%lx != %lx)\\n\",\n                     op->frame, act->frame);\n\n        op->map->flags &= ~GNTMAP_device_map;\n    }\n\n    if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n    {\n        if ( (rc = replace_grant_host_mapping(op->host_addr,\n                                              op->frame, op->new_addr, \n                                              op->flags)) < 0 )\n            goto act_release_out;\n\n        op->map->flags &= ~GNTMAP_host_map;\n    }\n\n act_release_out:\n    active_entry_release(act);\n unmap_out:\n    grant_read_unlock(rgt);\n\n    if ( rc == GNTST_okay && gnttab_need_iommu_mapping(ld) )\n    {\n        unsigned int kind;\n        int err = 0;\n\n        double_gt_lock(lgt, rgt);\n\n        kind = mapkind(lgt, rd, op->frame);\n        if ( !kind )\n            err = iommu_unmap_page(ld, op->frame);\n        else if ( !(kind & MAPKIND_WRITE) )\n            err = iommu_map_page(ld, op->frame, op->frame, IOMMUF_readable);\n\n        double_gt_unlock(lgt, rgt);\n\n        if ( err )\n            rc = GNTST_general_error;\n    }\n\n    /* If just unmapped a writable mapping, mark as dirtied */\n    if ( rc == GNTST_okay && !(op->flags & GNTMAP_readonly) )\n         gnttab_mark_dirty(rd, op->frame);\n\n    op->status = rc;\n    rcu_unlock_domain(rd);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -78,15 +78,8 @@\n             PIN_FAIL(act_release_out, GNTST_general_error,\n                      \"Bad frame number doesn't match gntref. (%lx != %lx)\\n\",\n                      op->frame, act->frame);\n-        if ( op->flags & GNTMAP_device_map )\n-        {\n-            ASSERT(act->pin & (GNTPIN_devw_mask | GNTPIN_devr_mask));\n-            op->map->flags &= ~GNTMAP_device_map;\n-            if ( op->flags & GNTMAP_readonly )\n-                act->pin -= GNTPIN_devr_inc;\n-            else\n-                act->pin -= GNTPIN_devw_inc;\n-        }\n+\n+        op->map->flags &= ~GNTMAP_device_map;\n     }\n \n     if ( (op->host_addr != 0) && (op->flags & GNTMAP_host_map) )\n@@ -96,12 +89,7 @@\n                                               op->flags)) < 0 )\n             goto act_release_out;\n \n-        ASSERT(act->pin & (GNTPIN_hstw_mask | GNTPIN_hstr_mask));\n         op->map->flags &= ~GNTMAP_host_map;\n-        if ( op->flags & GNTMAP_readonly )\n-            act->pin -= GNTPIN_hstr_inc;\n-        else\n-            act->pin -= GNTPIN_hstw_inc;\n     }\n \n  act_release_out:",
        "diff_line_info": {
            "deleted_lines": [
                "        if ( op->flags & GNTMAP_device_map )",
                "        {",
                "            ASSERT(act->pin & (GNTPIN_devw_mask | GNTPIN_devr_mask));",
                "            op->map->flags &= ~GNTMAP_device_map;",
                "            if ( op->flags & GNTMAP_readonly )",
                "                act->pin -= GNTPIN_devr_inc;",
                "            else",
                "                act->pin -= GNTPIN_devw_inc;",
                "        }",
                "        ASSERT(act->pin & (GNTPIN_hstw_mask | GNTPIN_hstr_mask));",
                "        if ( op->flags & GNTMAP_readonly )",
                "            act->pin -= GNTPIN_hstr_inc;",
                "        else",
                "            act->pin -= GNTPIN_hstw_inc;"
            ],
            "added_lines": [
                "",
                "        op->map->flags &= ~GNTMAP_device_map;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-10915",
        "func_name": "xen-project/xen/emulate_gva_to_mfn",
        "description": "The shadow-paging feature in Xen through 4.8.x mismanages page references and consequently introduces a race condition, which allows guest OS users to obtain Xen privileges, aka XSA-219.",
        "git_url": "https://github.com/xen-project/xen/commit/26217aff67ae1538d4e1b2226afab6993cdbe772",
        "commit_title": "x86/shadow: hold references for the duration of emulated writes",
        "commit_text": " The (misnamed) emulate_gva_to_mfn() function translates a linear address to an mfn, but releases its page reference before returning the mfn to its caller.  sh_emulate_map_dest() uses the results of one or two translations to construct a virtual mapping to the underlying frames, completes an emulated write/cmpxchg, then unmaps the virtual mappings.  The page references need holding until the mappings are unmapped, or the frames can change ownership before the writes occurs.  This is XSA-219. ",
        "func_before": "static mfn_t emulate_gva_to_mfn(struct vcpu *v, unsigned long vaddr,\n                                struct sh_emulate_ctxt *sh_ctxt)\n{\n    unsigned long gfn;\n    struct page_info *page;\n    mfn_t mfn;\n    p2m_type_t p2mt;\n    uint32_t pfec = PFEC_page_present | PFEC_write_access;\n\n    /* Translate the VA to a GFN. */\n    gfn = paging_get_hostmode(v)->gva_to_gfn(v, NULL, vaddr, &pfec);\n    if ( gfn == gfn_x(INVALID_GFN) )\n    {\n        x86_emul_pagefault(pfec, vaddr, &sh_ctxt->ctxt);\n\n        return _mfn(BAD_GVA_TO_GFN);\n    }\n\n    /* Translate the GFN to an MFN. */\n    ASSERT(!paging_locked_by_me(v->domain));\n\n    page = get_page_from_gfn(v->domain, gfn, &p2mt, P2M_ALLOC);\n\n    /* Sanity checking. */\n    if ( page == NULL )\n    {\n        return _mfn(BAD_GFN_TO_MFN);\n    }\n    if ( p2m_is_discard_write(p2mt) )\n    {\n        put_page(page);\n        return _mfn(READONLY_GFN);\n    }\n    if ( !p2m_is_ram(p2mt) )\n    {\n        put_page(page);\n        return _mfn(BAD_GFN_TO_MFN);\n    }\n    mfn = page_to_mfn(page);\n    ASSERT(mfn_valid(mfn));\n\n    v->arch.paging.last_write_was_pt = !!sh_mfn_is_a_page_table(mfn);\n    /*\n     * Note shadow cannot page out or unshare this mfn, so the map won't\n     * disappear. Otherwise, caller must hold onto page until done.\n     */\n    put_page(page);\n\n    return mfn;\n}",
        "func": "static mfn_t emulate_gva_to_mfn(struct vcpu *v, unsigned long vaddr,\n                                struct sh_emulate_ctxt *sh_ctxt)\n{\n    unsigned long gfn;\n    struct page_info *page;\n    mfn_t mfn;\n    p2m_type_t p2mt;\n    uint32_t pfec = PFEC_page_present | PFEC_write_access;\n\n    /* Translate the VA to a GFN. */\n    gfn = paging_get_hostmode(v)->gva_to_gfn(v, NULL, vaddr, &pfec);\n    if ( gfn == gfn_x(INVALID_GFN) )\n    {\n        x86_emul_pagefault(pfec, vaddr, &sh_ctxt->ctxt);\n\n        return _mfn(BAD_GVA_TO_GFN);\n    }\n\n    /* Translate the GFN to an MFN. */\n    ASSERT(!paging_locked_by_me(v->domain));\n\n    page = get_page_from_gfn(v->domain, gfn, &p2mt, P2M_ALLOC);\n\n    /* Sanity checking. */\n    if ( page == NULL )\n    {\n        return _mfn(BAD_GFN_TO_MFN);\n    }\n    if ( p2m_is_discard_write(p2mt) )\n    {\n        put_page(page);\n        return _mfn(READONLY_GFN);\n    }\n    if ( !p2m_is_ram(p2mt) )\n    {\n        put_page(page);\n        return _mfn(BAD_GFN_TO_MFN);\n    }\n    mfn = page_to_mfn(page);\n    ASSERT(mfn_valid(mfn));\n\n    v->arch.paging.last_write_was_pt = !!sh_mfn_is_a_page_table(mfn);\n\n    return mfn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,11 +40,6 @@\n     ASSERT(mfn_valid(mfn));\n \n     v->arch.paging.last_write_was_pt = !!sh_mfn_is_a_page_table(mfn);\n-    /*\n-     * Note shadow cannot page out or unshare this mfn, so the map won't\n-     * disappear. Otherwise, caller must hold onto page until done.\n-     */\n-    put_page(page);\n \n     return mfn;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    /*",
                "     * Note shadow cannot page out or unshare this mfn, so the map won't",
                "     * disappear. Otherwise, caller must hold onto page until done.",
                "     */",
                "    put_page(page);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2017-10915",
        "func_name": "xen-project/xen/sh_emulate_unmap_dest",
        "description": "The shadow-paging feature in Xen through 4.8.x mismanages page references and consequently introduces a race condition, which allows guest OS users to obtain Xen privileges, aka XSA-219.",
        "git_url": "https://github.com/xen-project/xen/commit/26217aff67ae1538d4e1b2226afab6993cdbe772",
        "commit_title": "x86/shadow: hold references for the duration of emulated writes",
        "commit_text": " The (misnamed) emulate_gva_to_mfn() function translates a linear address to an mfn, but releases its page reference before returning the mfn to its caller.  sh_emulate_map_dest() uses the results of one or two translations to construct a virtual mapping to the underlying frames, completes an emulated write/cmpxchg, then unmaps the virtual mappings.  The page references need holding until the mappings are unmapped, or the frames can change ownership before the writes occurs.  This is XSA-219. ",
        "func_before": "void sh_emulate_unmap_dest(struct vcpu *v, void *addr, unsigned int bytes,\n                           struct sh_emulate_ctxt *sh_ctxt)\n{\n    u32 b1 = bytes, b2 = 0, shflags;\n\n    /*\n     * We can avoid re-verifying the page contents after the write if:\n     *  - it was no larger than the PTE type of this pagetable;\n     *  - it was aligned to the PTE boundaries; and\n     *  - _PAGE_PRESENT was clear before and after the write.\n     */\n    shflags = mfn_to_page(sh_ctxt->mfn[0])->shadow_flags;\n#if (SHADOW_OPTIMIZATIONS & SHOPT_SKIP_VERIFY)\n    if ( sh_ctxt->low_bit_was_clear\n         && !(*(u8 *)addr & _PAGE_PRESENT)\n         && ((!(shflags & SHF_32)\n              /*\n               * Not shadowed 32-bit: aligned 64-bit writes that leave\n               * the present bit unset are safe to ignore.\n               */\n              && ((unsigned long)addr & 7) == 0\n              && bytes <= 8)\n             ||\n             (!(shflags & (SHF_PAE|SHF_64))\n              /*\n               * Not shadowed PAE/64-bit: aligned 32-bit writes that\n               * leave the present bit unset are safe to ignore.\n               */\n              && ((unsigned long)addr & 3) == 0\n              && bytes <= 4)) )\n    {\n        /* Writes with this alignment constraint can't possibly cross pages. */\n        ASSERT(!mfn_valid(sh_ctxt->mfn[1]));\n    }\n    else\n#endif /* SHADOW_OPTIMIZATIONS & SHOPT_SKIP_VERIFY */\n    {\n        if ( unlikely(mfn_valid(sh_ctxt->mfn[1])) )\n        {\n            /* Validate as two writes, one to each page. */\n            b1 = PAGE_SIZE - (((unsigned long)addr) & ~PAGE_MASK);\n            b2 = bytes - b1;\n            ASSERT(b2 < bytes);\n        }\n        if ( likely(b1 > 0) )\n            sh_validate_guest_pt_write(v, sh_ctxt->mfn[0], addr, b1);\n        if ( unlikely(b2 > 0) )\n            sh_validate_guest_pt_write(v, sh_ctxt->mfn[1], addr + b1, b2);\n    }\n\n    paging_mark_dirty(v->domain, sh_ctxt->mfn[0]);\n\n    if ( unlikely(mfn_valid(sh_ctxt->mfn[1])) )\n    {\n        paging_mark_dirty(v->domain, sh_ctxt->mfn[1]);\n        vunmap((void *)((unsigned long)addr & PAGE_MASK));\n    }\n    else\n        unmap_domain_page(addr);\n\n    atomic_inc(&v->domain->arch.paging.shadow.gtable_dirty_version);\n}",
        "func": "void sh_emulate_unmap_dest(struct vcpu *v, void *addr, unsigned int bytes,\n                           struct sh_emulate_ctxt *sh_ctxt)\n{\n    u32 b1 = bytes, b2 = 0, shflags;\n\n    /*\n     * We can avoid re-verifying the page contents after the write if:\n     *  - it was no larger than the PTE type of this pagetable;\n     *  - it was aligned to the PTE boundaries; and\n     *  - _PAGE_PRESENT was clear before and after the write.\n     */\n    shflags = mfn_to_page(sh_ctxt->mfn[0])->shadow_flags;\n#if (SHADOW_OPTIMIZATIONS & SHOPT_SKIP_VERIFY)\n    if ( sh_ctxt->low_bit_was_clear\n         && !(*(u8 *)addr & _PAGE_PRESENT)\n         && ((!(shflags & SHF_32)\n              /*\n               * Not shadowed 32-bit: aligned 64-bit writes that leave\n               * the present bit unset are safe to ignore.\n               */\n              && ((unsigned long)addr & 7) == 0\n              && bytes <= 8)\n             ||\n             (!(shflags & (SHF_PAE|SHF_64))\n              /*\n               * Not shadowed PAE/64-bit: aligned 32-bit writes that\n               * leave the present bit unset are safe to ignore.\n               */\n              && ((unsigned long)addr & 3) == 0\n              && bytes <= 4)) )\n    {\n        /* Writes with this alignment constraint can't possibly cross pages. */\n        ASSERT(!mfn_valid(sh_ctxt->mfn[1]));\n    }\n    else\n#endif /* SHADOW_OPTIMIZATIONS & SHOPT_SKIP_VERIFY */\n    {\n        if ( unlikely(mfn_valid(sh_ctxt->mfn[1])) )\n        {\n            /* Validate as two writes, one to each page. */\n            b1 = PAGE_SIZE - (((unsigned long)addr) & ~PAGE_MASK);\n            b2 = bytes - b1;\n            ASSERT(b2 < bytes);\n        }\n        if ( likely(b1 > 0) )\n            sh_validate_guest_pt_write(v, sh_ctxt->mfn[0], addr, b1);\n        if ( unlikely(b2 > 0) )\n            sh_validate_guest_pt_write(v, sh_ctxt->mfn[1], addr + b1, b2);\n    }\n\n    paging_mark_dirty(v->domain, sh_ctxt->mfn[0]);\n    put_page(mfn_to_page(sh_ctxt->mfn[0]));\n\n    if ( unlikely(mfn_valid(sh_ctxt->mfn[1])) )\n    {\n        paging_mark_dirty(v->domain, sh_ctxt->mfn[1]);\n        put_page(mfn_to_page(sh_ctxt->mfn[1]));\n        vunmap((void *)((unsigned long)addr & PAGE_MASK));\n    }\n    else\n        unmap_domain_page(addr);\n\n    atomic_inc(&v->domain->arch.paging.shadow.gtable_dirty_version);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -49,10 +49,12 @@\n     }\n \n     paging_mark_dirty(v->domain, sh_ctxt->mfn[0]);\n+    put_page(mfn_to_page(sh_ctxt->mfn[0]));\n \n     if ( unlikely(mfn_valid(sh_ctxt->mfn[1])) )\n     {\n         paging_mark_dirty(v->domain, sh_ctxt->mfn[1]);\n+        put_page(mfn_to_page(sh_ctxt->mfn[1]));\n         vunmap((void *)((unsigned long)addr & PAGE_MASK));\n     }\n     else",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    put_page(mfn_to_page(sh_ctxt->mfn[0]));",
                "        put_page(mfn_to_page(sh_ctxt->mfn[1]));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-10915",
        "func_name": "xen-project/xen/sh_emulate_map_dest",
        "description": "The shadow-paging feature in Xen through 4.8.x mismanages page references and consequently introduces a race condition, which allows guest OS users to obtain Xen privileges, aka XSA-219.",
        "git_url": "https://github.com/xen-project/xen/commit/26217aff67ae1538d4e1b2226afab6993cdbe772",
        "commit_title": "x86/shadow: hold references for the duration of emulated writes",
        "commit_text": " The (misnamed) emulate_gva_to_mfn() function translates a linear address to an mfn, but releases its page reference before returning the mfn to its caller.  sh_emulate_map_dest() uses the results of one or two translations to construct a virtual mapping to the underlying frames, completes an emulated write/cmpxchg, then unmaps the virtual mappings.  The page references need holding until the mappings are unmapped, or the frames can change ownership before the writes occurs.  This is XSA-219. ",
        "func_before": "void *sh_emulate_map_dest(struct vcpu *v, unsigned long vaddr,\n                          unsigned int bytes,\n                          struct sh_emulate_ctxt *sh_ctxt)\n{\n    struct domain *d = v->domain;\n    void *map;\n\n    sh_ctxt->mfn[0] = emulate_gva_to_mfn(v, vaddr, sh_ctxt);\n    if ( !mfn_valid(sh_ctxt->mfn[0]) )\n        return ((mfn_x(sh_ctxt->mfn[0]) == BAD_GVA_TO_GFN) ?\n                MAPPING_EXCEPTION :\n                (mfn_x(sh_ctxt->mfn[0]) == READONLY_GFN) ?\n                MAPPING_SILENT_FAIL : MAPPING_UNHANDLEABLE);\n\n#ifndef NDEBUG\n    /* We don't emulate user-mode writes to page tables. */\n    if ( is_hvm_domain(d) ? hvm_get_cpl(v) == 3\n                          : !guest_kernel_mode(v, guest_cpu_user_regs()) )\n    {\n        gdprintk(XENLOG_DEBUG, \"User-mode write to pagetable reached \"\n                 \"emulate_map_dest(). This should never happen!\\n\");\n        return MAPPING_UNHANDLEABLE;\n    }\n#endif\n\n    /* Unaligned writes mean probably this isn't a pagetable. */\n    if ( vaddr & (bytes - 1) )\n        sh_remove_shadows(d, sh_ctxt->mfn[0], 0, 0 /* Slow, can fail. */ );\n\n    if ( likely(((vaddr + bytes - 1) & PAGE_MASK) == (vaddr & PAGE_MASK)) )\n    {\n        /* Whole write fits on a single page. */\n        sh_ctxt->mfn[1] = INVALID_MFN;\n        map = map_domain_page(sh_ctxt->mfn[0]) + (vaddr & ~PAGE_MASK);\n    }\n    else if ( !is_hvm_domain(d) )\n    {\n        /*\n         * Cross-page emulated writes are only supported for HVM guests;\n         * PV guests ought to know better.\n         */\n        return MAPPING_UNHANDLEABLE;\n    }\n    else\n    {\n        /* This write crosses a page boundary. Translate the second page. */\n        sh_ctxt->mfn[1] = emulate_gva_to_mfn(\n            v, (vaddr + bytes - 1) & PAGE_MASK, sh_ctxt);\n        if ( !mfn_valid(sh_ctxt->mfn[1]) )\n            return ((mfn_x(sh_ctxt->mfn[1]) == BAD_GVA_TO_GFN) ?\n                    MAPPING_EXCEPTION :\n                    (mfn_x(sh_ctxt->mfn[1]) == READONLY_GFN) ?\n                    MAPPING_SILENT_FAIL : MAPPING_UNHANDLEABLE);\n\n        /* Cross-page writes mean probably not a pagetable. */\n        sh_remove_shadows(d, sh_ctxt->mfn[1], 0, 0 /* Slow, can fail. */ );\n\n        map = vmap(sh_ctxt->mfn, 2);\n        if ( !map )\n            return MAPPING_UNHANDLEABLE;\n        map += (vaddr & ~PAGE_MASK);\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_SKIP_VERIFY)\n    /*\n     * Remember if the bottom bit was clear, so we can choose not to run\n     * the change through the verify code if it's still clear afterwards.\n     */\n    sh_ctxt->low_bit_was_clear = map != NULL && !(*(u8 *)map & _PAGE_PRESENT);\n#endif\n\n    return map;\n}",
        "func": "void *sh_emulate_map_dest(struct vcpu *v, unsigned long vaddr,\n                          unsigned int bytes,\n                          struct sh_emulate_ctxt *sh_ctxt)\n{\n    struct domain *d = v->domain;\n    void *map;\n\n#ifndef NDEBUG\n    /* We don't emulate user-mode writes to page tables. */\n    if ( is_hvm_domain(d) ? hvm_get_cpl(v) == 3\n                          : !guest_kernel_mode(v, guest_cpu_user_regs()) )\n    {\n        gdprintk(XENLOG_DEBUG, \"User-mode write to pagetable reached \"\n                 \"emulate_map_dest(). This should never happen!\\n\");\n        return MAPPING_UNHANDLEABLE;\n    }\n#endif\n\n    sh_ctxt->mfn[0] = emulate_gva_to_mfn(v, vaddr, sh_ctxt);\n    if ( !mfn_valid(sh_ctxt->mfn[0]) )\n    {\n        switch ( mfn_x(sh_ctxt->mfn[0]) )\n        {\n        case BAD_GVA_TO_GFN: return MAPPING_EXCEPTION;\n        case READONLY_GFN:   return MAPPING_SILENT_FAIL;\n        default:             return MAPPING_UNHANDLEABLE;\n        }\n    }\n\n    /* Unaligned writes mean probably this isn't a pagetable. */\n    if ( vaddr & (bytes - 1) )\n        sh_remove_shadows(d, sh_ctxt->mfn[0], 0, 0 /* Slow, can fail. */ );\n\n    if ( likely(((vaddr + bytes - 1) & PAGE_MASK) == (vaddr & PAGE_MASK)) )\n    {\n        /* Whole write fits on a single page. */\n        sh_ctxt->mfn[1] = INVALID_MFN;\n        map = map_domain_page(sh_ctxt->mfn[0]) + (vaddr & ~PAGE_MASK);\n    }\n    else if ( !is_hvm_domain(d) )\n    {\n        /*\n         * Cross-page emulated writes are only supported for HVM guests;\n         * PV guests ought to know better.\n         */\n        put_page(mfn_to_page(sh_ctxt->mfn[0]));\n        return MAPPING_UNHANDLEABLE;\n    }\n    else\n    {\n        /* This write crosses a page boundary. Translate the second page. */\n        sh_ctxt->mfn[1] = emulate_gva_to_mfn(\n            v, (vaddr + bytes - 1) & PAGE_MASK, sh_ctxt);\n        if ( !mfn_valid(sh_ctxt->mfn[1]) )\n        {\n            put_page(mfn_to_page(sh_ctxt->mfn[0]));\n            switch ( mfn_x(sh_ctxt->mfn[1]) )\n            {\n            case BAD_GVA_TO_GFN: return MAPPING_EXCEPTION;\n            case READONLY_GFN:   return MAPPING_SILENT_FAIL;\n            default:             return MAPPING_UNHANDLEABLE;\n            }\n        }\n\n        /* Cross-page writes mean probably not a pagetable. */\n        sh_remove_shadows(d, sh_ctxt->mfn[1], 0, 0 /* Slow, can fail. */ );\n\n        map = vmap(sh_ctxt->mfn, 2);\n        if ( !map )\n        {\n            put_page(mfn_to_page(sh_ctxt->mfn[0]));\n            put_page(mfn_to_page(sh_ctxt->mfn[1]));\n            return MAPPING_UNHANDLEABLE;\n        }\n        map += (vaddr & ~PAGE_MASK);\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_SKIP_VERIFY)\n    /*\n     * Remember if the bottom bit was clear, so we can choose not to run\n     * the change through the verify code if it's still clear afterwards.\n     */\n    sh_ctxt->low_bit_was_clear = map != NULL && !(*(u8 *)map & _PAGE_PRESENT);\n#endif\n\n    return map;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,13 +4,6 @@\n {\n     struct domain *d = v->domain;\n     void *map;\n-\n-    sh_ctxt->mfn[0] = emulate_gva_to_mfn(v, vaddr, sh_ctxt);\n-    if ( !mfn_valid(sh_ctxt->mfn[0]) )\n-        return ((mfn_x(sh_ctxt->mfn[0]) == BAD_GVA_TO_GFN) ?\n-                MAPPING_EXCEPTION :\n-                (mfn_x(sh_ctxt->mfn[0]) == READONLY_GFN) ?\n-                MAPPING_SILENT_FAIL : MAPPING_UNHANDLEABLE);\n \n #ifndef NDEBUG\n     /* We don't emulate user-mode writes to page tables. */\n@@ -22,6 +15,17 @@\n         return MAPPING_UNHANDLEABLE;\n     }\n #endif\n+\n+    sh_ctxt->mfn[0] = emulate_gva_to_mfn(v, vaddr, sh_ctxt);\n+    if ( !mfn_valid(sh_ctxt->mfn[0]) )\n+    {\n+        switch ( mfn_x(sh_ctxt->mfn[0]) )\n+        {\n+        case BAD_GVA_TO_GFN: return MAPPING_EXCEPTION;\n+        case READONLY_GFN:   return MAPPING_SILENT_FAIL;\n+        default:             return MAPPING_UNHANDLEABLE;\n+        }\n+    }\n \n     /* Unaligned writes mean probably this isn't a pagetable. */\n     if ( vaddr & (bytes - 1) )\n@@ -39,6 +43,7 @@\n          * Cross-page emulated writes are only supported for HVM guests;\n          * PV guests ought to know better.\n          */\n+        put_page(mfn_to_page(sh_ctxt->mfn[0]));\n         return MAPPING_UNHANDLEABLE;\n     }\n     else\n@@ -47,17 +52,26 @@\n         sh_ctxt->mfn[1] = emulate_gva_to_mfn(\n             v, (vaddr + bytes - 1) & PAGE_MASK, sh_ctxt);\n         if ( !mfn_valid(sh_ctxt->mfn[1]) )\n-            return ((mfn_x(sh_ctxt->mfn[1]) == BAD_GVA_TO_GFN) ?\n-                    MAPPING_EXCEPTION :\n-                    (mfn_x(sh_ctxt->mfn[1]) == READONLY_GFN) ?\n-                    MAPPING_SILENT_FAIL : MAPPING_UNHANDLEABLE);\n+        {\n+            put_page(mfn_to_page(sh_ctxt->mfn[0]));\n+            switch ( mfn_x(sh_ctxt->mfn[1]) )\n+            {\n+            case BAD_GVA_TO_GFN: return MAPPING_EXCEPTION;\n+            case READONLY_GFN:   return MAPPING_SILENT_FAIL;\n+            default:             return MAPPING_UNHANDLEABLE;\n+            }\n+        }\n \n         /* Cross-page writes mean probably not a pagetable. */\n         sh_remove_shadows(d, sh_ctxt->mfn[1], 0, 0 /* Slow, can fail. */ );\n \n         map = vmap(sh_ctxt->mfn, 2);\n         if ( !map )\n+        {\n+            put_page(mfn_to_page(sh_ctxt->mfn[0]));\n+            put_page(mfn_to_page(sh_ctxt->mfn[1]));\n             return MAPPING_UNHANDLEABLE;\n+        }\n         map += (vaddr & ~PAGE_MASK);\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "    sh_ctxt->mfn[0] = emulate_gva_to_mfn(v, vaddr, sh_ctxt);",
                "    if ( !mfn_valid(sh_ctxt->mfn[0]) )",
                "        return ((mfn_x(sh_ctxt->mfn[0]) == BAD_GVA_TO_GFN) ?",
                "                MAPPING_EXCEPTION :",
                "                (mfn_x(sh_ctxt->mfn[0]) == READONLY_GFN) ?",
                "                MAPPING_SILENT_FAIL : MAPPING_UNHANDLEABLE);",
                "            return ((mfn_x(sh_ctxt->mfn[1]) == BAD_GVA_TO_GFN) ?",
                "                    MAPPING_EXCEPTION :",
                "                    (mfn_x(sh_ctxt->mfn[1]) == READONLY_GFN) ?",
                "                    MAPPING_SILENT_FAIL : MAPPING_UNHANDLEABLE);"
            ],
            "added_lines": [
                "",
                "    sh_ctxt->mfn[0] = emulate_gva_to_mfn(v, vaddr, sh_ctxt);",
                "    if ( !mfn_valid(sh_ctxt->mfn[0]) )",
                "    {",
                "        switch ( mfn_x(sh_ctxt->mfn[0]) )",
                "        {",
                "        case BAD_GVA_TO_GFN: return MAPPING_EXCEPTION;",
                "        case READONLY_GFN:   return MAPPING_SILENT_FAIL;",
                "        default:             return MAPPING_UNHANDLEABLE;",
                "        }",
                "    }",
                "        put_page(mfn_to_page(sh_ctxt->mfn[0]));",
                "        {",
                "            put_page(mfn_to_page(sh_ctxt->mfn[0]));",
                "            switch ( mfn_x(sh_ctxt->mfn[1]) )",
                "            {",
                "            case BAD_GVA_TO_GFN: return MAPPING_EXCEPTION;",
                "            case READONLY_GFN:   return MAPPING_SILENT_FAIL;",
                "            default:             return MAPPING_UNHANDLEABLE;",
                "            }",
                "        }",
                "        {",
                "            put_page(mfn_to_page(sh_ctxt->mfn[0]));",
                "            put_page(mfn_to_page(sh_ctxt->mfn[1]));",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-18203",
        "func_name": "torvalds/linux/dm_get_from_kobject",
        "description": "The dm_get_from_kobject function in drivers/md/dm.c in the Linux kernel before 4.14.3 allow local users to cause a denial of service (BUG) by leveraging a race condition with __dm_destroy during creation and removal of DM devices.",
        "git_url": "https://github.com/torvalds/linux/commit/b9a41d21dceadf8104812626ef85dc56ee8a60ed",
        "commit_title": "dm: fix race between dm_get_from_kobject() and __dm_destroy()",
        "commit_text": " The following BUG_ON was hit when testing repeat creation and removal of DM devices:      kernel BUG at drivers/md/dm.c:2919!     CPU: 7 PID: 750 Comm: systemd-udevd Not tainted 4.1.44     Call Trace:      [<ffffffff81649e8b>] dm_get_from_kobject+0x34/0x3a      [<ffffffff81650ef1>] dm_attr_show+0x2b/0x5e      [<ffffffff817b46d1>] ? mutex_lock+0x26/0x44      [<ffffffff811df7f5>] sysfs_kf_seq_show+0x83/0xcf      [<ffffffff811de257>] kernfs_seq_show+0x23/0x25      [<ffffffff81199118>] seq_read+0x16f/0x325      [<ffffffff811de994>] kernfs_fop_read+0x3a/0x13f      [<ffffffff8117b625>] __vfs_read+0x26/0x9d      [<ffffffff8130eb59>] ? security_file_permission+0x3c/0x44      [<ffffffff8117bdb8>] ? rw_verify_area+0x83/0xd9      [<ffffffff8117be9d>] vfs_read+0x8f/0xcf      [<ffffffff81193e34>] ? __fdget_pos+0x12/0x41      [<ffffffff8117c686>] SyS_read+0x4b/0x76      [<ffffffff817b606e>] system_call_fastpath+0x12/0x71  The bug can be easily triggered, if an extra delay (e.g. 10ms) is added between the test of DMF_FREEING & DMF_DELETING and dm_get() in dm_get_from_kobject().  To fix it, we need to ensure the test of DMF_FREEING & DMF_DELETING and dm_get() are done in an atomic way, so _minor_lock is used.  The other callers of dm_get() have also been checked to be OK: some callers invoke dm_get() under _minor_lock, some callers invoke it under _hash_lock, and dm_start_request() invoke it after increasing md->open_count.  Cc: stable@vger.kernel.org",
        "func_before": "struct mapped_device *dm_get_from_kobject(struct kobject *kobj)\n{\n\tstruct mapped_device *md;\n\n\tmd = container_of(kobj, struct mapped_device, kobj_holder.kobj);\n\n\tif (test_bit(DMF_FREEING, &md->flags) ||\n\t    dm_deleting_md(md))\n\t\treturn NULL;\n\n\tdm_get(md);\n\treturn md;\n}",
        "func": "struct mapped_device *dm_get_from_kobject(struct kobject *kobj)\n{\n\tstruct mapped_device *md;\n\n\tmd = container_of(kobj, struct mapped_device, kobj_holder.kobj);\n\n\tspin_lock(&_minor_lock);\n\tif (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {\n\t\tmd = NULL;\n\t\tgoto out;\n\t}\n\tdm_get(md);\nout:\n\tspin_unlock(&_minor_lock);\n\n\treturn md;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,10 +4,14 @@\n \n \tmd = container_of(kobj, struct mapped_device, kobj_holder.kobj);\n \n-\tif (test_bit(DMF_FREEING, &md->flags) ||\n-\t    dm_deleting_md(md))\n-\t\treturn NULL;\n+\tspin_lock(&_minor_lock);\n+\tif (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {\n+\t\tmd = NULL;\n+\t\tgoto out;\n+\t}\n+\tdm_get(md);\n+out:\n+\tspin_unlock(&_minor_lock);\n \n-\tdm_get(md);\n \treturn md;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (test_bit(DMF_FREEING, &md->flags) ||",
                "\t    dm_deleting_md(md))",
                "\t\treturn NULL;",
                "\tdm_get(md);"
            ],
            "added_lines": [
                "\tspin_lock(&_minor_lock);",
                "\tif (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {",
                "\t\tmd = NULL;",
                "\t\tgoto out;",
                "\t}",
                "\tdm_get(md);",
                "out:",
                "\tspin_unlock(&_minor_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-7995",
        "func_name": "kernel/git/tip/tip/set_ignore_ce",
        "description": "Race condition in the store_int_with_restart() function in arch/x86/kernel/cpu/mcheck/mce.c in the Linux kernel through 4.15.7 allows local users to cause a denial of service (panic) by leveraging root access to write to the check_interval file in a /sys/devices/system/machinecheck/machinecheck<cpu number> directory. NOTE: a third party has indicated that this report is not security relevant",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/commit/?h=b3b7c4795ccab5be71f080774c45bbbcc75c2aaf",
        "commit_title": "The check_interval file in",
        "commit_text": "   /sys/devices/system/machinecheck/machinecheck<cpu number>  directory is a global timer value for MCE polling. If it is changed by one CPU, mce_restart() broadcasts the event to other CPUs to delete and restart the MCE polling timer and __mcheck_cpu_init_timer() reinitializes the mce_timer variable.  If more than one CPU writes a specific value to the check_interval file concurrently, mce_timer is not protected from such concurrent accesses and all kinds of explosions happen. Since only root can write to those sysfs variables, the issue is not a big deal security-wise.  However, concurrent writes to these configuration variables is void of reason so the proper thing to do is to serialize the access with a mutex.  Boris:   - Make store_int_with_restart() use device_store_ulong() to filter out    negative intervals  - Limit min interval to 1 second  - Correct locking  - Massage commit message  Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org> Cc: Tony Luck <tony.luck@intel.com> Cc: linux-edac <linux-edac@vger.kernel.org> Cc: stable@vger.kernel.org Link: http://lkml.kernel.org/r/20180302202706.9434-1-kkamagui@gmail.com ",
        "func_before": "static ssize_t set_ignore_ce(struct device *s,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tif (mca_cfg.ignore_ce ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable ce features */\n\t\t\tmce_timer_delete_all();\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.ignore_ce = true;\n\t\t} else {\n\t\t\t/* enable ce features */\n\t\t\tmca_cfg.ignore_ce = false;\n\t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n\t\t}\n\t}\n\treturn size;\n}",
        "func": "static ssize_t set_ignore_ce(struct device *s,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tif (mca_cfg.ignore_ce ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable ce features */\n\t\t\tmce_timer_delete_all();\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.ignore_ce = true;\n\t\t} else {\n\t\t\t/* enable ce features */\n\t\t\tmca_cfg.ignore_ce = false;\n\t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n\t\t}\n\t}\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn size;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,7 @@\n \tif (kstrtou64(buf, 0, &new) < 0)\n \t\treturn -EINVAL;\n \n+\tmutex_lock(&mce_sysfs_mutex);\n \tif (mca_cfg.ignore_ce ^ !!new) {\n \t\tif (new) {\n \t\t\t/* disable ce features */\n@@ -19,5 +20,7 @@\n \t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n \t\t}\n \t}\n+\tmutex_unlock(&mce_sysfs_mutex);\n+\n \treturn size;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_lock(&mce_sysfs_mutex);",
                "\tmutex_unlock(&mce_sysfs_mutex);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-7995",
        "func_name": "kernel/git/tip/tip/set_cmci_disabled",
        "description": "Race condition in the store_int_with_restart() function in arch/x86/kernel/cpu/mcheck/mce.c in the Linux kernel through 4.15.7 allows local users to cause a denial of service (panic) by leveraging root access to write to the check_interval file in a /sys/devices/system/machinecheck/machinecheck<cpu number> directory. NOTE: a third party has indicated that this report is not security relevant",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/commit/?h=b3b7c4795ccab5be71f080774c45bbbcc75c2aaf",
        "commit_title": "The check_interval file in",
        "commit_text": "   /sys/devices/system/machinecheck/machinecheck<cpu number>  directory is a global timer value for MCE polling. If it is changed by one CPU, mce_restart() broadcasts the event to other CPUs to delete and restart the MCE polling timer and __mcheck_cpu_init_timer() reinitializes the mce_timer variable.  If more than one CPU writes a specific value to the check_interval file concurrently, mce_timer is not protected from such concurrent accesses and all kinds of explosions happen. Since only root can write to those sysfs variables, the issue is not a big deal security-wise.  However, concurrent writes to these configuration variables is void of reason so the proper thing to do is to serialize the access with a mutex.  Boris:   - Make store_int_with_restart() use device_store_ulong() to filter out    negative intervals  - Limit min interval to 1 second  - Correct locking  - Massage commit message  Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org> Cc: Tony Luck <tony.luck@intel.com> Cc: linux-edac <linux-edac@vger.kernel.org> Cc: stable@vger.kernel.org Link: http://lkml.kernel.org/r/20180302202706.9434-1-kkamagui@gmail.com ",
        "func_before": "static ssize_t set_cmci_disabled(struct device *s,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tif (mca_cfg.cmci_disabled ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable cmci */\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.cmci_disabled = true;\n\t\t} else {\n\t\t\t/* enable cmci */\n\t\t\tmca_cfg.cmci_disabled = false;\n\t\t\ton_each_cpu(mce_enable_ce, NULL, 1);\n\t\t}\n\t}\n\treturn size;\n}",
        "func": "static ssize_t set_cmci_disabled(struct device *s,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tif (mca_cfg.cmci_disabled ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable cmci */\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.cmci_disabled = true;\n\t\t} else {\n\t\t\t/* enable cmci */\n\t\t\tmca_cfg.cmci_disabled = false;\n\t\t\ton_each_cpu(mce_enable_ce, NULL, 1);\n\t\t}\n\t}\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn size;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,7 @@\n \tif (kstrtou64(buf, 0, &new) < 0)\n \t\treturn -EINVAL;\n \n+\tmutex_lock(&mce_sysfs_mutex);\n \tif (mca_cfg.cmci_disabled ^ !!new) {\n \t\tif (new) {\n \t\t\t/* disable cmci */\n@@ -18,5 +19,7 @@\n \t\t\ton_each_cpu(mce_enable_ce, NULL, 1);\n \t\t}\n \t}\n+\tmutex_unlock(&mce_sysfs_mutex);\n+\n \treturn size;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_lock(&mce_sysfs_mutex);",
                "\tmutex_unlock(&mce_sysfs_mutex);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-7995",
        "func_name": "kernel/git/tip/tip/store_int_with_restart",
        "description": "Race condition in the store_int_with_restart() function in arch/x86/kernel/cpu/mcheck/mce.c in the Linux kernel through 4.15.7 allows local users to cause a denial of service (panic) by leveraging root access to write to the check_interval file in a /sys/devices/system/machinecheck/machinecheck<cpu number> directory. NOTE: a third party has indicated that this report is not security relevant",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/commit/?h=b3b7c4795ccab5be71f080774c45bbbcc75c2aaf",
        "commit_title": "The check_interval file in",
        "commit_text": "   /sys/devices/system/machinecheck/machinecheck<cpu number>  directory is a global timer value for MCE polling. If it is changed by one CPU, mce_restart() broadcasts the event to other CPUs to delete and restart the MCE polling timer and __mcheck_cpu_init_timer() reinitializes the mce_timer variable.  If more than one CPU writes a specific value to the check_interval file concurrently, mce_timer is not protected from such concurrent accesses and all kinds of explosions happen. Since only root can write to those sysfs variables, the issue is not a big deal security-wise.  However, concurrent writes to these configuration variables is void of reason so the proper thing to do is to serialize the access with a mutex.  Boris:   - Make store_int_with_restart() use device_store_ulong() to filter out    negative intervals  - Limit min interval to 1 second  - Correct locking  - Massage commit message  Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org> Cc: Tony Luck <tony.luck@intel.com> Cc: linux-edac <linux-edac@vger.kernel.org> Cc: stable@vger.kernel.org Link: http://lkml.kernel.org/r/20180302202706.9434-1-kkamagui@gmail.com ",
        "func_before": "static ssize_t store_int_with_restart(struct device *s,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      const char *buf, size_t size)\n{\n\tssize_t ret = device_store_int(s, attr, buf, size);\n\tmce_restart();\n\treturn ret;\n}",
        "func": "static ssize_t store_int_with_restart(struct device *s,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      const char *buf, size_t size)\n{\n\tunsigned long old_check_interval = check_interval;\n\tssize_t ret = device_store_ulong(s, attr, buf, size);\n\n\tif (check_interval == old_check_interval)\n\t\treturn ret;\n\n\tif (check_interval < 1)\n\t\tcheck_interval = 1;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tmce_restart();\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,18 @@\n \t\t\t\t      struct device_attribute *attr,\n \t\t\t\t      const char *buf, size_t size)\n {\n-\tssize_t ret = device_store_int(s, attr, buf, size);\n+\tunsigned long old_check_interval = check_interval;\n+\tssize_t ret = device_store_ulong(s, attr, buf, size);\n+\n+\tif (check_interval == old_check_interval)\n+\t\treturn ret;\n+\n+\tif (check_interval < 1)\n+\t\tcheck_interval = 1;\n+\n+\tmutex_lock(&mce_sysfs_mutex);\n \tmce_restart();\n+\tmutex_unlock(&mce_sysfs_mutex);\n+\n \treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tssize_t ret = device_store_int(s, attr, buf, size);"
            ],
            "added_lines": [
                "\tunsigned long old_check_interval = check_interval;",
                "\tssize_t ret = device_store_ulong(s, attr, buf, size);",
                "",
                "\tif (check_interval == old_check_interval)",
                "\t\treturn ret;",
                "",
                "\tif (check_interval < 1)",
                "\t\tcheck_interval = 1;",
                "",
                "\tmutex_lock(&mce_sysfs_mutex);",
                "\tmutex_unlock(&mce_sysfs_mutex);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-7998",
        "func_name": "jcupitt/libvips/vips_foreign_load_start",
        "description": "In libvips before 8.6.3, a NULL function pointer dereference vulnerability was found in the vips_region_generate function in region.c, which allows remote attackers to cause a denial of service or possibly have unspecified other impact via a crafted image file. This occurs because of a race condition involving a failed delayed load and other worker threads.",
        "git_url": "https://github.com/jcupitt/libvips/commit/20d840e6da15c1574b3ed998bc92f91d1e36c2a5",
        "commit_title": "fix a crash with delayed load",
        "commit_text": " If a delayed load failed, it could leave the pipeline only half-set up. Sebsequent threads could then segv.  Set a load-has-failed flag and test before generate.  See https://github.com/jcupitt/libvips/issues/893",
        "func_before": "static void *\nvips_foreign_load_start( VipsImage *out, void *a, void *b )\n{\n\tVipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );\n\tVipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );\n\n\tif( !load->real ) {\n\t\tif( !(load->real = vips_foreign_load_temp( load )) )\n\t\t\treturn( NULL );\n\n#ifdef DEBUG\n\t\tprintf( \"vips_foreign_load_start: triggering ->load()\\n\" );\n#endif /*DEBUG*/\n\n\t\t/* Read the image in. This may involve a long computation and\n\t\t * will finish with load->real holding the decompressed image. \n\t\t *\n\t\t * We want our caller to be able to see this computation on\n\t\t * @out, so eval signals on ->real need to appear on ->out.\n\t\t */\n\t\tload->real->progress_signal = load->out;\n\n\t\t/* Note the load object on the image. Loaders can use \n\t\t * this to signal invalidate if they hit a load error. See\n\t\t * vips_foreign_load_invalidate() below.\n\t\t */\n\t\tg_object_set_qdata( G_OBJECT( load->real ), \n\t\t\tvips__foreign_load_operation, load ); \n\n\t\tif( class->load( load ) ||\n\t\t\tvips_image_pio_input( load->real ) ) \n\t\t\treturn( NULL );\n\n\t\t/* ->header() read the header into @out, load has read the\n\t\t * image into @real. They must match exactly in size, bands,\n\t\t * format and coding for the copy to work.  \n\t\t *\n\t\t * Some versions of ImageMagick give different results between\n\t\t * Ping and Load for some formats, for example.\n\t\t */\n\t\tif( !vips_foreign_load_iscompat( load->real, out ) )\n\t\t\treturn( NULL );\n\n\t\t/* We have to tell vips that out depends on real. We've set\n\t\t * the demand hint below, but not given an input there.\n\t\t */\n\t\tvips_image_pipelinev( load->out, load->out->dhint, \n\t\t\tload->real, NULL );\n\t}\n\n\treturn( vips_region_new( load->real ) );\n}",
        "func": "static void *\nvips_foreign_load_start( VipsImage *out, void *a, void *b )\n{\n\tVipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );\n\tVipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );\n\n\t/* If this start has failed before in another thread, we can fail now.\n\t */\n\tif( load->error )\n\t\treturn( NULL );\n\n\tif( !load->real ) {\n\t\tif( !(load->real = vips_foreign_load_temp( load )) )\n\t\t\treturn( NULL );\n\n#ifdef DEBUG\n\t\tprintf( \"vips_foreign_load_start: triggering ->load()\\n\" );\n#endif /*DEBUG*/\n\n\t\t/* Read the image in. This may involve a long computation and\n\t\t * will finish with load->real holding the decompressed image. \n\t\t *\n\t\t * We want our caller to be able to see this computation on\n\t\t * @out, so eval signals on ->real need to appear on ->out.\n\t\t */\n\t\tload->real->progress_signal = load->out;\n\n\t\t/* Note the load object on the image. Loaders can use \n\t\t * this to signal invalidate if they hit a load error. See\n\t\t * vips_foreign_load_invalidate() below.\n\t\t */\n\t\tg_object_set_qdata( G_OBJECT( load->real ), \n\t\t\tvips__foreign_load_operation, load ); \n\n\t\t/* Load the image and check the result.\n\t\t *\n\t\t * ->header() read the header into @out, load has read the\n\t\t * image into @real. They must match exactly in size, bands,\n\t\t * format and coding for the copy to work.  \n\t\t *\n\t\t * Some versions of ImageMagick give different results between\n\t\t * Ping and Load for some formats, for example.\n\t\t *\n\t\t * If the load fails, we need to stop\n\t\t */\n\t\tif( class->load( load ) ||\n\t\t\tvips_image_pio_input( load->real ) || \n\t\t\tvips_foreign_load_iscompat( load->real, out ) ) {\n\t\t\tvips_operation_invalidate( VIPS_OPERATION( load ) ); \n\t\t\tload->error = TRUE;\n\n\t\t\treturn( NULL );\n\t\t}\n\n\t\t/* We have to tell vips that out depends on real. We've set\n\t\t * the demand hint below, but not given an input there.\n\t\t */\n\t\tvips_image_pipelinev( load->out, load->out->dhint, \n\t\t\tload->real, NULL );\n\t}\n\n\treturn( vips_region_new( load->real ) );\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,11 @@\n {\n \tVipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );\n \tVipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );\n+\n+\t/* If this start has failed before in another thread, we can fail now.\n+\t */\n+\tif( load->error )\n+\t\treturn( NULL );\n \n \tif( !load->real ) {\n \t\tif( !(load->real = vips_foreign_load_temp( load )) )\n@@ -27,19 +32,25 @@\n \t\tg_object_set_qdata( G_OBJECT( load->real ), \n \t\t\tvips__foreign_load_operation, load ); \n \n-\t\tif( class->load( load ) ||\n-\t\t\tvips_image_pio_input( load->real ) ) \n-\t\t\treturn( NULL );\n-\n-\t\t/* ->header() read the header into @out, load has read the\n+\t\t/* Load the image and check the result.\n+\t\t *\n+\t\t * ->header() read the header into @out, load has read the\n \t\t * image into @real. They must match exactly in size, bands,\n \t\t * format and coding for the copy to work.  \n \t\t *\n \t\t * Some versions of ImageMagick give different results between\n \t\t * Ping and Load for some formats, for example.\n+\t\t *\n+\t\t * If the load fails, we need to stop\n \t\t */\n-\t\tif( !vips_foreign_load_iscompat( load->real, out ) )\n+\t\tif( class->load( load ) ||\n+\t\t\tvips_image_pio_input( load->real ) || \n+\t\t\tvips_foreign_load_iscompat( load->real, out ) ) {\n+\t\t\tvips_operation_invalidate( VIPS_OPERATION( load ) ); \n+\t\t\tload->error = TRUE;\n+\n \t\t\treturn( NULL );\n+\t\t}\n \n \t\t/* We have to tell vips that out depends on real. We've set\n \t\t * the demand hint below, but not given an input there.",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif( class->load( load ) ||",
                "\t\t\tvips_image_pio_input( load->real ) ) ",
                "\t\t\treturn( NULL );",
                "",
                "\t\t/* ->header() read the header into @out, load has read the",
                "\t\tif( !vips_foreign_load_iscompat( load->real, out ) )"
            ],
            "added_lines": [
                "",
                "\t/* If this start has failed before in another thread, we can fail now.",
                "\t */",
                "\tif( load->error )",
                "\t\treturn( NULL );",
                "\t\t/* Load the image and check the result.",
                "\t\t *",
                "\t\t * ->header() read the header into @out, load has read the",
                "\t\t *",
                "\t\t * If the load fails, we need to stop",
                "\t\tif( class->load( load ) ||",
                "\t\t\tvips_image_pio_input( load->real ) || ",
                "\t\t\tvips_foreign_load_iscompat( load->real, out ) ) {",
                "\t\t\tvips_operation_invalidate( VIPS_OPERATION( load ) ); ",
                "\t\t\tload->error = TRUE;",
                "",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10576",
        "func_name": "meetecho/janus-gateway/janus_voicemail_destroy_session",
        "description": "An issue was discovered in Janus through 0.9.1. plugins/janus_voicemail.c in the VoiceMail plugin has a race condition that could cause a server crash.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/eb99ac8674c554bab6775f4378b24e991b4a2fa1",
        "commit_title": "Fixes to leaks and race conditions in VoiceMail plugin",
        "commit_text": "",
        "func_before": "void janus_voicemail_destroy_session(janus_plugin_session *handle, int *error) {\n\tif(g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized)) {\n\t\t*error = -1;\n\t\treturn;\n\t}\n\tjanus_mutex_lock(&sessions_mutex);\n\tjanus_voicemail_session *session = janus_voicemail_lookup_session(handle);\n\tif(!session) {\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\tJANUS_LOG(LOG_ERR, \"No VoiceMail session associated with this handle...\\n\");\n\t\t*error = -2;\n\t\treturn;\n\t}\n\tJANUS_LOG(LOG_VERB, \"Removing VoiceMail session...\\n\");\n\tjanus_voicemail_hangup_media_internal(handle);\n\thandle->plugin_handle = NULL;\n\tg_hash_table_remove(sessions, handle);\n\tjanus_mutex_unlock(&sessions_mutex);\n\n\treturn;\n}",
        "func": "void janus_voicemail_destroy_session(janus_plugin_session *handle, int *error) {\n\tif(g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized)) {\n\t\t*error = -1;\n\t\treturn;\n\t}\n\tjanus_mutex_lock(&sessions_mutex);\n\tjanus_voicemail_session *session = janus_voicemail_lookup_session(handle);\n\tif(!session) {\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\tJANUS_LOG(LOG_ERR, \"No VoiceMail session associated with this handle...\\n\");\n\t\t*error = -2;\n\t\treturn;\n\t}\n\tJANUS_LOG(LOG_VERB, \"Removing VoiceMail session...\\n\");\n\tjanus_voicemail_hangup_media_internal(handle);\n\tg_hash_table_remove(sessions, handle);\n\tjanus_mutex_unlock(&sessions_mutex);\n\n\treturn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,6 @@\n \t}\n \tJANUS_LOG(LOG_VERB, \"Removing VoiceMail session...\\n\");\n \tjanus_voicemail_hangup_media_internal(handle);\n-\thandle->plugin_handle = NULL;\n \tg_hash_table_remove(sessions, handle);\n \tjanus_mutex_unlock(&sessions_mutex);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\thandle->plugin_handle = NULL;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2020-10576",
        "func_name": "meetecho/janus-gateway/janus_voicemail_incoming_rtp",
        "description": "An issue was discovered in Janus through 0.9.1. plugins/janus_voicemail.c in the VoiceMail plugin has a race condition that could cause a server crash.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/eb99ac8674c554bab6775f4378b24e991b4a2fa1",
        "commit_title": "Fixes to leaks and race conditions in VoiceMail plugin",
        "commit_text": "",
        "func_before": "void janus_voicemail_incoming_rtp(janus_plugin_session *handle, janus_plugin_rtp *packet) {\n\tif(handle == NULL || g_atomic_int_get(&handle->stopped) || g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized))\n\t\treturn;\n\tjanus_voicemail_session *session = (janus_voicemail_session *)handle->plugin_handle;\n\tif(!session || g_atomic_int_get(&session->destroyed) || session->stopping || !session->started || session->start_time == 0)\n\t\treturn;\n\tgint64 now = janus_get_monotonic_time();\n\t/* Have 10 seconds passed? */\n\tif((now-session->start_time) >= 10*G_USEC_PER_SEC) {\n\t\t/* FIXME Simulate a \"stop\" coming from the browser */\n\t\tsession->started = FALSE;\n\t\tjanus_refcount_increase(&session->ref);\n\t\tjanus_voicemail_message *msg = g_malloc(sizeof(janus_voicemail_message));\n\t\tmsg->handle = handle;\n\t\tmsg->message = json_pack(\"{ss}\", \"request\", \"stop\");\n\t\tmsg->transaction = NULL;\n\t\tmsg->jsep = NULL;\n\t\tg_async_queue_push(messages, msg);\n\t\treturn;\n\t}\n\t/* Save the frame */\n\tchar *buf = packet->buffer;\n\tuint16_t len = packet->length;\n\tjanus_rtp_header *rtp = (janus_rtp_header *)buf;\n\tuint16_t seq = ntohs(rtp->seq_number);\n\tif(session->seq == 0)\n\t\tsession->seq = seq;\n\tint plen = 0;\n\tconst unsigned char *payload = (const unsigned char *)janus_rtp_payload(buf, len, &plen);\n\tif(!payload) {\n\t\tJANUS_LOG(LOG_ERR, \"Ops! got an error accessing the RTP payload\\n\");\n\t\treturn;\n\t}\n\togg_packet *op = op_from_pkt(payload, plen);\n\t//~ JANUS_LOG(LOG_VERB, \"\\tWriting at position %d (%d)\\n\", seq-session->seq+1, 960*(seq-session->seq+1));\n\top->granulepos = 960*(seq-session->seq+1); // FIXME: get this from the toc byte\n\togg_stream_packetin(session->stream, op);\n\tg_free(op);\n\togg_write(session);\n}",
        "func": "void janus_voicemail_incoming_rtp(janus_plugin_session *handle, janus_plugin_rtp *packet) {\n\tif(handle == NULL || g_atomic_int_get(&handle->stopped) || g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized))\n\t\treturn;\n\tjanus_voicemail_session *session = (janus_voicemail_session *)handle->plugin_handle;\n\tif(!session || g_atomic_int_get(&session->destroyed) || g_atomic_int_get(&session->stopping) ||\n\t\t\t!g_atomic_int_get(&session->started) || session->start_time == 0)\n\t\treturn;\n\tgint64 now = janus_get_monotonic_time();\n\t/* Have 10 seconds passed? */\n\tif((now-session->start_time) >= 10*G_USEC_PER_SEC) {\n\t\t/* FIXME Simulate a \"stop\" coming from the browser */\n\t\tg_atomic_int_set(&session->started, 0);\n\t\tjanus_refcount_increase(&session->ref);\n\t\tjanus_voicemail_message *msg = g_malloc(sizeof(janus_voicemail_message));\n\t\tmsg->handle = handle;\n\t\tmsg->message = json_pack(\"{ss}\", \"request\", \"stop\");\n\t\tmsg->transaction = NULL;\n\t\tmsg->jsep = NULL;\n\t\tg_async_queue_push(messages, msg);\n\t\treturn;\n\t}\n\t/* Save the frame */\n\tchar *buf = packet->buffer;\n\tuint16_t len = packet->length;\n\tjanus_rtp_header *rtp = (janus_rtp_header *)buf;\n\tuint16_t seq = ntohs(rtp->seq_number);\n\tif(session->seq == 0)\n\t\tsession->seq = seq;\n\tint plen = 0;\n\tconst unsigned char *payload = (const unsigned char *)janus_rtp_payload(buf, len, &plen);\n\tif(!payload) {\n\t\tJANUS_LOG(LOG_ERR, \"Ops! got an error accessing the RTP payload\\n\");\n\t\treturn;\n\t}\n\togg_packet *op = op_from_pkt(payload, plen);\n\t//~ JANUS_LOG(LOG_VERB, \"\\tWriting at position %d (%d)\\n\", seq-session->seq+1, 960*(seq-session->seq+1));\n\top->granulepos = 960*(seq-session->seq+1); // FIXME: get this from the toc byte\n\togg_stream_packetin(session->stream, op);\n\tg_free(op);\n\togg_write(session);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,13 +2,14 @@\n \tif(handle == NULL || g_atomic_int_get(&handle->stopped) || g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized))\n \t\treturn;\n \tjanus_voicemail_session *session = (janus_voicemail_session *)handle->plugin_handle;\n-\tif(!session || g_atomic_int_get(&session->destroyed) || session->stopping || !session->started || session->start_time == 0)\n+\tif(!session || g_atomic_int_get(&session->destroyed) || g_atomic_int_get(&session->stopping) ||\n+\t\t\t!g_atomic_int_get(&session->started) || session->start_time == 0)\n \t\treturn;\n \tgint64 now = janus_get_monotonic_time();\n \t/* Have 10 seconds passed? */\n \tif((now-session->start_time) >= 10*G_USEC_PER_SEC) {\n \t\t/* FIXME Simulate a \"stop\" coming from the browser */\n-\t\tsession->started = FALSE;\n+\t\tg_atomic_int_set(&session->started, 0);\n \t\tjanus_refcount_increase(&session->ref);\n \t\tjanus_voicemail_message *msg = g_malloc(sizeof(janus_voicemail_message));\n \t\tmsg->handle = handle;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif(!session || g_atomic_int_get(&session->destroyed) || session->stopping || !session->started || session->start_time == 0)",
                "\t\tsession->started = FALSE;"
            ],
            "added_lines": [
                "\tif(!session || g_atomic_int_get(&session->destroyed) || g_atomic_int_get(&session->stopping) ||",
                "\t\t\t!g_atomic_int_get(&session->started) || session->start_time == 0)",
                "\t\tg_atomic_int_set(&session->started, 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10576",
        "func_name": "meetecho/janus-gateway/janus_voicemail_handler",
        "description": "An issue was discovered in Janus through 0.9.1. plugins/janus_voicemail.c in the VoiceMail plugin has a race condition that could cause a server crash.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/eb99ac8674c554bab6775f4378b24e991b4a2fa1",
        "commit_title": "Fixes to leaks and race conditions in VoiceMail plugin",
        "commit_text": "",
        "func_before": "static void *janus_voicemail_handler(void *data) {\n\tJANUS_LOG(LOG_VERB, \"Joining VoiceMail handler thread\\n\");\n\tjanus_voicemail_message *msg = NULL;\n\tint error_code = 0;\n\tchar error_cause[512];\n\tjson_t *root = NULL;\n\twhile(g_atomic_int_get(&initialized) && !g_atomic_int_get(&stopping)) {\n\t\tmsg = g_async_queue_pop(messages);\n\t\tif(msg == &exit_message)\n\t\t\tbreak;\n\t\tif(msg->handle == NULL) {\n\t\t\tjanus_voicemail_message_free(msg);\n\t\t\tcontinue;\n\t\t}\n\t\tjanus_mutex_lock(&sessions_mutex);\n\t\tjanus_voicemail_session *session = janus_voicemail_lookup_session(msg->handle);\n\t\tif(!session) {\n\t\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t\tJANUS_LOG(LOG_ERR, \"No session associated with this handle...\\n\");\n\t\t\tjanus_voicemail_message_free(msg);\n\t\t\tcontinue;\n\t\t}\n\t\tif(g_atomic_int_get(&session->destroyed)) {\n\t\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t\tjanus_voicemail_message_free(msg);\n\t\t\tcontinue;\n\t\t}\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t/* Handle request */\n\t\terror_code = 0;\n\t\troot = msg->message;\n\t\tif(msg->message == NULL) {\n\t\t\tJANUS_LOG(LOG_ERR, \"No message??\\n\");\n\t\t\terror_code = JANUS_VOICEMAIL_ERROR_NO_MESSAGE;\n\t\t\tg_snprintf(error_cause, 512, \"%s\", \"No message??\");\n\t\t\tgoto error;\n\t\t}\n\t\tif(!json_is_object(root)) {\n\t\t\tJANUS_LOG(LOG_ERR, \"JSON error: not an object\\n\");\n\t\t\terror_code = JANUS_VOICEMAIL_ERROR_INVALID_JSON;\n\t\t\tg_snprintf(error_cause, 512, \"JSON error: not an object\");\n\t\t\tgoto error;\n\t\t}\n\t\t/* Get the request first */\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, request_parameters,\n\t\t\terror_code, error_cause, TRUE,\n\t\t\tJANUS_VOICEMAIL_ERROR_MISSING_ELEMENT, JANUS_VOICEMAIL_ERROR_INVALID_ELEMENT);\n\t\tif(error_code != 0)\n\t\t\tgoto error;\n\t\tjson_t *request = json_object_get(root, \"request\");\n\t\tconst char *request_text = json_string_value(request);\n\t\tjson_t *event = NULL;\n\t\tgboolean sdp_update = FALSE;\n\t\tif(json_object_get(msg->jsep, \"update\") != NULL)\n\t\t\tsdp_update = json_is_true(json_object_get(msg->jsep, \"update\"));\n\t\tif(!strcasecmp(request_text, \"record\")) {\n\t\t\tJANUS_LOG(LOG_VERB, \"Starting new recording\\n\");\n\t\t\tif(session->file != NULL) {\n\t\t\t\tJANUS_LOG(LOG_ERR, \"Already recording (%s)\\n\", session->filename ? session->filename : \"??\");\n\t\t\t\terror_code = JANUS_VOICEMAIL_ERROR_ALREADY_RECORDING;\n\t\t\t\tg_snprintf(error_cause, 512, \"Already recording\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsession->stream = g_malloc0(sizeof(ogg_stream_state));\n\t\t\tif(ogg_stream_init(session->stream, rand()) < 0) {\n\t\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't initialize Ogg stream state\\n\");\n\t\t\t\terror_code = JANUS_VOICEMAIL_ERROR_LIBOGG_ERROR;\n\t\t\t\tg_snprintf(error_cause, 512, \"Couldn't initialize Ogg stream state\\n\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsession->file = fopen(session->filename, \"wb\");\n\t\t\tif(session->file == NULL) {\n\t\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't open output file\\n\");\n\t\t\t\terror_code = JANUS_VOICEMAIL_ERROR_IO_ERROR;\n\t\t\t\tg_snprintf(error_cause, 512, \"Couldn't open output file\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsession->seq = 0;\n\t\t\t/* Write stream headers */\n\t\t\togg_packet *op = op_opushead();\n\t\t\togg_stream_packetin(session->stream, op);\n\t\t\top_free(op);\n\t\t\top = op_opustags();\n\t\t\togg_stream_packetin(session->stream, op);\n\t\t\top_free(op);\n\t\t\togg_flush(session);\n\t\t\t/* Done: now wait for the setup_media callback to be called */\n\t\t\tevent = json_object();\n\t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\t\t\tjson_object_set_new(event, \"status\", json_string(session->started ? \"started\" : \"starting\"));\n\t\t\t/* Also notify event handlers */\n\t\t\tif(notify_events && gateway->events_is_enabled()) {\n\t\t\t\tjson_t *info = json_object();\n\t\t\t\tjson_object_set_new(info, \"event\", json_string(\"starting\"));\n\t\t\t\tgateway->notify_event(&janus_voicemail_plugin, session->handle, info);\n\t\t\t}\n\t\t} else if(!strcasecmp(request_text, \"update\")) {\n\t\t\t/* Only needed in case of renegotiations and ICE restarts (but with 10s messages is this worth it?) */\n\t\t\tJANUS_LOG(LOG_VERB, \"Updating existing recording\\n\");\n\t\t\tif(session->stream == NULL || !session->started) {\n\t\t\t\tJANUS_LOG(LOG_ERR, \"Invalid state (not recording)\\n\");\n\t\t\t\terror_code = JANUS_VOICEMAIL_ERROR_INVALID_STATE;\n\t\t\t\tg_snprintf(error_cause, 512, \"Invalid state (not recording)\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsdp_update = TRUE;\n\t\t\tevent = json_object();\n\t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\t\t\tjson_object_set_new(event, \"status\", json_string(\"updating\"));\n\t\t} else if(!strcasecmp(request_text, \"stop\")) {\n\t\t\t/* Stop the recording */\n\t\t\tsession->started = FALSE;\n\t\t\tsession->stopping = TRUE;\n\t\t\tif(session->file)\n\t\t\t\tfclose(session->file);\n\t\t\tsession->file = NULL;\n\t\t\tif(session->stream)\n\t\t\t\togg_stream_destroy(session->stream);\n\t\t\tsession->stream = NULL;\n\t\t\t/* Done: send the event and close the handle */\n\t\t\tevent = json_object();\n\t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\t\t\tjson_object_set_new(event, \"status\", json_string(\"done\"));\n\t\t\tchar url[1024];\n\t\t\tg_snprintf(url, 1024, \"%s/janus-voicemail-%\"SCNu64\".opus\", recordings_base, session->recording_id);\n\t\t\tjson_object_set_new(event, \"recording\", json_string(url));\n\t\t\t/* Also notify event handlers */\n\t\t\tif(notify_events && gateway->events_is_enabled()) {\n\t\t\t\tjson_t *info = json_object();\n\t\t\t\tjson_object_set_new(info, \"event\", json_string(\"done\"));\n\t\t\t\tgateway->notify_event(&janus_voicemail_plugin, session->handle, info);\n\t\t\t}\n\t\t} else {\n\t\t\tJANUS_LOG(LOG_ERR, \"Unknown request '%s'\\n\", request_text);\n\t\t\terror_code = JANUS_VOICEMAIL_ERROR_INVALID_REQUEST;\n\t\t\tg_snprintf(error_cause, 512, \"Unknown request '%s'\", request_text);\n\t\t\tgoto error;\n\t\t}\n\n\t\t/* Prepare JSON event */\n\t\tJANUS_LOG(LOG_VERB, \"Preparing JSON event as a reply\\n\");\n\t\t/* Any SDP to handle? */\n\t\tconst char *msg_sdp_type = json_string_value(json_object_get(msg->jsep, \"type\"));\n\t\tconst char *msg_sdp = json_string_value(json_object_get(msg->jsep, \"sdp\"));\n\t\tif(!msg_sdp) {\n\t\t\tint ret = gateway->push_event(msg->handle, &janus_voicemail_plugin, msg->transaction, event, NULL);\n\t\t\tJANUS_LOG(LOG_VERB, \"  >> %d (%s)\\n\", ret, janus_get_api_error(ret));\n\t\t\tjson_decref(event);\n\t\t} else {\n\t\t\tJANUS_LOG(LOG_VERB, \"This is involving a negotiation (%s) as well:\\n%s\\n\", msg_sdp_type, msg_sdp);\n\t\t\tconst char *type = NULL;\n\t\t\tif(!strcasecmp(msg_sdp_type, \"offer\"))\n\t\t\t\ttype = \"answer\";\n\t\t\tif(!strcasecmp(msg_sdp_type, \"answer\"))\n\t\t\t\ttype = \"offer\";\n\t\t\tif(sdp_update) {\n\t\t\t\t/* Renegotiation: make sure the user provided an offer, and send answer */\n\t\t\t\tJANUS_LOG(LOG_VERB, \"Request to update existing connection\\n\");\n\t\t\t\tsession->sdp_version++;\t\t/* This needs to be increased when it changes */\n\t\t\t} else {\n\t\t\t\t/* New PeerConnection */\n\t\t\t\tsession->sdp_version = 1;\t/* This needs to be increased when it changes */\n\t\t\t\tsession->sdp_sessid = janus_get_real_time();\n\t\t\t}\n\t\t\t/* Fill the SDP template and use that as our answer */\n\t\t\tchar sdp[1024];\n\t\t\t/* What is the Opus payload type? */\n\t\t\tint opus_pt = janus_get_codec_pt(msg_sdp, \"opus\");\n\t\t\tJANUS_LOG(LOG_VERB, \"Opus payload type is %d\\n\", opus_pt);\n\t\t\tg_snprintf(sdp, 1024, sdp_template,\n\t\t\t\tsession->sdp_sessid,\n\t\t\t\tsession->sdp_version,\n\t\t\t\tsession->recording_id,\t\t\t/* Recording ID */\n\t\t\t\topus_pt,\t\t\t\t\t\t/* Opus payload type */\n\t\t\t\topus_pt\t\t\t\t\t\t\t/* Opus payload type */);\n\t\t\t/* Did the peer negotiate video? */\n\t\t\tif(strstr(msg_sdp, \"m=video\") != NULL) {\n\t\t\t\t/* If so, reject it */\n\t\t\t\tg_strlcat(sdp, \"m=video 0 RTP/SAVPF 0\\r\\n\", 1024);\n\t\t\t}\n\t\t\tjson_t *jsep = json_pack(\"{ssss}\", \"type\", type, \"sdp\", sdp);\n\t\t\t/* How long will the Janus core take to push the event? */\n\t\t\tg_atomic_int_set(&session->hangingup, 0);\n\t\t\tgint64 start = janus_get_monotonic_time();\n\t\t\tint res = gateway->push_event(msg->handle, &janus_voicemail_plugin, msg->transaction, event, jsep);\n\t\t\tJANUS_LOG(LOG_VERB, \"  >> Pushing event: %d (took %\"SCNu64\" us)\\n\", res, janus_get_monotonic_time()-start);\n\t\t\tjson_decref(event);\n\t\t\tjson_decref(jsep);\n\t\t\tif(res != JANUS_OK) {\n\t\t\t\t/* TODO Failed to negotiate? We should remove this participant */\n\t\t\t}\n\t\t}\n\t\tjanus_voicemail_message_free(msg);\n\n\t\tif(session->stopping) {\n\t\t\tgateway->end_session(session->handle);\n\t\t}\n\n\t\tcontinue;\n\nerror:\n\t\t{\n\t\t\t/* Prepare JSON error event */\n\t\t\tjson_t *event = json_object();\n\t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\t\t\tjson_object_set_new(event, \"error_code\", json_integer(error_code));\n\t\t\tjson_object_set_new(event, \"error\", json_string(error_cause));\n\t\t\tint ret = gateway->push_event(msg->handle, &janus_voicemail_plugin, msg->transaction, event, NULL);\n\t\t\tJANUS_LOG(LOG_VERB, \"  >> Pushing event: %d (%s)\\n\", ret, janus_get_api_error(ret));\n\t\t\tjson_decref(event);\n\t\t\tjanus_voicemail_message_free(msg);\n\t\t}\n\t}\n\tJANUS_LOG(LOG_VERB, \"Leaving VoiceMail handler thread\\n\");\n\treturn NULL;\n}",
        "func": "static void *janus_voicemail_handler(void *data) {\n\tJANUS_LOG(LOG_VERB, \"Joining VoiceMail handler thread\\n\");\n\tjanus_voicemail_message *msg = NULL;\n\tint error_code = 0;\n\tchar error_cause[512];\n\tjson_t *root = NULL;\n\twhile(g_atomic_int_get(&initialized) && !g_atomic_int_get(&stopping)) {\n\t\tmsg = g_async_queue_pop(messages);\n\t\tif(msg == &exit_message)\n\t\t\tbreak;\n\t\tif(msg->handle == NULL) {\n\t\t\tjanus_voicemail_message_free(msg);\n\t\t\tcontinue;\n\t\t}\n\t\tjanus_mutex_lock(&sessions_mutex);\n\t\tjanus_voicemail_session *session = janus_voicemail_lookup_session(msg->handle);\n\t\tif(!session) {\n\t\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t\tJANUS_LOG(LOG_ERR, \"No session associated with this handle...\\n\");\n\t\t\tjanus_voicemail_message_free(msg);\n\t\t\tcontinue;\n\t\t}\n\t\tif(g_atomic_int_get(&session->destroyed)) {\n\t\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t\tjanus_voicemail_message_free(msg);\n\t\t\tcontinue;\n\t\t}\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t/* Handle request */\n\t\terror_code = 0;\n\t\troot = msg->message;\n\t\tif(msg->message == NULL) {\n\t\t\tJANUS_LOG(LOG_ERR, \"No message??\\n\");\n\t\t\terror_code = JANUS_VOICEMAIL_ERROR_NO_MESSAGE;\n\t\t\tg_snprintf(error_cause, 512, \"%s\", \"No message??\");\n\t\t\tgoto error;\n\t\t}\n\t\tif(!json_is_object(root)) {\n\t\t\tJANUS_LOG(LOG_ERR, \"JSON error: not an object\\n\");\n\t\t\terror_code = JANUS_VOICEMAIL_ERROR_INVALID_JSON;\n\t\t\tg_snprintf(error_cause, 512, \"JSON error: not an object\");\n\t\t\tgoto error;\n\t\t}\n\t\t/* Get the request first */\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, request_parameters,\n\t\t\terror_code, error_cause, TRUE,\n\t\t\tJANUS_VOICEMAIL_ERROR_MISSING_ELEMENT, JANUS_VOICEMAIL_ERROR_INVALID_ELEMENT);\n\t\tif(error_code != 0)\n\t\t\tgoto error;\n\t\tjson_t *request = json_object_get(root, \"request\");\n\t\tconst char *request_text = json_string_value(request);\n\t\tjson_t *event = NULL;\n\t\tgboolean sdp_update = FALSE;\n\t\tif(json_object_get(msg->jsep, \"update\") != NULL)\n\t\t\tsdp_update = json_is_true(json_object_get(msg->jsep, \"update\"));\n\t\tif(!strcasecmp(request_text, \"record\")) {\n\t\t\tJANUS_LOG(LOG_VERB, \"Starting new recording\\n\");\n\t\t\tif(session->file != NULL) {\n\t\t\t\tJANUS_LOG(LOG_ERR, \"Already recording (%s)\\n\", session->filename ? session->filename : \"??\");\n\t\t\t\terror_code = JANUS_VOICEMAIL_ERROR_ALREADY_RECORDING;\n\t\t\t\tg_snprintf(error_cause, 512, \"Already recording\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsession->stream = g_malloc0(sizeof(ogg_stream_state));\n\t\t\tif(ogg_stream_init(session->stream, rand()) < 0) {\n\t\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't initialize Ogg stream state\\n\");\n\t\t\t\terror_code = JANUS_VOICEMAIL_ERROR_LIBOGG_ERROR;\n\t\t\t\tg_snprintf(error_cause, 512, \"Couldn't initialize Ogg stream state\\n\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsession->file = fopen(session->filename, \"wb\");\n\t\t\tif(session->file == NULL) {\n\t\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't open output file\\n\");\n\t\t\t\terror_code = JANUS_VOICEMAIL_ERROR_IO_ERROR;\n\t\t\t\tg_snprintf(error_cause, 512, \"Couldn't open output file\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsession->seq = 0;\n\t\t\t/* Write stream headers */\n\t\t\togg_packet *op = op_opushead();\n\t\t\togg_stream_packetin(session->stream, op);\n\t\t\top_free(op);\n\t\t\top = op_opustags();\n\t\t\togg_stream_packetin(session->stream, op);\n\t\t\top_free(op);\n\t\t\togg_flush(session);\n\t\t\t/* Done: now wait for the setup_media callback to be called */\n\t\t\tevent = json_object();\n\t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\t\t\tjson_object_set_new(event, \"status\", json_string(g_atomic_int_get(&session->started) ? \"started\" : \"starting\"));\n\t\t\t/* Also notify event handlers */\n\t\t\tif(notify_events && gateway->events_is_enabled()) {\n\t\t\t\tjson_t *info = json_object();\n\t\t\t\tjson_object_set_new(info, \"event\", json_string(\"starting\"));\n\t\t\t\tgateway->notify_event(&janus_voicemail_plugin, session->handle, info);\n\t\t\t}\n\t\t} else if(!strcasecmp(request_text, \"update\")) {\n\t\t\t/* Only needed in case of renegotiations and ICE restarts (but with 10s messages is this worth it?) */\n\t\t\tJANUS_LOG(LOG_VERB, \"Updating existing recording\\n\");\n\t\t\tif(session->stream == NULL || !g_atomic_int_get(&session->started)) {\n\t\t\t\tJANUS_LOG(LOG_ERR, \"Invalid state (not recording)\\n\");\n\t\t\t\terror_code = JANUS_VOICEMAIL_ERROR_INVALID_STATE;\n\t\t\t\tg_snprintf(error_cause, 512, \"Invalid state (not recording)\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsdp_update = TRUE;\n\t\t\tevent = json_object();\n\t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\t\t\tjson_object_set_new(event, \"status\", json_string(\"updating\"));\n\t\t} else if(!strcasecmp(request_text, \"stop\")) {\n\t\t\t/* Stop the recording */\n\t\t\tg_atomic_int_set(&session->started, 0);\n\t\t\tg_atomic_int_set(&session->stopping, 1);\n\t\t\tif(session->file)\n\t\t\t\tfclose(session->file);\n\t\t\tsession->file = NULL;\n\t\t\tif(session->stream)\n\t\t\t\togg_stream_destroy(session->stream);\n\t\t\tsession->stream = NULL;\n\t\t\t/* Done: send the event and close the handle */\n\t\t\tevent = json_object();\n\t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\t\t\tjson_object_set_new(event, \"status\", json_string(\"done\"));\n\t\t\tif(session->recording_id > 0) {\n\t\t\t\tchar url[1024];\n\t\t\t\tg_snprintf(url, 1024, \"%s/janus-voicemail-%\"SCNu64\".opus\", recordings_base, session->recording_id);\n\t\t\t\tjson_object_set_new(event, \"recording\", json_string(url));\n\t\t\t}\n\t\t\t/* Also notify event handlers */\n\t\t\tif(notify_events && gateway->events_is_enabled()) {\n\t\t\t\tjson_t *info = json_object();\n\t\t\t\tjson_object_set_new(info, \"event\", json_string(\"done\"));\n\t\t\t\tgateway->notify_event(&janus_voicemail_plugin, session->handle, info);\n\t\t\t}\n\t\t} else {\n\t\t\tJANUS_LOG(LOG_ERR, \"Unknown request '%s'\\n\", request_text);\n\t\t\terror_code = JANUS_VOICEMAIL_ERROR_INVALID_REQUEST;\n\t\t\tg_snprintf(error_cause, 512, \"Unknown request '%s'\", request_text);\n\t\t\tgoto error;\n\t\t}\n\n\t\t/* Prepare JSON event */\n\t\tJANUS_LOG(LOG_VERB, \"Preparing JSON event as a reply\\n\");\n\t\t/* Any SDP to handle? */\n\t\tconst char *msg_sdp_type = json_string_value(json_object_get(msg->jsep, \"type\"));\n\t\tconst char *msg_sdp = json_string_value(json_object_get(msg->jsep, \"sdp\"));\n\t\tif(!msg_sdp) {\n\t\t\tint ret = gateway->push_event(msg->handle, &janus_voicemail_plugin, msg->transaction, event, NULL);\n\t\t\tJANUS_LOG(LOG_VERB, \"  >> %d (%s)\\n\", ret, janus_get_api_error(ret));\n\t\t\tjson_decref(event);\n\t\t} else {\n\t\t\tJANUS_LOG(LOG_VERB, \"This is involving a negotiation (%s) as well:\\n%s\\n\", msg_sdp_type, msg_sdp);\n\t\t\tconst char *type = NULL;\n\t\t\tif(!strcasecmp(msg_sdp_type, \"offer\"))\n\t\t\t\ttype = \"answer\";\n\t\t\tif(!strcasecmp(msg_sdp_type, \"answer\"))\n\t\t\t\ttype = \"offer\";\n\t\t\tif(sdp_update) {\n\t\t\t\t/* Renegotiation: make sure the user provided an offer, and send answer */\n\t\t\t\tJANUS_LOG(LOG_VERB, \"Request to update existing connection\\n\");\n\t\t\t\tsession->sdp_version++;\t\t/* This needs to be increased when it changes */\n\t\t\t} else {\n\t\t\t\t/* New PeerConnection */\n\t\t\t\tsession->sdp_version = 1;\t/* This needs to be increased when it changes */\n\t\t\t\tsession->sdp_sessid = janus_get_real_time();\n\t\t\t}\n\t\t\t/* Fill the SDP template and use that as our answer */\n\t\t\tchar sdp[1024];\n\t\t\t/* What is the Opus payload type? */\n\t\t\tint opus_pt = janus_get_codec_pt(msg_sdp, \"opus\");\n\t\t\tJANUS_LOG(LOG_VERB, \"Opus payload type is %d\\n\", opus_pt);\n\t\t\tg_snprintf(sdp, 1024, sdp_template,\n\t\t\t\tsession->sdp_sessid,\n\t\t\t\tsession->sdp_version,\n\t\t\t\tsession->recording_id,\t\t\t/* Recording ID */\n\t\t\t\topus_pt,\t\t\t\t\t\t/* Opus payload type */\n\t\t\t\topus_pt\t\t\t\t\t\t\t/* Opus payload type */);\n\t\t\t/* Did the peer negotiate video? */\n\t\t\tif(strstr(msg_sdp, \"m=video\") != NULL) {\n\t\t\t\t/* If so, reject it */\n\t\t\t\tg_strlcat(sdp, \"m=video 0 RTP/SAVPF 0\\r\\n\", 1024);\n\t\t\t}\n\t\t\tjson_t *jsep = json_pack(\"{ssss}\", \"type\", type, \"sdp\", sdp);\n\t\t\t/* How long will the Janus core take to push the event? */\n\t\t\tg_atomic_int_set(&session->hangingup, 0);\n\t\t\tgint64 start = janus_get_monotonic_time();\n\t\t\tint res = gateway->push_event(msg->handle, &janus_voicemail_plugin, msg->transaction, event, jsep);\n\t\t\tJANUS_LOG(LOG_VERB, \"  >> Pushing event: %d (took %\"SCNu64\" us)\\n\", res, janus_get_monotonic_time()-start);\n\t\t\tjson_decref(event);\n\t\t\tjson_decref(jsep);\n\t\t\tif(res != JANUS_OK) {\n\t\t\t\t/* TODO Failed to negotiate? We should remove this participant */\n\t\t\t}\n\t\t}\n\n\t\t/* Tear down the session if we're done */\n\t\tif(g_atomic_int_get(&session->stopping))\n\t\t\tgateway->end_session(session->handle);\n\t\tjanus_voicemail_message_free(msg);\n\n\t\tcontinue;\n\nerror:\n\t\t{\n\t\t\t/* Prepare JSON error event */\n\t\t\tjson_t *event = json_object();\n\t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\t\t\tjson_object_set_new(event, \"error_code\", json_integer(error_code));\n\t\t\tjson_object_set_new(event, \"error\", json_string(error_cause));\n\t\t\tint ret = gateway->push_event(msg->handle, &janus_voicemail_plugin, msg->transaction, event, NULL);\n\t\t\tJANUS_LOG(LOG_VERB, \"  >> Pushing event: %d (%s)\\n\", ret, janus_get_api_error(ret));\n\t\t\tjson_decref(event);\n\t\t\tjanus_voicemail_message_free(msg);\n\t\t}\n\t}\n\tJANUS_LOG(LOG_VERB, \"Leaving VoiceMail handler thread\\n\");\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -87,7 +87,7 @@\n \t\t\t/* Done: now wait for the setup_media callback to be called */\n \t\t\tevent = json_object();\n \t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n-\t\t\tjson_object_set_new(event, \"status\", json_string(session->started ? \"started\" : \"starting\"));\n+\t\t\tjson_object_set_new(event, \"status\", json_string(g_atomic_int_get(&session->started) ? \"started\" : \"starting\"));\n \t\t\t/* Also notify event handlers */\n \t\t\tif(notify_events && gateway->events_is_enabled()) {\n \t\t\t\tjson_t *info = json_object();\n@@ -97,7 +97,7 @@\n \t\t} else if(!strcasecmp(request_text, \"update\")) {\n \t\t\t/* Only needed in case of renegotiations and ICE restarts (but with 10s messages is this worth it?) */\n \t\t\tJANUS_LOG(LOG_VERB, \"Updating existing recording\\n\");\n-\t\t\tif(session->stream == NULL || !session->started) {\n+\t\t\tif(session->stream == NULL || !g_atomic_int_get(&session->started)) {\n \t\t\t\tJANUS_LOG(LOG_ERR, \"Invalid state (not recording)\\n\");\n \t\t\t\terror_code = JANUS_VOICEMAIL_ERROR_INVALID_STATE;\n \t\t\t\tg_snprintf(error_cause, 512, \"Invalid state (not recording)\");\n@@ -109,8 +109,8 @@\n \t\t\tjson_object_set_new(event, \"status\", json_string(\"updating\"));\n \t\t} else if(!strcasecmp(request_text, \"stop\")) {\n \t\t\t/* Stop the recording */\n-\t\t\tsession->started = FALSE;\n-\t\t\tsession->stopping = TRUE;\n+\t\t\tg_atomic_int_set(&session->started, 0);\n+\t\t\tg_atomic_int_set(&session->stopping, 1);\n \t\t\tif(session->file)\n \t\t\t\tfclose(session->file);\n \t\t\tsession->file = NULL;\n@@ -121,9 +121,11 @@\n \t\t\tevent = json_object();\n \t\t\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n \t\t\tjson_object_set_new(event, \"status\", json_string(\"done\"));\n-\t\t\tchar url[1024];\n-\t\t\tg_snprintf(url, 1024, \"%s/janus-voicemail-%\"SCNu64\".opus\", recordings_base, session->recording_id);\n-\t\t\tjson_object_set_new(event, \"recording\", json_string(url));\n+\t\t\tif(session->recording_id > 0) {\n+\t\t\t\tchar url[1024];\n+\t\t\t\tg_snprintf(url, 1024, \"%s/janus-voicemail-%\"SCNu64\".opus\", recordings_base, session->recording_id);\n+\t\t\t\tjson_object_set_new(event, \"recording\", json_string(url));\n+\t\t\t}\n \t\t\t/* Also notify event handlers */\n \t\t\tif(notify_events && gateway->events_is_enabled()) {\n \t\t\t\tjson_t *info = json_object();\n@@ -190,11 +192,11 @@\n \t\t\t\t/* TODO Failed to negotiate? We should remove this participant */\n \t\t\t}\n \t\t}\n+\n+\t\t/* Tear down the session if we're done */\n+\t\tif(g_atomic_int_get(&session->stopping))\n+\t\t\tgateway->end_session(session->handle);\n \t\tjanus_voicemail_message_free(msg);\n-\n-\t\tif(session->stopping) {\n-\t\t\tgateway->end_session(session->handle);\n-\t\t}\n \n \t\tcontinue;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tjson_object_set_new(event, \"status\", json_string(session->started ? \"started\" : \"starting\"));",
                "\t\t\tif(session->stream == NULL || !session->started) {",
                "\t\t\tsession->started = FALSE;",
                "\t\t\tsession->stopping = TRUE;",
                "\t\t\tchar url[1024];",
                "\t\t\tg_snprintf(url, 1024, \"%s/janus-voicemail-%\"SCNu64\".opus\", recordings_base, session->recording_id);",
                "\t\t\tjson_object_set_new(event, \"recording\", json_string(url));",
                "",
                "\t\tif(session->stopping) {",
                "\t\t\tgateway->end_session(session->handle);",
                "\t\t}"
            ],
            "added_lines": [
                "\t\t\tjson_object_set_new(event, \"status\", json_string(g_atomic_int_get(&session->started) ? \"started\" : \"starting\"));",
                "\t\t\tif(session->stream == NULL || !g_atomic_int_get(&session->started)) {",
                "\t\t\tg_atomic_int_set(&session->started, 0);",
                "\t\t\tg_atomic_int_set(&session->stopping, 1);",
                "\t\t\tif(session->recording_id > 0) {",
                "\t\t\t\tchar url[1024];",
                "\t\t\t\tg_snprintf(url, 1024, \"%s/janus-voicemail-%\"SCNu64\".opus\", recordings_base, session->recording_id);",
                "\t\t\t\tjson_object_set_new(event, \"recording\", json_string(url));",
                "\t\t\t}",
                "",
                "\t\t/* Tear down the session if we're done */",
                "\t\tif(g_atomic_int_get(&session->stopping))",
                "\t\t\tgateway->end_session(session->handle);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10576",
        "func_name": "meetecho/janus-gateway/janus_voicemail_hangup_media_internal",
        "description": "An issue was discovered in Janus through 0.9.1. plugins/janus_voicemail.c in the VoiceMail plugin has a race condition that could cause a server crash.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/eb99ac8674c554bab6775f4378b24e991b4a2fa1",
        "commit_title": "Fixes to leaks and race conditions in VoiceMail plugin",
        "commit_text": "",
        "func_before": "static void janus_voicemail_hangup_media_internal(janus_plugin_session *handle) {\n\tif(g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized))\n\t\treturn;\n\tjanus_voicemail_session *session = janus_voicemail_lookup_session(handle);\n\tif(!session) {\n\t\tJANUS_LOG(LOG_ERR, \"No session associated with this handle...\\n\");\n\t\treturn;\n\t}\n\tsession->started = FALSE;\n\tif(g_atomic_int_get(&session->destroyed))\n\t\treturn;\n\tif(!g_atomic_int_compare_and_exchange(&session->hangingup, 0, 1))\n\t\treturn;\n\t/* Close and reset stuff */\n\tif(session->file)\n\t\tfclose(session->file);\n\tsession->file = NULL;\n\tif(session->stream)\n\t\togg_stream_destroy(session->stream);\n\tsession->stream = NULL;\n\tg_atomic_int_set(&session->hangingup, 0);\n}",
        "func": "static void janus_voicemail_hangup_media_internal(janus_plugin_session *handle) {\n\tif(g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized))\n\t\treturn;\n\tjanus_voicemail_session *session = janus_voicemail_lookup_session(handle);\n\tif(!session) {\n\t\tJANUS_LOG(LOG_ERR, \"No session associated with this handle...\\n\");\n\t\treturn;\n\t}\n\tg_atomic_int_set(&session->started, 0);\n\tif(g_atomic_int_get(&session->destroyed))\n\t\treturn;\n\tif(!g_atomic_int_compare_and_exchange(&session->hangingup, 0, 1))\n\t\treturn;\n\t/* Close and reset stuff */\n\tif(session->file)\n\t\tfclose(session->file);\n\tsession->file = NULL;\n\tif(session->stream)\n\t\togg_stream_destroy(session->stream);\n\tsession->stream = NULL;\n\tg_atomic_int_set(&session->hangingup, 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n \t\tJANUS_LOG(LOG_ERR, \"No session associated with this handle...\\n\");\n \t\treturn;\n \t}\n-\tsession->started = FALSE;\n+\tg_atomic_int_set(&session->started, 0);\n \tif(g_atomic_int_get(&session->destroyed))\n \t\treturn;\n \tif(!g_atomic_int_compare_and_exchange(&session->hangingup, 0, 1))",
        "diff_line_info": {
            "deleted_lines": [
                "\tsession->started = FALSE;"
            ],
            "added_lines": [
                "\tg_atomic_int_set(&session->started, 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10576",
        "func_name": "meetecho/janus-gateway/janus_voicemail_create_session",
        "description": "An issue was discovered in Janus through 0.9.1. plugins/janus_voicemail.c in the VoiceMail plugin has a race condition that could cause a server crash.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/eb99ac8674c554bab6775f4378b24e991b4a2fa1",
        "commit_title": "Fixes to leaks and race conditions in VoiceMail plugin",
        "commit_text": "",
        "func_before": "void janus_voicemail_create_session(janus_plugin_session *handle, int *error) {\n\tif(g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized)) {\n\t\t*error = -1;\n\t\treturn;\n\t}\n\tjanus_voicemail_session *session = g_malloc0(sizeof(janus_voicemail_session));\n\tsession->handle = handle;\n\tsession->recording_id = janus_random_uint64();\n\tsession->start_time = 0;\n\tsession->stream = NULL;\n\tchar f[255];\n\tg_snprintf(f, 255, \"%s/janus-voicemail-%\"SCNu64\".opus\", recordings_path, session->recording_id);\n\tsession->filename = g_strdup(f);\n\tsession->file = NULL;\n\tsession->seq = 0;\n\tsession->started = FALSE;\n\tsession->stopping = FALSE;\n\tg_atomic_int_set(&session->hangingup, 0);\n\tg_atomic_int_set(&session->destroyed, 0);\n\tjanus_refcount_init(&session->ref, janus_voicemail_session_free);\n\thandle->plugin_handle = session;\n\n\tjanus_mutex_lock(&sessions_mutex);\n\tg_hash_table_insert(sessions, handle, session);\n\tjanus_mutex_unlock(&sessions_mutex);\n\n\treturn;\n}",
        "func": "void janus_voicemail_create_session(janus_plugin_session *handle, int *error) {\n\tif(g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized)) {\n\t\t*error = -1;\n\t\treturn;\n\t}\n\tjanus_voicemail_session *session = g_malloc0(sizeof(janus_voicemail_session));\n\tsession->handle = handle;\n\tsession->recording_id = janus_random_uint64();\n\tsession->start_time = 0;\n\tsession->stream = NULL;\n\tchar f[255];\n\tg_snprintf(f, 255, \"%s/janus-voicemail-%\"SCNu64\".opus\", recordings_path, session->recording_id);\n\tsession->filename = g_strdup(f);\n\tsession->file = NULL;\n\tsession->seq = 0;\n\tg_atomic_int_set(&session->started, 0);\n\tg_atomic_int_set(&session->stopping, 0);\n\tg_atomic_int_set(&session->hangingup, 0);\n\tg_atomic_int_set(&session->destroyed, 0);\n\tjanus_refcount_init(&session->ref, janus_voicemail_session_free);\n\thandle->plugin_handle = session;\n\n\tjanus_mutex_lock(&sessions_mutex);\n\tg_hash_table_insert(sessions, handle, session);\n\tjanus_mutex_unlock(&sessions_mutex);\n\n\treturn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,8 +13,8 @@\n \tsession->filename = g_strdup(f);\n \tsession->file = NULL;\n \tsession->seq = 0;\n-\tsession->started = FALSE;\n-\tsession->stopping = FALSE;\n+\tg_atomic_int_set(&session->started, 0);\n+\tg_atomic_int_set(&session->stopping, 0);\n \tg_atomic_int_set(&session->hangingup, 0);\n \tg_atomic_int_set(&session->destroyed, 0);\n \tjanus_refcount_init(&session->ref, janus_voicemail_session_free);",
        "diff_line_info": {
            "deleted_lines": [
                "\tsession->started = FALSE;",
                "\tsession->stopping = FALSE;"
            ],
            "added_lines": [
                "\tg_atomic_int_set(&session->started, 0);",
                "\tg_atomic_int_set(&session->stopping, 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10576",
        "func_name": "meetecho/janus-gateway/janus_voicemail_session_free",
        "description": "An issue was discovered in Janus through 0.9.1. plugins/janus_voicemail.c in the VoiceMail plugin has a race condition that could cause a server crash.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/eb99ac8674c554bab6775f4378b24e991b4a2fa1",
        "commit_title": "Fixes to leaks and race conditions in VoiceMail plugin",
        "commit_text": "",
        "func_before": "static void janus_voicemail_session_free(const janus_refcount *session_ref) {\n\tjanus_voicemail_session *session = janus_refcount_containerof(session_ref, janus_voicemail_session, ref);\n\t/* Remove the reference to the core plugin session */\n\tjanus_refcount_decrease(&session->handle->ref);\n\t/* This session can be destroyed, free all the resources */\n\tg_free(session);\n}",
        "func": "static void janus_voicemail_session_free(const janus_refcount *session_ref) {\n\tjanus_voicemail_session *session = janus_refcount_containerof(session_ref, janus_voicemail_session, ref);\n\t/* Remove the reference to the core plugin session */\n\tjanus_refcount_decrease(&session->handle->ref);\n\t/* This session can be destroyed, free all the resources */\n\tg_free(session->filename);\n\tif(session->file)\n\t\tfclose(session->file);\n\tg_free(session);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,5 +3,8 @@\n \t/* Remove the reference to the core plugin session */\n \tjanus_refcount_decrease(&session->handle->ref);\n \t/* This session can be destroyed, free all the resources */\n+\tg_free(session->filename);\n+\tif(session->file)\n+\t\tfclose(session->file);\n \tg_free(session);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tg_free(session->filename);",
                "\tif(session->file)",
                "\t\tfclose(session->file);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10576",
        "func_name": "meetecho/janus-gateway/janus_voicemail_setup_media",
        "description": "An issue was discovered in Janus through 0.9.1. plugins/janus_voicemail.c in the VoiceMail plugin has a race condition that could cause a server crash.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/eb99ac8674c554bab6775f4378b24e991b4a2fa1",
        "commit_title": "Fixes to leaks and race conditions in VoiceMail plugin",
        "commit_text": "",
        "func_before": "void janus_voicemail_setup_media(janus_plugin_session *handle) {\n\tJANUS_LOG(LOG_INFO, \"[%s-%p] WebRTC media is now available\\n\", JANUS_VOICEMAIL_PACKAGE, handle);\n\tif(g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized))\n\t\treturn;\n\tjanus_mutex_lock(&sessions_mutex);\n\tjanus_voicemail_session *session = janus_voicemail_lookup_session(handle);\n\tif(!session) {\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\tJANUS_LOG(LOG_ERR, \"No session associated with this handle...\\n\");\n\t\treturn;\n\t}\n\tif(g_atomic_int_get(&session->destroyed)) {\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\treturn;\n\t}\n\tjanus_refcount_increase(&session->ref);\n\tjanus_mutex_unlock(&sessions_mutex);\n\tg_atomic_int_set(&session->hangingup, 0);\n\t/* Only start recording this peer when we get this event */\n\tsession->start_time = janus_get_monotonic_time();\n\tsession->started = TRUE;\n\t/* Prepare JSON event */\n\tjson_t *event = json_object();\n\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\tjson_object_set_new(event, \"status\", json_string(\"started\"));\n\tint ret = gateway->push_event(handle, &janus_voicemail_plugin, NULL, event, NULL);\n\tJANUS_LOG(LOG_VERB, \"  >> Pushing event: %d (%s)\\n\", ret, janus_get_api_error(ret));\n\tjson_decref(event);\n\tjanus_refcount_decrease(&session->ref);\n}",
        "func": "void janus_voicemail_setup_media(janus_plugin_session *handle) {\n\tJANUS_LOG(LOG_INFO, \"[%s-%p] WebRTC media is now available\\n\", JANUS_VOICEMAIL_PACKAGE, handle);\n\tif(g_atomic_int_get(&stopping) || !g_atomic_int_get(&initialized))\n\t\treturn;\n\tjanus_mutex_lock(&sessions_mutex);\n\tjanus_voicemail_session *session = janus_voicemail_lookup_session(handle);\n\tif(!session) {\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\tJANUS_LOG(LOG_ERR, \"No session associated with this handle...\\n\");\n\t\treturn;\n\t}\n\tif(g_atomic_int_get(&session->destroyed)) {\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\treturn;\n\t}\n\tjanus_refcount_increase(&session->ref);\n\tjanus_mutex_unlock(&sessions_mutex);\n\tg_atomic_int_set(&session->hangingup, 0);\n\t/* Only start recording this peer when we get this event */\n\tsession->start_time = janus_get_monotonic_time();\n\tg_atomic_int_set(&session->started, 1);\n\t/* Prepare JSON event */\n\tjson_t *event = json_object();\n\tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));\n\tjson_object_set_new(event, \"status\", json_string(\"started\"));\n\tint ret = gateway->push_event(handle, &janus_voicemail_plugin, NULL, event, NULL);\n\tJANUS_LOG(LOG_VERB, \"  >> Pushing event: %d (%s)\\n\", ret, janus_get_api_error(ret));\n\tjson_decref(event);\n\tjanus_refcount_decrease(&session->ref);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,7 +18,7 @@\n \tg_atomic_int_set(&session->hangingup, 0);\n \t/* Only start recording this peer when we get this event */\n \tsession->start_time = janus_get_monotonic_time();\n-\tsession->started = TRUE;\n+\tg_atomic_int_set(&session->started, 1);\n \t/* Prepare JSON event */\n \tjson_t *event = json_object();\n \tjson_object_set_new(event, \"voicemail\", json_string(\"event\"));",
        "diff_line_info": {
            "deleted_lines": [
                "\tsession->started = TRUE;"
            ],
            "added_lines": [
                "\tg_atomic_int_set(&session->started, 1);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10577",
        "func_name": "meetecho/janus-gateway/janus_process_incoming_request",
        "description": "An issue was discovered in Janus through 0.9.1. janus.c has multiple concurrent threads that misuse the source property of a session, leading to a race condition when claiming sessions.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/bc319056603295953f4a70999bc4553785c6e5ef",
        "commit_title": "Fix rare race condition when claiming sessions",
        "commit_text": "",
        "func_before": "int janus_process_incoming_request(janus_request *request) {\n\tint ret = -1;\n\tif(request == NULL) {\n\t\tJANUS_LOG(LOG_ERR, \"Missing request or payload to process, giving up...\\n\");\n\t\treturn ret;\n\t}\n\tint error_code = 0;\n\tchar error_cause[100];\n\tjson_t *root = request->message;\n\t/* Ok, let's start with the ids */\n\tguint64 session_id = 0, handle_id = 0;\n\tjson_t *s = json_object_get(root, \"session_id\");\n\tif(s && json_is_integer(s))\n\t\tsession_id = json_integer_value(s);\n\tjson_t *h = json_object_get(root, \"handle_id\");\n\tif(h && json_is_integer(h))\n\t\thandle_id = json_integer_value(h);\n\n\tjanus_session *session = NULL;\n\tjanus_ice_handle *handle = NULL;\n\n\t/* Get transaction and message request */\n\tJANUS_VALIDATE_JSON_OBJECT(root, incoming_request_parameters,\n\t\terror_code, error_cause, FALSE,\n\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\tif(error_code != 0) {\n\t\tret = janus_process_error_string(request, session_id, NULL, error_code, error_cause);\n\t\tgoto jsondone;\n\t}\n\tjson_t *transaction = json_object_get(root, \"transaction\");\n\tconst gchar *transaction_text = json_string_value(transaction);\n\tjson_t *message = json_object_get(root, \"janus\");\n\tconst gchar *message_text = json_string_value(message);\n\n\tif(session_id == 0 && handle_id == 0) {\n\t\t/* Can only be a 'Create new session', a 'Get info' or a 'Ping/Pong' request */\n\t\tif(!strcasecmp(message_text, \"info\")) {\n\t\t\tret = janus_process_success(request, janus_info(transaction_text));\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(!strcasecmp(message_text, \"ping\")) {\n\t\t\t/* Prepare JSON reply */\n\t\t\tjson_t *reply = janus_create_message(\"pong\", 0, transaction_text);\n\t\t\tret = janus_process_success(request, reply);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(strcasecmp(message_text, \"create\")) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Make sure we're accepting new sessions */\n\t\tif(!accept_new_sessions) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_NOT_ACCEPTING_SESSIONS, NULL);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Any secret/token to check? */\n\t\tret = janus_request_check_secret(request, session_id, transaction_text);\n\t\tif(ret != 0) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED, NULL);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tsession_id = 0;\n\t\tjson_t *id = json_object_get(root, \"id\");\n\t\tif(id != NULL) {\n\t\t\t/* The application provided the session ID to use */\n\t\t\tsession_id = json_integer_value(id);\n\t\t\tif(session_id > 0 && (session = janus_session_find(session_id)) != NULL) {\n\t\t\t\t/* Session ID already taken */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_CONFLICT, \"Session ID already in use\");\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t}\n\t\t/* Handle it */\n\t\tsession = janus_session_create(session_id);\n\t\tif(session == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Memory error\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tsession_id = session->session_id;\n\t\t/* We increase the counter as this request is using the session */\n\t\tjanus_refcount_increase(&session->ref);\n\t\t/* Take note of the request source that originated this session (HTTP, WebSockets, RabbitMQ?) */\n\t\tsession->source = janus_request_new(request->transport, request->instance, NULL, FALSE, NULL);\n\t\t/* Notify the source that a new session has been created */\n\t\trequest->transport->session_created(request->instance, session->session_id);\n\t\t/* Notify event handlers */\n\t\tif(janus_events_is_enabled()) {\n\t\t\t/* Session created, add info on the transport that originated it */\n\t\t\tjson_t *transport = json_object();\n\t\t\tjson_object_set_new(transport, \"transport\", json_string(session->source->transport->get_package()));\n\t\t\tchar id[32];\n\t\t\tmemset(id, 0, sizeof(id));\n\t\t\tg_snprintf(id, sizeof(id), \"%p\", session->source->instance);\n\t\t\tjson_object_set_new(transport, \"id\", json_string(id));\n\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_SESSION, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\tsession_id, \"created\", transport);\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", 0, transaction_text);\n\t\tjson_t *data = json_object();\n\t\tjson_object_set_new(data, \"id\", json_integer(session_id));\n\t\tjson_object_set_new(reply, \"data\", data);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t\tgoto jsondone;\n\t}\n\tif(session_id < 1) {\n\t\tJANUS_LOG(LOG_ERR, \"Invalid session\\n\");\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, NULL);\n\t\tgoto jsondone;\n\t}\n\tif(h && handle_id < 1) {\n\t\tJANUS_LOG(LOG_ERR, \"Invalid handle\\n\");\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, NULL);\n\t\tgoto jsondone;\n\t}\n\n\t/* Go on with the processing */\n\tret = janus_request_check_secret(request, session_id, transaction_text);\n\tif(ret != 0) {\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED, NULL);\n\t\tgoto jsondone;\n\t}\n\n\t/* If we got here, make sure we have a session (and/or a handle) */\n\tsession = janus_session_find(session_id);\n\tif(!session) {\n\t\tJANUS_LOG(LOG_ERR, \"Couldn't find any session %\"SCNu64\"...\\n\", session_id);\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, \"No such session %\"SCNu64\"\", session_id);\n\t\tgoto jsondone;\n\t}\n\t/* Update the last activity timer */\n\tsession->last_activity = janus_get_monotonic_time();\n\thandle = NULL;\n\tif(handle_id > 0) {\n\t\thandle = janus_session_handles_find(session, handle_id);\n\t\tif(!handle) {\n\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't find any handle %\"SCNu64\" in session %\"SCNu64\"...\\n\", handle_id, session_id);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_HANDLE_NOT_FOUND, \"No such handle %\"SCNu64\" in session %\"SCNu64\"\", handle_id, session_id);\n\t\t\tgoto jsondone;\n\t\t}\n\t}\n\n\t/* What is this? */\n\tif(!strcasecmp(message_text, \"keepalive\")) {\n\t\t/* Just a keep-alive message, reply with an ack */\n\t\tJANUS_LOG(LOG_VERB, \"Got a keep-alive on session %\"SCNu64\"\\n\", session_id);\n\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"attach\")) {\n\t\tif(handle != NULL) {\n\t\t\t/* Attach is a session-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, attach_parameters,\n\t\t\terror_code, error_cause, FALSE,\n\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\tif(error_code != 0) {\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *plugin = json_object_get(root, \"plugin\");\n\t\tconst gchar *plugin_text = json_string_value(plugin);\n\t\tjanus_plugin *plugin_t = janus_plugin_find(plugin_text);\n\t\tif(plugin_t == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_NOT_FOUND, \"No such plugin '%s'\", plugin_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* If the auth token mechanism is enabled, we should check if this token can access this plugin */\n\t\tif(janus_auth_is_enabled()) {\n\t\t\tjson_t *token = json_object_get(root, \"token\");\n\t\t\tif(token != NULL) {\n\t\t\t\tconst char *token_value = json_string_value(token);\n\t\t\t\tif(token_value && !janus_auth_check_plugin(token_value, plugin_t)) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Token '%s' can't access plugin '%s'\\n\", token_value, plugin_text);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED_PLUGIN, \"Provided token can't access plugin '%s'\", plugin_text);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tjson_t *opaque = json_object_get(root, \"opaque_id\");\n\t\tconst char *opaque_id = opaque ? json_string_value(opaque) : NULL;\n\t\t/* Create handle */\n\t\thandle = janus_ice_handle_create(session, opaque_id);\n\t\tif(handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Memory error\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\thandle_id = handle->handle_id;\n\t\t/* We increase the counter as this request is using the handle */\n\t\tjanus_refcount_increase(&handle->ref);\n\t\t/* Attach to the plugin */\n\t\tint error = 0;\n\t\tif((error = janus_ice_handle_attach_plugin(session, handle, plugin_t)) != 0) {\n\t\t\t/* TODO Make error struct to pass verbose information */\n\t\t\tjanus_session_handles_remove(session, handle);\n\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't attach to plugin '%s', error '%d'\\n\", plugin_text, error);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_ATTACH, \"Couldn't attach to plugin: error '%d'\", error);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\tjson_t *data = json_object();\n\t\tjson_object_set_new(data, \"id\", json_integer(handle_id));\n\t\tjson_object_set_new(reply, \"data\", data);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"destroy\")) {\n\t\tif(handle != NULL) {\n\t\t\t/* Query is a session-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_mutex_lock(&sessions_mutex);\n\t\tg_hash_table_remove(sessions, &session->session_id);\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t/* Notify the source that the session has been destroyed */\n\t\tif(session->source && session->source->transport) {\n\t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, FALSE);\n\t\t}\n\t\t/* Schedule the session for deletion */\n\t\tjanus_session_destroy(session);\n\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t\t/* Notify event handlers as well */\n\t\tif(janus_events_is_enabled())\n\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_SESSION, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\tsession_id, \"destroyed\", NULL);\n\t} else if(!strcasecmp(message_text, \"detach\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"No plugin to detach from\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tint error = janus_session_handles_remove(session, handle);\n\t\tif(error != 0) {\n\t\t\t/* TODO Make error struct to pass verbose information */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"Couldn't detach from plugin: error '%d'\", error);\n\t\t\t/* TODO Delete handle instance */\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"hangup\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"No plugin attached\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_ice_webrtc_hangup(handle, \"Janus API\");\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"claim\")) {\n\t\tjanus_mutex_lock(&session->mutex);\n\t\tif(session->source != NULL) {\n\t\t\t/* Notify the old transport that this session is over for them, but has been reclaimed */\n\t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, TRUE);\n\t\t\tjanus_request_destroy(session->source);\n\t\t\tsession->source = NULL;\n\t\t}\n\t\tsession->source = janus_request_new(request->transport, request->instance, NULL, FALSE, NULL);\n\t\t/* Notify the new transport that it has claimed a session */\n\t\tsession->source->transport->session_claimed(session->source->instance, session->session_id);\n\t\t/* Previous transport may be gone, clear flag. */\n\t\tg_atomic_int_set(&session->transport_gone, 0);\n\t\tjanus_mutex_unlock(&session->mutex);\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = json_object();\n\t\tjson_object_set_new(reply, \"janus\", json_string(\"success\"));\n\t\tjson_object_set_new(reply, \"session_id\", json_integer(session_id));\n\t\tjson_object_set_new(reply, \"transaction\", json_string(transaction_text));\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"message\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this message\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_plugin *plugin_t = (janus_plugin *)handle->app;\n\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] There's a message for %s\\n\", handle->handle_id, plugin_t->get_name());\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, body_parameters,\n\t\t\terror_code, error_cause, FALSE,\n\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\tif(error_code != 0) {\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *body = json_object_get(root, \"body\");\n\t\t/* Is there an SDP attached? */\n\t\tjson_t *jsep = json_object_get(root, \"jsep\");\n\t\tchar *jsep_type = NULL;\n\t\tchar *jsep_sdp = NULL, *jsep_sdp_stripped = NULL;\n\t\tgboolean renegotiation = FALSE;\n\t\tif(jsep != NULL) {\n\t\t\tif(!json_is_object(jsep)) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_JSON_OBJECT, \"Invalid jsep object\");\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tJANUS_VALIDATE_JSON_OBJECT_FORMAT(\"JSEP error: missing mandatory element (%s)\",\n\t\t\t\t\"JSEP error: invalid element type (%s should be %s)\",\n\t\t\t\tjsep, jsep_parameters, error_code, error_cause, FALSE,\n\t\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\t\tif(error_code != 0) {\n\t\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjson_t *type = json_object_get(jsep, \"type\");\n\t\t\tjsep_type = g_strdup(json_string_value(type));\n\t\t\ttype = NULL;\n\t\t\tgboolean do_trickle = TRUE;\n\t\t\tjson_t *jsep_trickle = json_object_get(jsep, \"trickle\");\n\t\t\tdo_trickle = jsep_trickle ? json_is_true(jsep_trickle) : TRUE;\n\t\t\t/* Are we still cleaning up from a previous media session? */\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Still cleaning up from a previous media session, let's wait a bit...\\n\", handle->handle_id);\n\t\t\t\tgint64 waited = 0;\n\t\t\t\twhile(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\t\t\tg_usleep(100000);\n\t\t\t\t\twaited += 100000;\n\t\t\t\t\tif(waited >= 3*G_USEC_PER_SEC) {\n\t\t\t\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"]   -- Waited 3 seconds, that's enough!\\n\", handle->handle_id);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_WEBRTC_STATE, \"Still cleaning a previous session\");\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Check if we're renegotiating (if we have an answer, we did an offer/answer round already) */\n\t\t\trenegotiation = janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_NEGOTIATED);\n\t\t\t/* Check the JSEP type */\n\t\t\tjanus_mutex_lock(&handle->mutex);\n\t\t\tint offer = 0;\n\t\t\tif(!strcasecmp(jsep_type, \"offer\")) {\n\t\t\t\toffer = 1;\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER);\n\t\t\t} else if(!strcasecmp(jsep_type, \"answer\")) {\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER);\n\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER))\n\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_NEGOTIATED);\n\t\t\t\toffer = 0;\n\t\t\t} else {\n\t\t\t\t/* TODO Handle other message types as well */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_UNKNOWN_TYPE, \"JSEP error: unknown message type '%s'\", jsep_type);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjson_t *sdp = json_object_get(jsep, \"sdp\");\n\t\t\tjsep_sdp = (char *)json_string_value(sdp);\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Remote SDP:\\n%s\", handle->handle_id, jsep_sdp);\n\t\t\t/* Is this valid SDP? */\n\t\t\tchar error_str[512];\n\t\t\tint audio = 0, video = 0, data = 0;\n\t\t\tjanus_sdp *parsed_sdp = janus_sdp_preparse(handle, jsep_sdp, error_str, sizeof(error_str), &audio, &video, &data);\n\t\t\tif(parsed_sdp == NULL) {\n\t\t\t\t/* Invalid SDP */\n\t\t\t\tret = janus_process_error_string(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, error_str);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\t/* Notify event handlers */\n\t\t\tif(janus_events_is_enabled()) {\n\t\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_JSEP, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\t\tsession_id, handle_id, handle->opaque_id, \"remote\", jsep_type, jsep_sdp);\n\t\t\t}\n\t\t\t/* FIXME We're only handling single audio/video lines for now... */\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Audio %s been negotiated, Video %s been negotiated, SCTP/DataChannels %s been negotiated\\n\",\n\t\t\t                    handle->handle_id,\n\t\t\t                    audio ? \"has\" : \"has NOT\",\n\t\t\t                    video ? \"has\" : \"has NOT\",\n\t\t\t                    data ? \"have\" : \"have NOT\");\n\t\t\tif(audio > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one audio line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n\t\t\tif(video > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one video line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n\t\t\tif(data > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one data line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n#ifndef HAVE_SCTP\n\t\t\tif(data) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"]   -- DataChannels have been negotiated, but support for them has not been compiled...\\n\", handle->handle_id);\n\t\t\t}\n#endif\n\t\t\t/* We behave differently if it's a new session or an update... */\n\t\t\tif(!renegotiation) {\n\t\t\t\t/* New session */\n\t\t\t\tif(offer) {\n\t\t\t\t\t/* Setup ICE locally (we received an offer) */\n\t\t\t\t\tif(janus_ice_setup_local(handle, offer, audio, video, data, do_trickle) < 0) {\n\t\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error setting ICE locally\\n\");\n\t\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Error setting ICE locally\");\n\t\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t/* Make sure we're waiting for an ANSWER in the first place */\n\t\t\t\t\tif(!handle->agent) {\n\t\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Unexpected ANSWER (did we offer?)\\n\");\n\t\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNEXPECTED_ANSWER, \"Unexpected ANSWER (did we offer?)\");\n\t\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(janus_sdp_process(handle, parsed_sdp, FALSE) < 0) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error processing SDP\\n\");\n\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, \"Error processing SDP\");\n\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t\tif(!offer) {\n\t\t\t\t\t/* Set remote candidates now (we received an answer) */\n\t\t\t\t\tif(do_trickle) {\n\t\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t\t\t\t}\n\t\t\t\t\tjanus_request_ice_handle_answer(handle, audio, video, data, jsep_sdp);\n\t\t\t\t} else {\n\t\t\t\t\t/* Check if the mid RTP extension is being negotiated */\n\t\t\t\t\thandle->stream->mid_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_MID);\n\t\t\t\t\t/* Check if the RTP Stream ID extension is being negotiated */\n\t\t\t\t\thandle->stream->rid_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_RID);\n\t\t\t\t\thandle->stream->ridrtx_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_REPAIRED_RID);\n\t\t\t\t\t/* Check if the audio level ID extension is being negotiated */\n\t\t\t\t\thandle->stream->audiolevel_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_AUDIO_LEVEL);\n\t\t\t\t\t/* Check if the video orientation ID extension is being negotiated */\n\t\t\t\t\thandle->stream->videoorientation_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_VIDEO_ORIENTATION);\n\t\t\t\t\t/* Check if the frame marking ID extension is being negotiated */\n\t\t\t\t\thandle->stream->framemarking_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_FRAME_MARKING);\n\t\t\t\t\t/* Check if transport wide CC is supported */\n\t\t\t\t\tint transport_wide_cc_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_TRANSPORT_WIDE_CC);\n\t\t\t\t\thandle->stream->do_transport_wide_cc = transport_wide_cc_ext_id > 0 ? TRUE : FALSE;\n\t\t\t\t\thandle->stream->transport_wide_cc_ext_id = transport_wide_cc_ext_id;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* FIXME This is a renegotiation: we can currently only handle simple changes in media\n\t\t\t\t * direction and ICE restarts: anything more complex than that will result in an error */\n\t\t\t\tJANUS_LOG(LOG_INFO, \"[%\"SCNu64\"] Negotiation update, checking what changed...\\n\", handle->handle_id);\n\t\t\t\tif(janus_sdp_process(handle, parsed_sdp, TRUE) < 0) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error processing SDP\\n\");\n\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNEXPECTED_ANSWER, \"Error processing SDP\");\n\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_ICE_RESTART)) {\n\t\t\t\t\tJANUS_LOG(LOG_INFO, \"[%\"SCNu64\"] Restarting ICE...\\n\", handle->handle_id);\n\t\t\t\t\t/* Update remote credentials for ICE */\n\t\t\t\t\tif(handle->stream) {\n\t\t\t\t\t\tnice_agent_set_remote_credentials(handle->agent, handle->stream->stream_id,\n\t\t\t\t\t\t\thandle->stream->ruser, handle->stream->rpass);\n\t\t\t\t\t}\n\t\t\t\t\t/* FIXME We only need to do that for offers: if it's an answer, we did that already */\n\t\t\t\t\tif(offer) {\n\t\t\t\t\t\tjanus_ice_restart(handle);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_ICE_RESTART);\n\t\t\t\t\t}\n\t\t\t\t\t/* If we're full-trickling, we'll need to resend the candidates later */\n\t\t\t\t\tif(janus_ice_is_full_trickle_enabled()) {\n\t\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_RESEND_TRICKLES);\n\t\t\t\t\t}\n\t\t\t\t}\n#ifdef HAVE_SCTP\n\t\t\t\tif(!offer) {\n\t\t\t\t\t/* Were datachannels just added? */\n\t\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_DATA_CHANNELS)) {\n\t\t\t\t\t\tjanus_ice_stream *stream = handle->stream;\n\t\t\t\t\t\tif(stream != NULL && stream->component != NULL\n\t\t\t\t\t\t\t\t&& stream->component->dtls != NULL && stream->component->dtls->sctp == NULL) {\n\t\t\t\t\t\t\t/* Create SCTP association as well */\n\t\t\t\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] Creating datachannels...\\n\", handle->handle_id);\n\t\t\t\t\t\t\tjanus_dtls_srtp_create_sctp(stream->component->dtls);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n#endif\n\t\t\t}\n\t\t\tchar *tmp = handle->remote_sdp;\n\t\t\thandle->remote_sdp = g_strdup(jsep_sdp);\n\t\t\tg_free(tmp);\n\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t/* Anonymize SDP */\n\t\t\tif(janus_sdp_anonymize(parsed_sdp) < 0) {\n\t\t\t\t/* Invalid SDP */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, \"JSEP error: invalid SDP\");\n\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjsep_sdp_stripped = janus_sdp_write(parsed_sdp);\n\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\tsdp = NULL;\n\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t}\n\n\t\t/* Make sure the app handle is still valid */\n\t\tif(handle->app == NULL || !janus_plugin_session_is_alive(handle->app_handle)) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this message\");\n\t\t\tg_free(jsep_type);\n\t\t\tg_free(jsep_sdp_stripped);\n\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\tgoto jsondone;\n\t\t}\n\n\t\t/* Send the message to the plugin (which must eventually free transaction_text and unref the two objects, body and jsep) */\n\t\tjson_incref(body);\n\t\tjson_t *body_jsep = NULL;\n\t\tif(jsep_sdp_stripped) {\n\t\t\tbody_jsep = json_pack(\"{ssss}\", \"type\", jsep_type, \"sdp\", jsep_sdp_stripped);\n\t\t\t/* Check if simulcasting is enabled */\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_HAS_VIDEO)) {\n\t\t\t\tif(handle->stream && (handle->stream->rid[0] || handle->stream->video_ssrc_peer[1])) {\n\t\t\t\t\tjson_t *simulcast = json_object();\n\t\t\t\t\t/* If we have rids, pass those, otherwise pass the SSRCs */\n\t\t\t\t\tif(handle->stream->rid[0]) {\n\t\t\t\t\t\tjson_t *rids = json_array();\n\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[0]));\n\t\t\t\t\t\tif(handle->stream->rid[1])\n\t\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[1]));\n\t\t\t\t\t\tif(handle->stream->rid[2])\n\t\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[2]));\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"rids\", rids);\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"rid-ext\", json_integer(handle->stream->rid_ext_id));\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjson_t *ssrcs = json_array();\n\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[0]));\n\t\t\t\t\t\tif(handle->stream->video_ssrc_peer[1])\n\t\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[1]));\n\t\t\t\t\t\tif(handle->stream->video_ssrc_peer[2])\n\t\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[2]));\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"ssrcs\", ssrcs);\n\t\t\t\t\t}\n\t\t\t\t\tif(handle->stream->framemarking_ext_id > 0)\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"framemarking-ext\", json_integer(handle->stream->framemarking_ext_id));\n\t\t\t\t\tjson_object_set_new(body_jsep, \"simulcast\", simulcast);\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Check if this is a renegotiation or update */\n\t\t\tif(renegotiation)\n\t\t\t\tjson_object_set_new(body_jsep, \"update\", json_true());\n\t\t}\n\t\tjanus_plugin_result *result = plugin_t->handle_message(handle->app_handle,\n\t\t\tg_strdup((char *)transaction_text), body, body_jsep);\n\t\tg_free(jsep_type);\n\t\tg_free(jsep_sdp_stripped);\n\t\tif(result == NULL) {\n\t\t\t/* Something went horribly wrong! */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"Plugin didn't give a result\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(result->type == JANUS_PLUGIN_OK) {\n\t\t\t/* The plugin gave a result already (synchronous request/response) */\n\t\t\tif(result->content == NULL || !json_is_object(result->content)) {\n\t\t\t\t/* Missing content, or not a JSON object */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE,\n\t\t\t\t\tresult->content == NULL ?\n\t\t\t\t\t\t\"Plugin didn't provide any content for this synchronous response\" :\n\t\t\t\t\t\t\"Plugin returned an invalid JSON response\");\n\t\t\t\tjanus_plugin_result_destroy(result);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\t/* Reference the content, as destroying the result instance will decref it */\n\t\t\tjson_incref(result->content);\n\t\t\t/* Prepare JSON response */\n\t\t\tjson_t *reply = janus_create_message(\"success\", session->session_id, transaction_text);\n\t\t\tjson_object_set_new(reply, \"sender\", json_integer(handle->handle_id));\n\t\t\tif(janus_is_opaqueid_in_api_enabled() && handle->opaque_id != NULL)\n\t\t\t\tjson_object_set_new(reply, \"opaque_id\", json_string(handle->opaque_id));\n\t\t\tjson_t *plugin_data = json_object();\n\t\t\tjson_object_set_new(plugin_data, \"plugin\", json_string(plugin_t->get_package()));\n\t\t\tjson_object_set_new(plugin_data, \"data\", result->content);\n\t\t\tjson_object_set_new(reply, \"plugindata\", plugin_data);\n\t\t\t/* Send the success reply */\n\t\t\tret = janus_process_success(request, reply);\n\t\t} else if(result->type == JANUS_PLUGIN_OK_WAIT) {\n\t\t\t/* The plugin received the request but didn't process it yet, send an ack (asynchronous notifications may follow) */\n\t\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t\tif(result->text)\n\t\t\t\tjson_object_set_new(reply, \"hint\", json_string(result->text));\n\t\t\t/* Send the success reply */\n\t\t\tret = janus_process_success(request, reply);\n\t\t} else {\n\t\t\t/* Something went horribly wrong! */\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE,\n\t\t\t\t(char *)(result->text ? result->text : \"Plugin returned a severe (unknown) error\"));\n\t\t\tjanus_plugin_result_destroy(result);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_plugin_result_destroy(result);\n\t} else if(!strcasecmp(message_text, \"trickle\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Trickle is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || !janus_plugin_session_is_alive(handle->app_handle)) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this trickle candidate\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *candidate = json_object_get(root, \"candidate\");\n\t\tjson_t *candidates = json_object_get(root, \"candidates\");\n\t\tif(candidate == NULL && candidates == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_MISSING_MANDATORY_ELEMENT, \"Missing mandatory element (candidate|candidates)\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(candidate != NULL && candidates != NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_JSON, \"Can't have both candidate and candidates\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\tJANUS_LOG(LOG_ERR, \"[%\"SCNu64\"] Received a trickle, but still cleaning a previous session\\n\", handle->handle_id);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_WEBRTC_STATE, \"Still cleaning a previous session\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_mutex_lock(&handle->mutex);\n\t\tif(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE)) {\n\t\t\t/* It looks like this peer supports Trickle, after all */\n\t\t\tJANUS_LOG(LOG_VERB, \"Handle %\"SCNu64\" supports trickle even if it didn't negotiate it...\\n\", handle->handle_id);\n\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t}\n\t\t/* Is there any stream ready? this trickle may get here before the SDP it relates to */\n\t\tif(handle->stream == NULL) {\n\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] No stream, queueing this trickle as it got here before the SDP...\\n\", handle->handle_id);\n\t\t\t/* Enqueue this trickle candidate(s), we'll process this later */\n\t\t\tjanus_ice_trickle *early_trickle = janus_ice_trickle_new(transaction_text, candidate ? candidate : candidates);\n\t\t\thandle->pending_trickles = g_list_append(handle->pending_trickles, early_trickle);\n\t\t\t/* Send the ack right away, an event will tell the application if the candidate(s) failed */\n\t\t\tgoto trickledone;\n\t\t}\n\t\t/* Is the ICE stack ready already? */\n\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER) ||\n\t\t\t\t!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER) ||\n\t\t\t\t!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER)) {\n\t\t\tconst char *cause = NULL;\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER))\n\t\t\t\tcause = \"processing the offer\";\n\t\t\telse if(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER))\n\t\t\t\tcause = \"waiting for the answer\";\n\t\t\telse if(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER))\n\t\t\t\tcause = \"waiting for the offer\";\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Still %s, queueing this trickle to wait until we're done there...\\n\",\n\t\t\t\thandle->handle_id, cause);\n\t\t\t/* Enqueue this trickle candidate(s), we'll process this later */\n\t\t\tjanus_ice_trickle *early_trickle = janus_ice_trickle_new(transaction_text, candidate ? candidate : candidates);\n\t\t\thandle->pending_trickles = g_list_append(handle->pending_trickles, early_trickle);\n\t\t\t/* Send the ack right away, an event will tell the application if the candidate(s) failed */\n\t\t\tgoto trickledone;\n\t\t}\n\t\tif(candidate != NULL) {\n\t\t\t/* We got a single candidate */\n\t\t\tint error = 0;\n\t\t\tconst char *error_string = NULL;\n\t\t\tif((error = janus_ice_trickle_parse(handle, candidate, &error_string)) != 0) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, error, \"%s\", error_string);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t} else {\n\t\t\t/* We got multiple candidates in an array */\n\t\t\tif(!json_is_array(candidates)) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_ELEMENT_TYPE, \"candidates is not an array\");\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tJANUS_LOG(LOG_VERB, \"Got multiple candidates (%zu)\\n\", json_array_size(candidates));\n\t\t\tif(json_array_size(candidates) > 0) {\n\t\t\t\t/* Handle remote candidates */\n\t\t\t\tsize_t i = 0;\n\t\t\t\tfor(i=0; i<json_array_size(candidates); i++) {\n\t\t\t\t\tjson_t *c = json_array_get(candidates, i);\n\t\t\t\t\t/* FIXME We don't care if any trickle fails to parse */\n\t\t\t\t\tjanus_ice_trickle_parse(handle, c, NULL);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\ntrickledone:\n\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t/* We reply right away, not to block the web server... */\n\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else {\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN_REQUEST, \"Unknown request '%s'\", message_text);\n\t}\n\njsondone:\n\t/* Done processing */\n\tif(handle != NULL)\n\t\tjanus_refcount_decrease(&handle->ref);\n\tif(session != NULL)\n\t\tjanus_refcount_decrease(&session->ref);\n\treturn ret;\n}",
        "func": "int janus_process_incoming_request(janus_request *request) {\n\tint ret = -1;\n\tif(request == NULL) {\n\t\tJANUS_LOG(LOG_ERR, \"Missing request or payload to process, giving up...\\n\");\n\t\treturn ret;\n\t}\n\tint error_code = 0;\n\tchar error_cause[100];\n\tjson_t *root = request->message;\n\t/* Ok, let's start with the ids */\n\tguint64 session_id = 0, handle_id = 0;\n\tjson_t *s = json_object_get(root, \"session_id\");\n\tif(s && json_is_integer(s))\n\t\tsession_id = json_integer_value(s);\n\tjson_t *h = json_object_get(root, \"handle_id\");\n\tif(h && json_is_integer(h))\n\t\thandle_id = json_integer_value(h);\n\n\tjanus_session *session = NULL;\n\tjanus_ice_handle *handle = NULL;\n\n\t/* Get transaction and message request */\n\tJANUS_VALIDATE_JSON_OBJECT(root, incoming_request_parameters,\n\t\terror_code, error_cause, FALSE,\n\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\tif(error_code != 0) {\n\t\tret = janus_process_error_string(request, session_id, NULL, error_code, error_cause);\n\t\tgoto jsondone;\n\t}\n\tjson_t *transaction = json_object_get(root, \"transaction\");\n\tconst gchar *transaction_text = json_string_value(transaction);\n\tjson_t *message = json_object_get(root, \"janus\");\n\tconst gchar *message_text = json_string_value(message);\n\n\tif(session_id == 0 && handle_id == 0) {\n\t\t/* Can only be a 'Create new session', a 'Get info' or a 'Ping/Pong' request */\n\t\tif(!strcasecmp(message_text, \"info\")) {\n\t\t\tret = janus_process_success(request, janus_info(transaction_text));\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(!strcasecmp(message_text, \"ping\")) {\n\t\t\t/* Prepare JSON reply */\n\t\t\tjson_t *reply = janus_create_message(\"pong\", 0, transaction_text);\n\t\t\tret = janus_process_success(request, reply);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(strcasecmp(message_text, \"create\")) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Make sure we're accepting new sessions */\n\t\tif(!accept_new_sessions) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_NOT_ACCEPTING_SESSIONS, NULL);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Any secret/token to check? */\n\t\tret = janus_request_check_secret(request, session_id, transaction_text);\n\t\tif(ret != 0) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED, NULL);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tsession_id = 0;\n\t\tjson_t *id = json_object_get(root, \"id\");\n\t\tif(id != NULL) {\n\t\t\t/* The application provided the session ID to use */\n\t\t\tsession_id = json_integer_value(id);\n\t\t\tif(session_id > 0 && (session = janus_session_find(session_id)) != NULL) {\n\t\t\t\t/* Session ID already taken */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_CONFLICT, \"Session ID already in use\");\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t}\n\t\t/* Handle it */\n\t\tsession = janus_session_create(session_id);\n\t\tif(session == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Memory error\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tsession_id = session->session_id;\n\t\t/* We increase the counter as this request is using the session */\n\t\tjanus_refcount_increase(&session->ref);\n\t\t/* Take note of the request source that originated this session (HTTP, WebSockets, RabbitMQ?) */\n\t\tsession->source = janus_request_new(request->transport, request->instance, NULL, FALSE, NULL);\n\t\t/* Notify the source that a new session has been created */\n\t\trequest->transport->session_created(request->instance, session->session_id);\n\t\t/* Notify event handlers */\n\t\tif(janus_events_is_enabled()) {\n\t\t\t/* Session created, add info on the transport that originated it */\n\t\t\tjson_t *transport = json_object();\n\t\t\tjson_object_set_new(transport, \"transport\", json_string(session->source->transport->get_package()));\n\t\t\tchar id[32];\n\t\t\tmemset(id, 0, sizeof(id));\n\t\t\tg_snprintf(id, sizeof(id), \"%p\", session->source->instance);\n\t\t\tjson_object_set_new(transport, \"id\", json_string(id));\n\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_SESSION, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\tsession_id, \"created\", transport);\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", 0, transaction_text);\n\t\tjson_t *data = json_object();\n\t\tjson_object_set_new(data, \"id\", json_integer(session_id));\n\t\tjson_object_set_new(reply, \"data\", data);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t\tgoto jsondone;\n\t}\n\tif(session_id < 1) {\n\t\tJANUS_LOG(LOG_ERR, \"Invalid session\\n\");\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, NULL);\n\t\tgoto jsondone;\n\t}\n\tif(h && handle_id < 1) {\n\t\tJANUS_LOG(LOG_ERR, \"Invalid handle\\n\");\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, NULL);\n\t\tgoto jsondone;\n\t}\n\n\t/* Go on with the processing */\n\tret = janus_request_check_secret(request, session_id, transaction_text);\n\tif(ret != 0) {\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED, NULL);\n\t\tgoto jsondone;\n\t}\n\n\t/* If we got here, make sure we have a session (and/or a handle) */\n\tsession = janus_session_find(session_id);\n\tif(!session) {\n\t\tJANUS_LOG(LOG_ERR, \"Couldn't find any session %\"SCNu64\"...\\n\", session_id);\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, \"No such session %\"SCNu64\"\", session_id);\n\t\tgoto jsondone;\n\t}\n\t/* Update the last activity timer */\n\tsession->last_activity = janus_get_monotonic_time();\n\thandle = NULL;\n\tif(handle_id > 0) {\n\t\thandle = janus_session_handles_find(session, handle_id);\n\t\tif(!handle) {\n\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't find any handle %\"SCNu64\" in session %\"SCNu64\"...\\n\", handle_id, session_id);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_HANDLE_NOT_FOUND, \"No such handle %\"SCNu64\" in session %\"SCNu64\"\", handle_id, session_id);\n\t\t\tgoto jsondone;\n\t\t}\n\t}\n\n\t/* What is this? */\n\tif(!strcasecmp(message_text, \"keepalive\")) {\n\t\t/* Just a keep-alive message, reply with an ack */\n\t\tJANUS_LOG(LOG_VERB, \"Got a keep-alive on session %\"SCNu64\"\\n\", session_id);\n\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"attach\")) {\n\t\tif(handle != NULL) {\n\t\t\t/* Attach is a session-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, attach_parameters,\n\t\t\terror_code, error_cause, FALSE,\n\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\tif(error_code != 0) {\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *plugin = json_object_get(root, \"plugin\");\n\t\tconst gchar *plugin_text = json_string_value(plugin);\n\t\tjanus_plugin *plugin_t = janus_plugin_find(plugin_text);\n\t\tif(plugin_t == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_NOT_FOUND, \"No such plugin '%s'\", plugin_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* If the auth token mechanism is enabled, we should check if this token can access this plugin */\n\t\tif(janus_auth_is_enabled()) {\n\t\t\tjson_t *token = json_object_get(root, \"token\");\n\t\t\tif(token != NULL) {\n\t\t\t\tconst char *token_value = json_string_value(token);\n\t\t\t\tif(token_value && !janus_auth_check_plugin(token_value, plugin_t)) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Token '%s' can't access plugin '%s'\\n\", token_value, plugin_text);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED_PLUGIN, \"Provided token can't access plugin '%s'\", plugin_text);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tjson_t *opaque = json_object_get(root, \"opaque_id\");\n\t\tconst char *opaque_id = opaque ? json_string_value(opaque) : NULL;\n\t\t/* Create handle */\n\t\thandle = janus_ice_handle_create(session, opaque_id);\n\t\tif(handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Memory error\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\thandle_id = handle->handle_id;\n\t\t/* We increase the counter as this request is using the handle */\n\t\tjanus_refcount_increase(&handle->ref);\n\t\t/* Attach to the plugin */\n\t\tint error = 0;\n\t\tif((error = janus_ice_handle_attach_plugin(session, handle, plugin_t)) != 0) {\n\t\t\t/* TODO Make error struct to pass verbose information */\n\t\t\tjanus_session_handles_remove(session, handle);\n\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't attach to plugin '%s', error '%d'\\n\", plugin_text, error);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_ATTACH, \"Couldn't attach to plugin: error '%d'\", error);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\tjson_t *data = json_object();\n\t\tjson_object_set_new(data, \"id\", json_integer(handle_id));\n\t\tjson_object_set_new(reply, \"data\", data);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"destroy\")) {\n\t\tif(handle != NULL) {\n\t\t\t/* Query is a session-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_mutex_lock(&sessions_mutex);\n\t\tg_hash_table_remove(sessions, &session->session_id);\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t/* Notify the source that the session has been destroyed */\n\t\tjanus_mutex_lock(&session->mutex);\n\t\tif(session->source && session->source->transport)\n\t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, FALSE);\n\t\tjanus_mutex_unlock(&session->mutex);\n\t\t/* Schedule the session for deletion */\n\t\tjanus_session_destroy(session);\n\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t\t/* Notify event handlers as well */\n\t\tif(janus_events_is_enabled())\n\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_SESSION, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\tsession_id, \"destroyed\", NULL);\n\t} else if(!strcasecmp(message_text, \"detach\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"No plugin to detach from\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tint error = janus_session_handles_remove(session, handle);\n\t\tif(error != 0) {\n\t\t\t/* TODO Make error struct to pass verbose information */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"Couldn't detach from plugin: error '%d'\", error);\n\t\t\t/* TODO Delete handle instance */\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"hangup\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"No plugin attached\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_ice_webrtc_hangup(handle, \"Janus API\");\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"claim\")) {\n\t\tjanus_mutex_lock(&session->mutex);\n\t\tif(session->source != NULL) {\n\t\t\t/* Notify the old transport that this session is over for them, but has been reclaimed */\n\t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, TRUE);\n\t\t\tjanus_request_destroy(session->source);\n\t\t\tsession->source = NULL;\n\t\t}\n\t\tsession->source = janus_request_new(request->transport, request->instance, NULL, FALSE, NULL);\n\t\t/* Notify the new transport that it has claimed a session */\n\t\tsession->source->transport->session_claimed(session->source->instance, session->session_id);\n\t\t/* Previous transport may be gone, clear flag. */\n\t\tg_atomic_int_set(&session->transport_gone, 0);\n\t\tjanus_mutex_unlock(&session->mutex);\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = json_object();\n\t\tjson_object_set_new(reply, \"janus\", json_string(\"success\"));\n\t\tjson_object_set_new(reply, \"session_id\", json_integer(session_id));\n\t\tjson_object_set_new(reply, \"transaction\", json_string(transaction_text));\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"message\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this message\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_plugin *plugin_t = (janus_plugin *)handle->app;\n\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] There's a message for %s\\n\", handle->handle_id, plugin_t->get_name());\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, body_parameters,\n\t\t\terror_code, error_cause, FALSE,\n\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\tif(error_code != 0) {\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *body = json_object_get(root, \"body\");\n\t\t/* Is there an SDP attached? */\n\t\tjson_t *jsep = json_object_get(root, \"jsep\");\n\t\tchar *jsep_type = NULL;\n\t\tchar *jsep_sdp = NULL, *jsep_sdp_stripped = NULL;\n\t\tgboolean renegotiation = FALSE;\n\t\tif(jsep != NULL) {\n\t\t\tif(!json_is_object(jsep)) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_JSON_OBJECT, \"Invalid jsep object\");\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tJANUS_VALIDATE_JSON_OBJECT_FORMAT(\"JSEP error: missing mandatory element (%s)\",\n\t\t\t\t\"JSEP error: invalid element type (%s should be %s)\",\n\t\t\t\tjsep, jsep_parameters, error_code, error_cause, FALSE,\n\t\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\t\tif(error_code != 0) {\n\t\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjson_t *type = json_object_get(jsep, \"type\");\n\t\t\tjsep_type = g_strdup(json_string_value(type));\n\t\t\ttype = NULL;\n\t\t\tgboolean do_trickle = TRUE;\n\t\t\tjson_t *jsep_trickle = json_object_get(jsep, \"trickle\");\n\t\t\tdo_trickle = jsep_trickle ? json_is_true(jsep_trickle) : TRUE;\n\t\t\t/* Are we still cleaning up from a previous media session? */\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Still cleaning up from a previous media session, let's wait a bit...\\n\", handle->handle_id);\n\t\t\t\tgint64 waited = 0;\n\t\t\t\twhile(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\t\t\tg_usleep(100000);\n\t\t\t\t\twaited += 100000;\n\t\t\t\t\tif(waited >= 3*G_USEC_PER_SEC) {\n\t\t\t\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"]   -- Waited 3 seconds, that's enough!\\n\", handle->handle_id);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_WEBRTC_STATE, \"Still cleaning a previous session\");\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Check if we're renegotiating (if we have an answer, we did an offer/answer round already) */\n\t\t\trenegotiation = janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_NEGOTIATED);\n\t\t\t/* Check the JSEP type */\n\t\t\tjanus_mutex_lock(&handle->mutex);\n\t\t\tint offer = 0;\n\t\t\tif(!strcasecmp(jsep_type, \"offer\")) {\n\t\t\t\toffer = 1;\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER);\n\t\t\t} else if(!strcasecmp(jsep_type, \"answer\")) {\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER);\n\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER))\n\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_NEGOTIATED);\n\t\t\t\toffer = 0;\n\t\t\t} else {\n\t\t\t\t/* TODO Handle other message types as well */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_UNKNOWN_TYPE, \"JSEP error: unknown message type '%s'\", jsep_type);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjson_t *sdp = json_object_get(jsep, \"sdp\");\n\t\t\tjsep_sdp = (char *)json_string_value(sdp);\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Remote SDP:\\n%s\", handle->handle_id, jsep_sdp);\n\t\t\t/* Is this valid SDP? */\n\t\t\tchar error_str[512];\n\t\t\tint audio = 0, video = 0, data = 0;\n\t\t\tjanus_sdp *parsed_sdp = janus_sdp_preparse(handle, jsep_sdp, error_str, sizeof(error_str), &audio, &video, &data);\n\t\t\tif(parsed_sdp == NULL) {\n\t\t\t\t/* Invalid SDP */\n\t\t\t\tret = janus_process_error_string(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, error_str);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\t/* Notify event handlers */\n\t\t\tif(janus_events_is_enabled()) {\n\t\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_JSEP, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\t\tsession_id, handle_id, handle->opaque_id, \"remote\", jsep_type, jsep_sdp);\n\t\t\t}\n\t\t\t/* FIXME We're only handling single audio/video lines for now... */\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Audio %s been negotiated, Video %s been negotiated, SCTP/DataChannels %s been negotiated\\n\",\n\t\t\t                    handle->handle_id,\n\t\t\t                    audio ? \"has\" : \"has NOT\",\n\t\t\t                    video ? \"has\" : \"has NOT\",\n\t\t\t                    data ? \"have\" : \"have NOT\");\n\t\t\tif(audio > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one audio line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n\t\t\tif(video > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one video line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n\t\t\tif(data > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one data line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n#ifndef HAVE_SCTP\n\t\t\tif(data) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"]   -- DataChannels have been negotiated, but support for them has not been compiled...\\n\", handle->handle_id);\n\t\t\t}\n#endif\n\t\t\t/* We behave differently if it's a new session or an update... */\n\t\t\tif(!renegotiation) {\n\t\t\t\t/* New session */\n\t\t\t\tif(offer) {\n\t\t\t\t\t/* Setup ICE locally (we received an offer) */\n\t\t\t\t\tif(janus_ice_setup_local(handle, offer, audio, video, data, do_trickle) < 0) {\n\t\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error setting ICE locally\\n\");\n\t\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Error setting ICE locally\");\n\t\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t/* Make sure we're waiting for an ANSWER in the first place */\n\t\t\t\t\tif(!handle->agent) {\n\t\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Unexpected ANSWER (did we offer?)\\n\");\n\t\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNEXPECTED_ANSWER, \"Unexpected ANSWER (did we offer?)\");\n\t\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(janus_sdp_process(handle, parsed_sdp, FALSE) < 0) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error processing SDP\\n\");\n\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, \"Error processing SDP\");\n\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t\tif(!offer) {\n\t\t\t\t\t/* Set remote candidates now (we received an answer) */\n\t\t\t\t\tif(do_trickle) {\n\t\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t\t\t\t}\n\t\t\t\t\tjanus_request_ice_handle_answer(handle, audio, video, data, jsep_sdp);\n\t\t\t\t} else {\n\t\t\t\t\t/* Check if the mid RTP extension is being negotiated */\n\t\t\t\t\thandle->stream->mid_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_MID);\n\t\t\t\t\t/* Check if the RTP Stream ID extension is being negotiated */\n\t\t\t\t\thandle->stream->rid_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_RID);\n\t\t\t\t\thandle->stream->ridrtx_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_REPAIRED_RID);\n\t\t\t\t\t/* Check if the audio level ID extension is being negotiated */\n\t\t\t\t\thandle->stream->audiolevel_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_AUDIO_LEVEL);\n\t\t\t\t\t/* Check if the video orientation ID extension is being negotiated */\n\t\t\t\t\thandle->stream->videoorientation_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_VIDEO_ORIENTATION);\n\t\t\t\t\t/* Check if the frame marking ID extension is being negotiated */\n\t\t\t\t\thandle->stream->framemarking_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_FRAME_MARKING);\n\t\t\t\t\t/* Check if transport wide CC is supported */\n\t\t\t\t\tint transport_wide_cc_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_TRANSPORT_WIDE_CC);\n\t\t\t\t\thandle->stream->do_transport_wide_cc = transport_wide_cc_ext_id > 0 ? TRUE : FALSE;\n\t\t\t\t\thandle->stream->transport_wide_cc_ext_id = transport_wide_cc_ext_id;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* FIXME This is a renegotiation: we can currently only handle simple changes in media\n\t\t\t\t * direction and ICE restarts: anything more complex than that will result in an error */\n\t\t\t\tJANUS_LOG(LOG_INFO, \"[%\"SCNu64\"] Negotiation update, checking what changed...\\n\", handle->handle_id);\n\t\t\t\tif(janus_sdp_process(handle, parsed_sdp, TRUE) < 0) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error processing SDP\\n\");\n\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNEXPECTED_ANSWER, \"Error processing SDP\");\n\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_ICE_RESTART)) {\n\t\t\t\t\tJANUS_LOG(LOG_INFO, \"[%\"SCNu64\"] Restarting ICE...\\n\", handle->handle_id);\n\t\t\t\t\t/* Update remote credentials for ICE */\n\t\t\t\t\tif(handle->stream) {\n\t\t\t\t\t\tnice_agent_set_remote_credentials(handle->agent, handle->stream->stream_id,\n\t\t\t\t\t\t\thandle->stream->ruser, handle->stream->rpass);\n\t\t\t\t\t}\n\t\t\t\t\t/* FIXME We only need to do that for offers: if it's an answer, we did that already */\n\t\t\t\t\tif(offer) {\n\t\t\t\t\t\tjanus_ice_restart(handle);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_ICE_RESTART);\n\t\t\t\t\t}\n\t\t\t\t\t/* If we're full-trickling, we'll need to resend the candidates later */\n\t\t\t\t\tif(janus_ice_is_full_trickle_enabled()) {\n\t\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_RESEND_TRICKLES);\n\t\t\t\t\t}\n\t\t\t\t}\n#ifdef HAVE_SCTP\n\t\t\t\tif(!offer) {\n\t\t\t\t\t/* Were datachannels just added? */\n\t\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_DATA_CHANNELS)) {\n\t\t\t\t\t\tjanus_ice_stream *stream = handle->stream;\n\t\t\t\t\t\tif(stream != NULL && stream->component != NULL\n\t\t\t\t\t\t\t\t&& stream->component->dtls != NULL && stream->component->dtls->sctp == NULL) {\n\t\t\t\t\t\t\t/* Create SCTP association as well */\n\t\t\t\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] Creating datachannels...\\n\", handle->handle_id);\n\t\t\t\t\t\t\tjanus_dtls_srtp_create_sctp(stream->component->dtls);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n#endif\n\t\t\t}\n\t\t\tchar *tmp = handle->remote_sdp;\n\t\t\thandle->remote_sdp = g_strdup(jsep_sdp);\n\t\t\tg_free(tmp);\n\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t/* Anonymize SDP */\n\t\t\tif(janus_sdp_anonymize(parsed_sdp) < 0) {\n\t\t\t\t/* Invalid SDP */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, \"JSEP error: invalid SDP\");\n\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjsep_sdp_stripped = janus_sdp_write(parsed_sdp);\n\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\tsdp = NULL;\n\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t}\n\n\t\t/* Make sure the app handle is still valid */\n\t\tif(handle->app == NULL || !janus_plugin_session_is_alive(handle->app_handle)) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this message\");\n\t\t\tg_free(jsep_type);\n\t\t\tg_free(jsep_sdp_stripped);\n\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\tgoto jsondone;\n\t\t}\n\n\t\t/* Send the message to the plugin (which must eventually free transaction_text and unref the two objects, body and jsep) */\n\t\tjson_incref(body);\n\t\tjson_t *body_jsep = NULL;\n\t\tif(jsep_sdp_stripped) {\n\t\t\tbody_jsep = json_pack(\"{ssss}\", \"type\", jsep_type, \"sdp\", jsep_sdp_stripped);\n\t\t\t/* Check if simulcasting is enabled */\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_HAS_VIDEO)) {\n\t\t\t\tif(handle->stream && (handle->stream->rid[0] || handle->stream->video_ssrc_peer[1])) {\n\t\t\t\t\tjson_t *simulcast = json_object();\n\t\t\t\t\t/* If we have rids, pass those, otherwise pass the SSRCs */\n\t\t\t\t\tif(handle->stream->rid[0]) {\n\t\t\t\t\t\tjson_t *rids = json_array();\n\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[0]));\n\t\t\t\t\t\tif(handle->stream->rid[1])\n\t\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[1]));\n\t\t\t\t\t\tif(handle->stream->rid[2])\n\t\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[2]));\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"rids\", rids);\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"rid-ext\", json_integer(handle->stream->rid_ext_id));\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjson_t *ssrcs = json_array();\n\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[0]));\n\t\t\t\t\t\tif(handle->stream->video_ssrc_peer[1])\n\t\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[1]));\n\t\t\t\t\t\tif(handle->stream->video_ssrc_peer[2])\n\t\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[2]));\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"ssrcs\", ssrcs);\n\t\t\t\t\t}\n\t\t\t\t\tif(handle->stream->framemarking_ext_id > 0)\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"framemarking-ext\", json_integer(handle->stream->framemarking_ext_id));\n\t\t\t\t\tjson_object_set_new(body_jsep, \"simulcast\", simulcast);\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Check if this is a renegotiation or update */\n\t\t\tif(renegotiation)\n\t\t\t\tjson_object_set_new(body_jsep, \"update\", json_true());\n\t\t}\n\t\tjanus_plugin_result *result = plugin_t->handle_message(handle->app_handle,\n\t\t\tg_strdup((char *)transaction_text), body, body_jsep);\n\t\tg_free(jsep_type);\n\t\tg_free(jsep_sdp_stripped);\n\t\tif(result == NULL) {\n\t\t\t/* Something went horribly wrong! */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"Plugin didn't give a result\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(result->type == JANUS_PLUGIN_OK) {\n\t\t\t/* The plugin gave a result already (synchronous request/response) */\n\t\t\tif(result->content == NULL || !json_is_object(result->content)) {\n\t\t\t\t/* Missing content, or not a JSON object */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE,\n\t\t\t\t\tresult->content == NULL ?\n\t\t\t\t\t\t\"Plugin didn't provide any content for this synchronous response\" :\n\t\t\t\t\t\t\"Plugin returned an invalid JSON response\");\n\t\t\t\tjanus_plugin_result_destroy(result);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\t/* Reference the content, as destroying the result instance will decref it */\n\t\t\tjson_incref(result->content);\n\t\t\t/* Prepare JSON response */\n\t\t\tjson_t *reply = janus_create_message(\"success\", session->session_id, transaction_text);\n\t\t\tjson_object_set_new(reply, \"sender\", json_integer(handle->handle_id));\n\t\t\tif(janus_is_opaqueid_in_api_enabled() && handle->opaque_id != NULL)\n\t\t\t\tjson_object_set_new(reply, \"opaque_id\", json_string(handle->opaque_id));\n\t\t\tjson_t *plugin_data = json_object();\n\t\t\tjson_object_set_new(plugin_data, \"plugin\", json_string(plugin_t->get_package()));\n\t\t\tjson_object_set_new(plugin_data, \"data\", result->content);\n\t\t\tjson_object_set_new(reply, \"plugindata\", plugin_data);\n\t\t\t/* Send the success reply */\n\t\t\tret = janus_process_success(request, reply);\n\t\t} else if(result->type == JANUS_PLUGIN_OK_WAIT) {\n\t\t\t/* The plugin received the request but didn't process it yet, send an ack (asynchronous notifications may follow) */\n\t\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t\tif(result->text)\n\t\t\t\tjson_object_set_new(reply, \"hint\", json_string(result->text));\n\t\t\t/* Send the success reply */\n\t\t\tret = janus_process_success(request, reply);\n\t\t} else {\n\t\t\t/* Something went horribly wrong! */\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE,\n\t\t\t\t(char *)(result->text ? result->text : \"Plugin returned a severe (unknown) error\"));\n\t\t\tjanus_plugin_result_destroy(result);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_plugin_result_destroy(result);\n\t} else if(!strcasecmp(message_text, \"trickle\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Trickle is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || !janus_plugin_session_is_alive(handle->app_handle)) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this trickle candidate\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *candidate = json_object_get(root, \"candidate\");\n\t\tjson_t *candidates = json_object_get(root, \"candidates\");\n\t\tif(candidate == NULL && candidates == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_MISSING_MANDATORY_ELEMENT, \"Missing mandatory element (candidate|candidates)\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(candidate != NULL && candidates != NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_JSON, \"Can't have both candidate and candidates\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\tJANUS_LOG(LOG_ERR, \"[%\"SCNu64\"] Received a trickle, but still cleaning a previous session\\n\", handle->handle_id);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_WEBRTC_STATE, \"Still cleaning a previous session\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_mutex_lock(&handle->mutex);\n\t\tif(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE)) {\n\t\t\t/* It looks like this peer supports Trickle, after all */\n\t\t\tJANUS_LOG(LOG_VERB, \"Handle %\"SCNu64\" supports trickle even if it didn't negotiate it...\\n\", handle->handle_id);\n\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t}\n\t\t/* Is there any stream ready? this trickle may get here before the SDP it relates to */\n\t\tif(handle->stream == NULL) {\n\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] No stream, queueing this trickle as it got here before the SDP...\\n\", handle->handle_id);\n\t\t\t/* Enqueue this trickle candidate(s), we'll process this later */\n\t\t\tjanus_ice_trickle *early_trickle = janus_ice_trickle_new(transaction_text, candidate ? candidate : candidates);\n\t\t\thandle->pending_trickles = g_list_append(handle->pending_trickles, early_trickle);\n\t\t\t/* Send the ack right away, an event will tell the application if the candidate(s) failed */\n\t\t\tgoto trickledone;\n\t\t}\n\t\t/* Is the ICE stack ready already? */\n\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER) ||\n\t\t\t\t!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER) ||\n\t\t\t\t!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER)) {\n\t\t\tconst char *cause = NULL;\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER))\n\t\t\t\tcause = \"processing the offer\";\n\t\t\telse if(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER))\n\t\t\t\tcause = \"waiting for the answer\";\n\t\t\telse if(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER))\n\t\t\t\tcause = \"waiting for the offer\";\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Still %s, queueing this trickle to wait until we're done there...\\n\",\n\t\t\t\thandle->handle_id, cause);\n\t\t\t/* Enqueue this trickle candidate(s), we'll process this later */\n\t\t\tjanus_ice_trickle *early_trickle = janus_ice_trickle_new(transaction_text, candidate ? candidate : candidates);\n\t\t\thandle->pending_trickles = g_list_append(handle->pending_trickles, early_trickle);\n\t\t\t/* Send the ack right away, an event will tell the application if the candidate(s) failed */\n\t\t\tgoto trickledone;\n\t\t}\n\t\tif(candidate != NULL) {\n\t\t\t/* We got a single candidate */\n\t\t\tint error = 0;\n\t\t\tconst char *error_string = NULL;\n\t\t\tif((error = janus_ice_trickle_parse(handle, candidate, &error_string)) != 0) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, error, \"%s\", error_string);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t} else {\n\t\t\t/* We got multiple candidates in an array */\n\t\t\tif(!json_is_array(candidates)) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_ELEMENT_TYPE, \"candidates is not an array\");\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tJANUS_LOG(LOG_VERB, \"Got multiple candidates (%zu)\\n\", json_array_size(candidates));\n\t\t\tif(json_array_size(candidates) > 0) {\n\t\t\t\t/* Handle remote candidates */\n\t\t\t\tsize_t i = 0;\n\t\t\t\tfor(i=0; i<json_array_size(candidates); i++) {\n\t\t\t\t\tjson_t *c = json_array_get(candidates, i);\n\t\t\t\t\t/* FIXME We don't care if any trickle fails to parse */\n\t\t\t\t\tjanus_ice_trickle_parse(handle, c, NULL);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\ntrickledone:\n\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t/* We reply right away, not to block the web server... */\n\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else {\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN_REQUEST, \"Unknown request '%s'\", message_text);\n\t}\n\njsondone:\n\t/* Done processing */\n\tif(handle != NULL)\n\t\tjanus_refcount_decrease(&handle->ref);\n\tif(session != NULL)\n\t\tjanus_refcount_decrease(&session->ref);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -217,9 +217,10 @@\n \t\tg_hash_table_remove(sessions, &session->session_id);\n \t\tjanus_mutex_unlock(&sessions_mutex);\n \t\t/* Notify the source that the session has been destroyed */\n-\t\tif(session->source && session->source->transport) {\n+\t\tjanus_mutex_lock(&session->mutex);\n+\t\tif(session->source && session->source->transport)\n \t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, FALSE);\n-\t\t}\n+\t\tjanus_mutex_unlock(&session->mutex);\n \t\t/* Schedule the session for deletion */\n \t\tjanus_session_destroy(session);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif(session->source && session->source->transport) {",
                "\t\t}"
            ],
            "added_lines": [
                "\t\tjanus_mutex_lock(&session->mutex);",
                "\t\tif(session->source && session->source->transport)",
                "\t\tjanus_mutex_unlock(&session->mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10577",
        "func_name": "meetecho/janus-gateway/janus_session_notify_event",
        "description": "An issue was discovered in Janus through 0.9.1. janus.c has multiple concurrent threads that misuse the source property of a session, leading to a race condition when claiming sessions.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/bc319056603295953f4a70999bc4553785c6e5ef",
        "commit_title": "Fix rare race condition when claiming sessions",
        "commit_text": "",
        "func_before": "void janus_session_notify_event(janus_session *session, json_t *event) {\n\tif(session != NULL && !g_atomic_int_get(&session->destroyed) && session->source != NULL && session->source->transport != NULL) {\n\t\t/* Send this to the transport client */\n\t\tJANUS_LOG(LOG_HUGE, \"Sending event to %s (%p)\\n\", session->source->transport->get_package(), session->source->instance);\n\t\tsession->source->transport->send_message(session->source->instance, NULL, FALSE, event);\n\t} else {\n\t\t/* No transport, free the event */\n\t\tjson_decref(event);\n\t}\n}",
        "func": "void janus_session_notify_event(janus_session *session, json_t *event) {\n\tif(session != NULL && !g_atomic_int_get(&session->destroyed)) {\n\t\tjanus_mutex_lock(&session->mutex);\n\t\tif(session->source != NULL && session->source->transport != NULL) {\n\t\t\t/* Send this to the transport client */\n\t\t\tJANUS_LOG(LOG_HUGE, \"Sending event to %s (%p)\\n\", session->source->transport->get_package(), session->source->instance);\n\t\t\tsession->source->transport->send_message(session->source->instance, NULL, FALSE, event);\n\t\t} else {\n\t\t\t/* No transport, free the event */\n\t\t\tjson_decref(event);\n\t\t}\n\t\tjanus_mutex_unlock(&session->mutex);\n\t} else {\n\t\t/* No session, free the event */\n\t\tjson_decref(event);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,17 @@\n void janus_session_notify_event(janus_session *session, json_t *event) {\n-\tif(session != NULL && !g_atomic_int_get(&session->destroyed) && session->source != NULL && session->source->transport != NULL) {\n-\t\t/* Send this to the transport client */\n-\t\tJANUS_LOG(LOG_HUGE, \"Sending event to %s (%p)\\n\", session->source->transport->get_package(), session->source->instance);\n-\t\tsession->source->transport->send_message(session->source->instance, NULL, FALSE, event);\n+\tif(session != NULL && !g_atomic_int_get(&session->destroyed)) {\n+\t\tjanus_mutex_lock(&session->mutex);\n+\t\tif(session->source != NULL && session->source->transport != NULL) {\n+\t\t\t/* Send this to the transport client */\n+\t\t\tJANUS_LOG(LOG_HUGE, \"Sending event to %s (%p)\\n\", session->source->transport->get_package(), session->source->instance);\n+\t\t\tsession->source->transport->send_message(session->source->instance, NULL, FALSE, event);\n+\t\t} else {\n+\t\t\t/* No transport, free the event */\n+\t\t\tjson_decref(event);\n+\t\t}\n+\t\tjanus_mutex_unlock(&session->mutex);\n \t} else {\n-\t\t/* No transport, free the event */\n+\t\t/* No session, free the event */\n \t\tjson_decref(event);\n \t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif(session != NULL && !g_atomic_int_get(&session->destroyed) && session->source != NULL && session->source->transport != NULL) {",
                "\t\t/* Send this to the transport client */",
                "\t\tJANUS_LOG(LOG_HUGE, \"Sending event to %s (%p)\\n\", session->source->transport->get_package(), session->source->instance);",
                "\t\tsession->source->transport->send_message(session->source->instance, NULL, FALSE, event);",
                "\t\t/* No transport, free the event */"
            ],
            "added_lines": [
                "\tif(session != NULL && !g_atomic_int_get(&session->destroyed)) {",
                "\t\tjanus_mutex_lock(&session->mutex);",
                "\t\tif(session->source != NULL && session->source->transport != NULL) {",
                "\t\t\t/* Send this to the transport client */",
                "\t\t\tJANUS_LOG(LOG_HUGE, \"Sending event to %s (%p)\\n\", session->source->transport->get_package(), session->source->instance);",
                "\t\t\tsession->source->transport->send_message(session->source->instance, NULL, FALSE, event);",
                "\t\t} else {",
                "\t\t\t/* No transport, free the event */",
                "\t\t\tjson_decref(event);",
                "\t\t}",
                "\t\tjanus_mutex_unlock(&session->mutex);",
                "\t\t/* No session, free the event */"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10577",
        "func_name": "meetecho/janus-gateway/janus_transport_gone",
        "description": "An issue was discovered in Janus through 0.9.1. janus.c has multiple concurrent threads that misuse the source property of a session, leading to a race condition when claiming sessions.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/bc319056603295953f4a70999bc4553785c6e5ef",
        "commit_title": "Fix rare race condition when claiming sessions",
        "commit_text": "",
        "func_before": "void janus_transport_gone(janus_transport *plugin, janus_transport_session *transport) {\n\t/* Get rid of sessions this transport was handling */\n\tJANUS_LOG(LOG_VERB, \"A %s transport instance has gone away (%p)\\n\", plugin->get_package(), transport);\n\tjanus_mutex_lock(&sessions_mutex);\n\tif(sessions && g_hash_table_size(sessions) > 0) {\n\t\tGHashTableIter iter;\n\t\tgpointer value;\n\t\tg_hash_table_iter_init(&iter, sessions);\n\t\twhile(g_hash_table_iter_next(&iter, NULL, &value)) {\n\t\t\tjanus_session *session = (janus_session *) value;\n\t\t\tif(!session || g_atomic_int_get(&session->destroyed) || g_atomic_int_get(&session->timeout) || session->last_activity == 0)\n\t\t\t\tcontinue;\n\t\t\tif(session->source && session->source->instance == transport) {\n\t\t\t\tJANUS_LOG(LOG_VERB, \"  -- Session %\"SCNu64\" will be over if not reclaimed\\n\", session->session_id);\n\t\t\t\tJANUS_LOG(LOG_VERB, \"  -- Marking Session %\"SCNu64\" as over\\n\", session->session_id);\n\t\t\t\tif(reclaim_session_timeout < 1) { /* Reclaim session timeouts are disabled */\n\t\t\t\t\t/* Mark the session as destroyed */\n\t\t\t\t\tjanus_session_destroy(session);\n\t\t\t\t\tg_hash_table_iter_remove(&iter);\n\t\t\t\t} else {\n\t\t\t\t\t/* Set flag for transport_gone. The Janus sessions watchdog will clean this up if not reclaimed*/\n\t\t\t\t\tg_atomic_int_set(&session->transport_gone, 1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tjanus_mutex_unlock(&sessions_mutex);\n}",
        "func": "void janus_transport_gone(janus_transport *plugin, janus_transport_session *transport) {\n\t/* Get rid of sessions this transport was handling */\n\tJANUS_LOG(LOG_VERB, \"A %s transport instance has gone away (%p)\\n\", plugin->get_package(), transport);\n\tjanus_mutex_lock(&sessions_mutex);\n\tif(sessions && g_hash_table_size(sessions) > 0) {\n\t\tGHashTableIter iter;\n\t\tgpointer value;\n\t\tg_hash_table_iter_init(&iter, sessions);\n\t\twhile(g_hash_table_iter_next(&iter, NULL, &value)) {\n\t\t\tjanus_session *session = (janus_session *) value;\n\t\t\tif(!session || g_atomic_int_get(&session->destroyed) || g_atomic_int_get(&session->timeout) || session->last_activity == 0)\n\t\t\t\tcontinue;\n\t\t\tif(session->source && session->source->instance == transport) {\n\t\t\t\tJANUS_LOG(LOG_VERB, \"  -- Session %\"SCNu64\" will be over if not reclaimed\\n\", session->session_id);\n\t\t\t\tJANUS_LOG(LOG_VERB, \"  -- Marking Session %\"SCNu64\" as over\\n\", session->session_id);\n\t\t\t\tif(reclaim_session_timeout < 1) { /* Reclaim session timeouts are disabled */\n\t\t\t\t\t/* Mark the session as destroyed */\n\t\t\t\t\tjanus_session_destroy(session);\n\t\t\t\t\tg_hash_table_iter_remove(&iter);\n\t\t\t\t} else {\n\t\t\t\t\t/* Set flag for transport_gone. The Janus sessions watchdog will clean this up if not reclaimed */\n\t\t\t\t\tg_atomic_int_set(&session->transport_gone, 1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tjanus_mutex_unlock(&sessions_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,7 +18,7 @@\n \t\t\t\t\tjanus_session_destroy(session);\n \t\t\t\t\tg_hash_table_iter_remove(&iter);\n \t\t\t\t} else {\n-\t\t\t\t\t/* Set flag for transport_gone. The Janus sessions watchdog will clean this up if not reclaimed*/\n+\t\t\t\t\t/* Set flag for transport_gone. The Janus sessions watchdog will clean this up if not reclaimed */\n \t\t\t\t\tg_atomic_int_set(&session->transport_gone, 1);\n \t\t\t\t}\n \t\t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t\t/* Set flag for transport_gone. The Janus sessions watchdog will clean this up if not reclaimed*/"
            ],
            "added_lines": [
                "\t\t\t\t\t/* Set flag for transport_gone. The Janus sessions watchdog will clean this up if not reclaimed */"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-10577",
        "func_name": "meetecho/janus-gateway/janus_process_incoming_request",
        "description": "An issue was discovered in Janus through 0.9.1. janus.c has multiple concurrent threads that misuse the source property of a session, leading to a race condition when claiming sessions.",
        "git_url": "https://github.com/meetecho/janus-gateway/commit/4dee754d7902cc767132ad713fb185371382cd65",
        "commit_title": "If claiming from the same transport, do nothing",
        "commit_text": "",
        "func_before": "int janus_process_incoming_request(janus_request *request) {\n\tint ret = -1;\n\tif(request == NULL) {\n\t\tJANUS_LOG(LOG_ERR, \"Missing request or payload to process, giving up...\\n\");\n\t\treturn ret;\n\t}\n\tint error_code = 0;\n\tchar error_cause[100];\n\tjson_t *root = request->message;\n\t/* Ok, let's start with the ids */\n\tguint64 session_id = 0, handle_id = 0;\n\tjson_t *s = json_object_get(root, \"session_id\");\n\tif(s && json_is_integer(s))\n\t\tsession_id = json_integer_value(s);\n\tjson_t *h = json_object_get(root, \"handle_id\");\n\tif(h && json_is_integer(h))\n\t\thandle_id = json_integer_value(h);\n\n\tjanus_session *session = NULL;\n\tjanus_ice_handle *handle = NULL;\n\n\t/* Get transaction and message request */\n\tJANUS_VALIDATE_JSON_OBJECT(root, incoming_request_parameters,\n\t\terror_code, error_cause, FALSE,\n\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\tif(error_code != 0) {\n\t\tret = janus_process_error_string(request, session_id, NULL, error_code, error_cause);\n\t\tgoto jsondone;\n\t}\n\tjson_t *transaction = json_object_get(root, \"transaction\");\n\tconst gchar *transaction_text = json_string_value(transaction);\n\tjson_t *message = json_object_get(root, \"janus\");\n\tconst gchar *message_text = json_string_value(message);\n\n\tif(session_id == 0 && handle_id == 0) {\n\t\t/* Can only be a 'Create new session', a 'Get info' or a 'Ping/Pong' request */\n\t\tif(!strcasecmp(message_text, \"info\")) {\n\t\t\tret = janus_process_success(request, janus_info(transaction_text));\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(!strcasecmp(message_text, \"ping\")) {\n\t\t\t/* Prepare JSON reply */\n\t\t\tjson_t *reply = janus_create_message(\"pong\", 0, transaction_text);\n\t\t\tret = janus_process_success(request, reply);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(strcasecmp(message_text, \"create\")) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Make sure we're accepting new sessions */\n\t\tif(!accept_new_sessions) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_NOT_ACCEPTING_SESSIONS, NULL);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Any secret/token to check? */\n\t\tret = janus_request_check_secret(request, session_id, transaction_text);\n\t\tif(ret != 0) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED, NULL);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tsession_id = 0;\n\t\tjson_t *id = json_object_get(root, \"id\");\n\t\tif(id != NULL) {\n\t\t\t/* The application provided the session ID to use */\n\t\t\tsession_id = json_integer_value(id);\n\t\t\tif(session_id > 0 && (session = janus_session_find(session_id)) != NULL) {\n\t\t\t\t/* Session ID already taken */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_CONFLICT, \"Session ID already in use\");\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t}\n\t\t/* Handle it */\n\t\tsession = janus_session_create(session_id);\n\t\tif(session == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Memory error\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tsession_id = session->session_id;\n\t\t/* We increase the counter as this request is using the session */\n\t\tjanus_refcount_increase(&session->ref);\n\t\t/* Take note of the request source that originated this session (HTTP, WebSockets, RabbitMQ?) */\n\t\tsession->source = janus_request_new(request->transport, request->instance, NULL, FALSE, NULL);\n\t\t/* Notify the source that a new session has been created */\n\t\trequest->transport->session_created(request->instance, session->session_id);\n\t\t/* Notify event handlers */\n\t\tif(janus_events_is_enabled()) {\n\t\t\t/* Session created, add info on the transport that originated it */\n\t\t\tjson_t *transport = json_object();\n\t\t\tjson_object_set_new(transport, \"transport\", json_string(session->source->transport->get_package()));\n\t\t\tchar id[32];\n\t\t\tmemset(id, 0, sizeof(id));\n\t\t\tg_snprintf(id, sizeof(id), \"%p\", session->source->instance);\n\t\t\tjson_object_set_new(transport, \"id\", json_string(id));\n\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_SESSION, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\tsession_id, \"created\", transport);\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", 0, transaction_text);\n\t\tjson_t *data = json_object();\n\t\tjson_object_set_new(data, \"id\", json_integer(session_id));\n\t\tjson_object_set_new(reply, \"data\", data);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t\tgoto jsondone;\n\t}\n\tif(session_id < 1) {\n\t\tJANUS_LOG(LOG_ERR, \"Invalid session\\n\");\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, NULL);\n\t\tgoto jsondone;\n\t}\n\tif(h && handle_id < 1) {\n\t\tJANUS_LOG(LOG_ERR, \"Invalid handle\\n\");\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, NULL);\n\t\tgoto jsondone;\n\t}\n\n\t/* Go on with the processing */\n\tret = janus_request_check_secret(request, session_id, transaction_text);\n\tif(ret != 0) {\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED, NULL);\n\t\tgoto jsondone;\n\t}\n\n\t/* If we got here, make sure we have a session (and/or a handle) */\n\tsession = janus_session_find(session_id);\n\tif(!session) {\n\t\tJANUS_LOG(LOG_ERR, \"Couldn't find any session %\"SCNu64\"...\\n\", session_id);\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, \"No such session %\"SCNu64\"\", session_id);\n\t\tgoto jsondone;\n\t}\n\t/* Update the last activity timer */\n\tsession->last_activity = janus_get_monotonic_time();\n\thandle = NULL;\n\tif(handle_id > 0) {\n\t\thandle = janus_session_handles_find(session, handle_id);\n\t\tif(!handle) {\n\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't find any handle %\"SCNu64\" in session %\"SCNu64\"...\\n\", handle_id, session_id);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_HANDLE_NOT_FOUND, \"No such handle %\"SCNu64\" in session %\"SCNu64\"\", handle_id, session_id);\n\t\t\tgoto jsondone;\n\t\t}\n\t}\n\n\t/* What is this? */\n\tif(!strcasecmp(message_text, \"keepalive\")) {\n\t\t/* Just a keep-alive message, reply with an ack */\n\t\tJANUS_LOG(LOG_VERB, \"Got a keep-alive on session %\"SCNu64\"\\n\", session_id);\n\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"attach\")) {\n\t\tif(handle != NULL) {\n\t\t\t/* Attach is a session-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, attach_parameters,\n\t\t\terror_code, error_cause, FALSE,\n\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\tif(error_code != 0) {\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *plugin = json_object_get(root, \"plugin\");\n\t\tconst gchar *plugin_text = json_string_value(plugin);\n\t\tjanus_plugin *plugin_t = janus_plugin_find(plugin_text);\n\t\tif(plugin_t == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_NOT_FOUND, \"No such plugin '%s'\", plugin_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* If the auth token mechanism is enabled, we should check if this token can access this plugin */\n\t\tif(janus_auth_is_enabled()) {\n\t\t\tjson_t *token = json_object_get(root, \"token\");\n\t\t\tif(token != NULL) {\n\t\t\t\tconst char *token_value = json_string_value(token);\n\t\t\t\tif(token_value && !janus_auth_check_plugin(token_value, plugin_t)) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Token '%s' can't access plugin '%s'\\n\", token_value, plugin_text);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED_PLUGIN, \"Provided token can't access plugin '%s'\", plugin_text);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tjson_t *opaque = json_object_get(root, \"opaque_id\");\n\t\tconst char *opaque_id = opaque ? json_string_value(opaque) : NULL;\n\t\t/* Create handle */\n\t\thandle = janus_ice_handle_create(session, opaque_id);\n\t\tif(handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Memory error\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\thandle_id = handle->handle_id;\n\t\t/* We increase the counter as this request is using the handle */\n\t\tjanus_refcount_increase(&handle->ref);\n\t\t/* Attach to the plugin */\n\t\tint error = 0;\n\t\tif((error = janus_ice_handle_attach_plugin(session, handle, plugin_t)) != 0) {\n\t\t\t/* TODO Make error struct to pass verbose information */\n\t\t\tjanus_session_handles_remove(session, handle);\n\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't attach to plugin '%s', error '%d'\\n\", plugin_text, error);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_ATTACH, \"Couldn't attach to plugin: error '%d'\", error);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\tjson_t *data = json_object();\n\t\tjson_object_set_new(data, \"id\", json_integer(handle_id));\n\t\tjson_object_set_new(reply, \"data\", data);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"destroy\")) {\n\t\tif(handle != NULL) {\n\t\t\t/* Query is a session-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_mutex_lock(&sessions_mutex);\n\t\tg_hash_table_remove(sessions, &session->session_id);\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t/* Notify the source that the session has been destroyed */\n\t\tjanus_mutex_lock(&session->mutex);\n\t\tif(session->source && session->source->transport)\n\t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, FALSE);\n\t\tjanus_mutex_unlock(&session->mutex);\n\t\t/* Schedule the session for deletion */\n\t\tjanus_session_destroy(session);\n\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t\t/* Notify event handlers as well */\n\t\tif(janus_events_is_enabled())\n\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_SESSION, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\tsession_id, \"destroyed\", NULL);\n\t} else if(!strcasecmp(message_text, \"detach\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"No plugin to detach from\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tint error = janus_session_handles_remove(session, handle);\n\t\tif(error != 0) {\n\t\t\t/* TODO Make error struct to pass verbose information */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"Couldn't detach from plugin: error '%d'\", error);\n\t\t\t/* TODO Delete handle instance */\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"hangup\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"No plugin attached\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_ice_webrtc_hangup(handle, \"Janus API\");\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"claim\")) {\n\t\tjanus_mutex_lock(&session->mutex);\n\t\tif(session->source != NULL) {\n\t\t\t/* Notify the old transport that this session is over for them, but has been reclaimed */\n\t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, TRUE);\n\t\t\tjanus_request_destroy(session->source);\n\t\t\tsession->source = NULL;\n\t\t}\n\t\tsession->source = janus_request_new(request->transport, request->instance, NULL, FALSE, NULL);\n\t\t/* Notify the new transport that it has claimed a session */\n\t\tsession->source->transport->session_claimed(session->source->instance, session->session_id);\n\t\t/* Previous transport may be gone, clear flag. */\n\t\tg_atomic_int_set(&session->transport_gone, 0);\n\t\tjanus_mutex_unlock(&session->mutex);\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = json_object();\n\t\tjson_object_set_new(reply, \"janus\", json_string(\"success\"));\n\t\tjson_object_set_new(reply, \"session_id\", json_integer(session_id));\n\t\tjson_object_set_new(reply, \"transaction\", json_string(transaction_text));\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"message\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this message\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_plugin *plugin_t = (janus_plugin *)handle->app;\n\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] There's a message for %s\\n\", handle->handle_id, plugin_t->get_name());\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, body_parameters,\n\t\t\terror_code, error_cause, FALSE,\n\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\tif(error_code != 0) {\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *body = json_object_get(root, \"body\");\n\t\t/* Is there an SDP attached? */\n\t\tjson_t *jsep = json_object_get(root, \"jsep\");\n\t\tchar *jsep_type = NULL;\n\t\tchar *jsep_sdp = NULL, *jsep_sdp_stripped = NULL;\n\t\tgboolean renegotiation = FALSE;\n\t\tif(jsep != NULL) {\n\t\t\tif(!json_is_object(jsep)) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_JSON_OBJECT, \"Invalid jsep object\");\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tJANUS_VALIDATE_JSON_OBJECT_FORMAT(\"JSEP error: missing mandatory element (%s)\",\n\t\t\t\t\"JSEP error: invalid element type (%s should be %s)\",\n\t\t\t\tjsep, jsep_parameters, error_code, error_cause, FALSE,\n\t\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\t\tif(error_code != 0) {\n\t\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjson_t *type = json_object_get(jsep, \"type\");\n\t\t\tjsep_type = g_strdup(json_string_value(type));\n\t\t\ttype = NULL;\n\t\t\tgboolean do_trickle = TRUE;\n\t\t\tjson_t *jsep_trickle = json_object_get(jsep, \"trickle\");\n\t\t\tdo_trickle = jsep_trickle ? json_is_true(jsep_trickle) : TRUE;\n\t\t\t/* Are we still cleaning up from a previous media session? */\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Still cleaning up from a previous media session, let's wait a bit...\\n\", handle->handle_id);\n\t\t\t\tgint64 waited = 0;\n\t\t\t\twhile(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\t\t\tg_usleep(100000);\n\t\t\t\t\twaited += 100000;\n\t\t\t\t\tif(waited >= 3*G_USEC_PER_SEC) {\n\t\t\t\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"]   -- Waited 3 seconds, that's enough!\\n\", handle->handle_id);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_WEBRTC_STATE, \"Still cleaning a previous session\");\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Check if we're renegotiating (if we have an answer, we did an offer/answer round already) */\n\t\t\trenegotiation = janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_NEGOTIATED);\n\t\t\t/* Check the JSEP type */\n\t\t\tjanus_mutex_lock(&handle->mutex);\n\t\t\tint offer = 0;\n\t\t\tif(!strcasecmp(jsep_type, \"offer\")) {\n\t\t\t\toffer = 1;\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER);\n\t\t\t} else if(!strcasecmp(jsep_type, \"answer\")) {\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER);\n\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER))\n\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_NEGOTIATED);\n\t\t\t\toffer = 0;\n\t\t\t} else {\n\t\t\t\t/* TODO Handle other message types as well */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_UNKNOWN_TYPE, \"JSEP error: unknown message type '%s'\", jsep_type);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjson_t *sdp = json_object_get(jsep, \"sdp\");\n\t\t\tjsep_sdp = (char *)json_string_value(sdp);\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Remote SDP:\\n%s\", handle->handle_id, jsep_sdp);\n\t\t\t/* Is this valid SDP? */\n\t\t\tchar error_str[512];\n\t\t\tint audio = 0, video = 0, data = 0;\n\t\t\tjanus_sdp *parsed_sdp = janus_sdp_preparse(handle, jsep_sdp, error_str, sizeof(error_str), &audio, &video, &data);\n\t\t\tif(parsed_sdp == NULL) {\n\t\t\t\t/* Invalid SDP */\n\t\t\t\tret = janus_process_error_string(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, error_str);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\t/* Notify event handlers */\n\t\t\tif(janus_events_is_enabled()) {\n\t\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_JSEP, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\t\tsession_id, handle_id, handle->opaque_id, \"remote\", jsep_type, jsep_sdp);\n\t\t\t}\n\t\t\t/* FIXME We're only handling single audio/video lines for now... */\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Audio %s been negotiated, Video %s been negotiated, SCTP/DataChannels %s been negotiated\\n\",\n\t\t\t                    handle->handle_id,\n\t\t\t                    audio ? \"has\" : \"has NOT\",\n\t\t\t                    video ? \"has\" : \"has NOT\",\n\t\t\t                    data ? \"have\" : \"have NOT\");\n\t\t\tif(audio > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one audio line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n\t\t\tif(video > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one video line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n\t\t\tif(data > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one data line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n#ifndef HAVE_SCTP\n\t\t\tif(data) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"]   -- DataChannels have been negotiated, but support for them has not been compiled...\\n\", handle->handle_id);\n\t\t\t}\n#endif\n\t\t\t/* We behave differently if it's a new session or an update... */\n\t\t\tif(!renegotiation) {\n\t\t\t\t/* New session */\n\t\t\t\tif(offer) {\n\t\t\t\t\t/* Setup ICE locally (we received an offer) */\n\t\t\t\t\tif(janus_ice_setup_local(handle, offer, audio, video, data, do_trickle) < 0) {\n\t\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error setting ICE locally\\n\");\n\t\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Error setting ICE locally\");\n\t\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t/* Make sure we're waiting for an ANSWER in the first place */\n\t\t\t\t\tif(!handle->agent) {\n\t\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Unexpected ANSWER (did we offer?)\\n\");\n\t\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNEXPECTED_ANSWER, \"Unexpected ANSWER (did we offer?)\");\n\t\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(janus_sdp_process(handle, parsed_sdp, FALSE) < 0) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error processing SDP\\n\");\n\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, \"Error processing SDP\");\n\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t\tif(!offer) {\n\t\t\t\t\t/* Set remote candidates now (we received an answer) */\n\t\t\t\t\tif(do_trickle) {\n\t\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t\t\t\t}\n\t\t\t\t\tjanus_request_ice_handle_answer(handle, audio, video, data, jsep_sdp);\n\t\t\t\t} else {\n\t\t\t\t\t/* Check if the mid RTP extension is being negotiated */\n\t\t\t\t\thandle->stream->mid_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_MID);\n\t\t\t\t\t/* Check if the RTP Stream ID extension is being negotiated */\n\t\t\t\t\thandle->stream->rid_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_RID);\n\t\t\t\t\thandle->stream->ridrtx_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_REPAIRED_RID);\n\t\t\t\t\t/* Check if the audio level ID extension is being negotiated */\n\t\t\t\t\thandle->stream->audiolevel_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_AUDIO_LEVEL);\n\t\t\t\t\t/* Check if the video orientation ID extension is being negotiated */\n\t\t\t\t\thandle->stream->videoorientation_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_VIDEO_ORIENTATION);\n\t\t\t\t\t/* Check if the frame marking ID extension is being negotiated */\n\t\t\t\t\thandle->stream->framemarking_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_FRAME_MARKING);\n\t\t\t\t\t/* Check if transport wide CC is supported */\n\t\t\t\t\tint transport_wide_cc_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_TRANSPORT_WIDE_CC);\n\t\t\t\t\thandle->stream->do_transport_wide_cc = transport_wide_cc_ext_id > 0 ? TRUE : FALSE;\n\t\t\t\t\thandle->stream->transport_wide_cc_ext_id = transport_wide_cc_ext_id;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* FIXME This is a renegotiation: we can currently only handle simple changes in media\n\t\t\t\t * direction and ICE restarts: anything more complex than that will result in an error */\n\t\t\t\tJANUS_LOG(LOG_INFO, \"[%\"SCNu64\"] Negotiation update, checking what changed...\\n\", handle->handle_id);\n\t\t\t\tif(janus_sdp_process(handle, parsed_sdp, TRUE) < 0) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error processing SDP\\n\");\n\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNEXPECTED_ANSWER, \"Error processing SDP\");\n\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_ICE_RESTART)) {\n\t\t\t\t\tJANUS_LOG(LOG_INFO, \"[%\"SCNu64\"] Restarting ICE...\\n\", handle->handle_id);\n\t\t\t\t\t/* Update remote credentials for ICE */\n\t\t\t\t\tif(handle->stream) {\n\t\t\t\t\t\tnice_agent_set_remote_credentials(handle->agent, handle->stream->stream_id,\n\t\t\t\t\t\t\thandle->stream->ruser, handle->stream->rpass);\n\t\t\t\t\t}\n\t\t\t\t\t/* FIXME We only need to do that for offers: if it's an answer, we did that already */\n\t\t\t\t\tif(offer) {\n\t\t\t\t\t\tjanus_ice_restart(handle);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_ICE_RESTART);\n\t\t\t\t\t}\n\t\t\t\t\t/* If we're full-trickling, we'll need to resend the candidates later */\n\t\t\t\t\tif(janus_ice_is_full_trickle_enabled()) {\n\t\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_RESEND_TRICKLES);\n\t\t\t\t\t}\n\t\t\t\t}\n#ifdef HAVE_SCTP\n\t\t\t\tif(!offer) {\n\t\t\t\t\t/* Were datachannels just added? */\n\t\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_DATA_CHANNELS)) {\n\t\t\t\t\t\tjanus_ice_stream *stream = handle->stream;\n\t\t\t\t\t\tif(stream != NULL && stream->component != NULL\n\t\t\t\t\t\t\t\t&& stream->component->dtls != NULL && stream->component->dtls->sctp == NULL) {\n\t\t\t\t\t\t\t/* Create SCTP association as well */\n\t\t\t\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] Creating datachannels...\\n\", handle->handle_id);\n\t\t\t\t\t\t\tjanus_dtls_srtp_create_sctp(stream->component->dtls);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n#endif\n\t\t\t}\n\t\t\tchar *tmp = handle->remote_sdp;\n\t\t\thandle->remote_sdp = g_strdup(jsep_sdp);\n\t\t\tg_free(tmp);\n\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t/* Anonymize SDP */\n\t\t\tif(janus_sdp_anonymize(parsed_sdp) < 0) {\n\t\t\t\t/* Invalid SDP */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, \"JSEP error: invalid SDP\");\n\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjsep_sdp_stripped = janus_sdp_write(parsed_sdp);\n\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\tsdp = NULL;\n\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t}\n\n\t\t/* Make sure the app handle is still valid */\n\t\tif(handle->app == NULL || !janus_plugin_session_is_alive(handle->app_handle)) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this message\");\n\t\t\tg_free(jsep_type);\n\t\t\tg_free(jsep_sdp_stripped);\n\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\tgoto jsondone;\n\t\t}\n\n\t\t/* Send the message to the plugin (which must eventually free transaction_text and unref the two objects, body and jsep) */\n\t\tjson_incref(body);\n\t\tjson_t *body_jsep = NULL;\n\t\tif(jsep_sdp_stripped) {\n\t\t\tbody_jsep = json_pack(\"{ssss}\", \"type\", jsep_type, \"sdp\", jsep_sdp_stripped);\n\t\t\t/* Check if simulcasting is enabled */\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_HAS_VIDEO)) {\n\t\t\t\tif(handle->stream && (handle->stream->rid[0] || handle->stream->video_ssrc_peer[1])) {\n\t\t\t\t\tjson_t *simulcast = json_object();\n\t\t\t\t\t/* If we have rids, pass those, otherwise pass the SSRCs */\n\t\t\t\t\tif(handle->stream->rid[0]) {\n\t\t\t\t\t\tjson_t *rids = json_array();\n\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[0]));\n\t\t\t\t\t\tif(handle->stream->rid[1])\n\t\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[1]));\n\t\t\t\t\t\tif(handle->stream->rid[2])\n\t\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[2]));\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"rids\", rids);\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"rid-ext\", json_integer(handle->stream->rid_ext_id));\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjson_t *ssrcs = json_array();\n\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[0]));\n\t\t\t\t\t\tif(handle->stream->video_ssrc_peer[1])\n\t\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[1]));\n\t\t\t\t\t\tif(handle->stream->video_ssrc_peer[2])\n\t\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[2]));\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"ssrcs\", ssrcs);\n\t\t\t\t\t}\n\t\t\t\t\tif(handle->stream->framemarking_ext_id > 0)\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"framemarking-ext\", json_integer(handle->stream->framemarking_ext_id));\n\t\t\t\t\tjson_object_set_new(body_jsep, \"simulcast\", simulcast);\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Check if this is a renegotiation or update */\n\t\t\tif(renegotiation)\n\t\t\t\tjson_object_set_new(body_jsep, \"update\", json_true());\n\t\t}\n\t\tjanus_plugin_result *result = plugin_t->handle_message(handle->app_handle,\n\t\t\tg_strdup((char *)transaction_text), body, body_jsep);\n\t\tg_free(jsep_type);\n\t\tg_free(jsep_sdp_stripped);\n\t\tif(result == NULL) {\n\t\t\t/* Something went horribly wrong! */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"Plugin didn't give a result\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(result->type == JANUS_PLUGIN_OK) {\n\t\t\t/* The plugin gave a result already (synchronous request/response) */\n\t\t\tif(result->content == NULL || !json_is_object(result->content)) {\n\t\t\t\t/* Missing content, or not a JSON object */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE,\n\t\t\t\t\tresult->content == NULL ?\n\t\t\t\t\t\t\"Plugin didn't provide any content for this synchronous response\" :\n\t\t\t\t\t\t\"Plugin returned an invalid JSON response\");\n\t\t\t\tjanus_plugin_result_destroy(result);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\t/* Reference the content, as destroying the result instance will decref it */\n\t\t\tjson_incref(result->content);\n\t\t\t/* Prepare JSON response */\n\t\t\tjson_t *reply = janus_create_message(\"success\", session->session_id, transaction_text);\n\t\t\tjson_object_set_new(reply, \"sender\", json_integer(handle->handle_id));\n\t\t\tif(janus_is_opaqueid_in_api_enabled() && handle->opaque_id != NULL)\n\t\t\t\tjson_object_set_new(reply, \"opaque_id\", json_string(handle->opaque_id));\n\t\t\tjson_t *plugin_data = json_object();\n\t\t\tjson_object_set_new(plugin_data, \"plugin\", json_string(plugin_t->get_package()));\n\t\t\tjson_object_set_new(plugin_data, \"data\", result->content);\n\t\t\tjson_object_set_new(reply, \"plugindata\", plugin_data);\n\t\t\t/* Send the success reply */\n\t\t\tret = janus_process_success(request, reply);\n\t\t} else if(result->type == JANUS_PLUGIN_OK_WAIT) {\n\t\t\t/* The plugin received the request but didn't process it yet, send an ack (asynchronous notifications may follow) */\n\t\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t\tif(result->text)\n\t\t\t\tjson_object_set_new(reply, \"hint\", json_string(result->text));\n\t\t\t/* Send the success reply */\n\t\t\tret = janus_process_success(request, reply);\n\t\t} else {\n\t\t\t/* Something went horribly wrong! */\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE,\n\t\t\t\t(char *)(result->text ? result->text : \"Plugin returned a severe (unknown) error\"));\n\t\t\tjanus_plugin_result_destroy(result);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_plugin_result_destroy(result);\n\t} else if(!strcasecmp(message_text, \"trickle\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Trickle is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || !janus_plugin_session_is_alive(handle->app_handle)) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this trickle candidate\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *candidate = json_object_get(root, \"candidate\");\n\t\tjson_t *candidates = json_object_get(root, \"candidates\");\n\t\tif(candidate == NULL && candidates == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_MISSING_MANDATORY_ELEMENT, \"Missing mandatory element (candidate|candidates)\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(candidate != NULL && candidates != NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_JSON, \"Can't have both candidate and candidates\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\tJANUS_LOG(LOG_ERR, \"[%\"SCNu64\"] Received a trickle, but still cleaning a previous session\\n\", handle->handle_id);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_WEBRTC_STATE, \"Still cleaning a previous session\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_mutex_lock(&handle->mutex);\n\t\tif(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE)) {\n\t\t\t/* It looks like this peer supports Trickle, after all */\n\t\t\tJANUS_LOG(LOG_VERB, \"Handle %\"SCNu64\" supports trickle even if it didn't negotiate it...\\n\", handle->handle_id);\n\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t}\n\t\t/* Is there any stream ready? this trickle may get here before the SDP it relates to */\n\t\tif(handle->stream == NULL) {\n\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] No stream, queueing this trickle as it got here before the SDP...\\n\", handle->handle_id);\n\t\t\t/* Enqueue this trickle candidate(s), we'll process this later */\n\t\t\tjanus_ice_trickle *early_trickle = janus_ice_trickle_new(transaction_text, candidate ? candidate : candidates);\n\t\t\thandle->pending_trickles = g_list_append(handle->pending_trickles, early_trickle);\n\t\t\t/* Send the ack right away, an event will tell the application if the candidate(s) failed */\n\t\t\tgoto trickledone;\n\t\t}\n\t\t/* Is the ICE stack ready already? */\n\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER) ||\n\t\t\t\t!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER) ||\n\t\t\t\t!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER)) {\n\t\t\tconst char *cause = NULL;\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER))\n\t\t\t\tcause = \"processing the offer\";\n\t\t\telse if(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER))\n\t\t\t\tcause = \"waiting for the answer\";\n\t\t\telse if(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER))\n\t\t\t\tcause = \"waiting for the offer\";\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Still %s, queueing this trickle to wait until we're done there...\\n\",\n\t\t\t\thandle->handle_id, cause);\n\t\t\t/* Enqueue this trickle candidate(s), we'll process this later */\n\t\t\tjanus_ice_trickle *early_trickle = janus_ice_trickle_new(transaction_text, candidate ? candidate : candidates);\n\t\t\thandle->pending_trickles = g_list_append(handle->pending_trickles, early_trickle);\n\t\t\t/* Send the ack right away, an event will tell the application if the candidate(s) failed */\n\t\t\tgoto trickledone;\n\t\t}\n\t\tif(candidate != NULL) {\n\t\t\t/* We got a single candidate */\n\t\t\tint error = 0;\n\t\t\tconst char *error_string = NULL;\n\t\t\tif((error = janus_ice_trickle_parse(handle, candidate, &error_string)) != 0) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, error, \"%s\", error_string);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t} else {\n\t\t\t/* We got multiple candidates in an array */\n\t\t\tif(!json_is_array(candidates)) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_ELEMENT_TYPE, \"candidates is not an array\");\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tJANUS_LOG(LOG_VERB, \"Got multiple candidates (%zu)\\n\", json_array_size(candidates));\n\t\t\tif(json_array_size(candidates) > 0) {\n\t\t\t\t/* Handle remote candidates */\n\t\t\t\tsize_t i = 0;\n\t\t\t\tfor(i=0; i<json_array_size(candidates); i++) {\n\t\t\t\t\tjson_t *c = json_array_get(candidates, i);\n\t\t\t\t\t/* FIXME We don't care if any trickle fails to parse */\n\t\t\t\t\tjanus_ice_trickle_parse(handle, c, NULL);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\ntrickledone:\n\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t/* We reply right away, not to block the web server... */\n\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else {\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN_REQUEST, \"Unknown request '%s'\", message_text);\n\t}\n\njsondone:\n\t/* Done processing */\n\tif(handle != NULL)\n\t\tjanus_refcount_decrease(&handle->ref);\n\tif(session != NULL)\n\t\tjanus_refcount_decrease(&session->ref);\n\treturn ret;\n}",
        "func": "int janus_process_incoming_request(janus_request *request) {\n\tint ret = -1;\n\tif(request == NULL) {\n\t\tJANUS_LOG(LOG_ERR, \"Missing request or payload to process, giving up...\\n\");\n\t\treturn ret;\n\t}\n\tint error_code = 0;\n\tchar error_cause[100];\n\tjson_t *root = request->message;\n\t/* Ok, let's start with the ids */\n\tguint64 session_id = 0, handle_id = 0;\n\tjson_t *s = json_object_get(root, \"session_id\");\n\tif(s && json_is_integer(s))\n\t\tsession_id = json_integer_value(s);\n\tjson_t *h = json_object_get(root, \"handle_id\");\n\tif(h && json_is_integer(h))\n\t\thandle_id = json_integer_value(h);\n\n\tjanus_session *session = NULL;\n\tjanus_ice_handle *handle = NULL;\n\n\t/* Get transaction and message request */\n\tJANUS_VALIDATE_JSON_OBJECT(root, incoming_request_parameters,\n\t\terror_code, error_cause, FALSE,\n\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\tif(error_code != 0) {\n\t\tret = janus_process_error_string(request, session_id, NULL, error_code, error_cause);\n\t\tgoto jsondone;\n\t}\n\tjson_t *transaction = json_object_get(root, \"transaction\");\n\tconst gchar *transaction_text = json_string_value(transaction);\n\tjson_t *message = json_object_get(root, \"janus\");\n\tconst gchar *message_text = json_string_value(message);\n\n\tif(session_id == 0 && handle_id == 0) {\n\t\t/* Can only be a 'Create new session', a 'Get info' or a 'Ping/Pong' request */\n\t\tif(!strcasecmp(message_text, \"info\")) {\n\t\t\tret = janus_process_success(request, janus_info(transaction_text));\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(!strcasecmp(message_text, \"ping\")) {\n\t\t\t/* Prepare JSON reply */\n\t\t\tjson_t *reply = janus_create_message(\"pong\", 0, transaction_text);\n\t\t\tret = janus_process_success(request, reply);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(strcasecmp(message_text, \"create\")) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Make sure we're accepting new sessions */\n\t\tif(!accept_new_sessions) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_NOT_ACCEPTING_SESSIONS, NULL);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Any secret/token to check? */\n\t\tret = janus_request_check_secret(request, session_id, transaction_text);\n\t\tif(ret != 0) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED, NULL);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tsession_id = 0;\n\t\tjson_t *id = json_object_get(root, \"id\");\n\t\tif(id != NULL) {\n\t\t\t/* The application provided the session ID to use */\n\t\t\tsession_id = json_integer_value(id);\n\t\t\tif(session_id > 0 && (session = janus_session_find(session_id)) != NULL) {\n\t\t\t\t/* Session ID already taken */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_CONFLICT, \"Session ID already in use\");\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t}\n\t\t/* Handle it */\n\t\tsession = janus_session_create(session_id);\n\t\tif(session == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Memory error\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tsession_id = session->session_id;\n\t\t/* We increase the counter as this request is using the session */\n\t\tjanus_refcount_increase(&session->ref);\n\t\t/* Take note of the request source that originated this session (HTTP, WebSockets, RabbitMQ?) */\n\t\tsession->source = janus_request_new(request->transport, request->instance, NULL, FALSE, NULL);\n\t\t/* Notify the source that a new session has been created */\n\t\trequest->transport->session_created(request->instance, session->session_id);\n\t\t/* Notify event handlers */\n\t\tif(janus_events_is_enabled()) {\n\t\t\t/* Session created, add info on the transport that originated it */\n\t\t\tjson_t *transport = json_object();\n\t\t\tjson_object_set_new(transport, \"transport\", json_string(session->source->transport->get_package()));\n\t\t\tchar id[32];\n\t\t\tmemset(id, 0, sizeof(id));\n\t\t\tg_snprintf(id, sizeof(id), \"%p\", session->source->instance);\n\t\t\tjson_object_set_new(transport, \"id\", json_string(id));\n\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_SESSION, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\tsession_id, \"created\", transport);\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", 0, transaction_text);\n\t\tjson_t *data = json_object();\n\t\tjson_object_set_new(data, \"id\", json_integer(session_id));\n\t\tjson_object_set_new(reply, \"data\", data);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t\tgoto jsondone;\n\t}\n\tif(session_id < 1) {\n\t\tJANUS_LOG(LOG_ERR, \"Invalid session\\n\");\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, NULL);\n\t\tgoto jsondone;\n\t}\n\tif(h && handle_id < 1) {\n\t\tJANUS_LOG(LOG_ERR, \"Invalid handle\\n\");\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, NULL);\n\t\tgoto jsondone;\n\t}\n\n\t/* Go on with the processing */\n\tret = janus_request_check_secret(request, session_id, transaction_text);\n\tif(ret != 0) {\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED, NULL);\n\t\tgoto jsondone;\n\t}\n\n\t/* If we got here, make sure we have a session (and/or a handle) */\n\tsession = janus_session_find(session_id);\n\tif(!session) {\n\t\tJANUS_LOG(LOG_ERR, \"Couldn't find any session %\"SCNu64\"...\\n\", session_id);\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_SESSION_NOT_FOUND, \"No such session %\"SCNu64\"\", session_id);\n\t\tgoto jsondone;\n\t}\n\t/* Update the last activity timer */\n\tsession->last_activity = janus_get_monotonic_time();\n\thandle = NULL;\n\tif(handle_id > 0) {\n\t\thandle = janus_session_handles_find(session, handle_id);\n\t\tif(!handle) {\n\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't find any handle %\"SCNu64\" in session %\"SCNu64\"...\\n\", handle_id, session_id);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_HANDLE_NOT_FOUND, \"No such handle %\"SCNu64\" in session %\"SCNu64\"\", handle_id, session_id);\n\t\t\tgoto jsondone;\n\t\t}\n\t}\n\n\t/* What is this? */\n\tif(!strcasecmp(message_text, \"keepalive\")) {\n\t\t/* Just a keep-alive message, reply with an ack */\n\t\tJANUS_LOG(LOG_VERB, \"Got a keep-alive on session %\"SCNu64\"\\n\", session_id);\n\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"attach\")) {\n\t\tif(handle != NULL) {\n\t\t\t/* Attach is a session-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, attach_parameters,\n\t\t\terror_code, error_cause, FALSE,\n\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\tif(error_code != 0) {\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *plugin = json_object_get(root, \"plugin\");\n\t\tconst gchar *plugin_text = json_string_value(plugin);\n\t\tjanus_plugin *plugin_t = janus_plugin_find(plugin_text);\n\t\tif(plugin_t == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_NOT_FOUND, \"No such plugin '%s'\", plugin_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* If the auth token mechanism is enabled, we should check if this token can access this plugin */\n\t\tif(janus_auth_is_enabled()) {\n\t\t\tjson_t *token = json_object_get(root, \"token\");\n\t\t\tif(token != NULL) {\n\t\t\t\tconst char *token_value = json_string_value(token);\n\t\t\t\tif(token_value && !janus_auth_check_plugin(token_value, plugin_t)) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Token '%s' can't access plugin '%s'\\n\", token_value, plugin_text);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNAUTHORIZED_PLUGIN, \"Provided token can't access plugin '%s'\", plugin_text);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tjson_t *opaque = json_object_get(root, \"opaque_id\");\n\t\tconst char *opaque_id = opaque ? json_string_value(opaque) : NULL;\n\t\t/* Create handle */\n\t\thandle = janus_ice_handle_create(session, opaque_id);\n\t\tif(handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Memory error\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\thandle_id = handle->handle_id;\n\t\t/* We increase the counter as this request is using the handle */\n\t\tjanus_refcount_increase(&handle->ref);\n\t\t/* Attach to the plugin */\n\t\tint error = 0;\n\t\tif((error = janus_ice_handle_attach_plugin(session, handle, plugin_t)) != 0) {\n\t\t\t/* TODO Make error struct to pass verbose information */\n\t\t\tjanus_session_handles_remove(session, handle);\n\t\t\tJANUS_LOG(LOG_ERR, \"Couldn't attach to plugin '%s', error '%d'\\n\", plugin_text, error);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_ATTACH, \"Couldn't attach to plugin: error '%d'\", error);\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\tjson_t *data = json_object();\n\t\tjson_object_set_new(data, \"id\", json_integer(handle_id));\n\t\tjson_object_set_new(reply, \"data\", data);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"destroy\")) {\n\t\tif(handle != NULL) {\n\t\t\t/* Query is a session-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_mutex_lock(&sessions_mutex);\n\t\tg_hash_table_remove(sessions, &session->session_id);\n\t\tjanus_mutex_unlock(&sessions_mutex);\n\t\t/* Notify the source that the session has been destroyed */\n\t\tjanus_mutex_lock(&session->mutex);\n\t\tif(session->source && session->source->transport)\n\t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, FALSE);\n\t\tjanus_mutex_unlock(&session->mutex);\n\t\t/* Schedule the session for deletion */\n\t\tjanus_session_destroy(session);\n\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t\t/* Notify event handlers as well */\n\t\tif(janus_events_is_enabled())\n\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_SESSION, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\tsession_id, \"destroyed\", NULL);\n\t} else if(!strcasecmp(message_text, \"detach\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"No plugin to detach from\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tint error = janus_session_handles_remove(session, handle);\n\t\tif(error != 0) {\n\t\t\t/* TODO Make error struct to pass verbose information */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"Couldn't detach from plugin: error '%d'\", error);\n\t\t\t/* TODO Delete handle instance */\n\t\t\tgoto jsondone;\n\t\t}\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"hangup\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_DETACH, \"No plugin attached\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_ice_webrtc_hangup(handle, \"Janus API\");\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = janus_create_message(\"success\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"claim\")) {\n\t\tjanus_mutex_lock(&session->mutex);\n\t\tif(session->source != NULL) {\n\t\t\t/* If we're claiming from the same transport, ignore */\n\t\t\tif(session->source->instance == request->instance) {\n\t\t\t\tjanus_mutex_unlock(&session->mutex);\n\t\t\t\t/* Prepare JSON reply */\n\t\t\t\tjson_t *reply = json_object();\n\t\t\t\tjson_object_set_new(reply, \"janus\", json_string(\"success\"));\n\t\t\t\tjson_object_set_new(reply, \"session_id\", json_integer(session_id));\n\t\t\t\tjson_object_set_new(reply, \"transaction\", json_string(transaction_text));\n\t\t\t\t/* Send the success reply */\n\t\t\t\tret = janus_process_success(request, reply);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\t/* Notify the old transport that this session is over for them, but has been reclaimed */\n\t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, TRUE);\n\t\t\tjanus_request_destroy(session->source);\n\t\t\tsession->source = NULL;\n\t\t}\n\t\tsession->source = janus_request_new(request->transport, request->instance, NULL, FALSE, NULL);\n\t\t/* Notify the new transport that it has claimed a session */\n\t\tsession->source->transport->session_claimed(session->source->instance, session->session_id);\n\t\t/* Previous transport may be gone, clear flag */\n\t\tg_atomic_int_set(&session->transport_gone, 0);\n\t\tjanus_mutex_unlock(&session->mutex);\n\t\t/* Prepare JSON reply */\n\t\tjson_t *reply = json_object();\n\t\tjson_object_set_new(reply, \"janus\", json_string(\"success\"));\n\t\tjson_object_set_new(reply, \"session_id\", json_integer(session_id));\n\t\tjson_object_set_new(reply, \"transaction\", json_string(transaction_text));\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else if(!strcasecmp(message_text, \"message\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Query is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || handle->app_handle == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this message\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_plugin *plugin_t = (janus_plugin *)handle->app;\n\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] There's a message for %s\\n\", handle->handle_id, plugin_t->get_name());\n\t\tJANUS_VALIDATE_JSON_OBJECT(root, body_parameters,\n\t\t\terror_code, error_cause, FALSE,\n\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\tif(error_code != 0) {\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *body = json_object_get(root, \"body\");\n\t\t/* Is there an SDP attached? */\n\t\tjson_t *jsep = json_object_get(root, \"jsep\");\n\t\tchar *jsep_type = NULL;\n\t\tchar *jsep_sdp = NULL, *jsep_sdp_stripped = NULL;\n\t\tgboolean renegotiation = FALSE;\n\t\tif(jsep != NULL) {\n\t\t\tif(!json_is_object(jsep)) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_JSON_OBJECT, \"Invalid jsep object\");\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tJANUS_VALIDATE_JSON_OBJECT_FORMAT(\"JSEP error: missing mandatory element (%s)\",\n\t\t\t\t\"JSEP error: invalid element type (%s should be %s)\",\n\t\t\t\tjsep, jsep_parameters, error_code, error_cause, FALSE,\n\t\t\t\tJANUS_ERROR_MISSING_MANDATORY_ELEMENT, JANUS_ERROR_INVALID_ELEMENT_TYPE);\n\t\t\tif(error_code != 0) {\n\t\t\t\tret = janus_process_error_string(request, session_id, transaction_text, error_code, error_cause);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjson_t *type = json_object_get(jsep, \"type\");\n\t\t\tjsep_type = g_strdup(json_string_value(type));\n\t\t\ttype = NULL;\n\t\t\tgboolean do_trickle = TRUE;\n\t\t\tjson_t *jsep_trickle = json_object_get(jsep, \"trickle\");\n\t\t\tdo_trickle = jsep_trickle ? json_is_true(jsep_trickle) : TRUE;\n\t\t\t/* Are we still cleaning up from a previous media session? */\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Still cleaning up from a previous media session, let's wait a bit...\\n\", handle->handle_id);\n\t\t\t\tgint64 waited = 0;\n\t\t\t\twhile(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\t\t\tg_usleep(100000);\n\t\t\t\t\twaited += 100000;\n\t\t\t\t\tif(waited >= 3*G_USEC_PER_SEC) {\n\t\t\t\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"]   -- Waited 3 seconds, that's enough!\\n\", handle->handle_id);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_WEBRTC_STATE, \"Still cleaning a previous session\");\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Check if we're renegotiating (if we have an answer, we did an offer/answer round already) */\n\t\t\trenegotiation = janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_NEGOTIATED);\n\t\t\t/* Check the JSEP type */\n\t\t\tjanus_mutex_lock(&handle->mutex);\n\t\t\tint offer = 0;\n\t\t\tif(!strcasecmp(jsep_type, \"offer\")) {\n\t\t\t\toffer = 1;\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER);\n\t\t\t} else if(!strcasecmp(jsep_type, \"answer\")) {\n\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER);\n\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER))\n\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_NEGOTIATED);\n\t\t\t\toffer = 0;\n\t\t\t} else {\n\t\t\t\t/* TODO Handle other message types as well */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_UNKNOWN_TYPE, \"JSEP error: unknown message type '%s'\", jsep_type);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjson_t *sdp = json_object_get(jsep, \"sdp\");\n\t\t\tjsep_sdp = (char *)json_string_value(sdp);\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Remote SDP:\\n%s\", handle->handle_id, jsep_sdp);\n\t\t\t/* Is this valid SDP? */\n\t\t\tchar error_str[512];\n\t\t\tint audio = 0, video = 0, data = 0;\n\t\t\tjanus_sdp *parsed_sdp = janus_sdp_preparse(handle, jsep_sdp, error_str, sizeof(error_str), &audio, &video, &data);\n\t\t\tif(parsed_sdp == NULL) {\n\t\t\t\t/* Invalid SDP */\n\t\t\t\tret = janus_process_error_string(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, error_str);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\t/* Notify event handlers */\n\t\t\tif(janus_events_is_enabled()) {\n\t\t\t\tjanus_events_notify_handlers(JANUS_EVENT_TYPE_JSEP, JANUS_EVENT_SUBTYPE_NONE,\n\t\t\t\t\tsession_id, handle_id, handle->opaque_id, \"remote\", jsep_type, jsep_sdp);\n\t\t\t}\n\t\t\t/* FIXME We're only handling single audio/video lines for now... */\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Audio %s been negotiated, Video %s been negotiated, SCTP/DataChannels %s been negotiated\\n\",\n\t\t\t                    handle->handle_id,\n\t\t\t                    audio ? \"has\" : \"has NOT\",\n\t\t\t                    video ? \"has\" : \"has NOT\",\n\t\t\t                    data ? \"have\" : \"have NOT\");\n\t\t\tif(audio > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one audio line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n\t\t\tif(video > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one video line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n\t\t\tif(data > 1) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] More than one data line? only going to negotiate one...\\n\", handle->handle_id);\n\t\t\t}\n#ifndef HAVE_SCTP\n\t\t\tif(data) {\n\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"]   -- DataChannels have been negotiated, but support for them has not been compiled...\\n\", handle->handle_id);\n\t\t\t}\n#endif\n\t\t\t/* We behave differently if it's a new session or an update... */\n\t\t\tif(!renegotiation) {\n\t\t\t\t/* New session */\n\t\t\t\tif(offer) {\n\t\t\t\t\t/* Setup ICE locally (we received an offer) */\n\t\t\t\t\tif(janus_ice_setup_local(handle, offer, audio, video, data, do_trickle) < 0) {\n\t\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error setting ICE locally\\n\");\n\t\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN, \"Error setting ICE locally\");\n\t\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t/* Make sure we're waiting for an ANSWER in the first place */\n\t\t\t\t\tif(!handle->agent) {\n\t\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Unexpected ANSWER (did we offer?)\\n\");\n\t\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNEXPECTED_ANSWER, \"Unexpected ANSWER (did we offer?)\");\n\t\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\t\tgoto jsondone;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(janus_sdp_process(handle, parsed_sdp, FALSE) < 0) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error processing SDP\\n\");\n\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, \"Error processing SDP\");\n\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t\tif(!offer) {\n\t\t\t\t\t/* Set remote candidates now (we received an answer) */\n\t\t\t\t\tif(do_trickle) {\n\t\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t\t\t\t}\n\t\t\t\t\tjanus_request_ice_handle_answer(handle, audio, video, data, jsep_sdp);\n\t\t\t\t} else {\n\t\t\t\t\t/* Check if the mid RTP extension is being negotiated */\n\t\t\t\t\thandle->stream->mid_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_MID);\n\t\t\t\t\t/* Check if the RTP Stream ID extension is being negotiated */\n\t\t\t\t\thandle->stream->rid_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_RID);\n\t\t\t\t\thandle->stream->ridrtx_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_REPAIRED_RID);\n\t\t\t\t\t/* Check if the audio level ID extension is being negotiated */\n\t\t\t\t\thandle->stream->audiolevel_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_AUDIO_LEVEL);\n\t\t\t\t\t/* Check if the video orientation ID extension is being negotiated */\n\t\t\t\t\thandle->stream->videoorientation_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_VIDEO_ORIENTATION);\n\t\t\t\t\t/* Check if the frame marking ID extension is being negotiated */\n\t\t\t\t\thandle->stream->framemarking_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_FRAME_MARKING);\n\t\t\t\t\t/* Check if transport wide CC is supported */\n\t\t\t\t\tint transport_wide_cc_ext_id = janus_rtp_header_extension_get_id(jsep_sdp, JANUS_RTP_EXTMAP_TRANSPORT_WIDE_CC);\n\t\t\t\t\thandle->stream->do_transport_wide_cc = transport_wide_cc_ext_id > 0 ? TRUE : FALSE;\n\t\t\t\t\thandle->stream->transport_wide_cc_ext_id = transport_wide_cc_ext_id;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* FIXME This is a renegotiation: we can currently only handle simple changes in media\n\t\t\t\t * direction and ICE restarts: anything more complex than that will result in an error */\n\t\t\t\tJANUS_LOG(LOG_INFO, \"[%\"SCNu64\"] Negotiation update, checking what changed...\\n\", handle->handle_id);\n\t\t\t\tif(janus_sdp_process(handle, parsed_sdp, TRUE) < 0) {\n\t\t\t\t\tJANUS_LOG(LOG_ERR, \"Error processing SDP\\n\");\n\t\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\t\tg_free(jsep_type);\n\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNEXPECTED_ANSWER, \"Error processing SDP\");\n\t\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\t\tgoto jsondone;\n\t\t\t\t}\n\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_ICE_RESTART)) {\n\t\t\t\t\tJANUS_LOG(LOG_INFO, \"[%\"SCNu64\"] Restarting ICE...\\n\", handle->handle_id);\n\t\t\t\t\t/* Update remote credentials for ICE */\n\t\t\t\t\tif(handle->stream) {\n\t\t\t\t\t\tnice_agent_set_remote_credentials(handle->agent, handle->stream->stream_id,\n\t\t\t\t\t\t\thandle->stream->ruser, handle->stream->rpass);\n\t\t\t\t\t}\n\t\t\t\t\t/* FIXME We only need to do that for offers: if it's an answer, we did that already */\n\t\t\t\t\tif(offer) {\n\t\t\t\t\t\tjanus_ice_restart(handle);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_ICE_RESTART);\n\t\t\t\t\t}\n\t\t\t\t\t/* If we're full-trickling, we'll need to resend the candidates later */\n\t\t\t\t\tif(janus_ice_is_full_trickle_enabled()) {\n\t\t\t\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_RESEND_TRICKLES);\n\t\t\t\t\t}\n\t\t\t\t}\n#ifdef HAVE_SCTP\n\t\t\t\tif(!offer) {\n\t\t\t\t\t/* Were datachannels just added? */\n\t\t\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_DATA_CHANNELS)) {\n\t\t\t\t\t\tjanus_ice_stream *stream = handle->stream;\n\t\t\t\t\t\tif(stream != NULL && stream->component != NULL\n\t\t\t\t\t\t\t\t&& stream->component->dtls != NULL && stream->component->dtls->sctp == NULL) {\n\t\t\t\t\t\t\t/* Create SCTP association as well */\n\t\t\t\t\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] Creating datachannels...\\n\", handle->handle_id);\n\t\t\t\t\t\t\tjanus_dtls_srtp_create_sctp(stream->component->dtls);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n#endif\n\t\t\t}\n\t\t\tchar *tmp = handle->remote_sdp;\n\t\t\thandle->remote_sdp = g_strdup(jsep_sdp);\n\t\t\tg_free(tmp);\n\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t/* Anonymize SDP */\n\t\t\tif(janus_sdp_anonymize(parsed_sdp) < 0) {\n\t\t\t\t/* Invalid SDP */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_JSEP_INVALID_SDP, \"JSEP error: invalid SDP\");\n\t\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\t\tg_free(jsep_type);\n\t\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tjsep_sdp_stripped = janus_sdp_write(parsed_sdp);\n\t\t\tjanus_sdp_destroy(parsed_sdp);\n\t\t\tsdp = NULL;\n\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t}\n\n\t\t/* Make sure the app handle is still valid */\n\t\tif(handle->app == NULL || !janus_plugin_session_is_alive(handle->app_handle)) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this message\");\n\t\t\tg_free(jsep_type);\n\t\t\tg_free(jsep_sdp_stripped);\n\t\t\tjanus_flags_clear(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER);\n\t\t\tgoto jsondone;\n\t\t}\n\n\t\t/* Send the message to the plugin (which must eventually free transaction_text and unref the two objects, body and jsep) */\n\t\tjson_incref(body);\n\t\tjson_t *body_jsep = NULL;\n\t\tif(jsep_sdp_stripped) {\n\t\t\tbody_jsep = json_pack(\"{ssss}\", \"type\", jsep_type, \"sdp\", jsep_sdp_stripped);\n\t\t\t/* Check if simulcasting is enabled */\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_HAS_VIDEO)) {\n\t\t\t\tif(handle->stream && (handle->stream->rid[0] || handle->stream->video_ssrc_peer[1])) {\n\t\t\t\t\tjson_t *simulcast = json_object();\n\t\t\t\t\t/* If we have rids, pass those, otherwise pass the SSRCs */\n\t\t\t\t\tif(handle->stream->rid[0]) {\n\t\t\t\t\t\tjson_t *rids = json_array();\n\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[0]));\n\t\t\t\t\t\tif(handle->stream->rid[1])\n\t\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[1]));\n\t\t\t\t\t\tif(handle->stream->rid[2])\n\t\t\t\t\t\t\tjson_array_append_new(rids, json_string(handle->stream->rid[2]));\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"rids\", rids);\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"rid-ext\", json_integer(handle->stream->rid_ext_id));\n\t\t\t\t\t} else {\n\t\t\t\t\t\tjson_t *ssrcs = json_array();\n\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[0]));\n\t\t\t\t\t\tif(handle->stream->video_ssrc_peer[1])\n\t\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[1]));\n\t\t\t\t\t\tif(handle->stream->video_ssrc_peer[2])\n\t\t\t\t\t\t\tjson_array_append_new(ssrcs, json_integer(handle->stream->video_ssrc_peer[2]));\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"ssrcs\", ssrcs);\n\t\t\t\t\t}\n\t\t\t\t\tif(handle->stream->framemarking_ext_id > 0)\n\t\t\t\t\t\tjson_object_set_new(simulcast, \"framemarking-ext\", json_integer(handle->stream->framemarking_ext_id));\n\t\t\t\t\tjson_object_set_new(body_jsep, \"simulcast\", simulcast);\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Check if this is a renegotiation or update */\n\t\t\tif(renegotiation)\n\t\t\t\tjson_object_set_new(body_jsep, \"update\", json_true());\n\t\t}\n\t\tjanus_plugin_result *result = plugin_t->handle_message(handle->app_handle,\n\t\t\tg_strdup((char *)transaction_text), body, body_jsep);\n\t\tg_free(jsep_type);\n\t\tg_free(jsep_sdp_stripped);\n\t\tif(result == NULL) {\n\t\t\t/* Something went horribly wrong! */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"Plugin didn't give a result\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(result->type == JANUS_PLUGIN_OK) {\n\t\t\t/* The plugin gave a result already (synchronous request/response) */\n\t\t\tif(result->content == NULL || !json_is_object(result->content)) {\n\t\t\t\t/* Missing content, or not a JSON object */\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE,\n\t\t\t\t\tresult->content == NULL ?\n\t\t\t\t\t\t\"Plugin didn't provide any content for this synchronous response\" :\n\t\t\t\t\t\t\"Plugin returned an invalid JSON response\");\n\t\t\t\tjanus_plugin_result_destroy(result);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\t/* Reference the content, as destroying the result instance will decref it */\n\t\t\tjson_incref(result->content);\n\t\t\t/* Prepare JSON response */\n\t\t\tjson_t *reply = janus_create_message(\"success\", session->session_id, transaction_text);\n\t\t\tjson_object_set_new(reply, \"sender\", json_integer(handle->handle_id));\n\t\t\tif(janus_is_opaqueid_in_api_enabled() && handle->opaque_id != NULL)\n\t\t\t\tjson_object_set_new(reply, \"opaque_id\", json_string(handle->opaque_id));\n\t\t\tjson_t *plugin_data = json_object();\n\t\t\tjson_object_set_new(plugin_data, \"plugin\", json_string(plugin_t->get_package()));\n\t\t\tjson_object_set_new(plugin_data, \"data\", result->content);\n\t\t\tjson_object_set_new(reply, \"plugindata\", plugin_data);\n\t\t\t/* Send the success reply */\n\t\t\tret = janus_process_success(request, reply);\n\t\t} else if(result->type == JANUS_PLUGIN_OK_WAIT) {\n\t\t\t/* The plugin received the request but didn't process it yet, send an ack (asynchronous notifications may follow) */\n\t\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t\tif(result->text)\n\t\t\t\tjson_object_set_new(reply, \"hint\", json_string(result->text));\n\t\t\t/* Send the success reply */\n\t\t\tret = janus_process_success(request, reply);\n\t\t} else {\n\t\t\t/* Something went horribly wrong! */\n\t\t\tret = janus_process_error_string(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE,\n\t\t\t\t(char *)(result->text ? result->text : \"Plugin returned a severe (unknown) error\"));\n\t\t\tjanus_plugin_result_destroy(result);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_plugin_result_destroy(result);\n\t} else if(!strcasecmp(message_text, \"trickle\")) {\n\t\tif(handle == NULL) {\n\t\t\t/* Trickle is an handle-level command */\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_REQUEST_PATH, \"Unhandled request '%s' at this path\", message_text);\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(handle->app == NULL || !janus_plugin_session_is_alive(handle->app_handle)) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_PLUGIN_MESSAGE, \"No plugin to handle this trickle candidate\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjson_t *candidate = json_object_get(root, \"candidate\");\n\t\tjson_t *candidates = json_object_get(root, \"candidates\");\n\t\tif(candidate == NULL && candidates == NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_MISSING_MANDATORY_ELEMENT, \"Missing mandatory element (candidate|candidates)\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(candidate != NULL && candidates != NULL) {\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_JSON, \"Can't have both candidate and candidates\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_CLEANING)) {\n\t\t\tJANUS_LOG(LOG_ERR, \"[%\"SCNu64\"] Received a trickle, but still cleaning a previous session\\n\", handle->handle_id);\n\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_WEBRTC_STATE, \"Still cleaning a previous session\");\n\t\t\tgoto jsondone;\n\t\t}\n\t\tjanus_mutex_lock(&handle->mutex);\n\t\tif(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE)) {\n\t\t\t/* It looks like this peer supports Trickle, after all */\n\t\t\tJANUS_LOG(LOG_VERB, \"Handle %\"SCNu64\" supports trickle even if it didn't negotiate it...\\n\", handle->handle_id);\n\t\t\tjanus_flags_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_TRICKLE);\n\t\t}\n\t\t/* Is there any stream ready? this trickle may get here before the SDP it relates to */\n\t\tif(handle->stream == NULL) {\n\t\t\tJANUS_LOG(LOG_WARN, \"[%\"SCNu64\"] No stream, queueing this trickle as it got here before the SDP...\\n\", handle->handle_id);\n\t\t\t/* Enqueue this trickle candidate(s), we'll process this later */\n\t\t\tjanus_ice_trickle *early_trickle = janus_ice_trickle_new(transaction_text, candidate ? candidate : candidates);\n\t\t\thandle->pending_trickles = g_list_append(handle->pending_trickles, early_trickle);\n\t\t\t/* Send the ack right away, an event will tell the application if the candidate(s) failed */\n\t\t\tgoto trickledone;\n\t\t}\n\t\t/* Is the ICE stack ready already? */\n\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER) ||\n\t\t\t\t!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER) ||\n\t\t\t\t!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER)) {\n\t\t\tconst char *cause = NULL;\n\t\t\tif(janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_PROCESSING_OFFER))\n\t\t\t\tcause = \"processing the offer\";\n\t\t\telse if(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_ANSWER))\n\t\t\t\tcause = \"waiting for the answer\";\n\t\t\telse if(!janus_flags_is_set(&handle->webrtc_flags, JANUS_ICE_HANDLE_WEBRTC_GOT_OFFER))\n\t\t\t\tcause = \"waiting for the offer\";\n\t\t\tJANUS_LOG(LOG_VERB, \"[%\"SCNu64\"] Still %s, queueing this trickle to wait until we're done there...\\n\",\n\t\t\t\thandle->handle_id, cause);\n\t\t\t/* Enqueue this trickle candidate(s), we'll process this later */\n\t\t\tjanus_ice_trickle *early_trickle = janus_ice_trickle_new(transaction_text, candidate ? candidate : candidates);\n\t\t\thandle->pending_trickles = g_list_append(handle->pending_trickles, early_trickle);\n\t\t\t/* Send the ack right away, an event will tell the application if the candidate(s) failed */\n\t\t\tgoto trickledone;\n\t\t}\n\t\tif(candidate != NULL) {\n\t\t\t/* We got a single candidate */\n\t\t\tint error = 0;\n\t\t\tconst char *error_string = NULL;\n\t\t\tif((error = janus_ice_trickle_parse(handle, candidate, &error_string)) != 0) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, error, \"%s\", error_string);\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t} else {\n\t\t\t/* We got multiple candidates in an array */\n\t\t\tif(!json_is_array(candidates)) {\n\t\t\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_INVALID_ELEMENT_TYPE, \"candidates is not an array\");\n\t\t\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t\t\tgoto jsondone;\n\t\t\t}\n\t\t\tJANUS_LOG(LOG_VERB, \"Got multiple candidates (%zu)\\n\", json_array_size(candidates));\n\t\t\tif(json_array_size(candidates) > 0) {\n\t\t\t\t/* Handle remote candidates */\n\t\t\t\tsize_t i = 0;\n\t\t\t\tfor(i=0; i<json_array_size(candidates); i++) {\n\t\t\t\t\tjson_t *c = json_array_get(candidates, i);\n\t\t\t\t\t/* FIXME We don't care if any trickle fails to parse */\n\t\t\t\t\tjanus_ice_trickle_parse(handle, c, NULL);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\ntrickledone:\n\t\tjanus_mutex_unlock(&handle->mutex);\n\t\t/* We reply right away, not to block the web server... */\n\t\tjson_t *reply = janus_create_message(\"ack\", session_id, transaction_text);\n\t\t/* Send the success reply */\n\t\tret = janus_process_success(request, reply);\n\t} else {\n\t\tret = janus_process_error(request, session_id, transaction_text, JANUS_ERROR_UNKNOWN_REQUEST, \"Unknown request '%s'\", message_text);\n\t}\n\njsondone:\n\t/* Done processing */\n\tif(handle != NULL)\n\t\tjanus_refcount_decrease(&handle->ref);\n\tif(session != NULL)\n\t\tjanus_refcount_decrease(&session->ref);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -271,6 +271,18 @@\n \t} else if(!strcasecmp(message_text, \"claim\")) {\n \t\tjanus_mutex_lock(&session->mutex);\n \t\tif(session->source != NULL) {\n+\t\t\t/* If we're claiming from the same transport, ignore */\n+\t\t\tif(session->source->instance == request->instance) {\n+\t\t\t\tjanus_mutex_unlock(&session->mutex);\n+\t\t\t\t/* Prepare JSON reply */\n+\t\t\t\tjson_t *reply = json_object();\n+\t\t\t\tjson_object_set_new(reply, \"janus\", json_string(\"success\"));\n+\t\t\t\tjson_object_set_new(reply, \"session_id\", json_integer(session_id));\n+\t\t\t\tjson_object_set_new(reply, \"transaction\", json_string(transaction_text));\n+\t\t\t\t/* Send the success reply */\n+\t\t\t\tret = janus_process_success(request, reply);\n+\t\t\t\tgoto jsondone;\n+\t\t\t}\n \t\t\t/* Notify the old transport that this session is over for them, but has been reclaimed */\n \t\t\tsession->source->transport->session_over(session->source->instance, session->session_id, FALSE, TRUE);\n \t\t\tjanus_request_destroy(session->source);\n@@ -279,7 +291,7 @@\n \t\tsession->source = janus_request_new(request->transport, request->instance, NULL, FALSE, NULL);\n \t\t/* Notify the new transport that it has claimed a session */\n \t\tsession->source->transport->session_claimed(session->source->instance, session->session_id);\n-\t\t/* Previous transport may be gone, clear flag. */\n+\t\t/* Previous transport may be gone, clear flag */\n \t\tg_atomic_int_set(&session->transport_gone, 0);\n \t\tjanus_mutex_unlock(&session->mutex);\n \t\t/* Prepare JSON reply */",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t/* Previous transport may be gone, clear flag. */"
            ],
            "added_lines": [
                "\t\t\t/* If we're claiming from the same transport, ignore */",
                "\t\t\tif(session->source->instance == request->instance) {",
                "\t\t\t\tjanus_mutex_unlock(&session->mutex);",
                "\t\t\t\t/* Prepare JSON reply */",
                "\t\t\t\tjson_t *reply = json_object();",
                "\t\t\t\tjson_object_set_new(reply, \"janus\", json_string(\"success\"));",
                "\t\t\t\tjson_object_set_new(reply, \"session_id\", json_integer(session_id));",
                "\t\t\t\tjson_object_set_new(reply, \"transaction\", json_string(transaction_text));",
                "\t\t\t\t/* Send the success reply */",
                "\t\t\t\tret = janus_process_success(request, reply);",
                "\t\t\t\tgoto jsondone;",
                "\t\t\t}",
                "\t\t/* Previous transport may be gone, clear flag */"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19580",
        "func_name": "xen-project/xen/free_l3_table",
        "description": "An issue was discovered in Xen through 4.12.x allowing x86 PV guest OS users to gain host OS privileges by leveraging race conditions in pagetable promotion and demotion operations, because of an incomplete fix for CVE-2019-18421. XSA-299 addressed several critical issues in restartable PV type change operations. Despite extensive testing and auditing, some corner cases were missed. A malicious PV guest administrator may be able to escalate their privilege to that of the host. All security-supported versions of Xen are vulnerable. Only x86 systems are affected. Arm systems are not affected. Only x86 PV guests can leverage the vulnerability. x86 HVM and PVH guests cannot leverage the vulnerability. Note that these attacks require very precise timing, which may be difficult to exploit in practice.",
        "git_url": "https://github.com/xen-project/xen/commit/4e70f4476c0c543559f971faecdd5f1300cddb0a",
        "commit_title": "x86/mm: alloc/free_lN_table: Retain partial_flags on -EINTR",
        "commit_text": " When validating or de-validating pages (in alloc_lN_table and free_lN_table respectively), the `partial_flags` local variable is used to keep track of whether the \"current\" PTE started the entire operation in a \"may be partial\" state.  One of the patches in XSA-299 addressed the fact that it is possible for a previously-partially-validated entry to subsequently be found to have invalid entries (indicated by returning -EINVAL); in which case page->partial_flags needs to be set to indicate that the current PTE may have the partial bit set (and thus _put_page_type() should be called with PTF_partial_set).  Unfortunately, the patches in XSA-299 assumed that once put_page_from_lNe() returned -ERESTART on a page, it was not possible for it to return -EINTR.  This turns out to be true for alloc_lN_table() and free_lN_table, but not for _get_page_type() and _put_page_type(): both can return -EINTR when called on pages with PGT_partial set.  In these cases, the pages PGT_partial will still be set; failing to set partial_flags appropriately may allow an attacker to do a privilege escalation similar to those described in XSA-299.  Fix this by always copying the local partial_flags variable into page->partial_flags when exiting early.  NB that on the \"get\" side, no adjustment to nr_validated_entries is needed: whether pte[i] is partially validated or entirely un-validated, we want nr_validated_entries = i.  On the \"put\" side, however, we need to adjust nr_validated_entries appropriately: if pte[i] is entirely validated, we want nr_validated_entries = i + 1; if pte[i] is partially validated, we want nr_validated_entries = i.  This is part of XSA-310. ",
        "func_before": "static int free_l3_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long pfn = mfn_x(page_to_mfn(page));\n    l3_pgentry_t *pl3e;\n    int rc = 0;\n    unsigned int partial_flags = page->partial_flags,\n        i = page->nr_validated_ptes - !(partial_flags & PTF_partial_set);\n\n    pl3e = map_domain_page(_mfn(pfn));\n\n    for ( ; ; )\n    {\n        rc = put_page_from_l3e(pl3e[i], pfn, partial_flags);\n        if ( rc < 0 )\n            break;\n\n        partial_flags = 0;\n        if ( rc == 0 )\n            pl3e[i] = unadjust_guest_l3e(pl3e[i], d);\n\n        if ( !i-- )\n            break;\n\n        if ( hypercall_preempt_check() )\n        {\n            rc = -EINTR;\n            break;\n        }\n    }\n\n    unmap_domain_page(pl3e);\n\n    if ( rc == -ERESTART )\n    {\n        page->nr_validated_ptes = i;\n        page->partial_flags = PTF_partial_set;\n    }\n    else if ( rc == -EINTR && i < L3_PAGETABLE_ENTRIES - 1 )\n    {\n        page->nr_validated_ptes = i + 1;\n        page->partial_flags = 0;\n        rc = -ERESTART;\n    }\n    return rc > 0 ? 0 : rc;\n}",
        "func": "static int free_l3_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long pfn = mfn_x(page_to_mfn(page));\n    l3_pgentry_t *pl3e;\n    int rc = 0;\n    unsigned int partial_flags = page->partial_flags,\n        i = page->nr_validated_ptes - !(partial_flags & PTF_partial_set);\n\n    pl3e = map_domain_page(_mfn(pfn));\n\n    for ( ; ; )\n    {\n        rc = put_page_from_l3e(pl3e[i], pfn, partial_flags);\n        if ( rc < 0 )\n            break;\n\n        partial_flags = 0;\n        if ( rc == 0 )\n            pl3e[i] = unadjust_guest_l3e(pl3e[i], d);\n\n        if ( !i-- )\n            break;\n\n        if ( hypercall_preempt_check() )\n        {\n            rc = -EINTR;\n            break;\n        }\n    }\n\n    unmap_domain_page(pl3e);\n\n    if ( rc == -ERESTART )\n    {\n        page->nr_validated_ptes = i;\n        page->partial_flags = PTF_partial_set;\n    }\n    else if ( rc == -EINTR && i < L3_PAGETABLE_ENTRIES - 1 )\n    {\n        page->nr_validated_ptes = i + !(partial_flags & PTF_partial_set);\n        page->partial_flags = partial_flags;\n        rc = -ERESTART;\n    }\n    return rc > 0 ? 0 : rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,8 +38,8 @@\n     }\n     else if ( rc == -EINTR && i < L3_PAGETABLE_ENTRIES - 1 )\n     {\n-        page->nr_validated_ptes = i + 1;\n-        page->partial_flags = 0;\n+        page->nr_validated_ptes = i + !(partial_flags & PTF_partial_set);\n+        page->partial_flags = partial_flags;\n         rc = -ERESTART;\n     }\n     return rc > 0 ? 0 : rc;",
        "diff_line_info": {
            "deleted_lines": [
                "        page->nr_validated_ptes = i + 1;",
                "        page->partial_flags = 0;"
            ],
            "added_lines": [
                "        page->nr_validated_ptes = i + !(partial_flags & PTF_partial_set);",
                "        page->partial_flags = partial_flags;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19580",
        "func_name": "xen-project/xen/alloc_l3_table",
        "description": "An issue was discovered in Xen through 4.12.x allowing x86 PV guest OS users to gain host OS privileges by leveraging race conditions in pagetable promotion and demotion operations, because of an incomplete fix for CVE-2019-18421. XSA-299 addressed several critical issues in restartable PV type change operations. Despite extensive testing and auditing, some corner cases were missed. A malicious PV guest administrator may be able to escalate their privilege to that of the host. All security-supported versions of Xen are vulnerable. Only x86 systems are affected. Arm systems are not affected. Only x86 PV guests can leverage the vulnerability. x86 HVM and PVH guests cannot leverage the vulnerability. Note that these attacks require very precise timing, which may be difficult to exploit in practice.",
        "git_url": "https://github.com/xen-project/xen/commit/4e70f4476c0c543559f971faecdd5f1300cddb0a",
        "commit_title": "x86/mm: alloc/free_lN_table: Retain partial_flags on -EINTR",
        "commit_text": " When validating or de-validating pages (in alloc_lN_table and free_lN_table respectively), the `partial_flags` local variable is used to keep track of whether the \"current\" PTE started the entire operation in a \"may be partial\" state.  One of the patches in XSA-299 addressed the fact that it is possible for a previously-partially-validated entry to subsequently be found to have invalid entries (indicated by returning -EINVAL); in which case page->partial_flags needs to be set to indicate that the current PTE may have the partial bit set (and thus _put_page_type() should be called with PTF_partial_set).  Unfortunately, the patches in XSA-299 assumed that once put_page_from_lNe() returned -ERESTART on a page, it was not possible for it to return -EINTR.  This turns out to be true for alloc_lN_table() and free_lN_table, but not for _get_page_type() and _put_page_type(): both can return -EINTR when called on pages with PGT_partial set.  In these cases, the pages PGT_partial will still be set; failing to set partial_flags appropriately may allow an attacker to do a privilege escalation similar to those described in XSA-299.  Fix this by always copying the local partial_flags variable into page->partial_flags when exiting early.  NB that on the \"get\" side, no adjustment to nr_validated_entries is needed: whether pte[i] is partially validated or entirely un-validated, we want nr_validated_entries = i.  On the \"put\" side, however, we need to adjust nr_validated_entries appropriately: if pte[i] is entirely validated, we want nr_validated_entries = i + 1; if pte[i] is partially validated, we want nr_validated_entries = i.  This is part of XSA-310. ",
        "func_before": "static int alloc_l3_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long  pfn = mfn_x(page_to_mfn(page));\n    l3_pgentry_t  *pl3e;\n    unsigned int   i;\n    int            rc = 0;\n    unsigned int   partial_flags = page->partial_flags;\n    l3_pgentry_t   l3e = l3e_empty();\n\n    pl3e = map_domain_page(_mfn(pfn));\n\n    /*\n     * PAE guests allocate full pages, but aren't required to initialize\n     * more than the first four entries; when running in compatibility\n     * mode, however, the full page is visible to the MMU, and hence all\n     * 512 entries must be valid/verified, which is most easily achieved\n     * by clearing them out.\n     */\n    if ( is_pv_32bit_domain(d) )\n        memset(pl3e + 4, 0, (L3_PAGETABLE_ENTRIES - 4) * sizeof(*pl3e));\n\n    for ( i = page->nr_validated_ptes; i < L3_PAGETABLE_ENTRIES;\n          i++, partial_flags = 0 )\n    {\n        l3e = pl3e[i];\n\n        if ( i > page->nr_validated_ptes && hypercall_preempt_check() )\n            rc = -EINTR;\n        else if ( is_pv_32bit_domain(d) && (i == 3) )\n        {\n            if ( !(l3e_get_flags(l3e) & _PAGE_PRESENT) ||\n                 (l3e_get_flags(l3e) & l3_disallow_mask(d)) )\n                rc = -EINVAL;\n            else\n                rc = get_page_and_type_from_mfn(\n                    l3e_get_mfn(l3e),\n                    PGT_l2_page_table | PGT_pae_xen_l2, d,\n                    partial_flags | PTF_preemptible | PTF_retain_ref_on_restart);\n        }\n        else if ( !(l3e_get_flags(l3e) & _PAGE_PRESENT) )\n        {\n            if ( !pv_l1tf_check_l3e(d, l3e) )\n                continue;\n            rc = -EINTR;\n        }\n        else\n            rc = get_page_from_l3e(l3e, pfn, d,\n                                   partial_flags | PTF_retain_ref_on_restart);\n\n        if ( rc == -ERESTART )\n        {\n            page->nr_validated_ptes = i;\n            /* Set 'set', leave 'general ref' set if this entry was set */\n            page->partial_flags = PTF_partial_set;\n        }\n        else if ( rc == -EINTR && i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_flags = 0;\n            rc = -ERESTART;\n        }\n        if ( rc < 0 )\n            break;\n\n        pl3e[i] = adjust_guest_l3e(l3e, d);\n    }\n\n    if ( !rc && !create_pae_xen_mappings(d, pl3e) )\n        rc = -EINVAL;\n    if ( rc < 0 && rc != -ERESTART && rc != -EINTR )\n    {\n        gdprintk(XENLOG_WARNING,\n                 \"Failure %d in alloc_l3_table: slot %#x\\n\", rc, i);\n        if ( i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_flags = partial_flags;\n            if ( current->arch.old_guest_table )\n            {\n                /*\n                 * We've experienced a validation failure.  If\n                 * old_guest_table is set, \"transfer\" the general\n                 * reference count to pl3e[nr_validated_ptes] by\n                 * setting PTF_partial_set.\n                 *\n                 * As a precaution, check that old_guest_table is the\n                 * page pointed to by pl3e[nr_validated_ptes].  If\n                 * not, it's safer to leak a type ref on production\n                 * builds.\n                 */\n                if ( current->arch.old_guest_table == l3e_get_page(l3e) )\n                {\n                    ASSERT(current->arch.old_guest_table_partial);\n                    page->partial_flags = PTF_partial_set;\n                }\n                else\n                    ASSERT_UNREACHABLE();\n            }\n            current->arch.old_guest_ptpg = NULL;\n            current->arch.old_guest_table = page;\n            current->arch.old_guest_table_partial = true;\n        }\n        while ( i-- > 0 )\n            pl3e[i] = unadjust_guest_l3e(pl3e[i], d);\n    }\n\n    unmap_domain_page(pl3e);\n    return rc;\n}",
        "func": "static int alloc_l3_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long  pfn = mfn_x(page_to_mfn(page));\n    l3_pgentry_t  *pl3e;\n    unsigned int   i;\n    int            rc = 0;\n    unsigned int   partial_flags = page->partial_flags;\n    l3_pgentry_t   l3e = l3e_empty();\n\n    pl3e = map_domain_page(_mfn(pfn));\n\n    /*\n     * PAE guests allocate full pages, but aren't required to initialize\n     * more than the first four entries; when running in compatibility\n     * mode, however, the full page is visible to the MMU, and hence all\n     * 512 entries must be valid/verified, which is most easily achieved\n     * by clearing them out.\n     */\n    if ( is_pv_32bit_domain(d) )\n        memset(pl3e + 4, 0, (L3_PAGETABLE_ENTRIES - 4) * sizeof(*pl3e));\n\n    for ( i = page->nr_validated_ptes; i < L3_PAGETABLE_ENTRIES;\n          i++, partial_flags = 0 )\n    {\n        l3e = pl3e[i];\n\n        if ( i > page->nr_validated_ptes && hypercall_preempt_check() )\n            rc = -EINTR;\n        else if ( is_pv_32bit_domain(d) && (i == 3) )\n        {\n            if ( !(l3e_get_flags(l3e) & _PAGE_PRESENT) ||\n                 (l3e_get_flags(l3e) & l3_disallow_mask(d)) )\n                rc = -EINVAL;\n            else\n                rc = get_page_and_type_from_mfn(\n                    l3e_get_mfn(l3e),\n                    PGT_l2_page_table | PGT_pae_xen_l2, d,\n                    partial_flags | PTF_preemptible | PTF_retain_ref_on_restart);\n        }\n        else if ( !(l3e_get_flags(l3e) & _PAGE_PRESENT) )\n        {\n            if ( !pv_l1tf_check_l3e(d, l3e) )\n                continue;\n            rc = -EINTR;\n        }\n        else\n            rc = get_page_from_l3e(l3e, pfn, d,\n                                   partial_flags | PTF_retain_ref_on_restart);\n\n        if ( rc == -ERESTART )\n        {\n            page->nr_validated_ptes = i;\n            /* Set 'set', leave 'general ref' set if this entry was set */\n            page->partial_flags = PTF_partial_set;\n        }\n        else if ( rc == -EINTR && i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_flags = partial_flags;\n            rc = -ERESTART;\n        }\n        if ( rc < 0 )\n            break;\n\n        pl3e[i] = adjust_guest_l3e(l3e, d);\n    }\n\n    if ( !rc && !create_pae_xen_mappings(d, pl3e) )\n        rc = -EINVAL;\n    if ( rc < 0 && rc != -ERESTART && rc != -EINTR )\n    {\n        gdprintk(XENLOG_WARNING,\n                 \"Failure %d in alloc_l3_table: slot %#x\\n\", rc, i);\n        if ( i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_flags = partial_flags;\n            if ( current->arch.old_guest_table )\n            {\n                /*\n                 * We've experienced a validation failure.  If\n                 * old_guest_table is set, \"transfer\" the general\n                 * reference count to pl3e[nr_validated_ptes] by\n                 * setting PTF_partial_set.\n                 *\n                 * As a precaution, check that old_guest_table is the\n                 * page pointed to by pl3e[nr_validated_ptes].  If\n                 * not, it's safer to leak a type ref on production\n                 * builds.\n                 */\n                if ( current->arch.old_guest_table == l3e_get_page(l3e) )\n                {\n                    ASSERT(current->arch.old_guest_table_partial);\n                    page->partial_flags = PTF_partial_set;\n                }\n                else\n                    ASSERT_UNREACHABLE();\n            }\n            current->arch.old_guest_ptpg = NULL;\n            current->arch.old_guest_table = page;\n            current->arch.old_guest_table_partial = true;\n        }\n        while ( i-- > 0 )\n            pl3e[i] = unadjust_guest_l3e(pl3e[i], d);\n    }\n\n    unmap_domain_page(pl3e);\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -57,7 +57,7 @@\n         else if ( rc == -EINTR && i )\n         {\n             page->nr_validated_ptes = i;\n-            page->partial_flags = 0;\n+            page->partial_flags = partial_flags;\n             rc = -ERESTART;\n         }\n         if ( rc < 0 )",
        "diff_line_info": {
            "deleted_lines": [
                "            page->partial_flags = 0;"
            ],
            "added_lines": [
                "            page->partial_flags = partial_flags;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19580",
        "func_name": "xen-project/xen/free_l2_table",
        "description": "An issue was discovered in Xen through 4.12.x allowing x86 PV guest OS users to gain host OS privileges by leveraging race conditions in pagetable promotion and demotion operations, because of an incomplete fix for CVE-2019-18421. XSA-299 addressed several critical issues in restartable PV type change operations. Despite extensive testing and auditing, some corner cases were missed. A malicious PV guest administrator may be able to escalate their privilege to that of the host. All security-supported versions of Xen are vulnerable. Only x86 systems are affected. Arm systems are not affected. Only x86 PV guests can leverage the vulnerability. x86 HVM and PVH guests cannot leverage the vulnerability. Note that these attacks require very precise timing, which may be difficult to exploit in practice.",
        "git_url": "https://github.com/xen-project/xen/commit/4e70f4476c0c543559f971faecdd5f1300cddb0a",
        "commit_title": "x86/mm: alloc/free_lN_table: Retain partial_flags on -EINTR",
        "commit_text": " When validating or de-validating pages (in alloc_lN_table and free_lN_table respectively), the `partial_flags` local variable is used to keep track of whether the \"current\" PTE started the entire operation in a \"may be partial\" state.  One of the patches in XSA-299 addressed the fact that it is possible for a previously-partially-validated entry to subsequently be found to have invalid entries (indicated by returning -EINVAL); in which case page->partial_flags needs to be set to indicate that the current PTE may have the partial bit set (and thus _put_page_type() should be called with PTF_partial_set).  Unfortunately, the patches in XSA-299 assumed that once put_page_from_lNe() returned -ERESTART on a page, it was not possible for it to return -EINTR.  This turns out to be true for alloc_lN_table() and free_lN_table, but not for _get_page_type() and _put_page_type(): both can return -EINTR when called on pages with PGT_partial set.  In these cases, the pages PGT_partial will still be set; failing to set partial_flags appropriately may allow an attacker to do a privilege escalation similar to those described in XSA-299.  Fix this by always copying the local partial_flags variable into page->partial_flags when exiting early.  NB that on the \"get\" side, no adjustment to nr_validated_entries is needed: whether pte[i] is partially validated or entirely un-validated, we want nr_validated_entries = i.  On the \"put\" side, however, we need to adjust nr_validated_entries appropriately: if pte[i] is entirely validated, we want nr_validated_entries = i + 1; if pte[i] is partially validated, we want nr_validated_entries = i.  This is part of XSA-310. ",
        "func_before": "static int free_l2_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long pfn = mfn_x(page_to_mfn(page));\n    l2_pgentry_t *pl2e;\n    int rc = 0;\n    unsigned int partial_flags = page->partial_flags,\n        i = page->nr_validated_ptes - !(partial_flags & PTF_partial_set);\n\n    pl2e = map_domain_page(_mfn(pfn));\n\n    for ( ; ; )\n    {\n        if ( is_guest_l2_slot(d, page->u.inuse.type_info, i) )\n            rc = put_page_from_l2e(pl2e[i], pfn, partial_flags);\n        if ( rc < 0 )\n            break;\n\n        partial_flags = 0;\n\n        if ( !i-- )\n            break;\n\n        if ( hypercall_preempt_check() )\n        {\n            rc = -EINTR;\n            break;\n        }\n    }\n\n    unmap_domain_page(pl2e);\n\n    if ( rc >= 0 )\n    {\n        page->u.inuse.type_info &= ~PGT_pae_xen_l2;\n        rc = 0;\n    }\n    else if ( rc == -ERESTART )\n    {\n        page->nr_validated_ptes = i;\n        page->partial_flags = PTF_partial_set;\n    }\n    else if ( rc == -EINTR && i < L2_PAGETABLE_ENTRIES - 1 )\n    {\n        page->nr_validated_ptes = i + 1;\n        page->partial_flags = 0;\n        rc = -ERESTART;\n    }\n\n    return rc;\n}",
        "func": "static int free_l2_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long pfn = mfn_x(page_to_mfn(page));\n    l2_pgentry_t *pl2e;\n    int rc = 0;\n    unsigned int partial_flags = page->partial_flags,\n        i = page->nr_validated_ptes - !(partial_flags & PTF_partial_set);\n\n    pl2e = map_domain_page(_mfn(pfn));\n\n    for ( ; ; )\n    {\n        if ( is_guest_l2_slot(d, page->u.inuse.type_info, i) )\n            rc = put_page_from_l2e(pl2e[i], pfn, partial_flags);\n        if ( rc < 0 )\n            break;\n\n        partial_flags = 0;\n\n        if ( !i-- )\n            break;\n\n        if ( hypercall_preempt_check() )\n        {\n            rc = -EINTR;\n            break;\n        }\n    }\n\n    unmap_domain_page(pl2e);\n\n    if ( rc >= 0 )\n    {\n        page->u.inuse.type_info &= ~PGT_pae_xen_l2;\n        rc = 0;\n    }\n    else if ( rc == -ERESTART )\n    {\n        page->nr_validated_ptes = i;\n        page->partial_flags = PTF_partial_set;\n    }\n    else if ( rc == -EINTR && i < L2_PAGETABLE_ENTRIES - 1 )\n    {\n        page->nr_validated_ptes = i + !(partial_flags & PTF_partial_set);\n        page->partial_flags = partial_flags;\n        rc = -ERESTART;\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -42,8 +42,8 @@\n     }\n     else if ( rc == -EINTR && i < L2_PAGETABLE_ENTRIES - 1 )\n     {\n-        page->nr_validated_ptes = i + 1;\n-        page->partial_flags = 0;\n+        page->nr_validated_ptes = i + !(partial_flags & PTF_partial_set);\n+        page->partial_flags = partial_flags;\n         rc = -ERESTART;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "        page->nr_validated_ptes = i + 1;",
                "        page->partial_flags = 0;"
            ],
            "added_lines": [
                "        page->nr_validated_ptes = i + !(partial_flags & PTF_partial_set);",
                "        page->partial_flags = partial_flags;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19580",
        "func_name": "xen-project/xen/free_l4_table",
        "description": "An issue was discovered in Xen through 4.12.x allowing x86 PV guest OS users to gain host OS privileges by leveraging race conditions in pagetable promotion and demotion operations, because of an incomplete fix for CVE-2019-18421. XSA-299 addressed several critical issues in restartable PV type change operations. Despite extensive testing and auditing, some corner cases were missed. A malicious PV guest administrator may be able to escalate their privilege to that of the host. All security-supported versions of Xen are vulnerable. Only x86 systems are affected. Arm systems are not affected. Only x86 PV guests can leverage the vulnerability. x86 HVM and PVH guests cannot leverage the vulnerability. Note that these attacks require very precise timing, which may be difficult to exploit in practice.",
        "git_url": "https://github.com/xen-project/xen/commit/4e70f4476c0c543559f971faecdd5f1300cddb0a",
        "commit_title": "x86/mm: alloc/free_lN_table: Retain partial_flags on -EINTR",
        "commit_text": " When validating or de-validating pages (in alloc_lN_table and free_lN_table respectively), the `partial_flags` local variable is used to keep track of whether the \"current\" PTE started the entire operation in a \"may be partial\" state.  One of the patches in XSA-299 addressed the fact that it is possible for a previously-partially-validated entry to subsequently be found to have invalid entries (indicated by returning -EINVAL); in which case page->partial_flags needs to be set to indicate that the current PTE may have the partial bit set (and thus _put_page_type() should be called with PTF_partial_set).  Unfortunately, the patches in XSA-299 assumed that once put_page_from_lNe() returned -ERESTART on a page, it was not possible for it to return -EINTR.  This turns out to be true for alloc_lN_table() and free_lN_table, but not for _get_page_type() and _put_page_type(): both can return -EINTR when called on pages with PGT_partial set.  In these cases, the pages PGT_partial will still be set; failing to set partial_flags appropriately may allow an attacker to do a privilege escalation similar to those described in XSA-299.  Fix this by always copying the local partial_flags variable into page->partial_flags when exiting early.  NB that on the \"get\" side, no adjustment to nr_validated_entries is needed: whether pte[i] is partially validated or entirely un-validated, we want nr_validated_entries = i.  On the \"put\" side, however, we need to adjust nr_validated_entries appropriately: if pte[i] is entirely validated, we want nr_validated_entries = i + 1; if pte[i] is partially validated, we want nr_validated_entries = i.  This is part of XSA-310. ",
        "func_before": "static int free_l4_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long pfn = mfn_x(page_to_mfn(page));\n    l4_pgentry_t *pl4e = map_domain_page(_mfn(pfn));\n    int rc = 0;\n    unsigned partial_flags = page->partial_flags,\n        i = page->nr_validated_ptes - !(partial_flags & PTF_partial_set);\n\n    do {\n        if ( is_guest_l4_slot(d, i) )\n            rc = put_page_from_l4e(pl4e[i], pfn, partial_flags);\n        if ( rc < 0 )\n            break;\n        partial_flags = 0;\n    } while ( i-- );\n\n    if ( rc == -ERESTART )\n    {\n        page->nr_validated_ptes = i;\n        page->partial_flags = PTF_partial_set;\n    }\n    else if ( rc == -EINTR && i < L4_PAGETABLE_ENTRIES - 1 )\n    {\n        page->nr_validated_ptes = i + 1;\n        page->partial_flags = 0;\n        rc = -ERESTART;\n    }\n\n    unmap_domain_page(pl4e);\n\n    if ( rc >= 0 )\n    {\n        atomic_dec(&d->arch.pv.nr_l4_pages);\n        rc = 0;\n    }\n\n    return rc;\n}",
        "func": "static int free_l4_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long pfn = mfn_x(page_to_mfn(page));\n    l4_pgentry_t *pl4e = map_domain_page(_mfn(pfn));\n    int rc = 0;\n    unsigned partial_flags = page->partial_flags,\n        i = page->nr_validated_ptes - !(partial_flags & PTF_partial_set);\n\n    do {\n        if ( is_guest_l4_slot(d, i) )\n            rc = put_page_from_l4e(pl4e[i], pfn, partial_flags);\n        if ( rc < 0 )\n            break;\n        partial_flags = 0;\n    } while ( i-- );\n\n    if ( rc == -ERESTART )\n    {\n        page->nr_validated_ptes = i;\n        page->partial_flags = PTF_partial_set;\n    }\n    else if ( rc == -EINTR && i < L4_PAGETABLE_ENTRIES - 1 )\n    {\n        page->nr_validated_ptes = i + !(partial_flags & PTF_partial_set);\n        page->partial_flags = partial_flags;\n        rc = -ERESTART;\n    }\n\n    unmap_domain_page(pl4e);\n\n    if ( rc >= 0 )\n    {\n        atomic_dec(&d->arch.pv.nr_l4_pages);\n        rc = 0;\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,8 +22,8 @@\n     }\n     else if ( rc == -EINTR && i < L4_PAGETABLE_ENTRIES - 1 )\n     {\n-        page->nr_validated_ptes = i + 1;\n-        page->partial_flags = 0;\n+        page->nr_validated_ptes = i + !(partial_flags & PTF_partial_set);\n+        page->partial_flags = partial_flags;\n         rc = -ERESTART;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "        page->nr_validated_ptes = i + 1;",
                "        page->partial_flags = 0;"
            ],
            "added_lines": [
                "        page->nr_validated_ptes = i + !(partial_flags & PTF_partial_set);",
                "        page->partial_flags = partial_flags;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19580",
        "func_name": "xen-project/xen/alloc_l2_table",
        "description": "An issue was discovered in Xen through 4.12.x allowing x86 PV guest OS users to gain host OS privileges by leveraging race conditions in pagetable promotion and demotion operations, because of an incomplete fix for CVE-2019-18421. XSA-299 addressed several critical issues in restartable PV type change operations. Despite extensive testing and auditing, some corner cases were missed. A malicious PV guest administrator may be able to escalate their privilege to that of the host. All security-supported versions of Xen are vulnerable. Only x86 systems are affected. Arm systems are not affected. Only x86 PV guests can leverage the vulnerability. x86 HVM and PVH guests cannot leverage the vulnerability. Note that these attacks require very precise timing, which may be difficult to exploit in practice.",
        "git_url": "https://github.com/xen-project/xen/commit/4e70f4476c0c543559f971faecdd5f1300cddb0a",
        "commit_title": "x86/mm: alloc/free_lN_table: Retain partial_flags on -EINTR",
        "commit_text": " When validating or de-validating pages (in alloc_lN_table and free_lN_table respectively), the `partial_flags` local variable is used to keep track of whether the \"current\" PTE started the entire operation in a \"may be partial\" state.  One of the patches in XSA-299 addressed the fact that it is possible for a previously-partially-validated entry to subsequently be found to have invalid entries (indicated by returning -EINVAL); in which case page->partial_flags needs to be set to indicate that the current PTE may have the partial bit set (and thus _put_page_type() should be called with PTF_partial_set).  Unfortunately, the patches in XSA-299 assumed that once put_page_from_lNe() returned -ERESTART on a page, it was not possible for it to return -EINTR.  This turns out to be true for alloc_lN_table() and free_lN_table, but not for _get_page_type() and _put_page_type(): both can return -EINTR when called on pages with PGT_partial set.  In these cases, the pages PGT_partial will still be set; failing to set partial_flags appropriately may allow an attacker to do a privilege escalation similar to those described in XSA-299.  Fix this by always copying the local partial_flags variable into page->partial_flags when exiting early.  NB that on the \"get\" side, no adjustment to nr_validated_entries is needed: whether pte[i] is partially validated or entirely un-validated, we want nr_validated_entries = i.  On the \"put\" side, however, we need to adjust nr_validated_entries appropriately: if pte[i] is entirely validated, we want nr_validated_entries = i + 1; if pte[i] is partially validated, we want nr_validated_entries = i.  This is part of XSA-310. ",
        "func_before": "static int alloc_l2_table(struct page_info *page, unsigned long type)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long  pfn = mfn_x(page_to_mfn(page));\n    l2_pgentry_t  *pl2e;\n    unsigned int   i;\n    int            rc = 0;\n    unsigned int   partial_flags = page->partial_flags;\n\n    pl2e = map_domain_page(_mfn(pfn));\n\n    /*\n     * NB that alloc_l2_table will never set partial_pte on an l2; but\n     * free_l2_table might if a linear_pagetable entry is interrupted\n     * partway through de-validation.  In that circumstance,\n     * get_page_from_l2e() will always return -EINVAL; and we must\n     * retain the type ref by doing the normal partial_flags tracking.\n     */\n\n    for ( i = page->nr_validated_ptes; i < L2_PAGETABLE_ENTRIES;\n          i++, partial_flags = 0 )\n    {\n        l2_pgentry_t l2e = pl2e[i];\n\n        if ( i > page->nr_validated_ptes && hypercall_preempt_check() )\n            rc = -EINTR;\n        else if ( !is_guest_l2_slot(d, type, i) )\n            continue;\n        else if ( !(l2e_get_flags(l2e) & _PAGE_PRESENT) )\n        {\n            if ( !pv_l1tf_check_l2e(d, l2e) )\n                continue;\n            rc = -EINTR;\n        }\n        else\n            rc = get_page_from_l2e(l2e, pfn, d, partial_flags);\n\n        /*\n         * It shouldn't be possible for get_page_from_l2e to return\n         * -ERESTART, since we never call this with PTF_preemptible.\n         * (alloc_l1_table may return -EINTR on an L1TF-vulnerable\n         * entry.)\n         *\n         * NB that while on a \"clean\" promotion, we can never get\n         * PGT_partial.  It is possible to arrange for an l2e to\n         * contain a partially-devalidated l2; but in that case, both\n         * of the following functions will fail anyway (the first\n         * because the page in question is not an l1; the second\n         * because the page is not fully validated).\n         */\n        ASSERT(rc != -ERESTART);\n\n        if ( rc == -EINTR && i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_flags = 0;\n            rc = -ERESTART;\n        }\n        else if ( rc < 0 && rc != -EINTR )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Failure %d in alloc_l2_table: slot %#x\\n\", rc, i);\n            ASSERT(current->arch.old_guest_table == NULL);\n            if ( i )\n            {\n                /*\n                 * alloc_l1_table() doesn't set old_guest_table; it does\n                 * its own tear-down immediately on failure.  If it\n                 * did we'd need to check it and set partial_flags as we\n                 * do in alloc_l[34]_table().\n                 *\n                 * Note on the use of ASSERT: if it's non-null and\n                 * hasn't been cleaned up yet, it should have\n                 * PGT_partial set; and so the type will be cleaned up\n                 * on domain destruction.  Unfortunately, we would\n                 * leak the general ref held by old_guest_table; but\n                 * leaking a page is less bad than a host crash.\n                 */\n                ASSERT(current->arch.old_guest_table == NULL);\n                page->nr_validated_ptes = i;\n                page->partial_flags = partial_flags;\n                current->arch.old_guest_ptpg = NULL;\n                current->arch.old_guest_table = page;\n                current->arch.old_guest_table_partial = true;\n            }\n        }\n        if ( rc < 0 )\n            break;\n\n        pl2e[i] = adjust_guest_l2e(l2e, d);\n    }\n\n    if ( !rc && (type & PGT_pae_xen_l2) )\n        init_xen_pae_l2_slots(pl2e, d);\n\n    unmap_domain_page(pl2e);\n    return rc;\n}",
        "func": "static int alloc_l2_table(struct page_info *page, unsigned long type)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long  pfn = mfn_x(page_to_mfn(page));\n    l2_pgentry_t  *pl2e;\n    unsigned int   i;\n    int            rc = 0;\n    unsigned int   partial_flags = page->partial_flags;\n\n    pl2e = map_domain_page(_mfn(pfn));\n\n    /*\n     * NB that alloc_l2_table will never set partial_pte on an l2; but\n     * free_l2_table might if a linear_pagetable entry is interrupted\n     * partway through de-validation.  In that circumstance,\n     * get_page_from_l2e() will always return -EINVAL; and we must\n     * retain the type ref by doing the normal partial_flags tracking.\n     */\n\n    for ( i = page->nr_validated_ptes; i < L2_PAGETABLE_ENTRIES;\n          i++, partial_flags = 0 )\n    {\n        l2_pgentry_t l2e = pl2e[i];\n\n        if ( i > page->nr_validated_ptes && hypercall_preempt_check() )\n            rc = -EINTR;\n        else if ( !is_guest_l2_slot(d, type, i) )\n            continue;\n        else if ( !(l2e_get_flags(l2e) & _PAGE_PRESENT) )\n        {\n            if ( !pv_l1tf_check_l2e(d, l2e) )\n                continue;\n            rc = -EINTR;\n        }\n        else\n            rc = get_page_from_l2e(l2e, pfn, d, partial_flags);\n\n        /*\n         * It shouldn't be possible for get_page_from_l2e to return\n         * -ERESTART, since we never call this with PTF_preemptible.\n         * (alloc_l1_table may return -EINTR on an L1TF-vulnerable\n         * entry.)\n         *\n         * NB that while on a \"clean\" promotion, we can never get\n         * PGT_partial.  It is possible to arrange for an l2e to\n         * contain a partially-devalidated l2; but in that case, both\n         * of the following functions will fail anyway (the first\n         * because the page in question is not an l1; the second\n         * because the page is not fully validated).\n         */\n        ASSERT(rc != -ERESTART);\n\n        if ( rc == -EINTR && i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_flags = partial_flags;;\n            rc = -ERESTART;\n        }\n        else if ( rc < 0 && rc != -EINTR )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Failure %d in alloc_l2_table: slot %#x\\n\", rc, i);\n            ASSERT(current->arch.old_guest_table == NULL);\n            if ( i )\n            {\n                /*\n                 * alloc_l1_table() doesn't set old_guest_table; it does\n                 * its own tear-down immediately on failure.  If it\n                 * did we'd need to check it and set partial_flags as we\n                 * do in alloc_l[34]_table().\n                 *\n                 * Note on the use of ASSERT: if it's non-null and\n                 * hasn't been cleaned up yet, it should have\n                 * PGT_partial set; and so the type will be cleaned up\n                 * on domain destruction.  Unfortunately, we would\n                 * leak the general ref held by old_guest_table; but\n                 * leaking a page is less bad than a host crash.\n                 */\n                ASSERT(current->arch.old_guest_table == NULL);\n                page->nr_validated_ptes = i;\n                page->partial_flags = partial_flags;\n                current->arch.old_guest_ptpg = NULL;\n                current->arch.old_guest_table = page;\n                current->arch.old_guest_table_partial = true;\n            }\n        }\n        if ( rc < 0 )\n            break;\n\n        pl2e[i] = adjust_guest_l2e(l2e, d);\n    }\n\n    if ( !rc && (type & PGT_pae_xen_l2) )\n        init_xen_pae_l2_slots(pl2e, d);\n\n    unmap_domain_page(pl2e);\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -53,7 +53,7 @@\n         if ( rc == -EINTR && i )\n         {\n             page->nr_validated_ptes = i;\n-            page->partial_flags = 0;\n+            page->partial_flags = partial_flags;;\n             rc = -ERESTART;\n         }\n         else if ( rc < 0 && rc != -EINTR )",
        "diff_line_info": {
            "deleted_lines": [
                "            page->partial_flags = 0;"
            ],
            "added_lines": [
                "            page->partial_flags = partial_flags;;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19580",
        "func_name": "xen-project/xen/relinquish_memory",
        "description": "An issue was discovered in Xen through 4.12.x allowing x86 PV guest OS users to gain host OS privileges by leveraging race conditions in pagetable promotion and demotion operations, because of an incomplete fix for CVE-2019-18421. XSA-299 addressed several critical issues in restartable PV type change operations. Despite extensive testing and auditing, some corner cases were missed. A malicious PV guest administrator may be able to escalate their privilege to that of the host. All security-supported versions of Xen are vulnerable. Only x86 systems are affected. Arm systems are not affected. Only x86 PV guests can leverage the vulnerability. x86 HVM and PVH guests cannot leverage the vulnerability. Note that these attacks require very precise timing, which may be difficult to exploit in practice.",
        "git_url": "https://github.com/xen-project/xen/commit/66bdc16aeed8ddb2ae724adc5ea6bde0dea78c3d",
        "commit_title": "x86/mm: relinquish_memory: Grab an extra type ref when setting PGT_partial",
        "commit_text": " The PGT_partial bit in page->type_info holds both a type count and a general ref count.  During domain tear-down, when free_page_type() returns -ERESTART, relinquish_memory() correctly handles the general ref count, but fails to grab an extra type count when setting PGT_partial.  When this bit is eventually cleared, type_count underflows and triggers the following BUG in page_alloc.c:free_domheap_pages():      BUG_ON((pg[i].u.inuse.type_info & PGT_count_mask) != 0);  As far as we can tell, this page underflow cannot be exploited any any other way: The page can't be used as a pagetable by the dying domain because it's dying; it can't be used as a pagetable by any other domain since it belongs to the dying domain; and ownership can't transfer to any other domain without hitting the BUG_ON() in free_domheap_pages().  (steal_page() won't work on a page in this state, since it requires PGC_allocated to be set, and PGC_allocated will already have been cleared.)  Fix this by grabbing an extra type ref if setting PGT_partial in relinquish_memory.  This is part of XSA-310. ",
        "func_before": "static int relinquish_memory(\n    struct domain *d, struct page_list_head *list, unsigned long type)\n{\n    struct page_info  *page;\n    unsigned long     x, y;\n    int               ret = 0;\n\n    /* Use a recursive lock, as we may enter 'free_domheap_page'. */\n    spin_lock_recursive(&d->page_alloc_lock);\n\n    while ( (page = page_list_remove_head(list)) )\n    {\n        /* Grab a reference to the page so it won't disappear from under us. */\n        if ( unlikely(!get_page(page, d)) )\n        {\n            /* Couldn't get a reference -- someone is freeing this page. */\n            page_list_add_tail(page, &d->arch.relmem_list);\n            continue;\n        }\n\n        if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )\n            ret = put_page_and_type_preemptible(page);\n        switch ( ret )\n        {\n        case 0:\n            break;\n        case -ERESTART:\n        case -EINTR:\n            /*\n             * -EINTR means PGT_validated has been re-set; re-set\n             * PGT_pinned again so that it gets picked up next time\n             * around.\n             *\n             * -ERESTART, OTOH, means PGT_partial is set instead.  Put\n             * it back on the list, but don't set PGT_pinned; the\n             * section below will finish off de-validation.  But we do\n             * need to drop the general ref associated with\n             * PGT_pinned, since put_page_and_type_preemptible()\n             * didn't do it.\n             *\n             * NB we can do an ASSERT for PGT_validated, since we\n             * \"own\" the type ref; but theoretically, the PGT_partial\n             * could be cleared by someone else.\n             */\n            if ( ret == -EINTR )\n            {\n                ASSERT(page->u.inuse.type_info & PGT_validated);\n                set_bit(_PGT_pinned, &page->u.inuse.type_info);\n            }\n            else\n                put_page(page);\n\n            ret = -ERESTART;\n\n            /* Put the page back on the list and drop the ref we grabbed above */\n            page_list_add(page, list);\n            put_page(page);\n            goto out;\n        default:\n            BUG();\n        }\n\n        put_page_alloc_ref(page);\n\n        /*\n         * Forcibly invalidate top-most, still valid page tables at this point\n         * to break circular 'linear page table' references as well as clean up\n         * partially validated pages. This is okay because MMU structures are\n         * not shared across domains and this domain is now dead. Thus top-most\n         * valid tables are not in use so a non-zero count means circular\n         * reference or partially validated.\n         */\n        y = page->u.inuse.type_info;\n        for ( ; ; )\n        {\n            x = y;\n            if ( likely((x & PGT_type_mask) != type) ||\n                 likely(!(x & (PGT_validated|PGT_partial))) )\n                break;\n\n            y = cmpxchg(&page->u.inuse.type_info, x,\n                        x & ~(PGT_validated|PGT_partial));\n            if ( likely(y == x) )\n            {\n                /* No need for atomic update of type_info here: noone else updates it. */\n                switch ( ret = free_page_type(page, x, 1) )\n                {\n                case 0:\n                    break;\n                case -EINTR:\n                    page_list_add(page, list);\n                    page->u.inuse.type_info |= PGT_validated;\n                    if ( x & PGT_partial )\n                        put_page(page);\n                    put_page(page);\n                    ret = -ERESTART;\n                    goto out;\n                case -ERESTART:\n                    page_list_add(page, list);\n                    page->u.inuse.type_info |= PGT_partial;\n                    if ( x & PGT_partial )\n                        put_page(page);\n                    goto out;\n                default:\n                    BUG();\n                }\n                if ( x & PGT_partial )\n                {\n                    page->u.inuse.type_info--;\n                    put_page(page);\n                }\n                break;\n            }\n        }\n\n        /* Put the page on the list and /then/ potentially free it. */\n        page_list_add_tail(page, &d->arch.relmem_list);\n        put_page(page);\n\n        if ( hypercall_preempt_check() )\n        {\n            ret = -ERESTART;\n            goto out;\n        }\n    }\n\n    /* list is empty at this point. */\n    page_list_move(list, &d->arch.relmem_list);\n\n out:\n    spin_unlock_recursive(&d->page_alloc_lock);\n    return ret;\n}",
        "func": "static int relinquish_memory(\n    struct domain *d, struct page_list_head *list, unsigned long type)\n{\n    struct page_info  *page;\n    unsigned long     x, y;\n    int               ret = 0;\n\n    /* Use a recursive lock, as we may enter 'free_domheap_page'. */\n    spin_lock_recursive(&d->page_alloc_lock);\n\n    while ( (page = page_list_remove_head(list)) )\n    {\n        /* Grab a reference to the page so it won't disappear from under us. */\n        if ( unlikely(!get_page(page, d)) )\n        {\n            /* Couldn't get a reference -- someone is freeing this page. */\n            page_list_add_tail(page, &d->arch.relmem_list);\n            continue;\n        }\n\n        if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )\n            ret = put_page_and_type_preemptible(page);\n        switch ( ret )\n        {\n        case 0:\n            break;\n        case -ERESTART:\n        case -EINTR:\n            /*\n             * -EINTR means PGT_validated has been re-set; re-set\n             * PGT_pinned again so that it gets picked up next time\n             * around.\n             *\n             * -ERESTART, OTOH, means PGT_partial is set instead.  Put\n             * it back on the list, but don't set PGT_pinned; the\n             * section below will finish off de-validation.  But we do\n             * need to drop the general ref associated with\n             * PGT_pinned, since put_page_and_type_preemptible()\n             * didn't do it.\n             *\n             * NB we can do an ASSERT for PGT_validated, since we\n             * \"own\" the type ref; but theoretically, the PGT_partial\n             * could be cleared by someone else.\n             */\n            if ( ret == -EINTR )\n            {\n                ASSERT(page->u.inuse.type_info & PGT_validated);\n                set_bit(_PGT_pinned, &page->u.inuse.type_info);\n            }\n            else\n                put_page(page);\n\n            ret = -ERESTART;\n\n            /* Put the page back on the list and drop the ref we grabbed above */\n            page_list_add(page, list);\n            put_page(page);\n            goto out;\n        default:\n            BUG();\n        }\n\n        put_page_alloc_ref(page);\n\n        /*\n         * Forcibly invalidate top-most, still valid page tables at this point\n         * to break circular 'linear page table' references as well as clean up\n         * partially validated pages. This is okay because MMU structures are\n         * not shared across domains and this domain is now dead. Thus top-most\n         * valid tables are not in use so a non-zero count means circular\n         * reference or partially validated.\n         */\n        y = page->u.inuse.type_info;\n        for ( ; ; )\n        {\n            x = y;\n            if ( likely((x & PGT_type_mask) != type) ||\n                 likely(!(x & (PGT_validated|PGT_partial))) )\n                break;\n\n            y = cmpxchg(&page->u.inuse.type_info, x,\n                        x & ~(PGT_validated|PGT_partial));\n            if ( likely(y == x) )\n            {\n                /* No need for atomic update of type_info here: noone else updates it. */\n                switch ( ret = free_page_type(page, x, 1) )\n                {\n                case 0:\n                    break;\n                case -EINTR:\n                    page_list_add(page, list);\n                    page->u.inuse.type_info |= PGT_validated;\n                    if ( x & PGT_partial )\n                        put_page(page);\n                    put_page(page);\n                    ret = -ERESTART;\n                    goto out;\n                case -ERESTART:\n                    page_list_add(page, list);\n                    /*\n                     * PGT_partial holds a type ref and a general ref.\n                     * If we came in with PGT_partial set, then we 1)\n                     * don't need to grab an extra type count, and 2)\n                     * do need to drop the extra page ref we grabbed\n                     * at the top of the loop.  If we didn't come in\n                     * with PGT_partial set, we 1) do need to drab an\n                     * extra type count, but 2) can transfer the page\n                     * ref we grabbed above to it.\n                     *\n                     * Note that we must increment type_info before\n                     * setting PGT_partial.  Theoretically it should\n                     * be safe to drop the page ref before setting\n                     * PGT_partial, but do it afterwards just to be\n                     * extra safe.\n                     */\n                    if ( !(x & PGT_partial) )\n                        page->u.inuse.type_info++;\n                    smp_wmb();\n                    page->u.inuse.type_info |= PGT_partial;\n                    if ( x & PGT_partial )\n                        put_page(page);\n                    goto out;\n                default:\n                    BUG();\n                }\n                if ( x & PGT_partial )\n                {\n                    page->u.inuse.type_info--;\n                    put_page(page);\n                }\n                break;\n            }\n        }\n\n        /* Put the page on the list and /then/ potentially free it. */\n        page_list_add_tail(page, &d->arch.relmem_list);\n        put_page(page);\n\n        if ( hypercall_preempt_check() )\n        {\n            ret = -ERESTART;\n            goto out;\n        }\n    }\n\n    /* list is empty at this point. */\n    page_list_move(list, &d->arch.relmem_list);\n\n out:\n    spin_unlock_recursive(&d->page_alloc_lock);\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -97,6 +97,25 @@\n                     goto out;\n                 case -ERESTART:\n                     page_list_add(page, list);\n+                    /*\n+                     * PGT_partial holds a type ref and a general ref.\n+                     * If we came in with PGT_partial set, then we 1)\n+                     * don't need to grab an extra type count, and 2)\n+                     * do need to drop the extra page ref we grabbed\n+                     * at the top of the loop.  If we didn't come in\n+                     * with PGT_partial set, we 1) do need to drab an\n+                     * extra type count, but 2) can transfer the page\n+                     * ref we grabbed above to it.\n+                     *\n+                     * Note that we must increment type_info before\n+                     * setting PGT_partial.  Theoretically it should\n+                     * be safe to drop the page ref before setting\n+                     * PGT_partial, but do it afterwards just to be\n+                     * extra safe.\n+                     */\n+                    if ( !(x & PGT_partial) )\n+                        page->u.inuse.type_info++;\n+                    smp_wmb();\n                     page->u.inuse.type_info |= PGT_partial;\n                     if ( x & PGT_partial )\n                         put_page(page);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                    /*",
                "                     * PGT_partial holds a type ref and a general ref.",
                "                     * If we came in with PGT_partial set, then we 1)",
                "                     * don't need to grab an extra type count, and 2)",
                "                     * do need to drop the extra page ref we grabbed",
                "                     * at the top of the loop.  If we didn't come in",
                "                     * with PGT_partial set, we 1) do need to drab an",
                "                     * extra type count, but 2) can transfer the page",
                "                     * ref we grabbed above to it.",
                "                     *",
                "                     * Note that we must increment type_info before",
                "                     * setting PGT_partial.  Theoretically it should",
                "                     * be safe to drop the page ref before setting",
                "                     * PGT_partial, but do it afterwards just to be",
                "                     * extra safe.",
                "                     */",
                "                    if ( !(x & PGT_partial) )",
                "                        page->u.inuse.type_info++;",
                "                    smp_wmb();"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19580",
        "func_name": "xen-project/xen/vcpu_destroy_pagetables",
        "description": "An issue was discovered in Xen through 4.12.x allowing x86 PV guest OS users to gain host OS privileges by leveraging race conditions in pagetable promotion and demotion operations, because of an incomplete fix for CVE-2019-18421. XSA-299 addressed several critical issues in restartable PV type change operations. Despite extensive testing and auditing, some corner cases were missed. A malicious PV guest administrator may be able to escalate their privilege to that of the host. All security-supported versions of Xen are vulnerable. Only x86 systems are affected. Arm systems are not affected. Only x86 PV guests can leverage the vulnerability. x86 HVM and PVH guests cannot leverage the vulnerability. Note that these attacks require very precise timing, which may be difficult to exploit in practice.",
        "git_url": "https://github.com/xen-project/xen/commit/ececa12b2c4c8e4433e4f9be83f5c668ae36fe08",
        "commit_title": "x86/mm: Set old_guest_table when destroying vcpu pagetables",
        "commit_text": " Changeset 6c4efc1eba (\"x86/mm: Don't drop a type ref unless you held a ref to begin with\"), part of XSA-299, changed the calling discipline of put_page_type() such that if put_page_type() returned -ERESTART (indicating a partially de-validated page), subsequent calls to put_page_type() must be called with PTF_partial_set.  If called on a partially de-validated page but without PTF_partial_set, Xen will BUG(), because to do otherwise would risk opening up the kind of privilege escalation bug described in XSA-299.  One place this was missed was in vcpu_destroy_pagetables(). put_page_and_type_preemptible() is called, but on -ERESTART, the entire operation is simply restarted, causing put_page_type() to be called on a partially de-validated page without PTF_partial_set.  The result was that if such an operation were interrupted, Xen would hit a BUG().  Fix this by having vcpu_destroy_pagetables() consistently pass off interrupted de-validations to put_old_page_type(): - Unconditionally clear references to the page, even if   put_page_and_type failed - Set old_guest_table and old_guest_table_partial appropriately  While here, do some refactoring:   - Move clearing of arch.cr3 to the top of the function   - Now that clearing is unconditional, move the unmap to the same    conditional as the l4tab mapping.  This also allows us to reduce    the scope of the l4tab variable.   - Avoid code duplication by looping to drop references on    guest_table_user  This is part of XSA-310. ",
        "func_before": "int vcpu_destroy_pagetables(struct vcpu *v)\n{\n    unsigned long mfn = pagetable_get_pfn(v->arch.guest_table);\n    struct page_info *page;\n    l4_pgentry_t *l4tab = NULL;\n    int rc = put_old_guest_table(v);\n\n    if ( rc )\n        return rc;\n\n    if ( is_pv_32bit_vcpu(v) )\n    {\n        l4tab = map_domain_page(_mfn(mfn));\n        mfn = l4e_get_pfn(*l4tab);\n    }\n\n    if ( mfn )\n    {\n        page = mfn_to_page(_mfn(mfn));\n        if ( paging_mode_refcounts(v->domain) )\n            put_page(page);\n        else\n            rc = put_page_and_type_preemptible(page);\n    }\n\n    if ( l4tab )\n    {\n        if ( !rc )\n            l4e_write(l4tab, l4e_empty());\n        unmap_domain_page(l4tab);\n    }\n    else if ( !rc )\n    {\n        v->arch.guest_table = pagetable_null();\n\n        /* Drop ref to guest_table_user (from MMUEXT_NEW_USER_BASEPTR) */\n        mfn = pagetable_get_pfn(v->arch.guest_table_user);\n        if ( mfn )\n        {\n            page = mfn_to_page(_mfn(mfn));\n            if ( paging_mode_refcounts(v->domain) )\n                put_page(page);\n            else\n                rc = put_page_and_type_preemptible(page);\n        }\n        if ( !rc )\n            v->arch.guest_table_user = pagetable_null();\n    }\n\n    v->arch.cr3 = 0;\n\n    /*\n     * put_page_and_type_preemptible() is liable to return -EINTR. The\n     * callers of us expect -ERESTART so convert it over.\n     */\n    return rc != -EINTR ? rc : -ERESTART;\n}",
        "func": "int vcpu_destroy_pagetables(struct vcpu *v)\n{\n    unsigned long mfn = pagetable_get_pfn(v->arch.guest_table);\n    struct page_info *page = NULL;\n    int rc = put_old_guest_table(v);\n    bool put_guest_table_user = false;\n\n    if ( rc )\n        return rc;\n\n    v->arch.cr3 = 0;\n\n    /*\n     * Get the top-level guest page; either the guest_table itself, for\n     * 64-bit, or the top-level l4 entry for 32-bit.  Either way, remove\n     * the reference to that page.\n     */\n    if ( is_pv_32bit_vcpu(v) )\n    {\n        l4_pgentry_t *l4tab = map_domain_page(_mfn(mfn));\n\n        mfn = l4e_get_pfn(*l4tab);\n        l4e_write(l4tab, l4e_empty());\n        unmap_domain_page(l4tab);\n    }\n    else\n    {\n        v->arch.guest_table = pagetable_null();\n        put_guest_table_user = true;\n    }\n\n    /* Free that page if non-zero */\n    do {\n        if ( mfn )\n        {\n            page = mfn_to_page(_mfn(mfn));\n            if ( paging_mode_refcounts(v->domain) )\n                put_page(page);\n            else\n                rc = put_page_and_type_preemptible(page);\n            mfn = 0;\n        }\n\n        if ( !rc && put_guest_table_user )\n        {\n            /* Drop ref to guest_table_user (from MMUEXT_NEW_USER_BASEPTR) */\n            mfn = pagetable_get_pfn(v->arch.guest_table_user);\n            v->arch.guest_table_user = pagetable_null();\n            put_guest_table_user = false;\n        }\n    } while ( mfn );\n\n    /*\n     * If a \"put\" operation was interrupted, finish things off in\n     * put_old_guest_table() when the operation is restarted.\n     */\n    switch ( rc )\n    {\n    case -EINTR:\n    case -ERESTART:\n        v->arch.old_guest_ptpg = NULL;\n        v->arch.old_guest_table = page;\n        v->arch.old_guest_table_partial = (rc == -ERESTART);\n        rc = -ERESTART;\n        break;\n    default:\n        /*\n         * Failure to 'put' a page may cause it to leak, but that's\n         * less bad than a crash.\n         */\n        ASSERT(rc == 0);\n        break;\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,40 +1,36 @@\n int vcpu_destroy_pagetables(struct vcpu *v)\n {\n     unsigned long mfn = pagetable_get_pfn(v->arch.guest_table);\n-    struct page_info *page;\n-    l4_pgentry_t *l4tab = NULL;\n+    struct page_info *page = NULL;\n     int rc = put_old_guest_table(v);\n+    bool put_guest_table_user = false;\n \n     if ( rc )\n         return rc;\n \n+    v->arch.cr3 = 0;\n+\n+    /*\n+     * Get the top-level guest page; either the guest_table itself, for\n+     * 64-bit, or the top-level l4 entry for 32-bit.  Either way, remove\n+     * the reference to that page.\n+     */\n     if ( is_pv_32bit_vcpu(v) )\n     {\n-        l4tab = map_domain_page(_mfn(mfn));\n+        l4_pgentry_t *l4tab = map_domain_page(_mfn(mfn));\n+\n         mfn = l4e_get_pfn(*l4tab);\n+        l4e_write(l4tab, l4e_empty());\n+        unmap_domain_page(l4tab);\n+    }\n+    else\n+    {\n+        v->arch.guest_table = pagetable_null();\n+        put_guest_table_user = true;\n     }\n \n-    if ( mfn )\n-    {\n-        page = mfn_to_page(_mfn(mfn));\n-        if ( paging_mode_refcounts(v->domain) )\n-            put_page(page);\n-        else\n-            rc = put_page_and_type_preemptible(page);\n-    }\n-\n-    if ( l4tab )\n-    {\n-        if ( !rc )\n-            l4e_write(l4tab, l4e_empty());\n-        unmap_domain_page(l4tab);\n-    }\n-    else if ( !rc )\n-    {\n-        v->arch.guest_table = pagetable_null();\n-\n-        /* Drop ref to guest_table_user (from MMUEXT_NEW_USER_BASEPTR) */\n-        mfn = pagetable_get_pfn(v->arch.guest_table_user);\n+    /* Free that page if non-zero */\n+    do {\n         if ( mfn )\n         {\n             page = mfn_to_page(_mfn(mfn));\n@@ -42,16 +38,39 @@\n                 put_page(page);\n             else\n                 rc = put_page_and_type_preemptible(page);\n+            mfn = 0;\n         }\n-        if ( !rc )\n+\n+        if ( !rc && put_guest_table_user )\n+        {\n+            /* Drop ref to guest_table_user (from MMUEXT_NEW_USER_BASEPTR) */\n+            mfn = pagetable_get_pfn(v->arch.guest_table_user);\n             v->arch.guest_table_user = pagetable_null();\n+            put_guest_table_user = false;\n+        }\n+    } while ( mfn );\n+\n+    /*\n+     * If a \"put\" operation was interrupted, finish things off in\n+     * put_old_guest_table() when the operation is restarted.\n+     */\n+    switch ( rc )\n+    {\n+    case -EINTR:\n+    case -ERESTART:\n+        v->arch.old_guest_ptpg = NULL;\n+        v->arch.old_guest_table = page;\n+        v->arch.old_guest_table_partial = (rc == -ERESTART);\n+        rc = -ERESTART;\n+        break;\n+    default:\n+        /*\n+         * Failure to 'put' a page may cause it to leak, but that's\n+         * less bad than a crash.\n+         */\n+        ASSERT(rc == 0);\n+        break;\n     }\n \n-    v->arch.cr3 = 0;\n-\n-    /*\n-     * put_page_and_type_preemptible() is liable to return -EINTR. The\n-     * callers of us expect -ERESTART so convert it over.\n-     */\n-    return rc != -EINTR ? rc : -ERESTART;\n+    return rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    struct page_info *page;",
                "    l4_pgentry_t *l4tab = NULL;",
                "        l4tab = map_domain_page(_mfn(mfn));",
                "    if ( mfn )",
                "    {",
                "        page = mfn_to_page(_mfn(mfn));",
                "        if ( paging_mode_refcounts(v->domain) )",
                "            put_page(page);",
                "        else",
                "            rc = put_page_and_type_preemptible(page);",
                "    }",
                "",
                "    if ( l4tab )",
                "    {",
                "        if ( !rc )",
                "            l4e_write(l4tab, l4e_empty());",
                "        unmap_domain_page(l4tab);",
                "    }",
                "    else if ( !rc )",
                "    {",
                "        v->arch.guest_table = pagetable_null();",
                "",
                "        /* Drop ref to guest_table_user (from MMUEXT_NEW_USER_BASEPTR) */",
                "        mfn = pagetable_get_pfn(v->arch.guest_table_user);",
                "        if ( !rc )",
                "    v->arch.cr3 = 0;",
                "",
                "    /*",
                "     * put_page_and_type_preemptible() is liable to return -EINTR. The",
                "     * callers of us expect -ERESTART so convert it over.",
                "     */",
                "    return rc != -EINTR ? rc : -ERESTART;"
            ],
            "added_lines": [
                "    struct page_info *page = NULL;",
                "    bool put_guest_table_user = false;",
                "    v->arch.cr3 = 0;",
                "",
                "    /*",
                "     * Get the top-level guest page; either the guest_table itself, for",
                "     * 64-bit, or the top-level l4 entry for 32-bit.  Either way, remove",
                "     * the reference to that page.",
                "     */",
                "        l4_pgentry_t *l4tab = map_domain_page(_mfn(mfn));",
                "",
                "        l4e_write(l4tab, l4e_empty());",
                "        unmap_domain_page(l4tab);",
                "    }",
                "    else",
                "    {",
                "        v->arch.guest_table = pagetable_null();",
                "        put_guest_table_user = true;",
                "    /* Free that page if non-zero */",
                "    do {",
                "            mfn = 0;",
                "",
                "        if ( !rc && put_guest_table_user )",
                "        {",
                "            /* Drop ref to guest_table_user (from MMUEXT_NEW_USER_BASEPTR) */",
                "            mfn = pagetable_get_pfn(v->arch.guest_table_user);",
                "            put_guest_table_user = false;",
                "        }",
                "    } while ( mfn );",
                "",
                "    /*",
                "     * If a \"put\" operation was interrupted, finish things off in",
                "     * put_old_guest_table() when the operation is restarted.",
                "     */",
                "    switch ( rc )",
                "    {",
                "    case -EINTR:",
                "    case -ERESTART:",
                "        v->arch.old_guest_ptpg = NULL;",
                "        v->arch.old_guest_table = page;",
                "        v->arch.old_guest_table_partial = (rc == -ERESTART);",
                "        rc = -ERESTART;",
                "        break;",
                "    default:",
                "        /*",
                "         * Failure to 'put' a page may cause it to leak, but that's",
                "         * less bad than a crash.",
                "         */",
                "        ASSERT(rc == 0);",
                "        break;",
                "    return rc;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5873",
        "func_name": "torvalds/linux/__ns_get_path",
        "description": "An issue was discovered in the __ns_get_path function in fs/nsfs.c in the Linux kernel before 4.11. Due to a race condition when accessing files, a Use After Free condition can occur. This also affects all Android releases from CAF using the Linux kernel (Android for MSM, Firefox OS for MSM, QRD Android) before security patch level 2018-07-05.",
        "git_url": "https://github.com/torvalds/linux/commit/073c516ff73557a8f7315066856c04b50383ac34",
        "commit_title": "nsfs: mark dentry with DCACHE_RCUACCESS",
        "commit_text": " Andrey reported a use-after-free in __ns_get_path():    spin_lock include/linux/spinlock.h:299 [inline]   lockref_get_not_dead+0x19/0x80 lib/lockref.c:179   __ns_get_path+0x197/0x860 fs/nsfs.c:66   open_related_ns+0xda/0x200 fs/nsfs.c:143   sock_ioctl+0x39d/0x440 net/socket.c:1001   vfs_ioctl fs/ioctl.c:45 [inline]   do_vfs_ioctl+0x1bf/0x1780 fs/ioctl.c:685   SYSC_ioctl fs/ioctl.c:700 [inline]   SyS_ioctl+0x8f/0xc0 fs/ioctl.c:691  We are under rcu read lock protection at that point:          rcu_read_lock();         d = atomic_long_read(&ns->stashed);         if (!d)                 goto slow;         dentry = (struct dentry *)d;         if (!lockref_get_not_dead(&dentry->d_lockref))                 goto slow;         rcu_read_unlock();  but don't use a proper RCU API on the free path, therefore a parallel __d_free() could free it at the same time.  We need to mark the stashed dentry with DCACHE_RCUACCESS so that __d_free() will be called after all readers leave RCU.  Cc: Alexander Viro <viro@zeniv.linux.org.uk> Cc: Andrew Morton <akpm@linux-foundation.org>",
        "func_before": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}",
        "func": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_flags |= DCACHE_RCUACCESS;\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -39,6 +39,7 @@\n \t\treturn ERR_PTR(-ENOMEM);\n \t}\n \td_instantiate(dentry, inode);\n+\tdentry->d_flags |= DCACHE_RCUACCESS;\n \tdentry->d_fsdata = (void *)ns->ops;\n \td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n \tif (d) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tdentry->d_flags |= DCACHE_RCUACCESS;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-2616",
        "func_name": "util-linux/create_watching_parent",
        "description": "A race condition was found in util-linux before 2.32.1 in the way su handled the management of child processes. A local authenticated attacker could use this flaw to kill other processes with root privileges under specific conditions.",
        "git_url": "https://github.com/util-linux/util-linux/commit/dffab154d29a288aa171ff50263ecc8f2e14a891",
        "commit_title": "su: properly clear child PID",
        "commit_text": "",
        "func_before": "static void\ncreate_watching_parent (void)\n{\n  pid_t child;\n  sigset_t ourset;\n  struct sigaction oldact[3];\n  int status = 0;\n  int retval;\n\n  retval = pam_open_session (pamh, 0);\n  if (is_pam_failure(retval))\n    {\n      cleanup_pam (retval);\n      errx (EXIT_FAILURE, _(\"cannot open session: %s\"),\n\t     pam_strerror (pamh, retval));\n    }\n  else\n    _pam_session_opened = 1;\n\n  memset(oldact, 0, sizeof(oldact));\n\n  child = fork ();\n  if (child == (pid_t) -1)\n    {\n      cleanup_pam (PAM_ABORT);\n      err (EXIT_FAILURE, _(\"cannot create child process\"));\n    }\n\n  /* the child proceeds to run the shell */\n  if (child == 0)\n    return;\n\n  /* In the parent watch the child.  */\n\n  /* su without pam support does not have a helper that keeps\n     sitting on any directory so let's go to /.  */\n  if (chdir (\"/\") != 0)\n    warn (_(\"cannot change directory to %s\"), \"/\");\n\n  sigfillset (&ourset);\n  if (sigprocmask (SIG_BLOCK, &ourset, NULL))\n    {\n      warn (_(\"cannot block signals\"));\n      caught_signal = true;\n    }\n  if (!caught_signal)\n    {\n      struct sigaction action;\n      action.sa_handler = su_catch_sig;\n      sigemptyset (&action.sa_mask);\n      action.sa_flags = 0;\n      sigemptyset (&ourset);\n    if (!same_session)\n      {\n        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))\n          {\n            warn (_(\"cannot set signal handler\"));\n            caught_signal = true;\n          }\n      }\n    if (!caught_signal && (sigaddset(&ourset, SIGTERM)\n                    || sigaddset(&ourset, SIGALRM)\n                    || sigaction(SIGTERM, &action, &oldact[0])\n                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {\n\t  warn (_(\"cannot set signal handler\"));\n\t  caught_signal = true;\n\t}\n    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])\n                                     || sigaction(SIGQUIT, &action, &oldact[2])))\n      {\n        warn (_(\"cannot set signal handler\"));\n        caught_signal = true;\n      }\n    }\n  if (!caught_signal)\n    {\n      pid_t pid;\n      for (;;)\n\t{\n\t  pid = waitpid (child, &status, WUNTRACED);\n\n\t  if (pid != (pid_t)-1 && WIFSTOPPED (status))\n\t    {\n\t      kill (getpid (), SIGSTOP);\n\t      /* once we get here, we must have resumed */\n\t      kill (pid, SIGCONT);\n\t    }\n\t  else\n\t    break;\n\t}\n      if (pid != (pid_t)-1)\n        {\n          if (WIFSIGNALED (status))\n            {\n              fprintf (stderr, \"%s%s\\n\", strsignal (WTERMSIG (status)),\n                       WCOREDUMP (status) ? _(\" (core dumped)\") : \"\");\n              status = WTERMSIG (status) + 128;\n            }\n          else\n            status = WEXITSTATUS (status);\n        }\n      else if (caught_signal)\n        status = caught_signal + 128;\n      else\n        status = 1;\n    }\n  else\n    status = 1;\n\n  if (caught_signal)\n    {\n      fprintf (stderr, _(\"\\nSession terminated, killing shell...\"));\n      kill (child, SIGTERM);\n    }\n\n  cleanup_pam (PAM_SUCCESS);\n\n  if (caught_signal)\n    {\n      sleep (2);\n      kill (child, SIGKILL);\n      fprintf (stderr, _(\" ...killed.\\n\"));\n\n      /* Let's terminate itself with the received signal.\n       *\n       * It seems that shells use WIFSIGNALED() rather than our exit status\n       * value to detect situations when is necessary to cleanup (reset)\n       * terminal settings (kzak -- Jun 2013).\n       */\n      switch (caught_signal) {\n        case SIGTERM:\n          sigaction(SIGTERM, &oldact[0], NULL);\n          break;\n        case SIGINT:\n          sigaction(SIGINT, &oldact[1], NULL);\n          break;\n        case SIGQUIT:\n          sigaction(SIGQUIT, &oldact[2], NULL);\n          break;\n        default:\n\t  /* just in case that signal stuff initialization failed and\n\t   * caught_signal = true */\n          caught_signal = SIGKILL;\n          break;\n      }\n      kill(getpid(), caught_signal);\n    }\n  exit (status);\n}",
        "func": "static void\ncreate_watching_parent (void)\n{\n  pid_t child;\n  sigset_t ourset;\n  struct sigaction oldact[3];\n  int status = 0;\n  int retval;\n\n  retval = pam_open_session (pamh, 0);\n  if (is_pam_failure(retval))\n    {\n      cleanup_pam (retval);\n      errx (EXIT_FAILURE, _(\"cannot open session: %s\"),\n\t     pam_strerror (pamh, retval));\n    }\n  else\n    _pam_session_opened = 1;\n\n  memset(oldact, 0, sizeof(oldact));\n\n  child = fork ();\n  if (child == (pid_t) -1)\n    {\n      cleanup_pam (PAM_ABORT);\n      err (EXIT_FAILURE, _(\"cannot create child process\"));\n    }\n\n  /* the child proceeds to run the shell */\n  if (child == 0)\n    return;\n\n  /* In the parent watch the child.  */\n\n  /* su without pam support does not have a helper that keeps\n     sitting on any directory so let's go to /.  */\n  if (chdir (\"/\") != 0)\n    warn (_(\"cannot change directory to %s\"), \"/\");\n\n  sigfillset (&ourset);\n  if (sigprocmask (SIG_BLOCK, &ourset, NULL))\n    {\n      warn (_(\"cannot block signals\"));\n      caught_signal = true;\n    }\n  if (!caught_signal)\n    {\n      struct sigaction action;\n      action.sa_handler = su_catch_sig;\n      sigemptyset (&action.sa_mask);\n      action.sa_flags = 0;\n      sigemptyset (&ourset);\n    if (!same_session)\n      {\n        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))\n          {\n            warn (_(\"cannot set signal handler\"));\n            caught_signal = true;\n          }\n      }\n    if (!caught_signal && (sigaddset(&ourset, SIGTERM)\n                    || sigaddset(&ourset, SIGALRM)\n                    || sigaction(SIGTERM, &action, &oldact[0])\n                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {\n\t  warn (_(\"cannot set signal handler\"));\n\t  caught_signal = true;\n\t}\n    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])\n                                     || sigaction(SIGQUIT, &action, &oldact[2])))\n      {\n        warn (_(\"cannot set signal handler\"));\n        caught_signal = true;\n      }\n    }\n  if (!caught_signal)\n    {\n      pid_t pid;\n      for (;;)\n\t{\n\t  pid = waitpid (child, &status, WUNTRACED);\n\n\t  if (pid != (pid_t)-1 && WIFSTOPPED (status))\n\t    {\n\t      kill (getpid (), SIGSTOP);\n\t      /* once we get here, we must have resumed */\n\t      kill (pid, SIGCONT);\n\t    }\n\t  else\n\t    break;\n\t}\n      if (pid != (pid_t)-1)\n        {\n          if (WIFSIGNALED (status))\n            {\n              fprintf (stderr, \"%s%s\\n\", strsignal (WTERMSIG (status)),\n                       WCOREDUMP (status) ? _(\" (core dumped)\") : \"\");\n              status = WTERMSIG (status) + 128;\n            }\n          else\n            status = WEXITSTATUS (status);\n\n\t  /* child is gone, don't use the PID anymore */\n\t  child = (pid_t) -1;\n        }\n      else if (caught_signal)\n        status = caught_signal + 128;\n      else\n        status = 1;\n    }\n  else\n    status = 1;\n\n  if (caught_signal && child != (pid_t)-1)\n    {\n      fprintf (stderr, _(\"\\nSession terminated, killing shell...\"));\n      kill (child, SIGTERM);\n    }\n\n  cleanup_pam (PAM_SUCCESS);\n\n  if (caught_signal)\n    {\n      if (child != (pid_t)-1)\n\t{\n\t  sleep (2);\n\t  kill (child, SIGKILL);\n\t  fprintf (stderr, _(\" ...killed.\\n\"));\n\t}\n\n      /* Let's terminate itself with the received signal.\n       *\n       * It seems that shells use WIFSIGNALED() rather than our exit status\n       * value to detect situations when is necessary to cleanup (reset)\n       * terminal settings (kzak -- Jun 2013).\n       */\n      switch (caught_signal) {\n        case SIGTERM:\n          sigaction(SIGTERM, &oldact[0], NULL);\n          break;\n        case SIGINT:\n          sigaction(SIGINT, &oldact[1], NULL);\n          break;\n        case SIGQUIT:\n          sigaction(SIGQUIT, &oldact[2], NULL);\n          break;\n        default:\n\t  /* just in case that signal stuff initialization failed and\n\t   * caught_signal = true */\n          caught_signal = SIGKILL;\n          break;\n      }\n      kill(getpid(), caught_signal);\n    }\n  exit (status);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -98,6 +98,9 @@\n             }\n           else\n             status = WEXITSTATUS (status);\n+\n+\t  /* child is gone, don't use the PID anymore */\n+\t  child = (pid_t) -1;\n         }\n       else if (caught_signal)\n         status = caught_signal + 128;\n@@ -107,7 +110,7 @@\n   else\n     status = 1;\n \n-  if (caught_signal)\n+  if (caught_signal && child != (pid_t)-1)\n     {\n       fprintf (stderr, _(\"\\nSession terminated, killing shell...\"));\n       kill (child, SIGTERM);\n@@ -117,9 +120,12 @@\n \n   if (caught_signal)\n     {\n-      sleep (2);\n-      kill (child, SIGKILL);\n-      fprintf (stderr, _(\" ...killed.\\n\"));\n+      if (child != (pid_t)-1)\n+\t{\n+\t  sleep (2);\n+\t  kill (child, SIGKILL);\n+\t  fprintf (stderr, _(\" ...killed.\\n\"));\n+\t}\n \n       /* Let's terminate itself with the received signal.\n        *",
        "diff_line_info": {
            "deleted_lines": [
                "  if (caught_signal)",
                "      sleep (2);",
                "      kill (child, SIGKILL);",
                "      fprintf (stderr, _(\" ...killed.\\n\"));"
            ],
            "added_lines": [
                "",
                "\t  /* child is gone, don't use the PID anymore */",
                "\t  child = (pid_t) -1;",
                "  if (caught_signal && child != (pid_t)-1)",
                "      if (child != (pid_t)-1)",
                "\t{",
                "\t  sleep (2);",
                "\t  kill (child, SIGKILL);",
                "\t  fprintf (stderr, _(\" ...killed.\\n\"));",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-15473",
        "func_name": "openbsd/src/userauth_gssapi",
        "description": "OpenSSH through 7.7 is prone to a user enumeration vulnerability due to not delaying bailout for an invalid authenticating user until after the packet containing the request has been fully parsed, related to auth2-gss.c, auth2-hostbased.c, and auth2-pubkey.c.",
        "git_url": "https://github.com/openbsd/src/commit/779974d35b4859c07bc3cb8a12c74b43b0a7d1e0",
        "commit_title": "delay bailout for invalid authenticating user until after the packet",
        "commit_text": "containing the request has been fully parsed. Reported by Dariusz Tytko and Michał Sajdak; ok deraadt",
        "func_before": "static int\nuserauth_gssapi(struct ssh *ssh)\n{\n\tAuthctxt *authctxt = ssh->authctxt;\n\tgss_OID_desc goid = {0, NULL};\n\tGssctxt *ctxt = NULL;\n\tint r, present;\n\tu_int mechs;\n\tOM_uint32 ms;\n\tsize_t len;\n\tu_char *doid = NULL;\n\n\tif (!authctxt->valid || authctxt->user == NULL)\n\t\treturn (0);\n\n\tif ((r = sshpkt_get_u32(ssh, &mechs)) != 0)\n\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\n\tif (mechs == 0) {\n\t\tdebug(\"Mechanism negotiation is not supported\");\n\t\treturn (0);\n\t}\n\n\tdo {\n\t\tmechs--;\n\n\t\tfree(doid);\n\n\t\tpresent = 0;\n\t\tif ((r = sshpkt_get_string(ssh, &doid, &len)) != 0)\n\t\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\n\t\tif (len > 2 && doid[0] == SSH_GSS_OIDTYPE &&\n\t\t    doid[1] == len - 2) {\n\t\t\tgoid.elements = doid + 2;\n\t\t\tgoid.length   = len - 2;\n\t\t\tssh_gssapi_test_oid_supported(&ms, &goid, &present);\n\t\t} else {\n\t\t\tlogit(\"Badly formed OID received\");\n\t\t}\n\t} while (mechs > 0 && !present);\n\n\tif (!present) {\n\t\tfree(doid);\n\t\tauthctxt->server_caused_failure = 1;\n\t\treturn (0);\n\t}\n\n\tif (GSS_ERROR(PRIVSEP(ssh_gssapi_server_ctx(&ctxt, &goid)))) {\n\t\tif (ctxt != NULL)\n\t\t\tssh_gssapi_delete_ctx(&ctxt);\n\t\tfree(doid);\n\t\tauthctxt->server_caused_failure = 1;\n\t\treturn (0);\n\t}\n\n\tauthctxt->methoddata = (void *)ctxt;\n\n\t/* Return the OID that we received */\n\tif ((r = sshpkt_start(ssh, SSH2_MSG_USERAUTH_GSSAPI_RESPONSE)) != 0 ||\n\t    (r = sshpkt_put_string(ssh, doid, len)) != 0 ||\n\t    (r = sshpkt_send(ssh)) != 0)\n\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\n\tfree(doid);\n\n\tssh_dispatch_set(ssh, SSH2_MSG_USERAUTH_GSSAPI_TOKEN, &input_gssapi_token);\n\tssh_dispatch_set(ssh, SSH2_MSG_USERAUTH_GSSAPI_ERRTOK, &input_gssapi_errtok);\n\tauthctxt->postponed = 1;\n\n\treturn (0);\n}",
        "func": "static int\nuserauth_gssapi(struct ssh *ssh)\n{\n\tAuthctxt *authctxt = ssh->authctxt;\n\tgss_OID_desc goid = {0, NULL};\n\tGssctxt *ctxt = NULL;\n\tint r, present;\n\tu_int mechs;\n\tOM_uint32 ms;\n\tsize_t len;\n\tu_char *doid = NULL;\n\n\tif ((r = sshpkt_get_u32(ssh, &mechs)) != 0)\n\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\n\tif (mechs == 0) {\n\t\tdebug(\"Mechanism negotiation is not supported\");\n\t\treturn (0);\n\t}\n\n\tdo {\n\t\tmechs--;\n\n\t\tfree(doid);\n\n\t\tpresent = 0;\n\t\tif ((r = sshpkt_get_string(ssh, &doid, &len)) != 0)\n\t\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\n\t\tif (len > 2 && doid[0] == SSH_GSS_OIDTYPE &&\n\t\t    doid[1] == len - 2) {\n\t\t\tgoid.elements = doid + 2;\n\t\t\tgoid.length   = len - 2;\n\t\t\tssh_gssapi_test_oid_supported(&ms, &goid, &present);\n\t\t} else {\n\t\t\tlogit(\"Badly formed OID received\");\n\t\t}\n\t} while (mechs > 0 && !present);\n\n\tif (!present) {\n\t\tfree(doid);\n\t\tauthctxt->server_caused_failure = 1;\n\t\treturn (0);\n\t}\n\n\tif (!authctxt->valid || authctxt->user == NULL) {\n\t\tdebug2(\"%s: disabled because of invalid user\", __func__);\n\t\tfree(doid);\n\t\treturn (0);\n\t}\n\n\tif (GSS_ERROR(PRIVSEP(ssh_gssapi_server_ctx(&ctxt, &goid)))) {\n\t\tif (ctxt != NULL)\n\t\t\tssh_gssapi_delete_ctx(&ctxt);\n\t\tfree(doid);\n\t\tauthctxt->server_caused_failure = 1;\n\t\treturn (0);\n\t}\n\n\tauthctxt->methoddata = (void *)ctxt;\n\n\t/* Return the OID that we received */\n\tif ((r = sshpkt_start(ssh, SSH2_MSG_USERAUTH_GSSAPI_RESPONSE)) != 0 ||\n\t    (r = sshpkt_put_string(ssh, doid, len)) != 0 ||\n\t    (r = sshpkt_send(ssh)) != 0)\n\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\n\tfree(doid);\n\n\tssh_dispatch_set(ssh, SSH2_MSG_USERAUTH_GSSAPI_TOKEN, &input_gssapi_token);\n\tssh_dispatch_set(ssh, SSH2_MSG_USERAUTH_GSSAPI_ERRTOK, &input_gssapi_errtok);\n\tauthctxt->postponed = 1;\n\n\treturn (0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,9 +9,6 @@\n \tOM_uint32 ms;\n \tsize_t len;\n \tu_char *doid = NULL;\n-\n-\tif (!authctxt->valid || authctxt->user == NULL)\n-\t\treturn (0);\n \n \tif ((r = sshpkt_get_u32(ssh, &mechs)) != 0)\n \t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n@@ -46,6 +43,12 @@\n \t\treturn (0);\n \t}\n \n+\tif (!authctxt->valid || authctxt->user == NULL) {\n+\t\tdebug2(\"%s: disabled because of invalid user\", __func__);\n+\t\tfree(doid);\n+\t\treturn (0);\n+\t}\n+\n \tif (GSS_ERROR(PRIVSEP(ssh_gssapi_server_ctx(&ctxt, &goid)))) {\n \t\tif (ctxt != NULL)\n \t\t\tssh_gssapi_delete_ctx(&ctxt);",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\tif (!authctxt->valid || authctxt->user == NULL)",
                "\t\treturn (0);"
            ],
            "added_lines": [
                "\tif (!authctxt->valid || authctxt->user == NULL) {",
                "\t\tdebug2(\"%s: disabled because of invalid user\", __func__);",
                "\t\tfree(doid);",
                "\t\treturn (0);",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-15473",
        "func_name": "openbsd/src/userauth_pubkey",
        "description": "OpenSSH through 7.7 is prone to a user enumeration vulnerability due to not delaying bailout for an invalid authenticating user until after the packet containing the request has been fully parsed, related to auth2-gss.c, auth2-hostbased.c, and auth2-pubkey.c.",
        "git_url": "https://github.com/openbsd/src/commit/779974d35b4859c07bc3cb8a12c74b43b0a7d1e0",
        "commit_title": "delay bailout for invalid authenticating user until after the packet",
        "commit_text": "containing the request has been fully parsed. Reported by Dariusz Tytko and Michał Sajdak; ok deraadt",
        "func_before": "static int\nuserauth_pubkey(struct ssh *ssh)\n{\n\tAuthctxt *authctxt = ssh->authctxt;\n\tstruct passwd *pw = authctxt->pw;\n\tstruct sshbuf *b;\n\tstruct sshkey *key = NULL;\n\tchar *pkalg, *userstyle = NULL, *key_s = NULL, *ca_s = NULL;\n\tu_char *pkblob, *sig, have_sig;\n\tsize_t blen, slen;\n\tint r, pktype;\n\tint authenticated = 0;\n\tstruct sshauthopt *authopts = NULL;\n\n\tif (!authctxt->valid) {\n\t\tdebug2(\"%s: disabled because of invalid user\", __func__);\n\t\treturn 0;\n\t}\n\tif ((r = sshpkt_get_u8(ssh, &have_sig)) != 0 ||\n\t    (r = sshpkt_get_cstring(ssh, &pkalg, NULL)) != 0 ||\n\t    (r = sshpkt_get_string(ssh, &pkblob, &blen)) != 0)\n\t\tfatal(\"%s: parse request failed: %s\", __func__, ssh_err(r));\n\tpktype = sshkey_type_from_name(pkalg);\n\tif (pktype == KEY_UNSPEC) {\n\t\t/* this is perfectly legal */\n\t\tverbose(\"%s: unsupported public key algorithm: %s\",\n\t\t    __func__, pkalg);\n\t\tgoto done;\n\t}\n\tif ((r = sshkey_from_blob(pkblob, blen, &key)) != 0) {\n\t\terror(\"%s: could not parse key: %s\", __func__, ssh_err(r));\n\t\tgoto done;\n\t}\n\tif (key == NULL) {\n\t\terror(\"%s: cannot decode key: %s\", __func__, pkalg);\n\t\tgoto done;\n\t}\n\tif (key->type != pktype) {\n\t\terror(\"%s: type mismatch for decoded key \"\n\t\t    \"(received %d, expected %d)\", __func__, key->type, pktype);\n\t\tgoto done;\n\t}\n\tif (sshkey_type_plain(key->type) == KEY_RSA &&\n\t    (ssh->compat & SSH_BUG_RSASIGMD5) != 0) {\n\t\tlogit(\"Refusing RSA key because client uses unsafe \"\n\t\t    \"signature scheme\");\n\t\tgoto done;\n\t}\n\tif (auth2_key_already_used(authctxt, key)) {\n\t\tlogit(\"refusing previously-used %s key\", sshkey_type(key));\n\t\tgoto done;\n\t}\n\tif (match_pattern_list(pkalg, options.pubkey_key_types, 0) != 1) {\n\t\tlogit(\"%s: key type %s not in PubkeyAcceptedKeyTypes\",\n\t\t    __func__, sshkey_ssh_name(key));\n\t\tgoto done;\n\t}\n\n\tkey_s = format_key(key);\n\tif (sshkey_is_cert(key))\n\t\tca_s = format_key(key->cert->signature_key);\n\n\tif (have_sig) {\n\t\tdebug3(\"%s: have %s signature for %s%s%s\",\n\t\t    __func__, pkalg, key_s,\n\t\t    ca_s == NULL ? \"\" : \" CA \",\n\t\t    ca_s == NULL ? \"\" : ca_s);\n\t\tif ((r = sshpkt_get_string(ssh, &sig, &slen)) != 0 ||\n\t\t    (r = sshpkt_get_end(ssh)) != 0)\n\t\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\t\tif ((b = sshbuf_new()) == NULL)\n\t\t\tfatal(\"%s: sshbuf_new failed\", __func__);\n\t\tif (ssh->compat & SSH_OLD_SESSIONID) {\n\t\t\tif ((r = sshbuf_put(b, session_id2,\n\t\t\t    session_id2_len)) != 0)\n\t\t\t\tfatal(\"%s: sshbuf_put session id: %s\",\n\t\t\t\t    __func__, ssh_err(r));\n\t\t} else {\n\t\t\tif ((r = sshbuf_put_string(b, session_id2,\n\t\t\t    session_id2_len)) != 0)\n\t\t\t\tfatal(\"%s: sshbuf_put_string session id: %s\",\n\t\t\t\t    __func__, ssh_err(r));\n\t\t}\n\t\t/* reconstruct packet */\n\t\txasprintf(&userstyle, \"%s%s%s\", authctxt->user,\n\t\t    authctxt->style ? \":\" : \"\",\n\t\t    authctxt->style ? authctxt->style : \"\");\n\t\tif ((r = sshbuf_put_u8(b, SSH2_MSG_USERAUTH_REQUEST)) != 0 ||\n\t\t    (r = sshbuf_put_cstring(b, userstyle)) != 0 ||\n\t\t    (r = sshbuf_put_cstring(b, authctxt->service)) != 0 ||\n\t\t    (r = sshbuf_put_cstring(b, \"publickey\")) != 0 ||\n\t\t    (r = sshbuf_put_u8(b, have_sig)) != 0 ||\n\t\t    (r = sshbuf_put_cstring(b, pkalg) != 0) ||\n\t\t    (r = sshbuf_put_string(b, pkblob, blen)) != 0)\n\t\t\tfatal(\"%s: build packet failed: %s\",\n\t\t\t    __func__, ssh_err(r));\n#ifdef DEBUG_PK\n\t\tsshbuf_dump(b, stderr);\n#endif\n\n\t\t/* test for correct signature */\n\t\tauthenticated = 0;\n\t\tif (PRIVSEP(user_key_allowed(ssh, pw, key, 1, &authopts)) &&\n\t\t    PRIVSEP(sshkey_verify(key, sig, slen,\n\t\t    sshbuf_ptr(b), sshbuf_len(b),\n\t\t    (ssh->compat & SSH_BUG_SIGTYPE) == 0 ? pkalg : NULL,\n\t\t    ssh->compat)) == 0) {\n\t\t\tauthenticated = 1;\n\t\t}\n\t\tsshbuf_free(b);\n\t\tfree(sig);\n\t\tauth2_record_key(authctxt, authenticated, key);\n\t} else {\n\t\tdebug(\"%s: test pkalg %s pkblob %s%s%s\",\n\t\t    __func__, pkalg, key_s,\n\t\t    ca_s == NULL ? \"\" : \" CA \",\n\t\t    ca_s == NULL ? \"\" : ca_s);\n\n\t\tif ((r = sshpkt_get_end(ssh)) != 0)\n\t\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\n\t\t/* XXX fake reply and always send PK_OK ? */\n\t\t/*\n\t\t * XXX this allows testing whether a user is allowed\n\t\t * to login: if you happen to have a valid pubkey this\n\t\t * message is sent. the message is NEVER sent at all\n\t\t * if a user is not allowed to login. is this an\n\t\t * issue? -markus\n\t\t */\n\t\tif (PRIVSEP(user_key_allowed(ssh, pw, key, 0, NULL))) {\n\t\t\tif ((r = sshpkt_start(ssh, SSH2_MSG_USERAUTH_PK_OK))\n\t\t\t    != 0 ||\n\t\t\t    (r = sshpkt_put_cstring(ssh, pkalg)) != 0 ||\n\t\t\t    (r = sshpkt_put_string(ssh, pkblob, blen)) != 0 ||\n\t\t\t    (r = sshpkt_send(ssh)) != 0 ||\n\t\t\t    (r = ssh_packet_write_wait(ssh)) != 0)\n\t\t\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\t\t\tauthctxt->postponed = 1;\n\t\t}\n\t}\ndone:\n\tif (authenticated == 1 && auth_activate_options(ssh, authopts) != 0) {\n\t\tdebug(\"%s: key options inconsistent with existing\", __func__);\n\t\tauthenticated = 0;\n\t}\n\tdebug2(\"%s: authenticated %d pkalg %s\", __func__, authenticated, pkalg);\n\n\tsshauthopt_free(authopts);\n\tsshkey_free(key);\n\tfree(userstyle);\n\tfree(pkalg);\n\tfree(pkblob);\n\tfree(key_s);\n\tfree(ca_s);\n\treturn authenticated;\n}",
        "func": "static int\nuserauth_pubkey(struct ssh *ssh)\n{\n\tAuthctxt *authctxt = ssh->authctxt;\n\tstruct passwd *pw = authctxt->pw;\n\tstruct sshbuf *b = NULL;\n\tstruct sshkey *key = NULL;\n\tchar *pkalg = NULL, *userstyle = NULL, *key_s = NULL, *ca_s = NULL;\n\tu_char *pkblob = NULL, *sig = NULL, have_sig;\n\tsize_t blen, slen;\n\tint r, pktype;\n\tint authenticated = 0;\n\tstruct sshauthopt *authopts = NULL;\n\n\tif ((r = sshpkt_get_u8(ssh, &have_sig)) != 0 ||\n\t    (r = sshpkt_get_cstring(ssh, &pkalg, NULL)) != 0 ||\n\t    (r = sshpkt_get_string(ssh, &pkblob, &blen)) != 0)\n\t\tfatal(\"%s: parse request failed: %s\", __func__, ssh_err(r));\n\tpktype = sshkey_type_from_name(pkalg);\n\tif (pktype == KEY_UNSPEC) {\n\t\t/* this is perfectly legal */\n\t\tverbose(\"%s: unsupported public key algorithm: %s\",\n\t\t    __func__, pkalg);\n\t\tgoto done;\n\t}\n\tif ((r = sshkey_from_blob(pkblob, blen, &key)) != 0) {\n\t\terror(\"%s: could not parse key: %s\", __func__, ssh_err(r));\n\t\tgoto done;\n\t}\n\tif (key == NULL) {\n\t\terror(\"%s: cannot decode key: %s\", __func__, pkalg);\n\t\tgoto done;\n\t}\n\tif (key->type != pktype) {\n\t\terror(\"%s: type mismatch for decoded key \"\n\t\t    \"(received %d, expected %d)\", __func__, key->type, pktype);\n\t\tgoto done;\n\t}\n\tif (sshkey_type_plain(key->type) == KEY_RSA &&\n\t    (ssh->compat & SSH_BUG_RSASIGMD5) != 0) {\n\t\tlogit(\"Refusing RSA key because client uses unsafe \"\n\t\t    \"signature scheme\");\n\t\tgoto done;\n\t}\n\tif (auth2_key_already_used(authctxt, key)) {\n\t\tlogit(\"refusing previously-used %s key\", sshkey_type(key));\n\t\tgoto done;\n\t}\n\tif (match_pattern_list(pkalg, options.pubkey_key_types, 0) != 1) {\n\t\tlogit(\"%s: key type %s not in PubkeyAcceptedKeyTypes\",\n\t\t    __func__, sshkey_ssh_name(key));\n\t\tgoto done;\n\t}\n\n\tkey_s = format_key(key);\n\tif (sshkey_is_cert(key))\n\t\tca_s = format_key(key->cert->signature_key);\n\n\tif (have_sig) {\n\t\tdebug3(\"%s: have %s signature for %s%s%s\",\n\t\t    __func__, pkalg, key_s,\n\t\t    ca_s == NULL ? \"\" : \" CA \",\n\t\t    ca_s == NULL ? \"\" : ca_s);\n\t\tif ((r = sshpkt_get_string(ssh, &sig, &slen)) != 0 ||\n\t\t    (r = sshpkt_get_end(ssh)) != 0)\n\t\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\t\tif ((b = sshbuf_new()) == NULL)\n\t\t\tfatal(\"%s: sshbuf_new failed\", __func__);\n\t\tif (ssh->compat & SSH_OLD_SESSIONID) {\n\t\t\tif ((r = sshbuf_put(b, session_id2,\n\t\t\t    session_id2_len)) != 0)\n\t\t\t\tfatal(\"%s: sshbuf_put session id: %s\",\n\t\t\t\t    __func__, ssh_err(r));\n\t\t} else {\n\t\t\tif ((r = sshbuf_put_string(b, session_id2,\n\t\t\t    session_id2_len)) != 0)\n\t\t\t\tfatal(\"%s: sshbuf_put_string session id: %s\",\n\t\t\t\t    __func__, ssh_err(r));\n\t\t}\n\t\tif (!authctxt->valid || authctxt->user == NULL) {\n\t\t\tdebug2(\"%s: disabled because of invalid user\",\n\t\t\t    __func__);\n\t\t\tgoto done;\n\t\t}\n\t\t/* reconstruct packet */\n\t\txasprintf(&userstyle, \"%s%s%s\", authctxt->user,\n\t\t    authctxt->style ? \":\" : \"\",\n\t\t    authctxt->style ? authctxt->style : \"\");\n\t\tif ((r = sshbuf_put_u8(b, SSH2_MSG_USERAUTH_REQUEST)) != 0 ||\n\t\t    (r = sshbuf_put_cstring(b, userstyle)) != 0 ||\n\t\t    (r = sshbuf_put_cstring(b, authctxt->service)) != 0 ||\n\t\t    (r = sshbuf_put_cstring(b, \"publickey\")) != 0 ||\n\t\t    (r = sshbuf_put_u8(b, have_sig)) != 0 ||\n\t\t    (r = sshbuf_put_cstring(b, pkalg) != 0) ||\n\t\t    (r = sshbuf_put_string(b, pkblob, blen)) != 0)\n\t\t\tfatal(\"%s: build packet failed: %s\",\n\t\t\t    __func__, ssh_err(r));\n#ifdef DEBUG_PK\n\t\tsshbuf_dump(b, stderr);\n#endif\n\t\t/* test for correct signature */\n\t\tauthenticated = 0;\n\t\tif (PRIVSEP(user_key_allowed(ssh, pw, key, 1, &authopts)) &&\n\t\t    PRIVSEP(sshkey_verify(key, sig, slen,\n\t\t    sshbuf_ptr(b), sshbuf_len(b),\n\t\t    (ssh->compat & SSH_BUG_SIGTYPE) == 0 ? pkalg : NULL,\n\t\t    ssh->compat)) == 0) {\n\t\t\tauthenticated = 1;\n\t\t}\n\t\tsshbuf_free(b);\n\t\tauth2_record_key(authctxt, authenticated, key);\n\t} else {\n\t\tdebug(\"%s: test pkalg %s pkblob %s%s%s\",\n\t\t    __func__, pkalg, key_s,\n\t\t    ca_s == NULL ? \"\" : \" CA \",\n\t\t    ca_s == NULL ? \"\" : ca_s);\n\n\t\tif ((r = sshpkt_get_end(ssh)) != 0)\n\t\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\n\t\tif (!authctxt->valid || authctxt->user == NULL) {\n\t\t\tdebug2(\"%s: disabled because of invalid user\",\n\t\t\t    __func__);\n\t\t\tgoto done;\n\t\t}\n\t\t/* XXX fake reply and always send PK_OK ? */\n\t\t/*\n\t\t * XXX this allows testing whether a user is allowed\n\t\t * to login: if you happen to have a valid pubkey this\n\t\t * message is sent. the message is NEVER sent at all\n\t\t * if a user is not allowed to login. is this an\n\t\t * issue? -markus\n\t\t */\n\t\tif (PRIVSEP(user_key_allowed(ssh, pw, key, 0, NULL))) {\n\t\t\tif ((r = sshpkt_start(ssh, SSH2_MSG_USERAUTH_PK_OK))\n\t\t\t    != 0 ||\n\t\t\t    (r = sshpkt_put_cstring(ssh, pkalg)) != 0 ||\n\t\t\t    (r = sshpkt_put_string(ssh, pkblob, blen)) != 0 ||\n\t\t\t    (r = sshpkt_send(ssh)) != 0 ||\n\t\t\t    (r = ssh_packet_write_wait(ssh)) != 0)\n\t\t\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n\t\t\tauthctxt->postponed = 1;\n\t\t}\n\t}\ndone:\n\tif (authenticated == 1 && auth_activate_options(ssh, authopts) != 0) {\n\t\tdebug(\"%s: key options inconsistent with existing\", __func__);\n\t\tauthenticated = 0;\n\t}\n\tdebug2(\"%s: authenticated %d pkalg %s\", __func__, authenticated, pkalg);\n\n\tsshauthopt_free(authopts);\n\tsshkey_free(key);\n\tfree(userstyle);\n\tfree(pkalg);\n\tfree(pkblob);\n\tfree(key_s);\n\tfree(ca_s);\n\tfree(sig);\n\treturn authenticated;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,19 +3,15 @@\n {\n \tAuthctxt *authctxt = ssh->authctxt;\n \tstruct passwd *pw = authctxt->pw;\n-\tstruct sshbuf *b;\n+\tstruct sshbuf *b = NULL;\n \tstruct sshkey *key = NULL;\n-\tchar *pkalg, *userstyle = NULL, *key_s = NULL, *ca_s = NULL;\n-\tu_char *pkblob, *sig, have_sig;\n+\tchar *pkalg = NULL, *userstyle = NULL, *key_s = NULL, *ca_s = NULL;\n+\tu_char *pkblob = NULL, *sig = NULL, have_sig;\n \tsize_t blen, slen;\n \tint r, pktype;\n \tint authenticated = 0;\n \tstruct sshauthopt *authopts = NULL;\n \n-\tif (!authctxt->valid) {\n-\t\tdebug2(\"%s: disabled because of invalid user\", __func__);\n-\t\treturn 0;\n-\t}\n \tif ((r = sshpkt_get_u8(ssh, &have_sig)) != 0 ||\n \t    (r = sshpkt_get_cstring(ssh, &pkalg, NULL)) != 0 ||\n \t    (r = sshpkt_get_string(ssh, &pkblob, &blen)) != 0)\n@@ -81,6 +77,11 @@\n \t\t\t\tfatal(\"%s: sshbuf_put_string session id: %s\",\n \t\t\t\t    __func__, ssh_err(r));\n \t\t}\n+\t\tif (!authctxt->valid || authctxt->user == NULL) {\n+\t\t\tdebug2(\"%s: disabled because of invalid user\",\n+\t\t\t    __func__);\n+\t\t\tgoto done;\n+\t\t}\n \t\t/* reconstruct packet */\n \t\txasprintf(&userstyle, \"%s%s%s\", authctxt->user,\n \t\t    authctxt->style ? \":\" : \"\",\n@@ -97,7 +98,6 @@\n #ifdef DEBUG_PK\n \t\tsshbuf_dump(b, stderr);\n #endif\n-\n \t\t/* test for correct signature */\n \t\tauthenticated = 0;\n \t\tif (PRIVSEP(user_key_allowed(ssh, pw, key, 1, &authopts)) &&\n@@ -108,7 +108,6 @@\n \t\t\tauthenticated = 1;\n \t\t}\n \t\tsshbuf_free(b);\n-\t\tfree(sig);\n \t\tauth2_record_key(authctxt, authenticated, key);\n \t} else {\n \t\tdebug(\"%s: test pkalg %s pkblob %s%s%s\",\n@@ -119,6 +118,11 @@\n \t\tif ((r = sshpkt_get_end(ssh)) != 0)\n \t\t\tfatal(\"%s: %s\", __func__, ssh_err(r));\n \n+\t\tif (!authctxt->valid || authctxt->user == NULL) {\n+\t\t\tdebug2(\"%s: disabled because of invalid user\",\n+\t\t\t    __func__);\n+\t\t\tgoto done;\n+\t\t}\n \t\t/* XXX fake reply and always send PK_OK ? */\n \t\t/*\n \t\t * XXX this allows testing whether a user is allowed\n@@ -152,5 +156,6 @@\n \tfree(pkblob);\n \tfree(key_s);\n \tfree(ca_s);\n+\tfree(sig);\n \treturn authenticated;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct sshbuf *b;",
                "\tchar *pkalg, *userstyle = NULL, *key_s = NULL, *ca_s = NULL;",
                "\tu_char *pkblob, *sig, have_sig;",
                "\tif (!authctxt->valid) {",
                "\t\tdebug2(\"%s: disabled because of invalid user\", __func__);",
                "\t\treturn 0;",
                "\t}",
                "",
                "\t\tfree(sig);"
            ],
            "added_lines": [
                "\tstruct sshbuf *b = NULL;",
                "\tchar *pkalg = NULL, *userstyle = NULL, *key_s = NULL, *ca_s = NULL;",
                "\tu_char *pkblob = NULL, *sig = NULL, have_sig;",
                "\t\tif (!authctxt->valid || authctxt->user == NULL) {",
                "\t\t\tdebug2(\"%s: disabled because of invalid user\",",
                "\t\t\t    __func__);",
                "\t\t\tgoto done;",
                "\t\t}",
                "\t\tif (!authctxt->valid || authctxt->user == NULL) {",
                "\t\t\tdebug2(\"%s: disabled because of invalid user\",",
                "\t\t\t    __func__);",
                "\t\t\tgoto done;",
                "\t\t}",
                "\tfree(sig);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-15473",
        "func_name": "openbsd/src/userauth_hostbased",
        "description": "OpenSSH through 7.7 is prone to a user enumeration vulnerability due to not delaying bailout for an invalid authenticating user until after the packet containing the request has been fully parsed, related to auth2-gss.c, auth2-hostbased.c, and auth2-pubkey.c.",
        "git_url": "https://github.com/openbsd/src/commit/779974d35b4859c07bc3cb8a12c74b43b0a7d1e0",
        "commit_title": "delay bailout for invalid authenticating user until after the packet",
        "commit_text": "containing the request has been fully parsed. Reported by Dariusz Tytko and Michał Sajdak; ok deraadt",
        "func_before": "static int\nuserauth_hostbased(struct ssh *ssh)\n{\n\tAuthctxt *authctxt = ssh->authctxt;\n\tstruct sshbuf *b;\n\tstruct sshkey *key = NULL;\n\tchar *pkalg, *cuser, *chost;\n\tu_char *pkblob, *sig;\n\tsize_t alen, blen, slen;\n\tint r, pktype, authenticated = 0;\n\n\tif (!authctxt->valid) {\n\t\tdebug2(\"%s: disabled because of invalid user\", __func__);\n\t\treturn 0;\n\t}\n\t/* XXX use sshkey_froms() */\n\tif ((r = sshpkt_get_cstring(ssh, &pkalg, &alen)) != 0 ||\n\t    (r = sshpkt_get_string(ssh, &pkblob, &blen)) != 0 ||\n\t    (r = sshpkt_get_cstring(ssh, &chost, NULL)) != 0 ||\n\t    (r = sshpkt_get_cstring(ssh, &cuser, NULL)) != 0 ||\n\t    (r = sshpkt_get_string(ssh, &sig, &slen)) != 0)\n\t\tfatal(\"%s: packet parsing: %s\", __func__, ssh_err(r));\n\n\tdebug(\"%s: cuser %s chost %s pkalg %s slen %zu\", __func__,\n\t    cuser, chost, pkalg, slen);\n#ifdef DEBUG_PK\n\tdebug(\"signature:\");\n\tsshbuf_dump_data(sig, siglen, stderr);\n#endif\n\tpktype = sshkey_type_from_name(pkalg);\n\tif (pktype == KEY_UNSPEC) {\n\t\t/* this is perfectly legal */\n\t\tlogit(\"%s: unsupported public key algorithm: %s\",\n\t\t    __func__, pkalg);\n\t\tgoto done;\n\t}\n\tif ((r = sshkey_from_blob(pkblob, blen, &key)) != 0) {\n\t\terror(\"%s: key_from_blob: %s\", __func__, ssh_err(r));\n\t\tgoto done;\n\t}\n\tif (key == NULL) {\n\t\terror(\"%s: cannot decode key: %s\", __func__, pkalg);\n\t\tgoto done;\n\t}\n\tif (key->type != pktype) {\n\t\terror(\"%s: type mismatch for decoded key \"\n\t\t    \"(received %d, expected %d)\", __func__, key->type, pktype);\n\t\tgoto done;\n\t}\n\tif (sshkey_type_plain(key->type) == KEY_RSA &&\n\t    (ssh->compat & SSH_BUG_RSASIGMD5) != 0) {\n\t\terror(\"Refusing RSA key because peer uses unsafe \"\n\t\t    \"signature format\");\n\t\tgoto done;\n\t}\n\tif (match_pattern_list(pkalg, options.hostbased_key_types, 0) != 1) {\n\t\tlogit(\"%s: key type %s not in HostbasedAcceptedKeyTypes\",\n\t\t    __func__, sshkey_type(key));\n\t\tgoto done;\n\t}\n\n\tif ((b = sshbuf_new()) == NULL)\n\t\tfatal(\"%s: sshbuf_new failed\", __func__);\n\t/* reconstruct packet */\n\tif ((r = sshbuf_put_string(b, session_id2, session_id2_len)) != 0 ||\n\t    (r = sshbuf_put_u8(b, SSH2_MSG_USERAUTH_REQUEST)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, authctxt->user)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, authctxt->service)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, \"hostbased\")) != 0 ||\n\t    (r = sshbuf_put_string(b, pkalg, alen)) != 0 ||\n\t    (r = sshbuf_put_string(b, pkblob, blen)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, chost)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, cuser)) != 0)\n\t\tfatal(\"%s: buffer error: %s\", __func__, ssh_err(r));\n#ifdef DEBUG_PK\n\tsshbuf_dump(b, stderr);\n#endif\n\n\tauth2_record_info(authctxt,\n\t    \"client user \\\"%.100s\\\", client host \\\"%.100s\\\"\", cuser, chost);\n\n\t/* test for allowed key and correct signature */\n\tauthenticated = 0;\n\tif (PRIVSEP(hostbased_key_allowed(authctxt->pw, cuser, chost, key)) &&\n\t    PRIVSEP(sshkey_verify(key, sig, slen,\n\t    sshbuf_ptr(b), sshbuf_len(b), pkalg, ssh->compat)) == 0)\n\t\tauthenticated = 1;\n\n\tauth2_record_key(authctxt, authenticated, key);\n\tsshbuf_free(b);\ndone:\n\tdebug2(\"%s: authenticated %d\", __func__, authenticated);\n\tsshkey_free(key);\n\tfree(pkalg);\n\tfree(pkblob);\n\tfree(cuser);\n\tfree(chost);\n\tfree(sig);\n\treturn authenticated;\n}",
        "func": "static int\nuserauth_hostbased(struct ssh *ssh)\n{\n\tAuthctxt *authctxt = ssh->authctxt;\n\tstruct sshbuf *b;\n\tstruct sshkey *key = NULL;\n\tchar *pkalg, *cuser, *chost;\n\tu_char *pkblob, *sig;\n\tsize_t alen, blen, slen;\n\tint r, pktype, authenticated = 0;\n\n\t/* XXX use sshkey_froms() */\n\tif ((r = sshpkt_get_cstring(ssh, &pkalg, &alen)) != 0 ||\n\t    (r = sshpkt_get_string(ssh, &pkblob, &blen)) != 0 ||\n\t    (r = sshpkt_get_cstring(ssh, &chost, NULL)) != 0 ||\n\t    (r = sshpkt_get_cstring(ssh, &cuser, NULL)) != 0 ||\n\t    (r = sshpkt_get_string(ssh, &sig, &slen)) != 0)\n\t\tfatal(\"%s: packet parsing: %s\", __func__, ssh_err(r));\n\n\tdebug(\"%s: cuser %s chost %s pkalg %s slen %zu\", __func__,\n\t    cuser, chost, pkalg, slen);\n#ifdef DEBUG_PK\n\tdebug(\"signature:\");\n\tsshbuf_dump_data(sig, siglen, stderr);\n#endif\n\tpktype = sshkey_type_from_name(pkalg);\n\tif (pktype == KEY_UNSPEC) {\n\t\t/* this is perfectly legal */\n\t\tlogit(\"%s: unsupported public key algorithm: %s\",\n\t\t    __func__, pkalg);\n\t\tgoto done;\n\t}\n\tif ((r = sshkey_from_blob(pkblob, blen, &key)) != 0) {\n\t\terror(\"%s: key_from_blob: %s\", __func__, ssh_err(r));\n\t\tgoto done;\n\t}\n\tif (key == NULL) {\n\t\terror(\"%s: cannot decode key: %s\", __func__, pkalg);\n\t\tgoto done;\n\t}\n\tif (key->type != pktype) {\n\t\terror(\"%s: type mismatch for decoded key \"\n\t\t    \"(received %d, expected %d)\", __func__, key->type, pktype);\n\t\tgoto done;\n\t}\n\tif (sshkey_type_plain(key->type) == KEY_RSA &&\n\t    (ssh->compat & SSH_BUG_RSASIGMD5) != 0) {\n\t\terror(\"Refusing RSA key because peer uses unsafe \"\n\t\t    \"signature format\");\n\t\tgoto done;\n\t}\n\tif (match_pattern_list(pkalg, options.hostbased_key_types, 0) != 1) {\n\t\tlogit(\"%s: key type %s not in HostbasedAcceptedKeyTypes\",\n\t\t    __func__, sshkey_type(key));\n\t\tgoto done;\n\t}\n\n\tif (!authctxt->valid || authctxt->user == NULL) {\n\t\tdebug2(\"%s: disabled because of invalid user\", __func__);\n\t\tgoto done;\n\t}\n\n\tif ((b = sshbuf_new()) == NULL)\n\t\tfatal(\"%s: sshbuf_new failed\", __func__);\n\t/* reconstruct packet */\n\tif ((r = sshbuf_put_string(b, session_id2, session_id2_len)) != 0 ||\n\t    (r = sshbuf_put_u8(b, SSH2_MSG_USERAUTH_REQUEST)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, authctxt->user)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, authctxt->service)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, \"hostbased\")) != 0 ||\n\t    (r = sshbuf_put_string(b, pkalg, alen)) != 0 ||\n\t    (r = sshbuf_put_string(b, pkblob, blen)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, chost)) != 0 ||\n\t    (r = sshbuf_put_cstring(b, cuser)) != 0)\n\t\tfatal(\"%s: buffer error: %s\", __func__, ssh_err(r));\n#ifdef DEBUG_PK\n\tsshbuf_dump(b, stderr);\n#endif\n\n\tauth2_record_info(authctxt,\n\t    \"client user \\\"%.100s\\\", client host \\\"%.100s\\\"\", cuser, chost);\n\n\t/* test for allowed key and correct signature */\n\tauthenticated = 0;\n\tif (PRIVSEP(hostbased_key_allowed(authctxt->pw, cuser, chost, key)) &&\n\t    PRIVSEP(sshkey_verify(key, sig, slen,\n\t    sshbuf_ptr(b), sshbuf_len(b), pkalg, ssh->compat)) == 0)\n\t\tauthenticated = 1;\n\n\tauth2_record_key(authctxt, authenticated, key);\n\tsshbuf_free(b);\ndone:\n\tdebug2(\"%s: authenticated %d\", __func__, authenticated);\n\tsshkey_free(key);\n\tfree(pkalg);\n\tfree(pkblob);\n\tfree(cuser);\n\tfree(chost);\n\tfree(sig);\n\treturn authenticated;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,10 +9,6 @@\n \tsize_t alen, blen, slen;\n \tint r, pktype, authenticated = 0;\n \n-\tif (!authctxt->valid) {\n-\t\tdebug2(\"%s: disabled because of invalid user\", __func__);\n-\t\treturn 0;\n-\t}\n \t/* XXX use sshkey_froms() */\n \tif ((r = sshpkt_get_cstring(ssh, &pkalg, &alen)) != 0 ||\n \t    (r = sshpkt_get_string(ssh, &pkblob, &blen)) != 0 ||\n@@ -59,6 +55,11 @@\n \t\tgoto done;\n \t}\n \n+\tif (!authctxt->valid || authctxt->user == NULL) {\n+\t\tdebug2(\"%s: disabled because of invalid user\", __func__);\n+\t\tgoto done;\n+\t}\n+\n \tif ((b = sshbuf_new()) == NULL)\n \t\tfatal(\"%s: sshbuf_new failed\", __func__);\n \t/* reconstruct packet */",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!authctxt->valid) {",
                "\t\tdebug2(\"%s: disabled because of invalid user\", __func__);",
                "\t\treturn 0;",
                "\t}"
            ],
            "added_lines": [
                "\tif (!authctxt->valid || authctxt->user == NULL) {",
                "\t\tdebug2(\"%s: disabled because of invalid user\", __func__);",
                "\t\tgoto done;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19489",
        "func_name": "qemu/v9fs_wstat",
        "description": "v9fs_wstat in hw/9pfs/9p.c in QEMU allows guest OS users to cause a denial of service (crash) because of a race condition during file renaming.",
        "git_url": "https://github.com/qemu/qemu/commit/1d20398694a3b67a388d955b7a945ba4aa90a8a8",
        "commit_title": "9p: fix QEMU crash when renaming files",
        "commit_text": " When using the 9P2000.u version of the protocol, the following shell command line in the guest can cause QEMU to crash:      while true; do rm -rf aa; mkdir -p a/b & touch a/b/c & mv a aa; done  With 9P2000.u, file renaming is handled by the WSTAT command. The v9fs_wstat() function calls v9fs_complete_rename(), which calls v9fs_fix_path() for every fid whose path is affected by the change. The involved calls to v9fs_path_copy() may race with any other access to the fid path performed by some worker thread, causing a crash like shown below:  Thread 12 \"qemu-system-x86\" received signal SIGSEGV, Segmentation fault. 0x0000555555a25da2 in local_open_nofollow (fs_ctx=0x555557d958b8, path=0x0,  flags=65536, mode=0) at hw/9pfs/9p-local.c:59 59          while (*path && fd != -1) { (gdb) bt #0  0x0000555555a25da2 in local_open_nofollow (fs_ctx=0x555557d958b8,  path=0x0, flags=65536, mode=0) at hw/9pfs/9p-local.c:59 #1  0x0000555555a25e0c in local_opendir_nofollow (fs_ctx=0x555557d958b8,  path=0x0) at hw/9pfs/9p-local.c:92 #2  0x0000555555a261b8 in local_lstat (fs_ctx=0x555557d958b8,  fs_path=0x555556b56858, stbuf=0x7fff84830ef0) at hw/9pfs/9p-local.c:185 #3  0x0000555555a2b367 in v9fs_co_lstat (pdu=0x555557d97498,  path=0x555556b56858, stbuf=0x7fff84830ef0) at hw/9pfs/cofile.c:53 #4  0x0000555555a1e9e2 in v9fs_stat (opaque=0x555557d97498)  at hw/9pfs/9p.c:1083 #5  0x0000555555e060a2 in coroutine_trampoline (i0=-669165424, i1=32767)  at util/coroutine-ucontext.c:116 #6  0x00007fffef4f5600 in __start_context () at /lib64/libc.so.6 #7  0x0000000000000000 in  () (gdb)  The fix is to take the path write lock when calling v9fs_complete_rename(), like in v9fs_rename().  Impact:  DoS triggered by unprivileged guest users.  Cc: P J P <ppandit@redhat.com>",
        "func_before": "static void coroutine_fn v9fs_wstat(void *opaque)\n{\n    int32_t fid;\n    int err = 0;\n    int16_t unused;\n    V9fsStat v9stat;\n    size_t offset = 7;\n    struct stat stbuf;\n    V9fsFidState *fidp;\n    V9fsPDU *pdu = opaque;\n\n    v9fs_stat_init(&v9stat);\n    err = pdu_unmarshal(pdu, offset, \"dwS\", &fid, &unused, &v9stat);\n    if (err < 0) {\n        goto out_nofid;\n    }\n    trace_v9fs_wstat(pdu->tag, pdu->id, fid,\n                     v9stat.mode, v9stat.atime, v9stat.mtime);\n\n    fidp = get_fid(pdu, fid);\n    if (fidp == NULL) {\n        err = -EINVAL;\n        goto out_nofid;\n    }\n    /* do we need to sync the file? */\n    if (donttouch_stat(&v9stat)) {\n        err = v9fs_co_fsync(pdu, fidp, 0);\n        goto out;\n    }\n    if (v9stat.mode != -1) {\n        uint32_t v9_mode;\n        err = v9fs_co_lstat(pdu, &fidp->path, &stbuf);\n        if (err < 0) {\n            goto out;\n        }\n        v9_mode = stat_to_v9mode(&stbuf);\n        if ((v9stat.mode & P9_STAT_MODE_TYPE_BITS) !=\n            (v9_mode & P9_STAT_MODE_TYPE_BITS)) {\n            /* Attempting to change the type */\n            err = -EIO;\n            goto out;\n        }\n        err = v9fs_co_chmod(pdu, &fidp->path,\n                            v9mode_to_mode(v9stat.mode,\n                                           &v9stat.extension));\n        if (err < 0) {\n            goto out;\n        }\n    }\n    if (v9stat.mtime != -1 || v9stat.atime != -1) {\n        struct timespec times[2];\n        if (v9stat.atime != -1) {\n            times[0].tv_sec = v9stat.atime;\n            times[0].tv_nsec = 0;\n        } else {\n            times[0].tv_nsec = UTIME_OMIT;\n        }\n        if (v9stat.mtime != -1) {\n            times[1].tv_sec = v9stat.mtime;\n            times[1].tv_nsec = 0;\n        } else {\n            times[1].tv_nsec = UTIME_OMIT;\n        }\n        err = v9fs_co_utimensat(pdu, &fidp->path, times);\n        if (err < 0) {\n            goto out;\n        }\n    }\n    if (v9stat.n_gid != -1 || v9stat.n_uid != -1) {\n        err = v9fs_co_chown(pdu, &fidp->path, v9stat.n_uid, v9stat.n_gid);\n        if (err < 0) {\n            goto out;\n        }\n    }\n    if (v9stat.name.size != 0) {\n        err = v9fs_complete_rename(pdu, fidp, -1, &v9stat.name);\n        if (err < 0) {\n            goto out;\n        }\n    }\n    if (v9stat.length != -1) {\n        err = v9fs_co_truncate(pdu, &fidp->path, v9stat.length);\n        if (err < 0) {\n            goto out;\n        }\n    }\n    err = offset;\nout:\n    put_fid(pdu, fidp);\nout_nofid:\n    v9fs_stat_free(&v9stat);\n    pdu_complete(pdu, err);\n}",
        "func": "static void coroutine_fn v9fs_wstat(void *opaque)\n{\n    int32_t fid;\n    int err = 0;\n    int16_t unused;\n    V9fsStat v9stat;\n    size_t offset = 7;\n    struct stat stbuf;\n    V9fsFidState *fidp;\n    V9fsPDU *pdu = opaque;\n    V9fsState *s = pdu->s;\n\n    v9fs_stat_init(&v9stat);\n    err = pdu_unmarshal(pdu, offset, \"dwS\", &fid, &unused, &v9stat);\n    if (err < 0) {\n        goto out_nofid;\n    }\n    trace_v9fs_wstat(pdu->tag, pdu->id, fid,\n                     v9stat.mode, v9stat.atime, v9stat.mtime);\n\n    fidp = get_fid(pdu, fid);\n    if (fidp == NULL) {\n        err = -EINVAL;\n        goto out_nofid;\n    }\n    /* do we need to sync the file? */\n    if (donttouch_stat(&v9stat)) {\n        err = v9fs_co_fsync(pdu, fidp, 0);\n        goto out;\n    }\n    if (v9stat.mode != -1) {\n        uint32_t v9_mode;\n        err = v9fs_co_lstat(pdu, &fidp->path, &stbuf);\n        if (err < 0) {\n            goto out;\n        }\n        v9_mode = stat_to_v9mode(&stbuf);\n        if ((v9stat.mode & P9_STAT_MODE_TYPE_BITS) !=\n            (v9_mode & P9_STAT_MODE_TYPE_BITS)) {\n            /* Attempting to change the type */\n            err = -EIO;\n            goto out;\n        }\n        err = v9fs_co_chmod(pdu, &fidp->path,\n                            v9mode_to_mode(v9stat.mode,\n                                           &v9stat.extension));\n        if (err < 0) {\n            goto out;\n        }\n    }\n    if (v9stat.mtime != -1 || v9stat.atime != -1) {\n        struct timespec times[2];\n        if (v9stat.atime != -1) {\n            times[0].tv_sec = v9stat.atime;\n            times[0].tv_nsec = 0;\n        } else {\n            times[0].tv_nsec = UTIME_OMIT;\n        }\n        if (v9stat.mtime != -1) {\n            times[1].tv_sec = v9stat.mtime;\n            times[1].tv_nsec = 0;\n        } else {\n            times[1].tv_nsec = UTIME_OMIT;\n        }\n        err = v9fs_co_utimensat(pdu, &fidp->path, times);\n        if (err < 0) {\n            goto out;\n        }\n    }\n    if (v9stat.n_gid != -1 || v9stat.n_uid != -1) {\n        err = v9fs_co_chown(pdu, &fidp->path, v9stat.n_uid, v9stat.n_gid);\n        if (err < 0) {\n            goto out;\n        }\n    }\n    if (v9stat.name.size != 0) {\n        v9fs_path_write_lock(s);\n        err = v9fs_complete_rename(pdu, fidp, -1, &v9stat.name);\n        v9fs_path_unlock(s);\n        if (err < 0) {\n            goto out;\n        }\n    }\n    if (v9stat.length != -1) {\n        err = v9fs_co_truncate(pdu, &fidp->path, v9stat.length);\n        if (err < 0) {\n            goto out;\n        }\n    }\n    err = offset;\nout:\n    put_fid(pdu, fidp);\nout_nofid:\n    v9fs_stat_free(&v9stat);\n    pdu_complete(pdu, err);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,7 @@\n     struct stat stbuf;\n     V9fsFidState *fidp;\n     V9fsPDU *pdu = opaque;\n+    V9fsState *s = pdu->s;\n \n     v9fs_stat_init(&v9stat);\n     err = pdu_unmarshal(pdu, offset, \"dwS\", &fid, &unused, &v9stat);\n@@ -73,7 +74,9 @@\n         }\n     }\n     if (v9stat.name.size != 0) {\n+        v9fs_path_write_lock(s);\n         err = v9fs_complete_rename(pdu, fidp, -1, &v9stat.name);\n+        v9fs_path_unlock(s);\n         if (err < 0) {\n             goto out;\n         }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    V9fsState *s = pdu->s;",
                "        v9fs_path_write_lock(s);",
                "        v9fs_path_unlock(s);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5676",
        "func_name": "eclipse-openj9/openj9/predefinedHandlerWrapper",
        "description": "In Eclipse OpenJ9 before version 0.41.0, the JVM can be forced into an infinite busy hang on a spinlock or a segmentation fault if a shutdown signal (SIGTERM, SIGINT or SIGHUP) is received before the JVM has finished initializing.\n",
        "git_url": "https://github.com/eclipse-openj9/openj9/commit/d4dfa6c62270ea34fcc859355a66bc25b8175237",
        "commit_title": "Don't invoke shutdown signal handler until JVM init completes",
        "commit_text": " JVM init path: J9_CreateJavaVM. JVM exit paths: protectedDestroyJavaVM and exitJavaVM.  A segfault or other side-effects can happen if the JVM init and exit paths execute concurrently.  The exit path can be taken if a shutdown signal is raised and the shutdown handler is invoked. JVM shutdown signals are SIGINT, SIGTERM and SIGHUP.  Preventing invocation of the exit path from the shutdown signal handler until the JVM initialization completes resolves the above side-effects.  Related: - #17101 - #17438 ",
        "func_before": "static UDATA\npredefinedHandlerWrapper(struct J9PortLibrary *portLibrary, U_32 gpType, void *gpInfo, void *userData)\n{\n\tJ9JavaVM *vm = (J9JavaVM *)userData;\n\tJ9JavaVMAttachArgs attachArgs = {0};\n\tJ9VMThread *vmThread = NULL;\n\tIDATA result = JNI_ERR;\n\tBOOLEAN shutdownStarted = FALSE;\n\tI_32 signal = 0;\n\tPORT_ACCESS_FROM_JAVAVM(vm);\n\n\tsignal = j9sig_map_portlib_signal_to_os_signal(gpType);\n\t/* Don't invoke handler if signal is 0 or negative, or if -Xrs or -Xrs:async is specified */\n\tif ((signal <= 0) || J9_ARE_ANY_BITS_SET(vm->sigFlags, J9_SIG_XRS_ASYNC)) {\n\t\treturn 1;\n\t}\n\n\t/* Don't invoke handler if JVM exit has started. */\n\tomrthread_monitor_enter(vm->runtimeFlagsMutex);\n\tif (J9_ARE_ANY_BITS_SET(vm->runtimeFlags, J9_RUNTIME_EXIT_STARTED)) {\n\t\tshutdownStarted = TRUE;\n\t}\n\tomrthread_monitor_exit(vm->runtimeFlagsMutex);\n\n\tif (shutdownStarted) {\n\t\treturn 1;\n\t}\n\n\tattachArgs.version = JNI_VERSION_1_8;\n\tattachArgs.name = \"JVM Signal Thread\";\n\tattachArgs.group = vm->systemThreadGroupRef;\n\n\t/* Attach current thread as a daemon thread */\n\tresult = internalAttachCurrentThread(vm, &vmThread, &attachArgs,\n\t\t\t\tJ9_PRIVATE_FLAGS_DAEMON_THREAD | J9_PRIVATE_FLAGS_SYSTEM_THREAD | J9_PRIVATE_FLAGS_ATTACHED_THREAD,\n\t\t\t\tomrthread_self());\n\n\tif (JNI_OK != result) {\n\t\t/* Thread couldn't be attached. So, we can't run Java code. */\n\t\treturn 1;\n\t}\n\n\t/* Run handler (Java code). */\n\tsignalDispatch(vmThread, signal);\n\n\tDetachCurrentThread((JavaVM *)vm);\n\n\treturn 0;\n}",
        "func": "static UDATA\npredefinedHandlerWrapper(struct J9PortLibrary *portLibrary, U_32 gpType, void *gpInfo, void *userData)\n{\n\tJ9JavaVM *vm = (J9JavaVM *)userData;\n\tJ9JavaVMAttachArgs attachArgs = {0};\n\tJ9VMThread *vmThread = NULL;\n\tIDATA result = JNI_ERR;\n\tBOOLEAN invokeHandler = TRUE;\n\tI_32 signal = 0;\n\tU_32 runtimeFlags = 0;\n\tPORT_ACCESS_FROM_JAVAVM(vm);\n\n\tsignal = j9sig_map_portlib_signal_to_os_signal(gpType);\n\t/* Don't invoke handler if signal is 0 or negative, or if -Xrs or -Xrs:async is specified */\n\tif ((signal <= 0) || J9_ARE_ANY_BITS_SET(vm->sigFlags, J9_SIG_XRS_ASYNC)) {\n\t\treturn 1;\n\t}\n\n\t/* Don't invoke handler if JVM hasn't initialized or JVM exit has started. */\n\tissueReadBarrier();\n\truntimeFlags = vm->runtimeFlags;\n\tif (J9_ARE_NO_BITS_SET(runtimeFlags, J9_RUNTIME_INITIALIZED)\n\t|| J9_ARE_ANY_BITS_SET(runtimeFlags, J9_RUNTIME_EXIT_STARTED)\n\t) {\n\t\tinvokeHandler = FALSE;\n\t}\n\n\tif (!invokeHandler) {\n\t\treturn 1;\n\t}\n\n\tattachArgs.version = JNI_VERSION_1_8;\n\tattachArgs.name = \"JVM Signal Thread\";\n\tattachArgs.group = vm->systemThreadGroupRef;\n\n\t/* Attach current thread as a daemon thread */\n\tresult = internalAttachCurrentThread(vm, &vmThread, &attachArgs,\n\t\t\t\tJ9_PRIVATE_FLAGS_DAEMON_THREAD | J9_PRIVATE_FLAGS_SYSTEM_THREAD | J9_PRIVATE_FLAGS_ATTACHED_THREAD,\n\t\t\t\tomrthread_self());\n\n\tif (JNI_OK != result) {\n\t\t/* Thread couldn't be attached. So, we can't run Java code. */\n\t\treturn 1;\n\t}\n\n\t/* Run handler (Java code). */\n\tsignalDispatch(vmThread, signal);\n\n\tDetachCurrentThread((JavaVM *)vm);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,8 +5,9 @@\n \tJ9JavaVMAttachArgs attachArgs = {0};\n \tJ9VMThread *vmThread = NULL;\n \tIDATA result = JNI_ERR;\n-\tBOOLEAN shutdownStarted = FALSE;\n+\tBOOLEAN invokeHandler = TRUE;\n \tI_32 signal = 0;\n+\tU_32 runtimeFlags = 0;\n \tPORT_ACCESS_FROM_JAVAVM(vm);\n \n \tsignal = j9sig_map_portlib_signal_to_os_signal(gpType);\n@@ -15,14 +16,16 @@\n \t\treturn 1;\n \t}\n \n-\t/* Don't invoke handler if JVM exit has started. */\n-\tomrthread_monitor_enter(vm->runtimeFlagsMutex);\n-\tif (J9_ARE_ANY_BITS_SET(vm->runtimeFlags, J9_RUNTIME_EXIT_STARTED)) {\n-\t\tshutdownStarted = TRUE;\n+\t/* Don't invoke handler if JVM hasn't initialized or JVM exit has started. */\n+\tissueReadBarrier();\n+\truntimeFlags = vm->runtimeFlags;\n+\tif (J9_ARE_NO_BITS_SET(runtimeFlags, J9_RUNTIME_INITIALIZED)\n+\t|| J9_ARE_ANY_BITS_SET(runtimeFlags, J9_RUNTIME_EXIT_STARTED)\n+\t) {\n+\t\tinvokeHandler = FALSE;\n \t}\n-\tomrthread_monitor_exit(vm->runtimeFlagsMutex);\n \n-\tif (shutdownStarted) {\n+\tif (!invokeHandler) {\n \t\treturn 1;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tBOOLEAN shutdownStarted = FALSE;",
                "\t/* Don't invoke handler if JVM exit has started. */",
                "\tomrthread_monitor_enter(vm->runtimeFlagsMutex);",
                "\tif (J9_ARE_ANY_BITS_SET(vm->runtimeFlags, J9_RUNTIME_EXIT_STARTED)) {",
                "\t\tshutdownStarted = TRUE;",
                "\tomrthread_monitor_exit(vm->runtimeFlagsMutex);",
                "\tif (shutdownStarted) {"
            ],
            "added_lines": [
                "\tBOOLEAN invokeHandler = TRUE;",
                "\tU_32 runtimeFlags = 0;",
                "\t/* Don't invoke handler if JVM hasn't initialized or JVM exit has started. */",
                "\tissueReadBarrier();",
                "\truntimeFlags = vm->runtimeFlags;",
                "\tif (J9_ARE_NO_BITS_SET(runtimeFlags, J9_RUNTIME_INITIALIZED)",
                "\t|| J9_ARE_ANY_BITS_SET(runtimeFlags, J9_RUNTIME_EXIT_STARTED)",
                "\t) {",
                "\t\tinvokeHandler = FALSE;",
                "\tif (!invokeHandler) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-9962",
        "func_name": "opencontainers/runc/nsexec",
        "description": "RunC allowed additional container processes via 'runc exec' to be ptraced by the pid 1 of the container.  This allows the main processes of the container, if running as root, to gain access to file-descriptors of these new processes during the initialization and can lead to container escapes or modification of runC state before the process is fully placed inside the container.",
        "git_url": "https://github.com/opencontainers/runc/commit/50a19c6ff828c58e5dab13830bd3dacde268afe5",
        "commit_title": "Set init processes as non-dumpable",
        "commit_text": " This sets the init processes that join and setup the container's namespaces as non-dumpable before they setns to the container's pid (or any other ) namespace.  This settings is automatically reset to the default after the Exec in the container so that it does not change functionality for the applications that are running inside, just our init processes.  This prevents parent processes, the pid 1 of the container, to ptrace the init process before it drops caps and other sets LSMs.  This patch also ensures that the stateDirFD being used is still closed prior to exec, even though it is set as O_CLOEXEC, because of the order in the kernel.  https://github.com/torvalds/linux/blob/v4.9/fs/exec.c#L1290-L1318  The order during the exec syscall is that the process is set back to dumpable before O_CLOEXEC are processed. ",
        "func_before": "void nsexec(void)\n{\n\tint pipenum;\n\tjmp_buf env;\n\tint syncpipe[2];\n\tstruct nlconfig_t config = {0};\n\n\t/*\n\t * If we don't have an init pipe, just return to the go routine.\n\t * We'll only get an init pipe for start or exec.\n\t */\n\tpipenum = initpipe();\n\tif (pipenum == -1)\n\t\treturn;\n\n\t/* Parse all of the netlink configuration. */\n\tnl_parse(pipenum, &config);\n\n\t/* clone(2) flags are mandatory. */\n\tif (config.cloneflags == -1)\n\t\tbail(\"missing clone_flags\");\n\n\t/* Pipe so we can tell the child when we've finished setting up. */\n\tif (pipe(syncpipe) < 0)\n\t\tbail(\"failed to setup sync pipe between parent and child\");\n\n\t/* Set up the jump point. */\n\tif (setjmp(env) == JUMP_VAL) {\n\t\t/*\n\t\t * We're inside the child now, having jumped from the\n\t\t * start_child() code after forking in the parent.\n\t\t */\n\t\tuint8_t s = 0;\n\t\tint consolefd = config.consolefd;\n\n\t\t/* Close the writing side of pipe. */\n\t\tclose(syncpipe[1]);\n\n\t\t/* Sync with parent. */\n\t\tif (read(syncpipe[0], &s, sizeof(s)) != sizeof(s) || s != SYNC_VAL)\n\t\t\tbail(\"failed to read sync byte from parent\");\n\n\t\tif (setsid() < 0)\n\t\t\tbail(\"setsid failed\");\n\n\t\tif (setuid(0) < 0)\n\t\t\tbail(\"setuid failed\");\n\n\t\tif (setgid(0) < 0)\n\t\t\tbail(\"setgid failed\");\n\n\t\tif (setgroups(0, NULL) < 0)\n\t\t\tbail(\"setgroups failed\");\n\n\t\tif (consolefd != -1) {\n\t\t\tif (ioctl(consolefd, TIOCSCTTY, 0) < 0)\n\t\t\t\tbail(\"ioctl TIOCSCTTY failed\");\n\t\t\tif (dup3(consolefd, STDIN_FILENO, 0) != STDIN_FILENO)\n\t\t\t\tbail(\"failed to dup stdin\");\n\t\t\tif (dup3(consolefd, STDOUT_FILENO, 0) != STDOUT_FILENO)\n\t\t\t\tbail(\"failed to dup stdout\");\n\t\t\tif (dup3(consolefd, STDERR_FILENO, 0) != STDERR_FILENO)\n\t\t\t\tbail(\"failed to dup stderr\");\n\t\t}\n\n\t\t/* Free netlink data. */\n\t\tnl_free(&config);\n\n\t\t/* Finish executing, let the Go runtime take over. */\n\t\treturn;\n\t}\n\n\t/* Run the parent code. */\n\tstart_child(pipenum, &env, syncpipe, &config);\n\n\t/* Should never be reached. */\n\tbail(\"should never be reached\");\n}",
        "func": "void nsexec(void)\n{\n\tint pipenum;\n\tjmp_buf env;\n\tint syncpipe[2];\n\tstruct nlconfig_t config = {0};\n\n\t/*\n\t * If we don't have an init pipe, just return to the go routine.\n\t * We'll only get an init pipe for start or exec.\n\t */\n\tpipenum = initpipe();\n\tif (pipenum == -1)\n\t\treturn;\n\n\t/* make the process non-dumpable */\n\tif (prctl(PR_SET_DUMPABLE, 0, 0, 0, 0) != 0) {\n\t\tbail(\"failed to set process as non-dumpable\");\n\t}\n\n\t/* Parse all of the netlink configuration. */\n\tnl_parse(pipenum, &config);\n\n\t/* clone(2) flags are mandatory. */\n\tif (config.cloneflags == -1)\n\t\tbail(\"missing clone_flags\");\n\n\t/* Pipe so we can tell the child when we've finished setting up. */\n\tif (pipe(syncpipe) < 0)\n\t\tbail(\"failed to setup sync pipe between parent and child\");\n\n\t/* Set up the jump point. */\n\tif (setjmp(env) == JUMP_VAL) {\n\t\t/*\n\t\t * We're inside the child now, having jumped from the\n\t\t * start_child() code after forking in the parent.\n\t\t */\n\t\tuint8_t s = 0;\n\t\tint consolefd = config.consolefd;\n\n\t\t/* Close the writing side of pipe. */\n\t\tclose(syncpipe[1]);\n\n\t\t/* Sync with parent. */\n\t\tif (read(syncpipe[0], &s, sizeof(s)) != sizeof(s) || s != SYNC_VAL)\n\t\t\tbail(\"failed to read sync byte from parent\");\n\n\t\tif (setsid() < 0)\n\t\t\tbail(\"setsid failed\");\n\n\t\tif (setuid(0) < 0)\n\t\t\tbail(\"setuid failed\");\n\n\t\tif (setgid(0) < 0)\n\t\t\tbail(\"setgid failed\");\n\n\t\tif (setgroups(0, NULL) < 0)\n\t\t\tbail(\"setgroups failed\");\n\n\t\tif (consolefd != -1) {\n\t\t\tif (ioctl(consolefd, TIOCSCTTY, 0) < 0)\n\t\t\t\tbail(\"ioctl TIOCSCTTY failed\");\n\t\t\tif (dup3(consolefd, STDIN_FILENO, 0) != STDIN_FILENO)\n\t\t\t\tbail(\"failed to dup stdin\");\n\t\t\tif (dup3(consolefd, STDOUT_FILENO, 0) != STDOUT_FILENO)\n\t\t\t\tbail(\"failed to dup stdout\");\n\t\t\tif (dup3(consolefd, STDERR_FILENO, 0) != STDERR_FILENO)\n\t\t\t\tbail(\"failed to dup stderr\");\n\t\t}\n\n\t\t/* Free netlink data. */\n\t\tnl_free(&config);\n\n\t\t/* Finish executing, let the Go runtime take over. */\n\t\treturn;\n\t}\n\n\t/* Run the parent code. */\n\tstart_child(pipenum, &env, syncpipe, &config);\n\n\t/* Should never be reached. */\n\tbail(\"should never be reached\");\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,11 @@\n \tpipenum = initpipe();\n \tif (pipenum == -1)\n \t\treturn;\n+\n+\t/* make the process non-dumpable */\n+\tif (prctl(PR_SET_DUMPABLE, 0, 0, 0, 0) != 0) {\n+\t\tbail(\"failed to set process as non-dumpable\");\n+\t}\n \n \t/* Parse all of the netlink configuration. */\n \tnl_parse(pipenum, &config);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t/* make the process non-dumpable */",
                "\tif (prctl(PR_SET_DUMPABLE, 0, 0, 0, 0) != 0) {",
                "\t\tbail(\"failed to set process as non-dumpable\");",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9914",
        "func_name": "torvalds/linux/ip4_datagram_release_cb",
        "description": "Race condition in the ip4_datagram_release_cb function in net/ipv4/datagram.c in the Linux kernel before 3.15.2 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect expectations about locking during multithreaded access to internal data structures for IPv4 UDP sockets.",
        "git_url": "https://github.com/torvalds/linux/commit/9709674e68646cee5a24e3000b3558d25412203a",
        "commit_title": "ipv4: fix a race in ip4_datagram_release_cb()",
        "commit_text": " Alexey gave a AddressSanitizer[1] report that finally gave a good hint at where was the origin of various problems already reported by Dormando in the past [2]  Problem comes from the fact that UDP can have a lockless TX path, and concurrent threads can manipulate sk_dst_cache, while another thread, is holding socket lock and calls __sk_dst_set() in ip4_datagram_release_cb() (this was added in linux-3.8)  It seems that all we need to do is to use sk_dst_check() and sk_dst_set() so that all the writers hold same spinlock (sk->sk_dst_lock) to prevent corruptions.  TCP stack do not need this protection, as all sk_dst_cache writers hold the socket lock.  [1] https://code.google.com/p/address-sanitizer/wiki/AddressSanitizerForKernel  AddressSanitizer: heap-use-after-free in ipv4_dst_check Read of size 2 by thread T15453:  [<ffffffff817daa3a>] ipv4_dst_check+0x1a/0x90 ./net/ipv4/route.c:1116  [<ffffffff8175b789>] __sk_dst_check+0x89/0xe0 ./net/core/sock.c:531  [<ffffffff81830a36>] ip4_datagram_release_cb+0x46/0x390 ??:0  [<ffffffff8175eaea>] release_sock+0x17a/0x230 ./net/core/sock.c:2413  [<ffffffff81830882>] ip4_datagram_connect+0x462/0x5d0 ??:0  [<ffffffff81846d06>] inet_dgram_connect+0x76/0xd0 ./net/ipv4/af_inet.c:534  [<ffffffff817580ac>] SYSC_connect+0x15c/0x1c0 ./net/socket.c:1701  [<ffffffff817596ce>] SyS_connect+0xe/0x10 ./net/socket.c:1682  [<ffffffff818b0a29>] system_call_fastpath+0x16/0x1b ./arch/x86/kernel/entry_64.S:629  Freed by thread T15455:  [<ffffffff8178d9b8>] dst_destroy+0xa8/0x160 ./net/core/dst.c:251  [<ffffffff8178de25>] dst_release+0x45/0x80 ./net/core/dst.c:280  [<ffffffff818304c1>] ip4_datagram_connect+0xa1/0x5d0 ??:0  [<ffffffff81846d06>] inet_dgram_connect+0x76/0xd0 ./net/ipv4/af_inet.c:534  [<ffffffff817580ac>] SYSC_connect+0x15c/0x1c0 ./net/socket.c:1701  [<ffffffff817596ce>] SyS_connect+0xe/0x10 ./net/socket.c:1682  [<ffffffff818b0a29>] system_call_fastpath+0x16/0x1b ./arch/x86/kernel/entry_64.S:629  Allocated by thread T15453:  [<ffffffff8178d291>] dst_alloc+0x81/0x2b0 ./net/core/dst.c:171  [<ffffffff817db3b7>] rt_dst_alloc+0x47/0x50 ./net/ipv4/route.c:1406  [<     inlined    >] __ip_route_output_key+0x3e8/0xf70 __mkroute_output ./net/ipv4/route.c:1939  [<ffffffff817dde08>] __ip_route_output_key+0x3e8/0xf70 ./net/ipv4/route.c:2161  [<ffffffff817deb34>] ip_route_output_flow+0x14/0x30 ./net/ipv4/route.c:2249  [<ffffffff81830737>] ip4_datagram_connect+0x317/0x5d0 ??:0  [<ffffffff81846d06>] inet_dgram_connect+0x76/0xd0 ./net/ipv4/af_inet.c:534  [<ffffffff817580ac>] SYSC_connect+0x15c/0x1c0 ./net/socket.c:1701  [<ffffffff817596ce>] SyS_connect+0xe/0x10 ./net/socket.c:1682  [<ffffffff818b0a29>] system_call_fastpath+0x16/0x1b ./arch/x86/kernel/entry_64.S:629  [2] <4>[196727.311203] general protection fault: 0000 [#1] SMP <4>[196727.311224] Modules linked in: xt_TEE xt_dscp xt_DSCP macvlan bridge coretemp crc32_pclmul ghash_clmulni_intel gpio_ich microcode ipmi_watchdog ipmi_devintf sb_edac edac_core lpc_ich mfd_core tpm_tis tpm tpm_bios ipmi_si ipmi_msghandler isci igb libsas i2c_algo_bit ixgbe ptp pps_core mdio <4>[196727.311333] CPU: 17 PID: 0 Comm: swapper/17 Not tainted 3.10.26 #1 <4>[196727.311344] Hardware name: Supermicro X9DRi-LN4+/X9DR3-LN4+/X9DRi-LN4+/X9DR3-LN4+, BIOS 3.0 07/05/2013 <4>[196727.311364] task: ffff885e6f069700 ti: ffff885e6f072000 task.ti: ffff885e6f072000 <4>[196727.311377] RIP: 0010:[<ffffffff815f8c7f>]  [<ffffffff815f8c7f>] ipv4_dst_destroy+0x4f/0x80 <4>[196727.311399] RSP: 0018:ffff885effd23a70  EFLAGS: 00010282 <4>[196727.311409] RAX: dead000000200200 RBX: ffff8854c398ecc0 RCX: 0000000000000040 <4>[196727.311423] RDX: dead000000100100 RSI: dead000000100100 RDI: dead000000200200 <4>[196727.311437] RBP: ffff885effd23a80 R08: ffffffff815fd9e0 R09: ffff885d5a590800 <4>[196727.311451] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000 <4>[196727.311464] R13: ffffffff81c8c280 R14: 0000000000000000 R15: ffff880e85ee16ce <4>[196727.311510] FS:  0000000000000000(0000) GS:ffff885effd20000(0000) knlGS:0000000000000000 <4>[196727.311554] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 <4>[196727.311581] CR2: 00007a46751eb000 CR3: 0000005e65688000 CR4: 00000000000407e0 <4>[196727.311625] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 <4>[196727.311669] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400 <4>[196727.311713] Stack: <4>[196727.311733]  ffff8854c398ecc0 ffff8854c398ecc0 ffff885effd23ab0 ffffffff815b7f42 <4>[196727.311784]  ffff88be6595bc00 ffff8854c398ecc0 0000000000000000 ffff8854c398ecc0 <4>[196727.311834]  ffff885effd23ad0 ffffffff815b86c6 ffff885d5a590800 ffff8816827821c0 <4>[196727.311885] Call Trace: <4>[196727.311907]  <IRQ> <4>[196727.311912]  [<ffffffff815b7f42>] dst_destroy+0x32/0xe0 <4>[196727.311959]  [<ffffffff815b86c6>] dst_release+0x56/0x80 <4>[196727.311986]  [<ffffffff81620bd5>] tcp_v4_do_rcv+0x2a5/0x4a0 <4>[196727.312013]  [<ffffffff81622b5a>] tcp_v4_rcv+0x7da/0x820 <4>[196727.312041]  [<ffffffff815fd9e0>] ? ip_rcv_finish+0x360/0x360 <4>[196727.312070]  [<ffffffff815de02d>] ? nf_hook_slow+0x7d/0x150 <4>[196727.312097]  [<ffffffff815fd9e0>] ? ip_rcv_finish+0x360/0x360 <4>[196727.312125]  [<ffffffff815fda92>] ip_local_deliver_finish+0xb2/0x230 <4>[196727.312154]  [<ffffffff815fdd9a>] ip_local_deliver+0x4a/0x90 <4>[196727.312183]  [<ffffffff815fd799>] ip_rcv_finish+0x119/0x360 <4>[196727.312212]  [<ffffffff815fe00b>] ip_rcv+0x22b/0x340 <4>[196727.312242]  [<ffffffffa0339680>] ? macvlan_broadcast+0x160/0x160 [macvlan] <4>[196727.312275]  [<ffffffff815b0c62>] __netif_receive_skb_core+0x512/0x640 <4>[196727.312308]  [<ffffffff811427fb>] ? kmem_cache_alloc+0x13b/0x150 <4>[196727.312338]  [<ffffffff815b0db1>] __netif_receive_skb+0x21/0x70 <4>[196727.312368]  [<ffffffff815b0fa1>] netif_receive_skb+0x31/0xa0 <4>[196727.312397]  [<ffffffff815b1ae8>] napi_gro_receive+0xe8/0x140 <4>[196727.312433]  [<ffffffffa00274f1>] ixgbe_poll+0x551/0x11f0 [ixgbe] <4>[196727.312463]  [<ffffffff815fe00b>] ? ip_rcv+0x22b/0x340 <4>[196727.312491]  [<ffffffff815b1691>] net_rx_action+0x111/0x210 <4>[196727.312521]  [<ffffffff815b0db1>] ? __netif_receive_skb+0x21/0x70 <4>[196727.312552]  [<ffffffff810519d0>] __do_softirq+0xd0/0x270 <4>[196727.312583]  [<ffffffff816cef3c>] call_softirq+0x1c/0x30 <4>[196727.312613]  [<ffffffff81004205>] do_softirq+0x55/0x90 <4>[196727.312640]  [<ffffffff81051c85>] irq_exit+0x55/0x60 <4>[196727.312668]  [<ffffffff816cf5c3>] do_IRQ+0x63/0xe0 <4>[196727.312696]  [<ffffffff816c5aaa>] common_interrupt+0x6a/0x6a <4>[196727.312722]  <EOI> <1>[196727.313071] RIP  [<ffffffff815f8c7f>] ipv4_dst_destroy+0x4f/0x80 <4>[196727.313100]  RSP <ffff885effd23a70> <4>[196727.313377] ---[ end trace 64b3f14fae0f2e29 ]--- <0>[196727.380908] Kernel panic - not syncing: Fatal exception in interrupt  Cc: Steffen Klassert <steffen.klassert@secunet.com>",
        "func_before": "void ip4_datagram_release_cb(struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))\n\t\treturn;\n\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt))\n\t\t__sk_dst_set(sk, &rt->dst);\n\trcu_read_unlock();\n}",
        "func": "void ip4_datagram_release_cb(struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct dst_entry *dst;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\trcu_read_lock();\n\n\tdst = __sk_dst_get(sk);\n\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\n\tdst = !IS_ERR(rt) ? &rt->dst : NULL;\n\tsk_dst_set(sk, dst);\n\n\trcu_read_unlock();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,13 +3,17 @@\n \tconst struct inet_sock *inet = inet_sk(sk);\n \tconst struct ip_options_rcu *inet_opt;\n \t__be32 daddr = inet->inet_daddr;\n+\tstruct dst_entry *dst;\n \tstruct flowi4 fl4;\n \tstruct rtable *rt;\n \n-\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))\n+\trcu_read_lock();\n+\n+\tdst = __sk_dst_get(sk);\n+\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {\n+\t\trcu_read_unlock();\n \t\treturn;\n-\n-\trcu_read_lock();\n+\t}\n \tinet_opt = rcu_dereference(inet->inet_opt);\n \tif (inet_opt && inet_opt->opt.srr)\n \t\tdaddr = inet_opt->opt.faddr;\n@@ -17,7 +21,9 @@\n \t\t\t\t   inet->inet_saddr, inet->inet_dport,\n \t\t\t\t   inet->inet_sport, sk->sk_protocol,\n \t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n-\tif (!IS_ERR(rt))\n-\t\t__sk_dst_set(sk, &rt->dst);\n+\n+\tdst = !IS_ERR(rt) ? &rt->dst : NULL;\n+\tsk_dst_set(sk, dst);\n+\n \trcu_read_unlock();\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))",
                "",
                "\trcu_read_lock();",
                "\tif (!IS_ERR(rt))",
                "\t\t__sk_dst_set(sk, &rt->dst);"
            ],
            "added_lines": [
                "\tstruct dst_entry *dst;",
                "\trcu_read_lock();",
                "",
                "\tdst = __sk_dst_get(sk);",
                "\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {",
                "\t\trcu_read_unlock();",
                "\t}",
                "",
                "\tdst = !IS_ERR(rt) ? &rt->dst : NULL;",
                "\tsk_dst_set(sk, dst);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-5986",
        "func_name": "torvalds/linux/sctp_wait_for_sndbuf",
        "description": "Race condition in the sctp_wait_for_sndbuf function in net/sctp/socket.c in the Linux kernel before 4.9.11 allows local users to cause a denial of service (assertion failure and panic) via a multithreaded application that peels off an association in a certain buffer-full state.",
        "git_url": "https://github.com/torvalds/linux/commit/2dcab598484185dea7ec22219c76dcdd59e3cb90",
        "commit_title": "sctp: avoid BUG_ON on sctp_wait_for_sndbuf",
        "commit_text": " Alexander Popov reported that an application may trigger a BUG_ON in sctp_wait_for_sndbuf if the socket tx buffer is full, a thread is waiting on it to queue more data and meanwhile another thread peels off the association being used by the first thread.  This patch replaces the BUG_ON call with a proper error handling. It will return -EPIPE to the original sendmsg call, similarly to what would have been done if the association wasn't found in the first place. ",
        "func_before": "static int sctp_wait_for_sndbuf(struct sctp_association *asoc, long *timeo_p,\n\t\t\t\tsize_t msg_len)\n{\n\tstruct sock *sk = asoc->base.sk;\n\tint err = 0;\n\tlong current_timeo = *timeo_p;\n\tDEFINE_WAIT(wait);\n\n\tpr_debug(\"%s: asoc:%p, timeo:%ld, msg_len:%zu\\n\", __func__, asoc,\n\t\t *timeo_p, msg_len);\n\n\t/* Increment the association's refcnt.  */\n\tsctp_association_hold(asoc);\n\n\t/* Wait on the association specific sndbuf space. */\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(&asoc->wait, &wait,\n\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\tif (!*timeo_p)\n\t\t\tgoto do_nonblock;\n\t\tif (sk->sk_err || asoc->state >= SCTP_STATE_SHUTDOWN_PENDING ||\n\t\t    asoc->base.dead)\n\t\t\tgoto do_error;\n\t\tif (signal_pending(current))\n\t\t\tgoto do_interrupted;\n\t\tif (msg_len <= sctp_wspace(asoc))\n\t\t\tbreak;\n\n\t\t/* Let another process have a go.  Since we are going\n\t\t * to sleep anyway.\n\t\t */\n\t\trelease_sock(sk);\n\t\tcurrent_timeo = schedule_timeout(current_timeo);\n\t\tBUG_ON(sk != asoc->base.sk);\n\t\tlock_sock(sk);\n\n\t\t*timeo_p = current_timeo;\n\t}\n\nout:\n\tfinish_wait(&asoc->wait, &wait);\n\n\t/* Release the association's refcnt.  */\n\tsctp_association_put(asoc);\n\n\treturn err;\n\ndo_error:\n\terr = -EPIPE;\n\tgoto out;\n\ndo_interrupted:\n\terr = sock_intr_errno(*timeo_p);\n\tgoto out;\n\ndo_nonblock:\n\terr = -EAGAIN;\n\tgoto out;\n}",
        "func": "static int sctp_wait_for_sndbuf(struct sctp_association *asoc, long *timeo_p,\n\t\t\t\tsize_t msg_len)\n{\n\tstruct sock *sk = asoc->base.sk;\n\tint err = 0;\n\tlong current_timeo = *timeo_p;\n\tDEFINE_WAIT(wait);\n\n\tpr_debug(\"%s: asoc:%p, timeo:%ld, msg_len:%zu\\n\", __func__, asoc,\n\t\t *timeo_p, msg_len);\n\n\t/* Increment the association's refcnt.  */\n\tsctp_association_hold(asoc);\n\n\t/* Wait on the association specific sndbuf space. */\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(&asoc->wait, &wait,\n\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\tif (!*timeo_p)\n\t\t\tgoto do_nonblock;\n\t\tif (sk->sk_err || asoc->state >= SCTP_STATE_SHUTDOWN_PENDING ||\n\t\t    asoc->base.dead)\n\t\t\tgoto do_error;\n\t\tif (signal_pending(current))\n\t\t\tgoto do_interrupted;\n\t\tif (msg_len <= sctp_wspace(asoc))\n\t\t\tbreak;\n\n\t\t/* Let another process have a go.  Since we are going\n\t\t * to sleep anyway.\n\t\t */\n\t\trelease_sock(sk);\n\t\tcurrent_timeo = schedule_timeout(current_timeo);\n\t\tif (sk != asoc->base.sk)\n\t\t\tgoto do_error;\n\t\tlock_sock(sk);\n\n\t\t*timeo_p = current_timeo;\n\t}\n\nout:\n\tfinish_wait(&asoc->wait, &wait);\n\n\t/* Release the association's refcnt.  */\n\tsctp_association_put(asoc);\n\n\treturn err;\n\ndo_error:\n\terr = -EPIPE;\n\tgoto out;\n\ndo_interrupted:\n\terr = sock_intr_errno(*timeo_p);\n\tgoto out;\n\ndo_nonblock:\n\terr = -EAGAIN;\n\tgoto out;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -31,7 +31,8 @@\n \t\t */\n \t\trelease_sock(sk);\n \t\tcurrent_timeo = schedule_timeout(current_timeo);\n-\t\tBUG_ON(sk != asoc->base.sk);\n+\t\tif (sk != asoc->base.sk)\n+\t\t\tgoto do_error;\n \t\tlock_sock(sk);\n \n \t\t*timeo_p = current_timeo;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tBUG_ON(sk != asoc->base.sk);"
            ],
            "added_lines": [
                "\t\tif (sk != asoc->base.sk)",
                "\t\t\tgoto do_error;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-23133",
        "func_name": "torvalds/linux/sctp_close",
        "description": "A race condition in Linux kernel SCTP sockets (net/sctp/socket.c) before 5.12-rc8 can lead to kernel privilege escalation from the context of a network service or an unprivileged process. If sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock then an element is removed from the auto_asconf_splist list without any proper locking. This can be exploited by an attacker with network service privileges to escalate to root or from the context of an unprivileged user directly if a BPF_CGROUP_INET_SOCK_CREATE is attached which denies creation of some SCTP socket.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b166a20b07382b8bc1dcee2a448715c9c2c81b5b",
        "commit_title": "If sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock",
        "commit_text": "held and sp->do_auto_asconf is true, then an element is removed from the auto_asconf_splist without any proper locking.  This can happen in the following functions: 1. In sctp_accept, if sctp_sock_migrate fails. 2. In inet_create or inet6_create, if there is a bpf program    attached to BPF_CGROUP_INET_SOCK_CREATE which denies    creation of the sctp socket.  The bug is fixed by acquiring addr_wq_lock in sctp_destroy_sock instead of sctp_close.  This addresses CVE-2021-23133.  ",
        "func_before": "static void sctp_close(struct sock *sk, long timeout)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *asoc;\n\tstruct list_head *pos, *temp;\n\tunsigned int data_was_unread;\n\n\tpr_debug(\"%s: sk:%p, timeout:%ld\\n\", __func__, sk, timeout);\n\n\tlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tinet_sk_set_state(sk, SCTP_SS_CLOSING);\n\n\tep = sctp_sk(sk)->ep;\n\n\t/* Clean up any skbs sitting on the receive queue.  */\n\tdata_was_unread = sctp_queue_purge_ulpevents(&sk->sk_receive_queue);\n\tdata_was_unread += sctp_queue_purge_ulpevents(&sctp_sk(sk)->pd_lobby);\n\n\t/* Walk all associations on an endpoint.  */\n\tlist_for_each_safe(pos, temp, &ep->asocs) {\n\t\tasoc = list_entry(pos, struct sctp_association, asocs);\n\n\t\tif (sctp_style(sk, TCP)) {\n\t\t\t/* A closed association can still be in the list if\n\t\t\t * it belongs to a TCP-style listening socket that is\n\t\t\t * not yet accepted. If so, free it. If not, send an\n\t\t\t * ABORT or SHUTDOWN based on the linger options.\n\t\t\t */\n\t\t\tif (sctp_state(asoc, CLOSED)) {\n\t\t\t\tsctp_association_free(asoc);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (data_was_unread || !skb_queue_empty(&asoc->ulpq.lobby) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm_uo) ||\n\t\t    (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)) {\n\t\t\tstruct sctp_chunk *chunk;\n\n\t\t\tchunk = sctp_make_abort_user(asoc, NULL, 0);\n\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t} else\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t}\n\n\t/* On a TCP-style socket, block for at most linger_time if set. */\n\tif (sctp_style(sk, TCP) && timeout)\n\t\tsctp_wait_for_close(sk, timeout);\n\n\t/* This will run the backlog queue.  */\n\trelease_sock(sk);\n\n\t/* Supposedly, no process has access to the socket, but\n\t * the net layers still may.\n\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock\n\t * held and that should be grabbed before socket lock.\n\t */\n\tspin_lock_bh(&net->sctp.addr_wq_lock);\n\tbh_lock_sock_nested(sk);\n\n\t/* Hold the sock, since sk_common_release() will put sock_put()\n\t * and we have just a little more cleanup.\n\t */\n\tsock_hold(sk);\n\tsk_common_release(sk);\n\n\tbh_unlock_sock(sk);\n\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n\n\tsock_put(sk);\n\n\tSCTP_DBG_OBJCNT_DEC(sock);\n}",
        "func": "static void sctp_close(struct sock *sk, long timeout)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *asoc;\n\tstruct list_head *pos, *temp;\n\tunsigned int data_was_unread;\n\n\tpr_debug(\"%s: sk:%p, timeout:%ld\\n\", __func__, sk, timeout);\n\n\tlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tinet_sk_set_state(sk, SCTP_SS_CLOSING);\n\n\tep = sctp_sk(sk)->ep;\n\n\t/* Clean up any skbs sitting on the receive queue.  */\n\tdata_was_unread = sctp_queue_purge_ulpevents(&sk->sk_receive_queue);\n\tdata_was_unread += sctp_queue_purge_ulpevents(&sctp_sk(sk)->pd_lobby);\n\n\t/* Walk all associations on an endpoint.  */\n\tlist_for_each_safe(pos, temp, &ep->asocs) {\n\t\tasoc = list_entry(pos, struct sctp_association, asocs);\n\n\t\tif (sctp_style(sk, TCP)) {\n\t\t\t/* A closed association can still be in the list if\n\t\t\t * it belongs to a TCP-style listening socket that is\n\t\t\t * not yet accepted. If so, free it. If not, send an\n\t\t\t * ABORT or SHUTDOWN based on the linger options.\n\t\t\t */\n\t\t\tif (sctp_state(asoc, CLOSED)) {\n\t\t\t\tsctp_association_free(asoc);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (data_was_unread || !skb_queue_empty(&asoc->ulpq.lobby) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm_uo) ||\n\t\t    (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)) {\n\t\t\tstruct sctp_chunk *chunk;\n\n\t\t\tchunk = sctp_make_abort_user(asoc, NULL, 0);\n\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t} else\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t}\n\n\t/* On a TCP-style socket, block for at most linger_time if set. */\n\tif (sctp_style(sk, TCP) && timeout)\n\t\tsctp_wait_for_close(sk, timeout);\n\n\t/* This will run the backlog queue.  */\n\trelease_sock(sk);\n\n\t/* Supposedly, no process has access to the socket, but\n\t * the net layers still may.\n\t */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\n\t/* Hold the sock, since sk_common_release() will put sock_put()\n\t * and we have just a little more cleanup.\n\t */\n\tsock_hold(sk);\n\tsk_common_release(sk);\n\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\n\tsock_put(sk);\n\n\tSCTP_DBG_OBJCNT_DEC(sock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -55,11 +55,9 @@\n \n \t/* Supposedly, no process has access to the socket, but\n \t * the net layers still may.\n-\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock\n-\t * held and that should be grabbed before socket lock.\n \t */\n-\tspin_lock_bh(&net->sctp.addr_wq_lock);\n-\tbh_lock_sock_nested(sk);\n+\tlocal_bh_disable();\n+\tbh_lock_sock(sk);\n \n \t/* Hold the sock, since sk_common_release() will put sock_put()\n \t * and we have just a little more cleanup.\n@@ -68,7 +66,7 @@\n \tsk_common_release(sk);\n \n \tbh_unlock_sock(sk);\n-\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n+\tlocal_bh_enable();\n \n \tsock_put(sk);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock",
                "\t * held and that should be grabbed before socket lock.",
                "\tspin_lock_bh(&net->sctp.addr_wq_lock);",
                "\tbh_lock_sock_nested(sk);",
                "\tspin_unlock_bh(&net->sctp.addr_wq_lock);"
            ],
            "added_lines": [
                "\tlocal_bh_disable();",
                "\tbh_lock_sock(sk);",
                "\tlocal_bh_enable();"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-23133",
        "func_name": "torvalds/linux/sctp_destroy_sock",
        "description": "A race condition in Linux kernel SCTP sockets (net/sctp/socket.c) before 5.12-rc8 can lead to kernel privilege escalation from the context of a network service or an unprivileged process. If sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock then an element is removed from the auto_asconf_splist list without any proper locking. This can be exploited by an attacker with network service privileges to escalate to root or from the context of an unprivileged user directly if a BPF_CGROUP_INET_SOCK_CREATE is attached which denies creation of some SCTP socket.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b166a20b07382b8bc1dcee2a448715c9c2c81b5b",
        "commit_title": "If sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock",
        "commit_text": "held and sp->do_auto_asconf is true, then an element is removed from the auto_asconf_splist without any proper locking.  This can happen in the following functions: 1. In sctp_accept, if sctp_sock_migrate fails. 2. In inet_create or inet6_create, if there is a bpf program    attached to BPF_CGROUP_INET_SOCK_CREATE which denies    creation of the sctp socket.  The bug is fixed by acquiring addr_wq_lock in sctp_destroy_sock instead of sctp_close.  This addresses CVE-2021-23133.  ",
        "func_before": "static void sctp_destroy_sock(struct sock *sk)\n{\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\t/* Release our hold on the endpoint. */\n\tsp = sctp_sk(sk);\n\t/* This could happen during socket init, thus we bail out\n\t * early, since the rest of the below is not setup either.\n\t */\n\tif (sp->ep == NULL)\n\t\treturn;\n\n\tif (sp->do_auto_asconf) {\n\t\tsp->do_auto_asconf = 0;\n\t\tlist_del(&sp->auto_asconf_list);\n\t}\n\tsctp_endpoint_free(sp->ep);\n\tlocal_bh_disable();\n\tsk_sockets_allocated_dec(sk);\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\tlocal_bh_enable();\n}",
        "func": "static void sctp_destroy_sock(struct sock *sk)\n{\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\t/* Release our hold on the endpoint. */\n\tsp = sctp_sk(sk);\n\t/* This could happen during socket init, thus we bail out\n\t * early, since the rest of the below is not setup either.\n\t */\n\tif (sp->ep == NULL)\n\t\treturn;\n\n\tif (sp->do_auto_asconf) {\n\t\tsp->do_auto_asconf = 0;\n\t\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_del(&sp->auto_asconf_list);\n\t\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\t}\n\tsctp_endpoint_free(sp->ep);\n\tlocal_bh_disable();\n\tsk_sockets_allocated_dec(sk);\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\tlocal_bh_enable();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,9 @@\n \n \tif (sp->do_auto_asconf) {\n \t\tsp->do_auto_asconf = 0;\n+\t\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n \t\tlist_del(&sp->auto_asconf_list);\n+\t\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n \t}\n \tsctp_endpoint_free(sp->ep);\n \tlocal_bh_disable();",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);",
                "\t\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-23133",
        "func_name": "torvalds/linux/sctp_init_sock",
        "description": "A race condition in Linux kernel SCTP sockets (net/sctp/socket.c) before 5.12-rc8 can lead to kernel privilege escalation from the context of a network service or an unprivileged process. If sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock then an element is removed from the auto_asconf_splist list without any proper locking. This can be exploited by an attacker with network service privileges to escalate to root or from the context of an unprivileged user directly if a BPF_CGROUP_INET_SOCK_CREATE is attached which denies creation of some SCTP socket.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b166a20b07382b8bc1dcee2a448715c9c2c81b5b",
        "commit_title": "If sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock",
        "commit_text": "held and sp->do_auto_asconf is true, then an element is removed from the auto_asconf_splist without any proper locking.  This can happen in the following functions: 1. In sctp_accept, if sctp_sock_migrate fails. 2. In inet_create or inet6_create, if there is a bpf program    attached to BPF_CGROUP_INET_SOCK_CREATE which denies    creation of the sctp socket.  The bug is fixed by acquiring addr_wq_lock in sctp_destroy_sock instead of sctp_close.  This addresses CVE-2021-23133.  ",
        "func_before": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\tsk->sk_gso_type = SKB_GSO_SCTP;\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tsp->subscribe = 0;\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->udp_port    = htons(net->sctp.udp_port);\n\tsp->encap_port  = htons(net->sctp.encap_port);\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pf_retrans  = net->sctp.pf_retrans;\n\tsp->ps_retrans  = net->sctp.ps_retrans;\n\tsp->pf_expose   = net->sctp.pf_expose;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\tsp->default_ss = SCTP_SS_DEFAULT;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tsk_sockets_allocated_inc(sk);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\n\t/* Nothing can fail after this block, otherwise\n\t * sctp_destroy_sock() will be called without addr_wq_lock held\n\t */\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n\t} else {\n\t\tsp->do_auto_asconf = 0;\n\t}\n\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "func": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\tsk->sk_gso_type = SKB_GSO_SCTP;\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tsp->subscribe = 0;\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->udp_port    = htons(net->sctp.udp_port);\n\tsp->encap_port  = htons(net->sctp.encap_port);\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pf_retrans  = net->sctp.pf_retrans;\n\tsp->ps_retrans  = net->sctp.ps_retrans;\n\tsp->pf_expose   = net->sctp.pf_expose;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\tsp->default_ss = SCTP_SS_DEFAULT;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tsk_sockets_allocated_inc(sk);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n\t} else {\n\t\tsp->do_auto_asconf = 0;\n\t}\n\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -134,9 +134,6 @@\n \tsk_sockets_allocated_inc(sk);\n \tsock_prot_inuse_add(net, sk->sk_prot, 1);\n \n-\t/* Nothing can fail after this block, otherwise\n-\t * sctp_destroy_sock() will be called without addr_wq_lock held\n-\t */\n \tif (net->sctp.default_auto_asconf) {\n \t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n \t\tlist_add_tail(&sp->auto_asconf_list,",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* Nothing can fail after this block, otherwise",
                "\t * sctp_destroy_sock() will be called without addr_wq_lock held",
                "\t */"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2021-32399",
        "func_name": "torvalds/linux/hci_req_sync",
        "description": "net/bluetooth/hci_request.c in the Linux kernel through 5.12.2 has a race condition for removal of the HCI controller.",
        "git_url": "https://github.com/torvalds/linux/commit/e2cb6b891ad2b8caa9131e3be70f45243df82a80",
        "commit_title": "bluetooth: eliminate the potential race condition when removing the HCI controller",
        "commit_text": " There is a possible race condition vulnerability between issuing a HCI command and removing the cont.  Specifically, functions hci_req_sync() and hci_dev_do_close() can race each other like below:  thread-A in hci_req_sync()      |   thread-B in hci_dev_do_close()                                 |   hci_req_sync_lock(hdev); test_bit(HCI_UP, &hdev->flags); | ...                             |   test_and_clear_bit(HCI_UP, &hdev->flags) hci_req_sync_lock(hdev);        |                                 | In this commit we alter the sequence in function hci_req_sync(). Hence, the thread-A cannot issue th.  Cc: Marcel Holtmann <marcel@holtmann.org>",
        "func_before": "int hci_req_sync(struct hci_dev *hdev, int (*req)(struct hci_request *req,\n\t\t\t\t\t\t  unsigned long opt),\n\t\t unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tint ret;\n\n\tif (!test_bit(HCI_UP, &hdev->flags))\n\t\treturn -ENETDOWN;\n\n\t/* Serialize all requests */\n\thci_req_sync_lock(hdev);\n\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n\thci_req_sync_unlock(hdev);\n\n\treturn ret;\n}",
        "func": "int hci_req_sync(struct hci_dev *hdev, int (*req)(struct hci_request *req,\n\t\t\t\t\t\t  unsigned long opt),\n\t\t unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tint ret;\n\n\t/* Serialize all requests */\n\thci_req_sync_lock(hdev);\n\t/* check the state after obtaing the lock to protect the HCI_UP\n\t * against any races from hci_dev_do_close when the controller\n\t * gets removed.\n\t */\n\tif (test_bit(HCI_UP, &hdev->flags))\n\t\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n\telse\n\t\tret = -ENETDOWN;\n\thci_req_sync_unlock(hdev);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,12 +4,16 @@\n {\n \tint ret;\n \n-\tif (!test_bit(HCI_UP, &hdev->flags))\n-\t\treturn -ENETDOWN;\n-\n \t/* Serialize all requests */\n \thci_req_sync_lock(hdev);\n-\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n+\t/* check the state after obtaing the lock to protect the HCI_UP\n+\t * against any races from hci_dev_do_close when the controller\n+\t * gets removed.\n+\t */\n+\tif (test_bit(HCI_UP, &hdev->flags))\n+\t\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n+\telse\n+\t\tret = -ENETDOWN;\n \thci_req_sync_unlock(hdev);\n \n \treturn ret;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!test_bit(HCI_UP, &hdev->flags))",
                "\t\treturn -ENETDOWN;",
                "",
                "\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);"
            ],
            "added_lines": [
                "\t/* check the state after obtaing the lock to protect the HCI_UP",
                "\t * against any races from hci_dev_do_close when the controller",
                "\t * gets removed.",
                "\t */",
                "\tif (test_bit(HCI_UP, &hdev->flags))",
                "\t\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);",
                "\telse",
                "\t\tret = -ENETDOWN;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-31083",
        "func_name": "torvalds/linux/hci_uart_tty_ioctl",
        "description": "An issue was discovered in drivers/bluetooth/hci_ldisc.c in the Linux kernel 6.2. In hci_uart_tty_ioctl, there is a race condition between HCIUARTSETPROTO and HCIUARTGETPROTO. HCI_UART_PROTO_SET is set before hu->proto is set. A NULL pointer dereference may occur.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=9c33663af9ad115f90c076a1828129a3fbadea98",
        "commit_title": "This patch adds code to check HCI_UART_PROTO_READY flag before",
        "commit_text": "accessing hci_uart->proto. It fixes the race condition in hci_uart_tty_ioctl() between HCIUARTSETPROTO and HCIUARTGETPROTO. This issue bug found by Yu Hao and Weiteng Chen:  BUG: general protection fault in hci_uart_tty_ioctl [1]  The information of C reproducer can also reference the link [2]  Closes: https://lore.kernel.org/all/CA+UBctC3p49aTgzbVgkSZ2+TQcqq4fPDO7yZitFT5uBPDeCO2g@mail.gmail.com/ [1] Closes: https://lore.kernel.org/lkml/CA+UBctDPEvHdkHMwD340=n02rh+jNRJNNQ5LBZNA+Wm4Keh2ow@mail.gmail.com/T/ [2] ",
        "func_before": "static int hci_uart_tty_ioctl(struct tty_struct *tty, unsigned int cmd,\n\t\t\t      unsigned long arg)\n{\n\tstruct hci_uart *hu = tty->disc_data;\n\tint err = 0;\n\n\tBT_DBG(\"\");\n\n\t/* Verify the status of the device */\n\tif (!hu)\n\t\treturn -EBADF;\n\n\tswitch (cmd) {\n\tcase HCIUARTSETPROTO:\n\t\tif (!test_and_set_bit(HCI_UART_PROTO_SET, &hu->flags)) {\n\t\t\terr = hci_uart_set_proto(hu, arg);\n\t\t\tif (err)\n\t\t\t\tclear_bit(HCI_UART_PROTO_SET, &hu->flags);\n\t\t} else\n\t\t\terr = -EBUSY;\n\t\tbreak;\n\n\tcase HCIUARTGETPROTO:\n\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))\n\t\t\terr = hu->proto->id;\n\t\telse\n\t\t\terr = -EUNATCH;\n\t\tbreak;\n\n\tcase HCIUARTGETDEVICE:\n\t\tif (test_bit(HCI_UART_REGISTERED, &hu->flags))\n\t\t\terr = hu->hdev->id;\n\t\telse\n\t\t\terr = -EUNATCH;\n\t\tbreak;\n\n\tcase HCIUARTSETFLAGS:\n\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\terr = hci_uart_set_flags(hu, arg);\n\t\tbreak;\n\n\tcase HCIUARTGETFLAGS:\n\t\terr = hu->hdev_flags;\n\t\tbreak;\n\n\tdefault:\n\t\terr = n_tty_ioctl_helper(tty, cmd, arg);\n\t\tbreak;\n\t}\n\n\treturn err;\n}",
        "func": "static int hci_uart_tty_ioctl(struct tty_struct *tty, unsigned int cmd,\n\t\t\t      unsigned long arg)\n{\n\tstruct hci_uart *hu = tty->disc_data;\n\tint err = 0;\n\n\tBT_DBG(\"\");\n\n\t/* Verify the status of the device */\n\tif (!hu)\n\t\treturn -EBADF;\n\n\tswitch (cmd) {\n\tcase HCIUARTSETPROTO:\n\t\tif (!test_and_set_bit(HCI_UART_PROTO_SET, &hu->flags)) {\n\t\t\terr = hci_uart_set_proto(hu, arg);\n\t\t\tif (err)\n\t\t\t\tclear_bit(HCI_UART_PROTO_SET, &hu->flags);\n\t\t} else\n\t\t\terr = -EBUSY;\n\t\tbreak;\n\n\tcase HCIUARTGETPROTO:\n\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags) &&\n\t\t    test_bit(HCI_UART_PROTO_READY, &hu->flags))\n\t\t\terr = hu->proto->id;\n\t\telse\n\t\t\terr = -EUNATCH;\n\t\tbreak;\n\n\tcase HCIUARTGETDEVICE:\n\t\tif (test_bit(HCI_UART_REGISTERED, &hu->flags))\n\t\t\terr = hu->hdev->id;\n\t\telse\n\t\t\terr = -EUNATCH;\n\t\tbreak;\n\n\tcase HCIUARTSETFLAGS:\n\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\terr = hci_uart_set_flags(hu, arg);\n\t\tbreak;\n\n\tcase HCIUARTGETFLAGS:\n\t\terr = hu->hdev_flags;\n\t\tbreak;\n\n\tdefault:\n\t\terr = n_tty_ioctl_helper(tty, cmd, arg);\n\t\tbreak;\n\t}\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,7 +21,8 @@\n \t\tbreak;\n \n \tcase HCIUARTGETPROTO:\n-\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))\n+\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags) &&\n+\t\t    test_bit(HCI_UART_PROTO_READY, &hu->flags))\n \t\t\terr = hu->proto->id;\n \t\telse\n \t\t\terr = -EUNATCH;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))"
            ],
            "added_lines": [
                "\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags) &&",
                "\t\t    test_bit(HCI_UART_PROTO_READY, &hu->flags))"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2006",
        "func_name": "torvalds/linux/rxrpc_put_bundle",
        "description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel.",
        "git_url": "https://github.com/torvalds/linux/commit/3bcd6c7eaa53b56c3f584da46a1f7652e759d0e5",
        "commit_title": "rxrpc: Fix race between conn bundle lookup and bundle removal [ZDI-CAN-15975]",
        "commit_text": " After rxrpc_unbundle_conn() has removed a connection from a bundle, it checks to see if there are any conns with available channels and, if not, removes and attempts to destroy the bundle.  Whilst it does check after grabbing client_bundles_lock that there are no connections attached, this races with rxrpc_look_up_bundle() retrieving the bundle, but not attaching a connection for the connection to be attached later.  There is therefore a window in which the bundle can get destroyed before we manage to attach a new connection to it.  Fix this by adding an \"active\" counter to struct rxrpc_bundle:   (1) rxrpc_connect_call() obtains an active count by prepping/looking up a      bundle and ditches it before returning.   (2) If, during rxrpc_connect_call(), a connection is added to the bundle,      this obtains an active count, which is held until the connection is      discarded.   (3) rxrpc_deactivate_bundle() is created to drop an active count on a      bundle and destroy it when the active count reaches 0.  The active      count is checked inside client_bundles_lock() to prevent a race with      rxrpc_look_up_bundle().   (4) rxrpc_unbundle_conn() then calls rxrpc_deactivate_bundle().  cc: Marc Dionne <marc.dionne@auristor.com> cc: linux-afs@lists.infradead.org",
        "func_before": "void rxrpc_put_bundle(struct rxrpc_bundle *bundle)\n{\n\tunsigned int d = bundle->debug_id;\n\tbool dead;\n\tint r;\n\n\tdead = __refcount_dec_and_test(&bundle->ref, &r);\n\n\t_debug(\"PUT B=%x %d\", d, r);\n\tif (dead)\n\t\trxrpc_free_bundle(bundle);\n}",
        "func": "void rxrpc_put_bundle(struct rxrpc_bundle *bundle)\n{\n\tunsigned int d = bundle->debug_id;\n\tbool dead;\n\tint r;\n\n\tdead = __refcount_dec_and_test(&bundle->ref, &r);\n\n\t_debug(\"PUT B=%x %d\", d, r - 1);\n\tif (dead)\n\t\trxrpc_free_bundle(bundle);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n \n \tdead = __refcount_dec_and_test(&bundle->ref, &r);\n \n-\t_debug(\"PUT B=%x %d\", d, r);\n+\t_debug(\"PUT B=%x %d\", d, r - 1);\n \tif (dead)\n \t\trxrpc_free_bundle(bundle);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t_debug(\"PUT B=%x %d\", d, r);"
            ],
            "added_lines": [
                "\t_debug(\"PUT B=%x %d\", d, r - 1);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2006",
        "func_name": "torvalds/linux/rxrpc_unbundle_conn",
        "description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel.",
        "git_url": "https://github.com/torvalds/linux/commit/3bcd6c7eaa53b56c3f584da46a1f7652e759d0e5",
        "commit_title": "rxrpc: Fix race between conn bundle lookup and bundle removal [ZDI-CAN-15975]",
        "commit_text": " After rxrpc_unbundle_conn() has removed a connection from a bundle, it checks to see if there are any conns with available channels and, if not, removes and attempts to destroy the bundle.  Whilst it does check after grabbing client_bundles_lock that there are no connections attached, this races with rxrpc_look_up_bundle() retrieving the bundle, but not attaching a connection for the connection to be attached later.  There is therefore a window in which the bundle can get destroyed before we manage to attach a new connection to it.  Fix this by adding an \"active\" counter to struct rxrpc_bundle:   (1) rxrpc_connect_call() obtains an active count by prepping/looking up a      bundle and ditches it before returning.   (2) If, during rxrpc_connect_call(), a connection is added to the bundle,      this obtains an active count, which is held until the connection is      discarded.   (3) rxrpc_deactivate_bundle() is created to drop an active count on a      bundle and destroy it when the active count reaches 0.  The active      count is checked inside client_bundles_lock() to prevent a race with      rxrpc_look_up_bundle().   (4) rxrpc_unbundle_conn() then calls rxrpc_deactivate_bundle().  cc: Marc Dionne <marc.dionne@auristor.com> cc: linux-afs@lists.infradead.org",
        "func_before": "static void rxrpc_unbundle_conn(struct rxrpc_connection *conn)\n{\n\tstruct rxrpc_bundle *bundle = conn->bundle;\n\tstruct rxrpc_local *local = bundle->params.local;\n\tunsigned int bindex;\n\tbool need_drop = false, need_put = false;\n\tint i;\n\n\t_enter(\"C=%x\", conn->debug_id);\n\n\tif (conn->flags & RXRPC_CONN_FINAL_ACK_MASK)\n\t\trxrpc_process_delayed_final_acks(conn, true);\n\n\tspin_lock(&bundle->channel_lock);\n\tbindex = conn->bundle_shift / RXRPC_MAXCALLS;\n\tif (bundle->conns[bindex] == conn) {\n\t\t_debug(\"clear slot %u\", bindex);\n\t\tbundle->conns[bindex] = NULL;\n\t\tfor (i = 0; i < RXRPC_MAXCALLS; i++)\n\t\t\tclear_bit(conn->bundle_shift + i, &bundle->avail_chans);\n\t\tneed_drop = true;\n\t}\n\tspin_unlock(&bundle->channel_lock);\n\n\t/* If there are no more connections, remove the bundle */\n\tif (!bundle->avail_chans) {\n\t\t_debug(\"maybe unbundle\");\n\t\tspin_lock(&local->client_bundles_lock);\n\n\t\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++)\n\t\t\tif (bundle->conns[i])\n\t\t\t\tbreak;\n\t\tif (i == ARRAY_SIZE(bundle->conns) && !bundle->params.exclusive) {\n\t\t\t_debug(\"erase bundle\");\n\t\t\trb_erase(&bundle->local_node, &local->client_bundles);\n\t\t\tneed_put = true;\n\t\t}\n\n\t\tspin_unlock(&local->client_bundles_lock);\n\t\tif (need_put)\n\t\t\trxrpc_put_bundle(bundle);\n\t}\n\n\tif (need_drop)\n\t\trxrpc_put_connection(conn);\n\t_leave(\"\");\n}",
        "func": "static void rxrpc_unbundle_conn(struct rxrpc_connection *conn)\n{\n\tstruct rxrpc_bundle *bundle = conn->bundle;\n\tunsigned int bindex;\n\tbool need_drop = false;\n\tint i;\n\n\t_enter(\"C=%x\", conn->debug_id);\n\n\tif (conn->flags & RXRPC_CONN_FINAL_ACK_MASK)\n\t\trxrpc_process_delayed_final_acks(conn, true);\n\n\tspin_lock(&bundle->channel_lock);\n\tbindex = conn->bundle_shift / RXRPC_MAXCALLS;\n\tif (bundle->conns[bindex] == conn) {\n\t\t_debug(\"clear slot %u\", bindex);\n\t\tbundle->conns[bindex] = NULL;\n\t\tfor (i = 0; i < RXRPC_MAXCALLS; i++)\n\t\t\tclear_bit(conn->bundle_shift + i, &bundle->avail_chans);\n\t\tneed_drop = true;\n\t}\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (need_drop) {\n\t\trxrpc_deactivate_bundle(bundle);\n\t\trxrpc_put_connection(conn);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,8 @@\n static void rxrpc_unbundle_conn(struct rxrpc_connection *conn)\n {\n \tstruct rxrpc_bundle *bundle = conn->bundle;\n-\tstruct rxrpc_local *local = bundle->params.local;\n \tunsigned int bindex;\n-\tbool need_drop = false, need_put = false;\n+\tbool need_drop = false;\n \tint i;\n \n \t_enter(\"C=%x\", conn->debug_id);\n@@ -22,26 +21,8 @@\n \t}\n \tspin_unlock(&bundle->channel_lock);\n \n-\t/* If there are no more connections, remove the bundle */\n-\tif (!bundle->avail_chans) {\n-\t\t_debug(\"maybe unbundle\");\n-\t\tspin_lock(&local->client_bundles_lock);\n-\n-\t\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++)\n-\t\t\tif (bundle->conns[i])\n-\t\t\t\tbreak;\n-\t\tif (i == ARRAY_SIZE(bundle->conns) && !bundle->params.exclusive) {\n-\t\t\t_debug(\"erase bundle\");\n-\t\t\trb_erase(&bundle->local_node, &local->client_bundles);\n-\t\t\tneed_put = true;\n-\t\t}\n-\n-\t\tspin_unlock(&local->client_bundles_lock);\n-\t\tif (need_put)\n-\t\t\trxrpc_put_bundle(bundle);\n+\tif (need_drop) {\n+\t\trxrpc_deactivate_bundle(bundle);\n+\t\trxrpc_put_connection(conn);\n \t}\n-\n-\tif (need_drop)\n-\t\trxrpc_put_connection(conn);\n-\t_leave(\"\");\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct rxrpc_local *local = bundle->params.local;",
                "\tbool need_drop = false, need_put = false;",
                "\t/* If there are no more connections, remove the bundle */",
                "\tif (!bundle->avail_chans) {",
                "\t\t_debug(\"maybe unbundle\");",
                "\t\tspin_lock(&local->client_bundles_lock);",
                "",
                "\t\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++)",
                "\t\t\tif (bundle->conns[i])",
                "\t\t\t\tbreak;",
                "\t\tif (i == ARRAY_SIZE(bundle->conns) && !bundle->params.exclusive) {",
                "\t\t\t_debug(\"erase bundle\");",
                "\t\t\trb_erase(&bundle->local_node, &local->client_bundles);",
                "\t\t\tneed_put = true;",
                "\t\t}",
                "",
                "\t\tspin_unlock(&local->client_bundles_lock);",
                "\t\tif (need_put)",
                "\t\t\trxrpc_put_bundle(bundle);",
                "",
                "\tif (need_drop)",
                "\t\trxrpc_put_connection(conn);",
                "\t_leave(\"\");"
            ],
            "added_lines": [
                "\tbool need_drop = false;",
                "\tif (need_drop) {",
                "\t\trxrpc_deactivate_bundle(bundle);",
                "\t\trxrpc_put_connection(conn);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2006",
        "func_name": "torvalds/linux/rxrpc_connect_call",
        "description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel.",
        "git_url": "https://github.com/torvalds/linux/commit/3bcd6c7eaa53b56c3f584da46a1f7652e759d0e5",
        "commit_title": "rxrpc: Fix race between conn bundle lookup and bundle removal [ZDI-CAN-15975]",
        "commit_text": " After rxrpc_unbundle_conn() has removed a connection from a bundle, it checks to see if there are any conns with available channels and, if not, removes and attempts to destroy the bundle.  Whilst it does check after grabbing client_bundles_lock that there are no connections attached, this races with rxrpc_look_up_bundle() retrieving the bundle, but not attaching a connection for the connection to be attached later.  There is therefore a window in which the bundle can get destroyed before we manage to attach a new connection to it.  Fix this by adding an \"active\" counter to struct rxrpc_bundle:   (1) rxrpc_connect_call() obtains an active count by prepping/looking up a      bundle and ditches it before returning.   (2) If, during rxrpc_connect_call(), a connection is added to the bundle,      this obtains an active count, which is held until the connection is      discarded.   (3) rxrpc_deactivate_bundle() is created to drop an active count on a      bundle and destroy it when the active count reaches 0.  The active      count is checked inside client_bundles_lock() to prevent a race with      rxrpc_look_up_bundle().   (4) rxrpc_unbundle_conn() then calls rxrpc_deactivate_bundle().  cc: Marc Dionne <marc.dionne@auristor.com> cc: linux-afs@lists.infradead.org",
        "func_before": "int rxrpc_connect_call(struct rxrpc_sock *rx,\n\t\t       struct rxrpc_call *call,\n\t\t       struct rxrpc_conn_parameters *cp,\n\t\t       struct sockaddr_rxrpc *srx,\n\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\tstruct rxrpc_net *rxnet = cp->local->rxnet;\n\tint ret = 0;\n\n\t_enter(\"{%d,%lx},\", call->debug_id, call->user_call_ID);\n\n\trxrpc_discard_expired_client_conns(&rxnet->client_conn_reaper);\n\n\tbundle = rxrpc_prep_call(rx, call, cp, srx, gfp);\n\tif (IS_ERR(bundle)) {\n\t\tret = PTR_ERR(bundle);\n\t\tgoto out;\n\t}\n\n\tif (call->state == RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = rxrpc_wait_for_channel(bundle, call, gfp);\n\t\tif (ret < 0)\n\t\t\tgoto wait_failed;\n\t}\n\ngranted_channel:\n\t/* Paired with the write barrier in rxrpc_activate_one_channel(). */\n\tsmp_rmb();\n\nout_put_bundle:\n\trxrpc_put_bundle(bundle);\nout:\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\nwait_failed:\n\tspin_lock(&bundle->channel_lock);\n\tlist_del_init(&call->chan_wait_link);\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (call->state != RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = 0;\n\t\tgoto granted_channel;\n\t}\n\n\ttrace_rxrpc_client(call->conn, ret, rxrpc_client_chan_wait_failed);\n\trxrpc_set_call_completion(call, RXRPC_CALL_LOCAL_ERROR, 0, ret);\n\trxrpc_disconnect_client_call(bundle, call);\n\tgoto out_put_bundle;\n}",
        "func": "int rxrpc_connect_call(struct rxrpc_sock *rx,\n\t\t       struct rxrpc_call *call,\n\t\t       struct rxrpc_conn_parameters *cp,\n\t\t       struct sockaddr_rxrpc *srx,\n\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\tstruct rxrpc_net *rxnet = cp->local->rxnet;\n\tint ret = 0;\n\n\t_enter(\"{%d,%lx},\", call->debug_id, call->user_call_ID);\n\n\trxrpc_discard_expired_client_conns(&rxnet->client_conn_reaper);\n\n\tbundle = rxrpc_prep_call(rx, call, cp, srx, gfp);\n\tif (IS_ERR(bundle)) {\n\t\tret = PTR_ERR(bundle);\n\t\tgoto out;\n\t}\n\n\tif (call->state == RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = rxrpc_wait_for_channel(bundle, call, gfp);\n\t\tif (ret < 0)\n\t\t\tgoto wait_failed;\n\t}\n\ngranted_channel:\n\t/* Paired with the write barrier in rxrpc_activate_one_channel(). */\n\tsmp_rmb();\n\nout_put_bundle:\n\trxrpc_deactivate_bundle(bundle);\n\trxrpc_put_bundle(bundle);\nout:\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\nwait_failed:\n\tspin_lock(&bundle->channel_lock);\n\tlist_del_init(&call->chan_wait_link);\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (call->state != RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = 0;\n\t\tgoto granted_channel;\n\t}\n\n\ttrace_rxrpc_client(call->conn, ret, rxrpc_client_chan_wait_failed);\n\trxrpc_set_call_completion(call, RXRPC_CALL_LOCAL_ERROR, 0, ret);\n\trxrpc_disconnect_client_call(bundle, call);\n\tgoto out_put_bundle;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -29,6 +29,7 @@\n \tsmp_rmb();\n \n out_put_bundle:\n+\trxrpc_deactivate_bundle(bundle);\n \trxrpc_put_bundle(bundle);\n out:\n \t_leave(\" = %d\", ret);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\trxrpc_deactivate_bundle(bundle);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2006",
        "func_name": "torvalds/linux/rxrpc_look_up_bundle",
        "description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel.",
        "git_url": "https://github.com/torvalds/linux/commit/3bcd6c7eaa53b56c3f584da46a1f7652e759d0e5",
        "commit_title": "rxrpc: Fix race between conn bundle lookup and bundle removal [ZDI-CAN-15975]",
        "commit_text": " After rxrpc_unbundle_conn() has removed a connection from a bundle, it checks to see if there are any conns with available channels and, if not, removes and attempts to destroy the bundle.  Whilst it does check after grabbing client_bundles_lock that there are no connections attached, this races with rxrpc_look_up_bundle() retrieving the bundle, but not attaching a connection for the connection to be attached later.  There is therefore a window in which the bundle can get destroyed before we manage to attach a new connection to it.  Fix this by adding an \"active\" counter to struct rxrpc_bundle:   (1) rxrpc_connect_call() obtains an active count by prepping/looking up a      bundle and ditches it before returning.   (2) If, during rxrpc_connect_call(), a connection is added to the bundle,      this obtains an active count, which is held until the connection is      discarded.   (3) rxrpc_deactivate_bundle() is created to drop an active count on a      bundle and destroy it when the active count reaches 0.  The active      count is checked inside client_bundles_lock() to prevent a race with      rxrpc_look_up_bundle().   (4) rxrpc_unbundle_conn() then calls rxrpc_deactivate_bundle().  cc: Marc Dionne <marc.dionne@auristor.com> cc: linux-afs@lists.infradead.org",
        "func_before": "static struct rxrpc_bundle *rxrpc_look_up_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t\t gfp_t gfp)\n{\n\tstatic atomic_t rxrpc_bundle_id;\n\tstruct rxrpc_bundle *bundle, *candidate;\n\tstruct rxrpc_local *local = cp->local;\n\tstruct rb_node *p, **pp, *parent;\n\tlong diff;\n\n\t_enter(\"{%px,%x,%u,%u}\",\n\t       cp->peer, key_serial(cp->key), cp->security_level, cp->upgrade);\n\n\tif (cp->exclusive)\n\t\treturn rxrpc_alloc_bundle(cp, gfp);\n\n\t/* First, see if the bundle is already there. */\n\t_debug(\"search 1\");\n\tspin_lock(&local->client_bundles_lock);\n\tp = local->client_bundles.rb_node;\n\twhile (p) {\n\t\tbundle = rb_entry(p, struct rxrpc_bundle, local_node);\n\n#define cmp(X) ((long)bundle->params.X - (long)cp->X)\n\t\tdiff = (cmp(peer) ?:\n\t\t\tcmp(key) ?:\n\t\t\tcmp(security_level) ?:\n\t\t\tcmp(upgrade));\n#undef cmp\n\t\tif (diff < 0)\n\t\t\tp = p->rb_left;\n\t\telse if (diff > 0)\n\t\t\tp = p->rb_right;\n\t\telse\n\t\t\tgoto found_bundle;\n\t}\n\tspin_unlock(&local->client_bundles_lock);\n\t_debug(\"not found\");\n\n\t/* It wasn't.  We need to add one. */\n\tcandidate = rxrpc_alloc_bundle(cp, gfp);\n\tif (!candidate)\n\t\treturn NULL;\n\n\t_debug(\"search 2\");\n\tspin_lock(&local->client_bundles_lock);\n\tpp = &local->client_bundles.rb_node;\n\tparent = NULL;\n\twhile (*pp) {\n\t\tparent = *pp;\n\t\tbundle = rb_entry(parent, struct rxrpc_bundle, local_node);\n\n#define cmp(X) ((long)bundle->params.X - (long)cp->X)\n\t\tdiff = (cmp(peer) ?:\n\t\t\tcmp(key) ?:\n\t\t\tcmp(security_level) ?:\n\t\t\tcmp(upgrade));\n#undef cmp\n\t\tif (diff < 0)\n\t\t\tpp = &(*pp)->rb_left;\n\t\telse if (diff > 0)\n\t\t\tpp = &(*pp)->rb_right;\n\t\telse\n\t\t\tgoto found_bundle_free;\n\t}\n\n\t_debug(\"new bundle\");\n\tcandidate->debug_id = atomic_inc_return(&rxrpc_bundle_id);\n\trb_link_node(&candidate->local_node, parent, pp);\n\trb_insert_color(&candidate->local_node, &local->client_bundles);\n\trxrpc_get_bundle(candidate);\n\tspin_unlock(&local->client_bundles_lock);\n\t_leave(\" = %u [new]\", candidate->debug_id);\n\treturn candidate;\n\nfound_bundle_free:\n\trxrpc_free_bundle(candidate);\nfound_bundle:\n\trxrpc_get_bundle(bundle);\n\tspin_unlock(&local->client_bundles_lock);\n\t_leave(\" = %u [found]\", bundle->debug_id);\n\treturn bundle;\n}\n\n/*\n * Create or find a client bundle to use for a call.\n *\n * If we return with a connection, the call will be on its waiting list.  It's\n * left to the caller to assign a channel and wake up the call.\n */\nstatic struct rxrpc_bundle *rxrpc_prep_call(struct rxrpc_sock *rx,\n\t\t\t\t\t    struct rxrpc_call *call,\n\t\t\t\t\t    struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t    struct sockaddr_rxrpc *srx,\n\t\t\t\t\t    gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\n\t_enter(\"{%d,%lx},\", call->debug_id, call->user_call_ID);\n\n\tcp->peer = rxrpc_lookup_peer(rx, cp->local, srx, gfp);\n\tif (!cp->peer)\n\t\tgoto error;\n\n\tcall->cong_cwnd = cp->peer->cong_cwnd;\n\tif (call->cong_cwnd >= call->cong_ssthresh)\n\t\tcall->cong_mode = RXRPC_CALL_CONGEST_AVOIDANCE;\n\telse\n\t\tcall->cong_mode = RXRPC_CALL_SLOW_START;\n\tif (cp->upgrade)\n\t\t__set_bit(RXRPC_CALL_UPGRADE, &call->flags);\n\n\t/* Find the client connection bundle. */\n\tbundle = rxrpc_look_up_bundle(cp, gfp);\n\tif (!bundle)\n\t\tgoto error;\n\n\t/* Get this call queued.  Someone else may activate it whilst we're\n\t * lining up a new connection, but that's fine.\n\t */\n\tspin_lock(&bundle->channel_lock);\n\tlist_add_tail(&call->chan_wait_link, &bundle->waiting_calls);\n\tspin_unlock(&bundle->channel_lock);\n\n\t_leave(\" = [B=%x]\", bundle->debug_id);\n\treturn bundle;\n\nerror:\n\t_leave(\" = -ENOMEM\");\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/*\n * Allocate a new connection and add it into a bundle.\n */\nstatic void rxrpc_add_conn_to_bundle(struct rxrpc_bundle *bundle, gfp_t gfp)\n\t__releases(bundle->channel_lock)\n{\n\tstruct rxrpc_connection *candidate = NULL, *old = NULL;\n\tbool conflict;\n\tint i;\n\n\t_enter(\"\");\n\n\tconflict = bundle->alloc_conn;\n\tif (!conflict)\n\t\tbundle->alloc_conn = true;\n\tspin_unlock(&bundle->channel_lock);\n\tif (conflict) {\n\t\t_leave(\" [conf]\");\n\t\treturn;\n\t}\n\n\tcandidate = rxrpc_alloc_client_connection(bundle, gfp);\n\n\tspin_lock(&bundle->channel_lock);\n\tbundle->alloc_conn = false;\n\n\tif (IS_ERR(candidate)) {\n\t\tbundle->alloc_error = PTR_ERR(candidate);\n\t\tspin_unlock(&bundle->channel_lock);\n\t\t_leave(\" [err %ld]\", PTR_ERR(candidate));\n\t\treturn;\n\t}\n\n\tbundle->alloc_error = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++) {\n\t\tunsigned int shift = i * RXRPC_MAXCALLS;\n\t\tint j;\n\n\t\told = bundle->conns[i];\n\t\tif (!rxrpc_may_reuse_conn(old)) {\n\t\t\tif (old)\n\t\t\t\ttrace_rxrpc_client(old, -1, rxrpc_client_replace);\n\t\t\tcandidate->bundle_shift = shift;\n\t\t\tbundle->conns[i] = candidate;\n\t\t\tfor (j = 0; j < RXRPC_MAXCALLS; j++)\n\t\t\t\tset_bit(shift + j, &bundle->avail_chans);\n\t\t\tcandidate = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\told = NULL;\n\t}\n\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (candidate) {\n\t\t_debug(\"discard C=%x\", candidate->debug_id);\n\t\ttrace_rxrpc_client(candidate, -1, rxrpc_client_duplicate);\n\t\trxrpc_put_connection(candidate);\n\t}\n\n\trxrpc_put_connection(old);\n\t_leave(\"\");\n}\n\n/*\n * Add a connection to a bundle if there are no usable connections or we have\n * connections waiting for extra capacity.\n */\nstatic void rxrpc_maybe_add_conn(struct rxrpc_bundle *bundle, gfp_t gfp)\n{\n\tstruct rxrpc_call *call;\n\tint i, usable;\n\n\t_enter(\"\");\n\n\tspin_lock(&bundle->channel_lock);\n\n\t/* See if there are any usable connections. */\n\tusable = 0;\n\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++)\n\t\tif (rxrpc_may_reuse_conn(bundle->conns[i]))\n\t\t\tusable++;\n\n\tif (!usable && !list_empty(&bundle->waiting_calls)) {\n\t\tcall = list_first_entry(&bundle->waiting_calls,\n\t\t\t\t\tstruct rxrpc_call, chan_wait_link);\n\t\tif (test_bit(RXRPC_CALL_UPGRADE, &call->flags))\n\t\t\tbundle->try_upgrade = true;\n\t}\n\n\tif (!usable)\n\t\tgoto alloc_conn;\n\n\tif (!bundle->avail_chans &&\n\t    !bundle->try_upgrade &&\n\t    !list_empty(&bundle->waiting_calls) &&\n\t    usable < ARRAY_SIZE(bundle->conns))\n\t\tgoto alloc_conn;\n\n\tspin_unlock(&bundle->channel_lock);\n\t_leave(\"\");\n\treturn;\n\nalloc_conn:\n\treturn rxrpc_add_conn_to_bundle(bundle, gfp);\n}\n\n/*\n * Assign a channel to the call at the front of the queue and wake the call up.\n * We don't increment the callNumber counter until this number has been exposed\n * to the world.\n */\nstatic void rxrpc_activate_one_channel(struct rxrpc_connection *conn,\n\t\t\t\t       unsigned int channel)\n{\n\tstruct rxrpc_channel *chan = &conn->channels[channel];\n\tstruct rxrpc_bundle *bundle = conn->bundle;\n\tstruct rxrpc_call *call = list_entry(bundle->waiting_calls.next,\n\t\t\t\t\t     struct rxrpc_call, chan_wait_link);\n\tu32 call_id = chan->call_counter + 1;\n\n\t_enter(\"C=%x,%u\", conn->debug_id, channel);\n\n\ttrace_rxrpc_client(conn, channel, rxrpc_client_chan_activate);\n\n\t/* Cancel the final ACK on the previous call if it hasn't been sent yet\n\t * as the DATA packet will implicitly ACK it.\n\t */\n\tclear_bit(RXRPC_CONN_FINAL_ACK_0 + channel, &conn->flags);\n\tclear_bit(conn->bundle_shift + channel, &bundle->avail_chans);\n\n\trxrpc_see_call(call);\n\tlist_del_init(&call->chan_wait_link);\n\tcall->peer\t= rxrpc_get_peer(conn->params.peer);\n\tcall->conn\t= rxrpc_get_connection(conn);\n\tcall->cid\t= conn->proto.cid | channel;\n\tcall->call_id\t= call_id;\n\tcall->security\t= conn->security;\n\tcall->security_ix = conn->security_ix;\n\tcall->service_id = conn->service_id;\n\n\ttrace_rxrpc_connect_call(call);\n\t_net(\"CONNECT call %08x:%08x as call %d on conn %d\",\n\t     call->cid, call->call_id, call->debug_id, conn->debug_id);\n\n\twrite_lock_bh(&call->state_lock);\n\tcall->state = RXRPC_CALL_CLIENT_SEND_REQUEST;\n\twrite_unlock_bh(&call->state_lock);\n\n\t/* Paired with the read barrier in rxrpc_connect_call().  This orders\n\t * cid and epoch in the connection wrt to call_id without the need to\n\t * take the channel_lock.\n\t *\n\t * We provisionally assign a callNumber at this point, but we don't\n\t * confirm it until the call is about to be exposed.\n\t *\n\t * TODO: Pair with a barrier in the data_ready handler when that looks\n\t * at the call ID through a connection channel.\n\t */\n\tsmp_wmb();\n\n\tchan->call_id\t\t= call_id;\n\tchan->call_debug_id\t= call->debug_id;\n\trcu_assign_pointer(chan->call, call);\n\twake_up(&call->waitq);\n}\n\n/*\n * Remove a connection from the idle list if it's on it.\n */\nstatic void rxrpc_unidle_conn(struct rxrpc_bundle *bundle, struct rxrpc_connection *conn)\n{\n\tstruct rxrpc_net *rxnet = bundle->params.local->rxnet;\n\tbool drop_ref;\n\n\tif (!list_empty(&conn->cache_link)) {\n\t\tdrop_ref = false;\n\t\tspin_lock(&rxnet->client_conn_cache_lock);\n\t\tif (!list_empty(&conn->cache_link)) {\n\t\t\tlist_del_init(&conn->cache_link);\n\t\t\tdrop_ref = true;\n\t\t}\n\t\tspin_unlock(&rxnet->client_conn_cache_lock);\n\t\tif (drop_ref)\n\t\t\trxrpc_put_connection(conn);\n\t}\n}\n\n/*\n * Assign channels and callNumbers to waiting calls with channel_lock\n * held by caller.\n */\nstatic void rxrpc_activate_channels_locked(struct rxrpc_bundle *bundle)\n{\n\tstruct rxrpc_connection *conn;\n\tunsigned long avail, mask;\n\tunsigned int channel, slot;\n\n\tif (bundle->try_upgrade)\n\t\tmask = 1;\n\telse\n\t\tmask = ULONG_MAX;\n\n\twhile (!list_empty(&bundle->waiting_calls)) {\n\t\tavail = bundle->avail_chans & mask;\n\t\tif (!avail)\n\t\t\tbreak;\n\t\tchannel = __ffs(avail);\n\t\tclear_bit(channel, &bundle->avail_chans);\n\n\t\tslot = channel / RXRPC_MAXCALLS;\n\t\tconn = bundle->conns[slot];\n\t\tif (!conn)\n\t\t\tbreak;\n\n\t\tif (bundle->try_upgrade)\n\t\t\tset_bit(RXRPC_CONN_PROBING_FOR_UPGRADE, &conn->flags);\n\t\trxrpc_unidle_conn(bundle, conn);\n\n\t\tchannel &= (RXRPC_MAXCALLS - 1);\n\t\tconn->act_chans\t|= 1 << channel;\n\t\trxrpc_activate_one_channel(conn, channel);\n\t}\n}\n\n/*\n * Assign channels and callNumbers to waiting calls.\n */\nstatic void rxrpc_activate_channels(struct rxrpc_bundle *bundle)\n{\n\t_enter(\"B=%x\", bundle->debug_id);\n\n\ttrace_rxrpc_client(NULL, -1, rxrpc_client_activate_chans);\n\n\tif (!bundle->avail_chans)\n\t\treturn;\n\n\tspin_lock(&bundle->channel_lock);\n\trxrpc_activate_channels_locked(bundle);\n\tspin_unlock(&bundle->channel_lock);\n\t_leave(\"\");\n}\n\n/*\n * Wait for a callNumber and a channel to be granted to a call.\n */\nstatic int rxrpc_wait_for_channel(struct rxrpc_bundle *bundle,\n\t\t\t\t  struct rxrpc_call *call, gfp_t gfp)\n{\n\tDECLARE_WAITQUEUE(myself, current);\n\tint ret = 0;\n\n\t_enter(\"%d\", call->debug_id);\n\n\tif (!gfpflags_allow_blocking(gfp)) {\n\t\trxrpc_maybe_add_conn(bundle, gfp);\n\t\trxrpc_activate_channels(bundle);\n\t\tret = bundle->alloc_error ?: -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tadd_wait_queue_exclusive(&call->waitq, &myself);\n\tfor (;;) {\n\t\trxrpc_maybe_add_conn(bundle, gfp);\n\t\trxrpc_activate_channels(bundle);\n\t\tret = bundle->alloc_error;\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tswitch (call->interruptibility) {\n\t\tcase RXRPC_INTERRUPTIBLE:\n\t\tcase RXRPC_PREINTERRUPTIBLE:\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tbreak;\n\t\tcase RXRPC_UNINTERRUPTIBLE:\n\t\tdefault:\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tbreak;\n\t\t}\n\t\tif (READ_ONCE(call->state) != RXRPC_CALL_CLIENT_AWAIT_CONN)\n\t\t\tbreak;\n\t\tif ((call->interruptibility == RXRPC_INTERRUPTIBLE ||\n\t\t     call->interruptibility == RXRPC_PREINTERRUPTIBLE) &&\n\t\t    signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tschedule();\n\t}",
        "func": "static struct rxrpc_bundle *rxrpc_look_up_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t\t gfp_t gfp)\n{\n\tstatic atomic_t rxrpc_bundle_id;\n\tstruct rxrpc_bundle *bundle, *candidate;\n\tstruct rxrpc_local *local = cp->local;\n\tstruct rb_node *p, **pp, *parent;\n\tlong diff;\n\n\t_enter(\"{%px,%x,%u,%u}\",\n\t       cp->peer, key_serial(cp->key), cp->security_level, cp->upgrade);\n\n\tif (cp->exclusive)\n\t\treturn rxrpc_alloc_bundle(cp, gfp);\n\n\t/* First, see if the bundle is already there. */\n\t_debug(\"search 1\");\n\tspin_lock(&local->client_bundles_lock);\n\tp = local->client_bundles.rb_node;\n\twhile (p) {\n\t\tbundle = rb_entry(p, struct rxrpc_bundle, local_node);\n\n#define cmp(X) ((long)bundle->params.X - (long)cp->X)\n\t\tdiff = (cmp(peer) ?:\n\t\t\tcmp(key) ?:\n\t\t\tcmp(security_level) ?:\n\t\t\tcmp(upgrade));\n#undef cmp\n\t\tif (diff < 0)\n\t\t\tp = p->rb_left;\n\t\telse if (diff > 0)\n\t\t\tp = p->rb_right;\n\t\telse\n\t\t\tgoto found_bundle;\n\t}\n\tspin_unlock(&local->client_bundles_lock);\n\t_debug(\"not found\");\n\n\t/* It wasn't.  We need to add one. */\n\tcandidate = rxrpc_alloc_bundle(cp, gfp);\n\tif (!candidate)\n\t\treturn NULL;\n\n\t_debug(\"search 2\");\n\tspin_lock(&local->client_bundles_lock);\n\tpp = &local->client_bundles.rb_node;\n\tparent = NULL;\n\twhile (*pp) {\n\t\tparent = *pp;\n\t\tbundle = rb_entry(parent, struct rxrpc_bundle, local_node);\n\n#define cmp(X) ((long)bundle->params.X - (long)cp->X)\n\t\tdiff = (cmp(peer) ?:\n\t\t\tcmp(key) ?:\n\t\t\tcmp(security_level) ?:\n\t\t\tcmp(upgrade));\n#undef cmp\n\t\tif (diff < 0)\n\t\t\tpp = &(*pp)->rb_left;\n\t\telse if (diff > 0)\n\t\t\tpp = &(*pp)->rb_right;\n\t\telse\n\t\t\tgoto found_bundle_free;\n\t}\n\n\t_debug(\"new bundle\");\n\tcandidate->debug_id = atomic_inc_return(&rxrpc_bundle_id);\n\trb_link_node(&candidate->local_node, parent, pp);\n\trb_insert_color(&candidate->local_node, &local->client_bundles);\n\trxrpc_get_bundle(candidate);\n\tspin_unlock(&local->client_bundles_lock);\n\t_leave(\" = %u [new]\", candidate->debug_id);\n\treturn candidate;\n\nfound_bundle_free:\n\trxrpc_free_bundle(candidate);\nfound_bundle:\n\trxrpc_get_bundle(bundle);\n\tatomic_inc(&bundle->active);\n\tspin_unlock(&local->client_bundles_lock);\n\t_leave(\" = %u [found]\", bundle->debug_id);\n\treturn bundle;\n}\n\n/*\n * Create or find a client bundle to use for a call.\n *\n * If we return with a connection, the call will be on its waiting list.  It's\n * left to the caller to assign a channel and wake up the call.\n */\nstatic struct rxrpc_bundle *rxrpc_prep_call(struct rxrpc_sock *rx,\n\t\t\t\t\t    struct rxrpc_call *call,\n\t\t\t\t\t    struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t    struct sockaddr_rxrpc *srx,\n\t\t\t\t\t    gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\n\t_enter(\"{%d,%lx},\", call->debug_id, call->user_call_ID);\n\n\tcp->peer = rxrpc_lookup_peer(rx, cp->local, srx, gfp);\n\tif (!cp->peer)\n\t\tgoto error;\n\n\tcall->cong_cwnd = cp->peer->cong_cwnd;\n\tif (call->cong_cwnd >= call->cong_ssthresh)\n\t\tcall->cong_mode = RXRPC_CALL_CONGEST_AVOIDANCE;\n\telse\n\t\tcall->cong_mode = RXRPC_CALL_SLOW_START;\n\tif (cp->upgrade)\n\t\t__set_bit(RXRPC_CALL_UPGRADE, &call->flags);\n\n\t/* Find the client connection bundle. */\n\tbundle = rxrpc_look_up_bundle(cp, gfp);\n\tif (!bundle)\n\t\tgoto error;\n\n\t/* Get this call queued.  Someone else may activate it whilst we're\n\t * lining up a new connection, but that's fine.\n\t */\n\tspin_lock(&bundle->channel_lock);\n\tlist_add_tail(&call->chan_wait_link, &bundle->waiting_calls);\n\tspin_unlock(&bundle->channel_lock);\n\n\t_leave(\" = [B=%x]\", bundle->debug_id);\n\treturn bundle;\n\nerror:\n\t_leave(\" = -ENOMEM\");\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/*\n * Allocate a new connection and add it into a bundle.\n */\nstatic void rxrpc_add_conn_to_bundle(struct rxrpc_bundle *bundle, gfp_t gfp)\n\t__releases(bundle->channel_lock)\n{\n\tstruct rxrpc_connection *candidate = NULL, *old = NULL;\n\tbool conflict;\n\tint i;\n\n\t_enter(\"\");\n\n\tconflict = bundle->alloc_conn;\n\tif (!conflict)\n\t\tbundle->alloc_conn = true;\n\tspin_unlock(&bundle->channel_lock);\n\tif (conflict) {\n\t\t_leave(\" [conf]\");\n\t\treturn;\n\t}\n\n\tcandidate = rxrpc_alloc_client_connection(bundle, gfp);\n\n\tspin_lock(&bundle->channel_lock);\n\tbundle->alloc_conn = false;\n\n\tif (IS_ERR(candidate)) {\n\t\tbundle->alloc_error = PTR_ERR(candidate);\n\t\tspin_unlock(&bundle->channel_lock);\n\t\t_leave(\" [err %ld]\", PTR_ERR(candidate));\n\t\treturn;\n\t}\n\n\tbundle->alloc_error = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++) {\n\t\tunsigned int shift = i * RXRPC_MAXCALLS;\n\t\tint j;\n\n\t\told = bundle->conns[i];\n\t\tif (!rxrpc_may_reuse_conn(old)) {\n\t\t\tif (old)\n\t\t\t\ttrace_rxrpc_client(old, -1, rxrpc_client_replace);\n\t\t\tcandidate->bundle_shift = shift;\n\t\t\tatomic_inc(&bundle->active);\n\t\t\tbundle->conns[i] = candidate;\n\t\t\tfor (j = 0; j < RXRPC_MAXCALLS; j++)\n\t\t\t\tset_bit(shift + j, &bundle->avail_chans);\n\t\t\tcandidate = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\told = NULL;\n\t}\n\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (candidate) {\n\t\t_debug(\"discard C=%x\", candidate->debug_id);\n\t\ttrace_rxrpc_client(candidate, -1, rxrpc_client_duplicate);\n\t\trxrpc_put_connection(candidate);\n\t}\n\n\trxrpc_put_connection(old);\n\t_leave(\"\");\n}\n\n/*\n * Add a connection to a bundle if there are no usable connections or we have\n * connections waiting for extra capacity.\n */\nstatic void rxrpc_maybe_add_conn(struct rxrpc_bundle *bundle, gfp_t gfp)\n{\n\tstruct rxrpc_call *call;\n\tint i, usable;\n\n\t_enter(\"\");\n\n\tspin_lock(&bundle->channel_lock);\n\n\t/* See if there are any usable connections. */\n\tusable = 0;\n\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++)\n\t\tif (rxrpc_may_reuse_conn(bundle->conns[i]))\n\t\t\tusable++;\n\n\tif (!usable && !list_empty(&bundle->waiting_calls)) {\n\t\tcall = list_first_entry(&bundle->waiting_calls,\n\t\t\t\t\tstruct rxrpc_call, chan_wait_link);\n\t\tif (test_bit(RXRPC_CALL_UPGRADE, &call->flags))\n\t\t\tbundle->try_upgrade = true;\n\t}\n\n\tif (!usable)\n\t\tgoto alloc_conn;\n\n\tif (!bundle->avail_chans &&\n\t    !bundle->try_upgrade &&\n\t    !list_empty(&bundle->waiting_calls) &&\n\t    usable < ARRAY_SIZE(bundle->conns))\n\t\tgoto alloc_conn;\n\n\tspin_unlock(&bundle->channel_lock);\n\t_leave(\"\");\n\treturn;\n\nalloc_conn:\n\treturn rxrpc_add_conn_to_bundle(bundle, gfp);\n}\n\n/*\n * Assign a channel to the call at the front of the queue and wake the call up.\n * We don't increment the callNumber counter until this number has been exposed\n * to the world.\n */\nstatic void rxrpc_activate_one_channel(struct rxrpc_connection *conn,\n\t\t\t\t       unsigned int channel)\n{\n\tstruct rxrpc_channel *chan = &conn->channels[channel];\n\tstruct rxrpc_bundle *bundle = conn->bundle;\n\tstruct rxrpc_call *call = list_entry(bundle->waiting_calls.next,\n\t\t\t\t\t     struct rxrpc_call, chan_wait_link);\n\tu32 call_id = chan->call_counter + 1;\n\n\t_enter(\"C=%x,%u\", conn->debug_id, channel);\n\n\ttrace_rxrpc_client(conn, channel, rxrpc_client_chan_activate);\n\n\t/* Cancel the final ACK on the previous call if it hasn't been sent yet\n\t * as the DATA packet will implicitly ACK it.\n\t */\n\tclear_bit(RXRPC_CONN_FINAL_ACK_0 + channel, &conn->flags);\n\tclear_bit(conn->bundle_shift + channel, &bundle->avail_chans);\n\n\trxrpc_see_call(call);\n\tlist_del_init(&call->chan_wait_link);\n\tcall->peer\t= rxrpc_get_peer(conn->params.peer);\n\tcall->conn\t= rxrpc_get_connection(conn);\n\tcall->cid\t= conn->proto.cid | channel;\n\tcall->call_id\t= call_id;\n\tcall->security\t= conn->security;\n\tcall->security_ix = conn->security_ix;\n\tcall->service_id = conn->service_id;\n\n\ttrace_rxrpc_connect_call(call);\n\t_net(\"CONNECT call %08x:%08x as call %d on conn %d\",\n\t     call->cid, call->call_id, call->debug_id, conn->debug_id);\n\n\twrite_lock_bh(&call->state_lock);\n\tcall->state = RXRPC_CALL_CLIENT_SEND_REQUEST;\n\twrite_unlock_bh(&call->state_lock);\n\n\t/* Paired with the read barrier in rxrpc_connect_call().  This orders\n\t * cid and epoch in the connection wrt to call_id without the need to\n\t * take the channel_lock.\n\t *\n\t * We provisionally assign a callNumber at this point, but we don't\n\t * confirm it until the call is about to be exposed.\n\t *\n\t * TODO: Pair with a barrier in the data_ready handler when that looks\n\t * at the call ID through a connection channel.\n\t */\n\tsmp_wmb();\n\n\tchan->call_id\t\t= call_id;\n\tchan->call_debug_id\t= call->debug_id;\n\trcu_assign_pointer(chan->call, call);\n\twake_up(&call->waitq);\n}\n\n/*\n * Remove a connection from the idle list if it's on it.\n */\nstatic void rxrpc_unidle_conn(struct rxrpc_bundle *bundle, struct rxrpc_connection *conn)\n{\n\tstruct rxrpc_net *rxnet = bundle->params.local->rxnet;\n\tbool drop_ref;\n\n\tif (!list_empty(&conn->cache_link)) {\n\t\tdrop_ref = false;\n\t\tspin_lock(&rxnet->client_conn_cache_lock);\n\t\tif (!list_empty(&conn->cache_link)) {\n\t\t\tlist_del_init(&conn->cache_link);\n\t\t\tdrop_ref = true;\n\t\t}\n\t\tspin_unlock(&rxnet->client_conn_cache_lock);\n\t\tif (drop_ref)\n\t\t\trxrpc_put_connection(conn);\n\t}\n}\n\n/*\n * Assign channels and callNumbers to waiting calls with channel_lock\n * held by caller.\n */\nstatic void rxrpc_activate_channels_locked(struct rxrpc_bundle *bundle)\n{\n\tstruct rxrpc_connection *conn;\n\tunsigned long avail, mask;\n\tunsigned int channel, slot;\n\n\tif (bundle->try_upgrade)\n\t\tmask = 1;\n\telse\n\t\tmask = ULONG_MAX;\n\n\twhile (!list_empty(&bundle->waiting_calls)) {\n\t\tavail = bundle->avail_chans & mask;\n\t\tif (!avail)\n\t\t\tbreak;\n\t\tchannel = __ffs(avail);\n\t\tclear_bit(channel, &bundle->avail_chans);\n\n\t\tslot = channel / RXRPC_MAXCALLS;\n\t\tconn = bundle->conns[slot];\n\t\tif (!conn)\n\t\t\tbreak;\n\n\t\tif (bundle->try_upgrade)\n\t\t\tset_bit(RXRPC_CONN_PROBING_FOR_UPGRADE, &conn->flags);\n\t\trxrpc_unidle_conn(bundle, conn);\n\n\t\tchannel &= (RXRPC_MAXCALLS - 1);\n\t\tconn->act_chans\t|= 1 << channel;\n\t\trxrpc_activate_one_channel(conn, channel);\n\t}\n}\n\n/*\n * Assign channels and callNumbers to waiting calls.\n */\nstatic void rxrpc_activate_channels(struct rxrpc_bundle *bundle)\n{\n\t_enter(\"B=%x\", bundle->debug_id);\n\n\ttrace_rxrpc_client(NULL, -1, rxrpc_client_activate_chans);\n\n\tif (!bundle->avail_chans)\n\t\treturn;\n\n\tspin_lock(&bundle->channel_lock);\n\trxrpc_activate_channels_locked(bundle);\n\tspin_unlock(&bundle->channel_lock);\n\t_leave(\"\");\n}\n\n/*\n * Wait for a callNumber and a channel to be granted to a call.\n */\nstatic int rxrpc_wait_for_channel(struct rxrpc_bundle *bundle,\n\t\t\t\t  struct rxrpc_call *call, gfp_t gfp)\n{\n\tDECLARE_WAITQUEUE(myself, current);\n\tint ret = 0;\n\n\t_enter(\"%d\", call->debug_id);\n\n\tif (!gfpflags_allow_blocking(gfp)) {\n\t\trxrpc_maybe_add_conn(bundle, gfp);\n\t\trxrpc_activate_channels(bundle);\n\t\tret = bundle->alloc_error ?: -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tadd_wait_queue_exclusive(&call->waitq, &myself);\n\tfor (;;) {\n\t\trxrpc_maybe_add_conn(bundle, gfp);\n\t\trxrpc_activate_channels(bundle);\n\t\tret = bundle->alloc_error;\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tswitch (call->interruptibility) {\n\t\tcase RXRPC_INTERRUPTIBLE:\n\t\tcase RXRPC_PREINTERRUPTIBLE:\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tbreak;\n\t\tcase RXRPC_UNINTERRUPTIBLE:\n\t\tdefault:\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tbreak;\n\t\t}\n\t\tif (READ_ONCE(call->state) != RXRPC_CALL_CLIENT_AWAIT_CONN)\n\t\t\tbreak;\n\t\tif ((call->interruptibility == RXRPC_INTERRUPTIBLE ||\n\t\t     call->interruptibility == RXRPC_PREINTERRUPTIBLE) &&\n\t\t    signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tschedule();\n\t}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -76,6 +76,7 @@\n \trxrpc_free_bundle(candidate);\n found_bundle:\n \trxrpc_get_bundle(bundle);\n+\tatomic_inc(&bundle->active);\n \tspin_unlock(&local->client_bundles_lock);\n \t_leave(\" = %u [found]\", bundle->debug_id);\n \treturn bundle;\n@@ -173,6 +174,7 @@\n \t\t\tif (old)\n \t\t\t\ttrace_rxrpc_client(old, -1, rxrpc_client_replace);\n \t\t\tcandidate->bundle_shift = shift;\n+\t\t\tatomic_inc(&bundle->active);\n \t\t\tbundle->conns[i] = candidate;\n \t\t\tfor (j = 0; j < RXRPC_MAXCALLS; j++)\n \t\t\t\tset_bit(shift + j, &bundle->avail_chans);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tatomic_inc(&bundle->active);",
                "\t\t\tatomic_inc(&bundle->active);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2006",
        "func_name": "torvalds/linux/rxrpc_alloc_bundle",
        "description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel.",
        "git_url": "https://github.com/torvalds/linux/commit/3bcd6c7eaa53b56c3f584da46a1f7652e759d0e5",
        "commit_title": "rxrpc: Fix race between conn bundle lookup and bundle removal [ZDI-CAN-15975]",
        "commit_text": " After rxrpc_unbundle_conn() has removed a connection from a bundle, it checks to see if there are any conns with available channels and, if not, removes and attempts to destroy the bundle.  Whilst it does check after grabbing client_bundles_lock that there are no connections attached, this races with rxrpc_look_up_bundle() retrieving the bundle, but not attaching a connection for the connection to be attached later.  There is therefore a window in which the bundle can get destroyed before we manage to attach a new connection to it.  Fix this by adding an \"active\" counter to struct rxrpc_bundle:   (1) rxrpc_connect_call() obtains an active count by prepping/looking up a      bundle and ditches it before returning.   (2) If, during rxrpc_connect_call(), a connection is added to the bundle,      this obtains an active count, which is held until the connection is      discarded.   (3) rxrpc_deactivate_bundle() is created to drop an active count on a      bundle and destroy it when the active count reaches 0.  The active      count is checked inside client_bundles_lock() to prevent a race with      rxrpc_look_up_bundle().   (4) rxrpc_unbundle_conn() then calls rxrpc_deactivate_bundle().  cc: Marc Dionne <marc.dionne@auristor.com> cc: linux-afs@lists.infradead.org",
        "func_before": "static struct rxrpc_bundle *rxrpc_alloc_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\n\tbundle = kzalloc(sizeof(*bundle), gfp);\n\tif (bundle) {\n\t\tbundle->params = *cp;\n\t\trxrpc_get_peer(bundle->params.peer);\n\t\trefcount_set(&bundle->ref, 1);\n\t\tspin_lock_init(&bundle->channel_lock);\n\t\tINIT_LIST_HEAD(&bundle->waiting_calls);\n\t}\n\treturn bundle;\n}",
        "func": "static struct rxrpc_bundle *rxrpc_alloc_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\n\tbundle = kzalloc(sizeof(*bundle), gfp);\n\tif (bundle) {\n\t\tbundle->params = *cp;\n\t\trxrpc_get_peer(bundle->params.peer);\n\t\trefcount_set(&bundle->ref, 1);\n\t\tatomic_set(&bundle->active, 1);\n\t\tspin_lock_init(&bundle->channel_lock);\n\t\tINIT_LIST_HEAD(&bundle->waiting_calls);\n\t}\n\treturn bundle;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,7 @@\n \t\tbundle->params = *cp;\n \t\trxrpc_get_peer(bundle->params.peer);\n \t\trefcount_set(&bundle->ref, 1);\n+\t\tatomic_set(&bundle->active, 1);\n \t\tspin_lock_init(&bundle->channel_lock);\n \t\tINIT_LIST_HEAD(&bundle->waiting_calls);\n \t}",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tatomic_set(&bundle->active, 1);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-33203",
        "func_name": "torvalds/linux/emac_remove",
        "description": "The Linux kernel before 6.2.9 has a race condition and resultant use-after-free in drivers/net/ethernet/qualcomm/emac/emac.c if a physically proximate attacker unplugs an emac based device.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=6b6bc5b8bd2d4ca9e1efa9ae0f98a0b0687ace75",
        "commit_title": "In emac_probe, &adpt->work_thread is bound with",
        "commit_text": "emac_work_thread. Then it will be started by timeout handler emac_tx_timeout or a IRQ handler emac_isr.  If we remove the driver which will call emac_remove   to make cleanup, there may be a unfinished work.  The possible sequence is as follows:  Fix it by finishing the work before cleanup in the emac_remove and disable timeout response.  CPU0                  CPU1                      |emac_work_thread emac_remove         | free_netdev         | kfree(netdev);      |                     |emac_reinit_locked                     |emac_mac_down                     |//use netdev  ",
        "func_before": "static int emac_remove(struct platform_device *pdev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(&pdev->dev);\n\tstruct emac_adapter *adpt = netdev_priv(netdev);\n\n\tunregister_netdev(netdev);\n\tnetif_napi_del(&adpt->rx_q.napi);\n\n\temac_clks_teardown(adpt);\n\n\tput_device(&adpt->phydev->mdio.dev);\n\tmdiobus_unregister(adpt->mii_bus);\n\n\tif (adpt->phy.digital)\n\t\tiounmap(adpt->phy.digital);\n\tiounmap(adpt->phy.base);\n\n\tfree_netdev(netdev);\n\n\treturn 0;\n}",
        "func": "static int emac_remove(struct platform_device *pdev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(&pdev->dev);\n\tstruct emac_adapter *adpt = netdev_priv(netdev);\n\n\tnetif_carrier_off(netdev);\n\tnetif_tx_disable(netdev);\n\n\tunregister_netdev(netdev);\n\tnetif_napi_del(&adpt->rx_q.napi);\n\n\tfree_irq(adpt->irq.irq, &adpt->irq);\n\tcancel_work_sync(&adpt->work_thread);\n\n\temac_clks_teardown(adpt);\n\n\tput_device(&adpt->phydev->mdio.dev);\n\tmdiobus_unregister(adpt->mii_bus);\n\n\tif (adpt->phy.digital)\n\t\tiounmap(adpt->phy.digital);\n\tiounmap(adpt->phy.base);\n\n\tfree_netdev(netdev);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,8 +3,14 @@\n \tstruct net_device *netdev = dev_get_drvdata(&pdev->dev);\n \tstruct emac_adapter *adpt = netdev_priv(netdev);\n \n+\tnetif_carrier_off(netdev);\n+\tnetif_tx_disable(netdev);\n+\n \tunregister_netdev(netdev);\n \tnetif_napi_del(&adpt->rx_q.napi);\n+\n+\tfree_irq(adpt->irq.irq, &adpt->irq);\n+\tcancel_work_sync(&adpt->work_thread);\n \n \temac_clks_teardown(adpt);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tnetif_carrier_off(netdev);",
                "\tnetif_tx_disable(netdev);",
                "",
                "",
                "\tfree_irq(adpt->irq.irq, &adpt->irq);",
                "\tcancel_work_sync(&adpt->work_thread);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-33974",
        "func_name": "RIOT-OS/RIOT/_handle_ack",
        "description": "RIOT-OS, an operating system for Internet of Things (IoT) devices, contains a network stack with the ability to process 6LoWPAN frames. In versions 2023.01 and prior, an attacker can send multiple crafted frames to the device to trigger a race condition. The race condition invalidates assumptions about the program state and leads to an invalid memory access resulting in denial of service. This issue is patched in pull request 19679. There are no known workarounds.",
        "git_url": "https://github.com/RIOT-OS/RIOT/commit/31c6191f6196f1a05c9765cffeadba868e3b0723",
        "commit_title": "gnrc_sixlowpan_frag_sfr: fix ARQ scheduler race-condition",
        "commit_text": "",
        "func_before": "static void _handle_ack(gnrc_netif_hdr_t *netif_hdr, gnrc_pktsnip_t *pkt,\n                        unsigned page)\n{\n    gnrc_sixlowpan_frag_vrb_t *vrbe;\n    sixlowpan_sfr_ack_t *hdr = pkt->data;\n    uint32_t recv_time = xtimer_now_usec();\n\n    (void)page;\n    DEBUG(\"6lo sfr: received ACK for datagram (%s, %02x): %02X%02X%02X%02X\\n\",\n          gnrc_netif_addr_to_str(gnrc_netif_hdr_get_src_addr(netif_hdr),\n                                 netif_hdr->src_l2addr_len,\n                                 addr_str), hdr->base.tag,\n          hdr->bitmap[0], hdr->bitmap[1], hdr->bitmap[2], hdr->bitmap[3]);\n    if ((vrbe = gnrc_sixlowpan_frag_vrb_reverse(\n            gnrc_netif_hdr_get_netif(netif_hdr),\n            gnrc_netif_hdr_get_src_addr(netif_hdr),\n            netif_hdr->src_l2addr_len, hdr->base.tag)) != NULL) {\n        /* we found a VRB entry by reverse lookup, forward ACK further down. */\n        sixlowpan_sfr_t mock_base = { .disp_ecn = hdr->base.disp_ecn,\n                                      .tag = vrbe->super.tag };\n        DEBUG(\"6lo sfr: forward ACK to (%s, %02x)\\n\",\n              gnrc_netif_addr_to_str(vrbe->super.src, vrbe->super.src_len,\n                                     addr_str), vrbe->super.tag);\n        _send_ack(vrbe->in_netif, vrbe->super.src, vrbe->super.src_len,\n                  &mock_base, hdr->bitmap);\n        if (IS_USED(MODULE_GNRC_SIXLOWPAN_FRAG_SFR_STATS)) {\n            _stats.acks.forwarded++;\n        }\n        if ((unaligned_get_u32(hdr->bitmap) == _full_bitmap.u32) ||\n            (unaligned_get_u32(hdr->bitmap) == _null_bitmap.u32)) {\n            if (CONFIG_GNRC_SIXLOWPAN_FRAG_RBUF_DEL_TIMER > 0) {\n                /* garbage-collect entry after CONFIG_GNRC_SIXLOWPAN_FRAG_RBUF_DEL_TIMER\n                 * microseconds */\n                vrbe->super.arrival = recv_time -\n                                      (CONFIG_GNRC_SIXLOWPAN_FRAG_VRB_TIMEOUT_US -\n                                       CONFIG_GNRC_SIXLOWPAN_FRAG_RBUF_DEL_TIMER);\n            }\n            else {\n                gnrc_sixlowpan_frag_vrb_rm(vrbe);\n            }\n        }\n        else {\n            vrbe->super.arrival = recv_time;\n        }\n    }\n    else {\n        gnrc_sixlowpan_frag_fb_t *fbuf;\n\n        if ((fbuf = gnrc_sixlowpan_frag_fb_get_by_tag(hdr->base.tag)) != NULL) {\n            /* ACK for pending ACK timeout received. removing ACK timeout */\n            DEBUG(\"6lo sfr: cancelling ARQ timeout\\n\");\n            evtimer_del((evtimer_t *)(&_arq_timer),\n                        &fbuf->sfr.arq_timeout_event.event);\n            fbuf->sfr.arq_timeout_event.msg.content.ptr = NULL;\n            if ((unaligned_get_u32(hdr->bitmap) == _null_bitmap.u32)) {\n                /* ACK indicates the reassembling endpoint canceled reassembly\n                 */\n                DEBUG(\"6lo sfr: fragmentation canceled\\n\");\n                /* Retry to send whole datagram if configured, otherwise\n                 * cancel fragmentation */\n                _retry_datagram(fbuf);\n            }\n            else {\n                /* Check and resent failed fragments within the current window\n                 */\n               _check_failed_frags(hdr, fbuf, recv_time / US_PER_MS);\n            }\n        }\n        else {\n            DEBUG(\"6lo sfr: no VRB or fragmentation buffer found\\n\");\n        }\n    }\n    gnrc_pktbuf_release(pkt);\n}",
        "func": "static void _handle_ack(gnrc_netif_hdr_t *netif_hdr, gnrc_pktsnip_t *pkt,\n                        unsigned page)\n{\n    gnrc_sixlowpan_frag_vrb_t *vrbe;\n    sixlowpan_sfr_ack_t *hdr = pkt->data;\n    uint32_t recv_time = xtimer_now_usec();\n\n    (void)page;\n    DEBUG(\"6lo sfr: received ACK for datagram (%s, %02x): %02X%02X%02X%02X\\n\",\n          gnrc_netif_addr_to_str(gnrc_netif_hdr_get_src_addr(netif_hdr),\n                                 netif_hdr->src_l2addr_len,\n                                 addr_str), hdr->base.tag,\n          hdr->bitmap[0], hdr->bitmap[1], hdr->bitmap[2], hdr->bitmap[3]);\n    if ((vrbe = gnrc_sixlowpan_frag_vrb_reverse(\n            gnrc_netif_hdr_get_netif(netif_hdr),\n            gnrc_netif_hdr_get_src_addr(netif_hdr),\n            netif_hdr->src_l2addr_len, hdr->base.tag)) != NULL) {\n        /* we found a VRB entry by reverse lookup, forward ACK further down. */\n        sixlowpan_sfr_t mock_base = { .disp_ecn = hdr->base.disp_ecn,\n                                      .tag = vrbe->super.tag };\n        DEBUG(\"6lo sfr: forward ACK to (%s, %02x)\\n\",\n              gnrc_netif_addr_to_str(vrbe->super.src, vrbe->super.src_len,\n                                     addr_str), vrbe->super.tag);\n        _send_ack(vrbe->in_netif, vrbe->super.src, vrbe->super.src_len,\n                  &mock_base, hdr->bitmap);\n        if (IS_USED(MODULE_GNRC_SIXLOWPAN_FRAG_SFR_STATS)) {\n            _stats.acks.forwarded++;\n        }\n        if ((unaligned_get_u32(hdr->bitmap) == _full_bitmap.u32) ||\n            (unaligned_get_u32(hdr->bitmap) == _null_bitmap.u32)) {\n            if (CONFIG_GNRC_SIXLOWPAN_FRAG_RBUF_DEL_TIMER > 0) {\n                /* garbage-collect entry after CONFIG_GNRC_SIXLOWPAN_FRAG_RBUF_DEL_TIMER\n                 * microseconds */\n                vrbe->super.arrival = recv_time -\n                                      (CONFIG_GNRC_SIXLOWPAN_FRAG_VRB_TIMEOUT_US -\n                                       CONFIG_GNRC_SIXLOWPAN_FRAG_RBUF_DEL_TIMER);\n            }\n            else {\n                gnrc_sixlowpan_frag_vrb_rm(vrbe);\n            }\n        }\n        else {\n            vrbe->super.arrival = recv_time;\n        }\n    }\n    else {\n        gnrc_sixlowpan_frag_fb_t *fbuf;\n\n        if ((fbuf = gnrc_sixlowpan_frag_fb_get_by_tag(hdr->base.tag)) != NULL) {\n            /* ACK for pending ACK timeout received. removing ACK timeout */\n            DEBUG(\"6lo sfr: cancelling ARQ timeout\\n\");\n            evtimer_del((evtimer_t *)(&_arq_timer),\n                        &fbuf->sfr.arq_timeout_event.event);\n            if ((unaligned_get_u32(hdr->bitmap) == _null_bitmap.u32)) {\n                /* ACK indicates the reassembling endpoint canceled reassembly\n                 */\n                DEBUG(\"6lo sfr: fragmentation canceled\\n\");\n                /* Retry to send whole datagram if configured, otherwise\n                 * cancel fragmentation */\n                _retry_datagram(fbuf);\n            }\n            else {\n                /* Check and resent failed fragments within the current window\n                 */\n               _check_failed_frags(hdr, fbuf, recv_time / US_PER_MS);\n            }\n        }\n        else {\n            DEBUG(\"6lo sfr: no VRB or fragmentation buffer found\\n\");\n        }\n    }\n    gnrc_pktbuf_release(pkt);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -51,7 +51,6 @@\n             DEBUG(\"6lo sfr: cancelling ARQ timeout\\n\");\n             evtimer_del((evtimer_t *)(&_arq_timer),\n                         &fbuf->sfr.arq_timeout_event.event);\n-            fbuf->sfr.arq_timeout_event.msg.content.ptr = NULL;\n             if ((unaligned_get_u32(hdr->bitmap) == _null_bitmap.u32)) {\n                 /* ACK indicates the reassembling endpoint canceled reassembly\n                  */",
        "diff_line_info": {
            "deleted_lines": [
                "            fbuf->sfr.arq_timeout_event.msg.content.ptr = NULL;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-33974",
        "func_name": "RIOT-OS/RIOT/_sched_arq_timeout",
        "description": "RIOT-OS, an operating system for Internet of Things (IoT) devices, contains a network stack with the ability to process 6LoWPAN frames. In versions 2023.01 and prior, an attacker can send multiple crafted frames to the device to trigger a race condition. The race condition invalidates assumptions about the program state and leads to an invalid memory access resulting in denial of service. This issue is patched in pull request 19679. There are no known workarounds.",
        "git_url": "https://github.com/RIOT-OS/RIOT/commit/31c6191f6196f1a05c9765cffeadba868e3b0723",
        "commit_title": "gnrc_sixlowpan_frag_sfr: fix ARQ scheduler race-condition",
        "commit_text": "",
        "func_before": "static void _sched_arq_timeout(gnrc_sixlowpan_frag_fb_t *fbuf, uint32_t offset)\n{\n    if (IS_ACTIVE(CONFIG_GNRC_SIXLOWPAN_SFR_MOCK_ARQ_TIMER)) {\n        /* mock does not need to be scheduled */\n        return;\n    }\n    if (fbuf->sfr.arq_timeout_event.msg.content.ptr != NULL) {\n        DEBUG(\"6lo sfr: ARQ timeout for datagram %u already scheduled\\n\",\n              (uint8_t)fbuf->tag);\n        return;\n    }\n    DEBUG(\"6lo sfr: arming ACK timeout in %lums for datagram %u\\n\",\n          (long unsigned)offset, fbuf->tag);\n    fbuf->sfr.arq_timeout_event.event.offset = offset;\n    fbuf->sfr.arq_timeout_event.msg.content.ptr = fbuf;\n    fbuf->sfr.arq_timeout_event.msg.type = GNRC_SIXLOWPAN_FRAG_SFR_ARQ_TIMEOUT_MSG;\n    evtimer_add_msg(&_arq_timer, &fbuf->sfr.arq_timeout_event,\n                    _getpid());\n}",
        "func": "static void _sched_arq_timeout(gnrc_sixlowpan_frag_fb_t *fbuf, uint32_t offset)\n{\n    if (IS_ACTIVE(CONFIG_GNRC_SIXLOWPAN_SFR_MOCK_ARQ_TIMER)) {\n        /* mock does not need to be scheduled */\n        return;\n    }\n    if (_arq_scheduled(fbuf)) {\n        DEBUG(\"6lo sfr: ARQ timeout for datagram %u already scheduled\\n\",\n              (uint8_t)fbuf->tag);\n        return;\n    }\n    DEBUG(\"6lo sfr: arming ACK timeout in %lums for datagram %u\\n\",\n          (long unsigned)offset, fbuf->tag);\n    fbuf->sfr.arq_timeout_event.event.offset = offset;\n    fbuf->sfr.arq_timeout_event.msg.content.ptr = fbuf;\n    fbuf->sfr.arq_timeout_event.msg.type = GNRC_SIXLOWPAN_FRAG_SFR_ARQ_TIMEOUT_MSG;\n    evtimer_add_msg(&_arq_timer, &fbuf->sfr.arq_timeout_event,\n                    _getpid());\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n         /* mock does not need to be scheduled */\n         return;\n     }\n-    if (fbuf->sfr.arq_timeout_event.msg.content.ptr != NULL) {\n+    if (_arq_scheduled(fbuf)) {\n         DEBUG(\"6lo sfr: ARQ timeout for datagram %u already scheduled\\n\",\n               (uint8_t)fbuf->tag);\n         return;",
        "diff_line_info": {
            "deleted_lines": [
                "    if (fbuf->sfr.arq_timeout_event.msg.content.ptr != NULL) {"
            ],
            "added_lines": [
                "    if (_arq_scheduled(fbuf)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-33974",
        "func_name": "RIOT-OS/RIOT/gnrc_sixlowpan_frag_sfr_arq_timeout",
        "description": "RIOT-OS, an operating system for Internet of Things (IoT) devices, contains a network stack with the ability to process 6LoWPAN frames. In versions 2023.01 and prior, an attacker can send multiple crafted frames to the device to trigger a race condition. The race condition invalidates assumptions about the program state and leads to an invalid memory access resulting in denial of service. This issue is patched in pull request 19679. There are no known workarounds.",
        "git_url": "https://github.com/RIOT-OS/RIOT/commit/31c6191f6196f1a05c9765cffeadba868e3b0723",
        "commit_title": "gnrc_sixlowpan_frag_sfr: fix ARQ scheduler race-condition",
        "commit_text": "",
        "func_before": "void gnrc_sixlowpan_frag_sfr_arq_timeout(gnrc_sixlowpan_frag_fb_t *fbuf)\n{\n    uint32_t now = xtimer_now_usec() / US_PER_MS;\n    _frag_desc_t *frag_desc = (_frag_desc_t *)fbuf->sfr.window.next;\n    uint32_t next_arq_offset = fbuf->sfr.arq_timeout;\n    bool reschedule_arq_timeout = false;\n    int error_no = ETIMEDOUT;   /* assume time out for fbuf->pkt */\n\n    DEBUG(\"6lo sfr: ARQ timeout for datagram %u\\n\", fbuf->tag);\n    fbuf->sfr.arq_timeout_event.msg.content.ptr = NULL;\n    if (IS_ACTIVE(CONFIG_GNRC_SIXLOWPAN_SFR_MOCK_ARQ_TIMER)) {\n        /* mock-up to emulate time having passed beyond (1us) the ARQ timeout */\n        now -= (fbuf->sfr.arq_timeout * US_PER_MS) + 1;\n    }\n    if (IS_USED(MODULE_GNRC_SIXLOWPAN_FRAG_SFR_CONGURE) && frag_desc) {\n        /* report timeout to CongURE state */\n        gnrc_sixlowpan_frag_sfr_congure_snd_report_frags_timeout(fbuf);\n        _shrink_window(fbuf);   /* potentially shrink window */\n        /* reassign frag_desc, in case window head changed */\n        frag_desc = (_frag_desc_t *)fbuf->sfr.window.next;\n    }\n    /* copying clist_foreach because we can't work just in function context */\n    _frag_desc_t * const head = frag_desc;\n    if (frag_desc) {\n        do {\n            uint32_t diff;\n\n            frag_desc = (_frag_desc_t *)frag_desc->super.super.next;\n            diff = now - frag_desc->super.send_time;\n            if (diff < fbuf->sfr.arq_timeout) {\n                /* this fragment's last was last sent < fbuf->sfr.arq_timeout\n                 * ago */\n                uint32_t offset = fbuf->sfr.arq_timeout - diff;\n\n                DEBUG(\"6lo sfr: wait for fragment %u in next reschedule\\n\",\n                      _frag_seq(frag_desc));\n                if (offset < next_arq_offset) {\n                    /* wait for this fragments ACK next */\n                    next_arq_offset = offset;\n                    DEBUG(\"         (next ARQ timeout in %lu)\\n\",\n                          (long unsigned)next_arq_offset);\n                }\n                /* this fragment is still waiting for its ACK,\n                 * reschedule the next ACK timeout to the difference\n                 * of the ACK timeout and the time of its last send */\n                reschedule_arq_timeout = true;\n            }\n            else if (_frag_ack_req(frag_desc)) {\n                /* for this fragment we requested an ACK which was not received\n                 * yet. Try to resend it */\n                if ((frag_desc->super.resends++) < CONFIG_GNRC_SIXLOWPAN_SFR_FRAG_RETRIES) {\n                    /* we have retries left for this fragment */\n                    DEBUG(\"6lo sfr: %u retries left for fragment (tag: %u, \"\n                          \"X: %i, seq: %u, frag_size: %u, offset: %u)\\n\",\n                          CONFIG_GNRC_SIXLOWPAN_SFR_FRAG_RETRIES -\n                          (frag_desc->super.resends - 1), (uint8_t)fbuf->tag,\n                          _frag_ack_req(frag_desc), _frag_seq(frag_desc),\n                          _frag_size(frag_desc), frag_desc->offset);\n                    if (_resend_frag(&frag_desc->super.super, fbuf) != 0) {\n                        /* _resend_frag failed due to a memory resource\n                         * problem */\n                        error_no = ENOMEM;\n                        goto error;\n                    }\n                    else {\n                        if (IS_USED(MODULE_GNRC_SIXLOWPAN_FRAG_SFR_CONGURE)) {\n                            /* fragment was resent successfully, report this to CongURE state\n                             * object */\n                            gnrc_sixlowpan_frag_sfr_congure_snd_report_frag_sent(fbuf);\n                        }\n                        if (IS_USED(MODULE_GNRC_SIXLOWPAN_FRAG_SFR_STATS)) {\n                            /* fragment was resent successfully, note this done\n                             * in the statistics */\n                            _stats.fragment_resends.by_timeout++;\n                        }\n                    }\n                    /* fragment was resent successfully, schedule next ACK\n                     * timeout */\n                    reschedule_arq_timeout = true;\n                }\n                else {\n                    /* out of retries */\n                    DEBUG(\"6lo sfr: no retries left for fragment \"\n                          \"(tag: %u, X: %i, seq: %u, frag_size: %u, \"\n                          \"offset: %u)\\n\",\n                          (uint8_t)fbuf->tag, _frag_ack_req(frag_desc),\n                          _frag_seq(frag_desc), _frag_size(frag_desc),\n                          frag_desc->offset);\n                    /* we are out of retries on the fragment level, but we\n                     * might be able to retry the datagram if retries for the\n                     * datagram are configured. */\n                    _retry_datagram(fbuf);\n                    return;\n                }\n            }\n            else {\n                /* Do not resend fragments that were not explicitly asking for\n                 * an ACK from the reassembling endpoint on ACK timeout.\n                 * If this is true for all fragments remaining in the fragment\n                 * buffer, the datagram is to be considered timed out, so\n                 * error_no should remain ETIMEDOUT */\n                DEBUG(\"6lo sfr: nothing to do for fragment %u\\n\",\n                      _frag_seq(frag_desc));\n            }\n        } while (frag_desc != head);\n        /* report all non-ack_req fragments in window also as sent, since even\n         * the lost fragments are still in flight (even though they were\n         * previously marked as timed out) */\n        clist_foreach(&fbuf->sfr.window, _report_non_ack_req_window_sent, fbuf);\n    }\n    else {\n        /* No fragments to resend, we can assume the packet was delivered\n         * successfully */\n        error_no = GNRC_NETERR_SUCCESS;\n    }\n    assert(fbuf->sfr.frags_sent == clist_count(&fbuf->sfr.window));\n    if (reschedule_arq_timeout) {\n        _sched_arq_timeout(fbuf, next_arq_offset);\n        return;\n    }\nerror:\n    /* don't check return value, as we don't want to wait for an ACK again ;-) */\n    _send_abort_frag(fbuf->pkt, fbuf, false, 0);\n    _clean_up_fbuf(fbuf, error_no);\n}",
        "func": "void gnrc_sixlowpan_frag_sfr_arq_timeout(gnrc_sixlowpan_frag_fb_t *fbuf)\n{\n    uint32_t now = xtimer_now_usec() / US_PER_MS;\n    _frag_desc_t *frag_desc = (_frag_desc_t *)fbuf->sfr.window.next;\n    uint32_t next_arq_offset = fbuf->sfr.arq_timeout;\n    bool reschedule_arq_timeout = false;\n    int error_no = ETIMEDOUT;   /* assume time out for fbuf->pkt */\n\n    DEBUG(\"6lo sfr: ARQ timeout for datagram %u\\n\", fbuf->tag);\n    if (IS_ACTIVE(CONFIG_GNRC_SIXLOWPAN_SFR_MOCK_ARQ_TIMER)) {\n        /* mock-up to emulate time having passed beyond (1us) the ARQ timeout */\n        now -= (fbuf->sfr.arq_timeout * US_PER_MS) + 1;\n    }\n    if (IS_USED(MODULE_GNRC_SIXLOWPAN_FRAG_SFR_CONGURE) && frag_desc) {\n        /* report timeout to CongURE state */\n        gnrc_sixlowpan_frag_sfr_congure_snd_report_frags_timeout(fbuf);\n        _shrink_window(fbuf);   /* potentially shrink window */\n        /* reassign frag_desc, in case window head changed */\n        frag_desc = (_frag_desc_t *)fbuf->sfr.window.next;\n    }\n    /* copying clist_foreach because we can't work just in function context */\n    _frag_desc_t * const head = frag_desc;\n    if (frag_desc) {\n        do {\n            uint32_t diff;\n\n            frag_desc = (_frag_desc_t *)frag_desc->super.super.next;\n            diff = now - frag_desc->super.send_time;\n            if (diff < fbuf->sfr.arq_timeout) {\n                /* this fragment's last was last sent < fbuf->sfr.arq_timeout\n                 * ago */\n                uint32_t offset = fbuf->sfr.arq_timeout - diff;\n\n                DEBUG(\"6lo sfr: wait for fragment %u in next reschedule\\n\",\n                      _frag_seq(frag_desc));\n                if (offset < next_arq_offset) {\n                    /* wait for this fragments ACK next */\n                    next_arq_offset = offset;\n                    DEBUG(\"         (next ARQ timeout in %lu)\\n\",\n                          (long unsigned)next_arq_offset);\n                }\n                /* this fragment is still waiting for its ACK,\n                 * reschedule the next ACK timeout to the difference\n                 * of the ACK timeout and the time of its last send */\n                reschedule_arq_timeout = true;\n            }\n            else if (_frag_ack_req(frag_desc)) {\n                /* for this fragment we requested an ACK which was not received\n                 * yet. Try to resend it */\n                if ((frag_desc->super.resends++) < CONFIG_GNRC_SIXLOWPAN_SFR_FRAG_RETRIES) {\n                    /* we have retries left for this fragment */\n                    DEBUG(\"6lo sfr: %u retries left for fragment (tag: %u, \"\n                          \"X: %i, seq: %u, frag_size: %u, offset: %u)\\n\",\n                          CONFIG_GNRC_SIXLOWPAN_SFR_FRAG_RETRIES -\n                          (frag_desc->super.resends - 1), (uint8_t)fbuf->tag,\n                          _frag_ack_req(frag_desc), _frag_seq(frag_desc),\n                          _frag_size(frag_desc), frag_desc->offset);\n                    if (_resend_frag(&frag_desc->super.super, fbuf) != 0) {\n                        /* _resend_frag failed due to a memory resource\n                         * problem */\n                        error_no = ENOMEM;\n                        goto error;\n                    }\n                    else {\n                        if (IS_USED(MODULE_GNRC_SIXLOWPAN_FRAG_SFR_CONGURE)) {\n                            /* fragment was resent successfully, report this to CongURE state\n                             * object */\n                            gnrc_sixlowpan_frag_sfr_congure_snd_report_frag_sent(fbuf);\n                        }\n                        if (IS_USED(MODULE_GNRC_SIXLOWPAN_FRAG_SFR_STATS)) {\n                            /* fragment was resent successfully, note this done\n                             * in the statistics */\n                            _stats.fragment_resends.by_timeout++;\n                        }\n                    }\n                    /* fragment was resent successfully, schedule next ACK\n                     * timeout */\n                    reschedule_arq_timeout = true;\n                }\n                else {\n                    /* out of retries */\n                    DEBUG(\"6lo sfr: no retries left for fragment \"\n                          \"(tag: %u, X: %i, seq: %u, frag_size: %u, \"\n                          \"offset: %u)\\n\",\n                          (uint8_t)fbuf->tag, _frag_ack_req(frag_desc),\n                          _frag_seq(frag_desc), _frag_size(frag_desc),\n                          frag_desc->offset);\n                    /* we are out of retries on the fragment level, but we\n                     * might be able to retry the datagram if retries for the\n                     * datagram are configured. */\n                    _retry_datagram(fbuf);\n                    return;\n                }\n            }\n            else {\n                /* Do not resend fragments that were not explicitly asking for\n                 * an ACK from the reassembling endpoint on ACK timeout.\n                 * If this is true for all fragments remaining in the fragment\n                 * buffer, the datagram is to be considered timed out, so\n                 * error_no should remain ETIMEDOUT */\n                DEBUG(\"6lo sfr: nothing to do for fragment %u\\n\",\n                      _frag_seq(frag_desc));\n            }\n        } while (frag_desc != head);\n        /* report all non-ack_req fragments in window also as sent, since even\n         * the lost fragments are still in flight (even though they were\n         * previously marked as timed out) */\n        clist_foreach(&fbuf->sfr.window, _report_non_ack_req_window_sent, fbuf);\n    }\n    else {\n        /* No fragments to resend, we can assume the packet was delivered\n         * successfully */\n        error_no = GNRC_NETERR_SUCCESS;\n    }\n    assert(fbuf->sfr.frags_sent == clist_count(&fbuf->sfr.window));\n    if (reschedule_arq_timeout) {\n        _sched_arq_timeout(fbuf, next_arq_offset);\n        return;\n    }\nerror:\n    /* don't check return value, as we don't want to wait for an ACK again ;-) */\n    _send_abort_frag(fbuf->pkt, fbuf, false, 0);\n    _clean_up_fbuf(fbuf, error_no);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,6 @@\n     int error_no = ETIMEDOUT;   /* assume time out for fbuf->pkt */\n \n     DEBUG(\"6lo sfr: ARQ timeout for datagram %u\\n\", fbuf->tag);\n-    fbuf->sfr.arq_timeout_event.msg.content.ptr = NULL;\n     if (IS_ACTIVE(CONFIG_GNRC_SIXLOWPAN_SFR_MOCK_ARQ_TIMER)) {\n         /* mock-up to emulate time having passed beyond (1us) the ARQ timeout */\n         now -= (fbuf->sfr.arq_timeout * US_PER_MS) + 1;",
        "diff_line_info": {
            "deleted_lines": [
                "    fbuf->sfr.arq_timeout_event.msg.content.ptr = NULL;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-33974",
        "func_name": "RIOT-OS/RIOT/_clean_slate_datagram",
        "description": "RIOT-OS, an operating system for Internet of Things (IoT) devices, contains a network stack with the ability to process 6LoWPAN frames. In versions 2023.01 and prior, an attacker can send multiple crafted frames to the device to trigger a race condition. The race condition invalidates assumptions about the program state and leads to an invalid memory access resulting in denial of service. This issue is patched in pull request 19679. There are no known workarounds.",
        "git_url": "https://github.com/RIOT-OS/RIOT/commit/31c6191f6196f1a05c9765cffeadba868e3b0723",
        "commit_title": "gnrc_sixlowpan_frag_sfr: fix ARQ scheduler race-condition",
        "commit_text": "",
        "func_before": "static void _clean_slate_datagram(gnrc_sixlowpan_frag_fb_t *fbuf)\n{\n    clist_node_t new_queue = { .next = NULL };\n\n    fbuf->sfr.arq_timeout_event.msg.content.ptr = NULL;\n    /* remove potentially scheduled timers for this datagram */\n    evtimer_del((evtimer_t *)(&_arq_timer),\n                &fbuf->sfr.arq_timeout_event.event);\n    fbuf->sfr.arq_timeout_event.event.next = NULL;\n    if (gnrc_sixlowpan_frag_sfr_congure_snd_has_inter_frame_gap()) {\n        for (clist_node_t *node = clist_lpop(&_frame_queue);\n             node != NULL; node = clist_lpop(&_frame_queue)) {\n            _frame_queue_t *entry = (_frame_queue_t *)node;\n            /* remove frames of this datagram from frame queue */\n            if (entry->datagram_tag == fbuf->tag) {\n                gnrc_pktbuf_release(entry->frame);\n                /* unset packet just to be safe */\n                entry->frame = NULL;\n                clist_rpush(&_frag_descs_free, node);\n            }\n            else {\n                clist_rpush(&new_queue, node);\n            }\n        }\n        /* reset frame queue with remaining frames */\n        _frame_queue = new_queue;\n    }\n    fbuf->offset = 0U;\n    fbuf->sfr.cur_seq = 0U;\n    fbuf->sfr.frags_sent = 0U;\n    for (clist_node_t *node = clist_lpop(&fbuf->sfr.window);\n         node != NULL; node = clist_lpop(&fbuf->sfr.window)) {\n        clist_rpush(&_frag_descs_free, node);\n    }\n}",
        "func": "static void _clean_slate_datagram(gnrc_sixlowpan_frag_fb_t *fbuf)\n{\n    clist_node_t new_queue = { .next = NULL };\n\n    fbuf->sfr.arq_timeout_event.msg.content.ptr = NULL;\n    /* remove potentially scheduled timers for this datagram */\n    evtimer_del((evtimer_t *)(&_arq_timer),\n                &fbuf->sfr.arq_timeout_event.event);\n    if (gnrc_sixlowpan_frag_sfr_congure_snd_has_inter_frame_gap()) {\n        for (clist_node_t *node = clist_lpop(&_frame_queue);\n             node != NULL; node = clist_lpop(&_frame_queue)) {\n            _frame_queue_t *entry = (_frame_queue_t *)node;\n            /* remove frames of this datagram from frame queue */\n            if (entry->datagram_tag == fbuf->tag) {\n                gnrc_pktbuf_release(entry->frame);\n                /* unset packet just to be safe */\n                entry->frame = NULL;\n                clist_rpush(&_frag_descs_free, node);\n            }\n            else {\n                clist_rpush(&new_queue, node);\n            }\n        }\n        /* reset frame queue with remaining frames */\n        _frame_queue = new_queue;\n    }\n    fbuf->offset = 0U;\n    fbuf->sfr.cur_seq = 0U;\n    fbuf->sfr.frags_sent = 0U;\n    for (clist_node_t *node = clist_lpop(&fbuf->sfr.window);\n         node != NULL; node = clist_lpop(&fbuf->sfr.window)) {\n        clist_rpush(&_frag_descs_free, node);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,6 @@\n     /* remove potentially scheduled timers for this datagram */\n     evtimer_del((evtimer_t *)(&_arq_timer),\n                 &fbuf->sfr.arq_timeout_event.event);\n-    fbuf->sfr.arq_timeout_event.event.next = NULL;\n     if (gnrc_sixlowpan_frag_sfr_congure_snd_has_inter_frame_gap()) {\n         for (clist_node_t *node = clist_lpop(&_frame_queue);\n              node != NULL; node = clist_lpop(&_frame_queue)) {",
        "diff_line_info": {
            "deleted_lines": [
                "    fbuf->sfr.arq_timeout_event.event.next = NULL;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2019-19537",
        "func_name": "torvalds/linux/usb_register_dev",
        "description": "In the Linux kernel before 5.2.10, there is a race condition bug that can be caused by a malicious USB device in the USB character device driver layer, aka CID-303911cfc5b9. This affects drivers/usb/core/file.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=303911cfc5b95d33687d9046133ff184cf5043ff",
        "commit_title": "The syzbot fuzzer has found two (!) races in the USB character device",
        "commit_text": "registration and deregistration routines.  This patch fixes the races.  The first race results from the fact that usb_deregister_dev() sets usb_minors[intf->minor] to NULL before calling device_destroy() on the class device.  This leaves a window during which another thread can allocate the same minor number but will encounter a duplicate name error when it tries to register its own class device.  A typical error message in the system log would look like:      sysfs: cannot create duplicate filename '/class/usbmisc/ldusb0'  The patch fixes this race by destroying the class device first.  The second race is in usb_register_dev().  When that routine runs, it first allocates a minor number, then drops minor_rwsem, and then creates the class device.  If the device creation fails, the minor number is deallocated and the whole routine returns an error.  But during the time while minor_rwsem was dropped, there is a window in which the minor number is allocated and so another thread can successfully open the device file.  Typically this results in use-after-free errors or invalid accesses when the other thread closes its open file reference, because the kernel then tries to release resources that were already deallocated when usb_register_dev() failed.  The patch fixes this race by keeping minor_rwsem locked throughout the entire routine.  Reported-and-tested-by: syzbot+30cf45ebfe0b0c4847a1@syzkaller.appspotmail.com Link: https://lore.kernel.org/r/Pine.LNX.4.44L0.1908121607590.1659-100000@iolanthe.rowland.org ",
        "func_before": "int usb_register_dev(struct usb_interface *intf,\n\t\t     struct usb_class_driver *class_driver)\n{\n\tint retval;\n\tint minor_base = class_driver->minor_base;\n\tint minor;\n\tchar name[20];\n\n#ifdef CONFIG_USB_DYNAMIC_MINORS\n\t/*\n\t * We don't care what the device tries to start at, we want to start\n\t * at zero to pack the devices into the smallest available space with\n\t * no holes in the minor range.\n\t */\n\tminor_base = 0;\n#endif\n\n\tif (class_driver->fops == NULL)\n\t\treturn -EINVAL;\n\tif (intf->minor >= 0)\n\t\treturn -EADDRINUSE;\n\n\tmutex_lock(&init_usb_class_mutex);\n\tretval = init_usb_class();\n\tmutex_unlock(&init_usb_class_mutex);\n\n\tif (retval)\n\t\treturn retval;\n\n\tdev_dbg(&intf->dev, \"looking for a minor, starting at %d\\n\", minor_base);\n\n\tdown_write(&minor_rwsem);\n\tfor (minor = minor_base; minor < MAX_USB_MINORS; ++minor) {\n\t\tif (usb_minors[minor])\n\t\t\tcontinue;\n\n\t\tusb_minors[minor] = class_driver->fops;\n\t\tintf->minor = minor;\n\t\tbreak;\n\t}\n\tup_write(&minor_rwsem);\n\tif (intf->minor < 0)\n\t\treturn -EXFULL;\n\n\t/* create a usb class device for this usb interface */\n\tsnprintf(name, sizeof(name), class_driver->name, minor - minor_base);\n\tintf->usb_dev = device_create(usb_class->class, &intf->dev,\n\t\t\t\t      MKDEV(USB_MAJOR, minor), class_driver,\n\t\t\t\t      \"%s\", kbasename(name));\n\tif (IS_ERR(intf->usb_dev)) {\n\t\tdown_write(&minor_rwsem);\n\t\tusb_minors[minor] = NULL;\n\t\tintf->minor = -1;\n\t\tup_write(&minor_rwsem);\n\t\tretval = PTR_ERR(intf->usb_dev);\n\t}\n\treturn retval;\n}",
        "func": "int usb_register_dev(struct usb_interface *intf,\n\t\t     struct usb_class_driver *class_driver)\n{\n\tint retval;\n\tint minor_base = class_driver->minor_base;\n\tint minor;\n\tchar name[20];\n\n#ifdef CONFIG_USB_DYNAMIC_MINORS\n\t/*\n\t * We don't care what the device tries to start at, we want to start\n\t * at zero to pack the devices into the smallest available space with\n\t * no holes in the minor range.\n\t */\n\tminor_base = 0;\n#endif\n\n\tif (class_driver->fops == NULL)\n\t\treturn -EINVAL;\n\tif (intf->minor >= 0)\n\t\treturn -EADDRINUSE;\n\n\tmutex_lock(&init_usb_class_mutex);\n\tretval = init_usb_class();\n\tmutex_unlock(&init_usb_class_mutex);\n\n\tif (retval)\n\t\treturn retval;\n\n\tdev_dbg(&intf->dev, \"looking for a minor, starting at %d\\n\", minor_base);\n\n\tdown_write(&minor_rwsem);\n\tfor (minor = minor_base; minor < MAX_USB_MINORS; ++minor) {\n\t\tif (usb_minors[minor])\n\t\t\tcontinue;\n\n\t\tusb_minors[minor] = class_driver->fops;\n\t\tintf->minor = minor;\n\t\tbreak;\n\t}\n\tif (intf->minor < 0) {\n\t\tup_write(&minor_rwsem);\n\t\treturn -EXFULL;\n\t}\n\n\t/* create a usb class device for this usb interface */\n\tsnprintf(name, sizeof(name), class_driver->name, minor - minor_base);\n\tintf->usb_dev = device_create(usb_class->class, &intf->dev,\n\t\t\t\t      MKDEV(USB_MAJOR, minor), class_driver,\n\t\t\t\t      \"%s\", kbasename(name));\n\tif (IS_ERR(intf->usb_dev)) {\n\t\tusb_minors[minor] = NULL;\n\t\tintf->minor = -1;\n\t\tretval = PTR_ERR(intf->usb_dev);\n\t}\n\tup_write(&minor_rwsem);\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,9 +38,10 @@\n \t\tintf->minor = minor;\n \t\tbreak;\n \t}\n-\tup_write(&minor_rwsem);\n-\tif (intf->minor < 0)\n+\tif (intf->minor < 0) {\n+\t\tup_write(&minor_rwsem);\n \t\treturn -EXFULL;\n+\t}\n \n \t/* create a usb class device for this usb interface */\n \tsnprintf(name, sizeof(name), class_driver->name, minor - minor_base);\n@@ -48,11 +49,10 @@\n \t\t\t\t      MKDEV(USB_MAJOR, minor), class_driver,\n \t\t\t\t      \"%s\", kbasename(name));\n \tif (IS_ERR(intf->usb_dev)) {\n-\t\tdown_write(&minor_rwsem);\n \t\tusb_minors[minor] = NULL;\n \t\tintf->minor = -1;\n-\t\tup_write(&minor_rwsem);\n \t\tretval = PTR_ERR(intf->usb_dev);\n \t}\n+\tup_write(&minor_rwsem);\n \treturn retval;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tup_write(&minor_rwsem);",
                "\tif (intf->minor < 0)",
                "\t\tdown_write(&minor_rwsem);",
                "\t\tup_write(&minor_rwsem);"
            ],
            "added_lines": [
                "\tif (intf->minor < 0) {",
                "\t\tup_write(&minor_rwsem);",
                "\t}",
                "\tup_write(&minor_rwsem);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19537",
        "func_name": "torvalds/linux/usb_deregister_dev",
        "description": "In the Linux kernel before 5.2.10, there is a race condition bug that can be caused by a malicious USB device in the USB character device driver layer, aka CID-303911cfc5b9. This affects drivers/usb/core/file.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=303911cfc5b95d33687d9046133ff184cf5043ff",
        "commit_title": "The syzbot fuzzer has found two (!) races in the USB character device",
        "commit_text": "registration and deregistration routines.  This patch fixes the races.  The first race results from the fact that usb_deregister_dev() sets usb_minors[intf->minor] to NULL before calling device_destroy() on the class device.  This leaves a window during which another thread can allocate the same minor number but will encounter a duplicate name error when it tries to register its own class device.  A typical error message in the system log would look like:      sysfs: cannot create duplicate filename '/class/usbmisc/ldusb0'  The patch fixes this race by destroying the class device first.  The second race is in usb_register_dev().  When that routine runs, it first allocates a minor number, then drops minor_rwsem, and then creates the class device.  If the device creation fails, the minor number is deallocated and the whole routine returns an error.  But during the time while minor_rwsem was dropped, there is a window in which the minor number is allocated and so another thread can successfully open the device file.  Typically this results in use-after-free errors or invalid accesses when the other thread closes its open file reference, because the kernel then tries to release resources that were already deallocated when usb_register_dev() failed.  The patch fixes this race by keeping minor_rwsem locked throughout the entire routine.  Reported-and-tested-by: syzbot+30cf45ebfe0b0c4847a1@syzkaller.appspotmail.com Link: https://lore.kernel.org/r/Pine.LNX.4.44L0.1908121607590.1659-100000@iolanthe.rowland.org ",
        "func_before": "void usb_deregister_dev(struct usb_interface *intf,\n\t\t\tstruct usb_class_driver *class_driver)\n{\n\tif (intf->minor == -1)\n\t\treturn;\n\n\tdev_dbg(&intf->dev, \"removing %d minor\\n\", intf->minor);\n\n\tdown_write(&minor_rwsem);\n\tusb_minors[intf->minor] = NULL;\n\tup_write(&minor_rwsem);\n\n\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));\n\tintf->usb_dev = NULL;\n\tintf->minor = -1;\n\tdestroy_usb_class();\n}",
        "func": "void usb_deregister_dev(struct usb_interface *intf,\n\t\t\tstruct usb_class_driver *class_driver)\n{\n\tif (intf->minor == -1)\n\t\treturn;\n\n\tdev_dbg(&intf->dev, \"removing %d minor\\n\", intf->minor);\n\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));\n\n\tdown_write(&minor_rwsem);\n\tusb_minors[intf->minor] = NULL;\n\tup_write(&minor_rwsem);\n\n\tintf->usb_dev = NULL;\n\tintf->minor = -1;\n\tdestroy_usb_class();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,12 +5,12 @@\n \t\treturn;\n \n \tdev_dbg(&intf->dev, \"removing %d minor\\n\", intf->minor);\n+\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));\n \n \tdown_write(&minor_rwsem);\n \tusb_minors[intf->minor] = NULL;\n \tup_write(&minor_rwsem);\n \n-\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));\n \tintf->usb_dev = NULL;\n \tintf->minor = -1;\n \tdestroy_usb_class();",
        "diff_line_info": {
            "deleted_lines": [
                "\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));"
            ],
            "added_lines": [
                "\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-7533",
        "func_name": "torvalds/linux/debugfs_rename",
        "description": "Race condition in the fsnotify implementation in the Linux kernel through 4.12.4 allows local users to gain privileges or cause a denial of service (memory corruption) via a crafted application that leverages simultaneous execution of the inotify_handle_event and vfs_rename functions.",
        "git_url": "https://github.com/torvalds/linux/commit/49d31c2f389acfe83417083e1208422b4091cd9e",
        "commit_title": "dentry name snapshots",
        "commit_text": " take_dentry_name_snapshot() takes a safe snapshot of dentry name; if the name is a short one, it gets copied into caller-supplied structure, otherwise an extra reference to external name is grabbed (those are never modified).  In either case the pointer to stable string is stored into the same structure.  dentry must be held by the caller of take_dentry_name_snapshot(), but may be freely dropped afterwards - the snapshot will stay until destroyed by release_dentry_name_snapshot().  Intended use: \tstruct name_snapshot s;  \ttake_dentry_name_snapshot(&s, dentry); \t... \taccess s.name \t... \trelease_dentry_name_snapshot(&s);  Replaces fsnotify_oldname_...(), gets used in fsnotify to obtain the name to pass down with event. ",
        "func_before": "struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n\t\tstruct dentry *new_dir, const char *new_name)\n{\n\tint error;\n\tstruct dentry *dentry = NULL, *trap;\n\tconst char *old_name;\n\n\ttrap = lock_rename(new_dir, old_dir);\n\t/* Source or destination directories don't exist? */\n\tif (d_really_is_negative(old_dir) || d_really_is_negative(new_dir))\n\t\tgoto exit;\n\t/* Source does not exist, cyclic rename, or mountpoint? */\n\tif (d_really_is_negative(old_dentry) || old_dentry == trap ||\n\t    d_mountpoint(old_dentry))\n\t\tgoto exit;\n\tdentry = lookup_one_len(new_name, new_dir, strlen(new_name));\n\t/* Lookup failed, cyclic rename or target exists? */\n\tif (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))\n\t\tgoto exit;\n\n\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n\n\terror = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),\n\t\t\t      dentry, 0);\n\tif (error) {\n\t\tfsnotify_oldname_free(old_name);\n\t\tgoto exit;\n\t}\n\td_move(old_dentry, dentry);\n\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,\n\t\td_is_dir(old_dentry),\n\t\tNULL, old_dentry);\n\tfsnotify_oldname_free(old_name);\n\tunlock_rename(new_dir, old_dir);\n\tdput(dentry);\n\treturn old_dentry;\nexit:\n\tif (dentry && !IS_ERR(dentry))\n\t\tdput(dentry);\n\tunlock_rename(new_dir, old_dir);\n\treturn NULL;\n}",
        "func": "struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n\t\tstruct dentry *new_dir, const char *new_name)\n{\n\tint error;\n\tstruct dentry *dentry = NULL, *trap;\n\tstruct name_snapshot old_name;\n\n\ttrap = lock_rename(new_dir, old_dir);\n\t/* Source or destination directories don't exist? */\n\tif (d_really_is_negative(old_dir) || d_really_is_negative(new_dir))\n\t\tgoto exit;\n\t/* Source does not exist, cyclic rename, or mountpoint? */\n\tif (d_really_is_negative(old_dentry) || old_dentry == trap ||\n\t    d_mountpoint(old_dentry))\n\t\tgoto exit;\n\tdentry = lookup_one_len(new_name, new_dir, strlen(new_name));\n\t/* Lookup failed, cyclic rename or target exists? */\n\tif (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))\n\t\tgoto exit;\n\n\ttake_dentry_name_snapshot(&old_name, old_dentry);\n\n\terror = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),\n\t\t\t      dentry, 0);\n\tif (error) {\n\t\trelease_dentry_name_snapshot(&old_name);\n\t\tgoto exit;\n\t}\n\td_move(old_dentry, dentry);\n\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,\n\t\td_is_dir(old_dentry),\n\t\tNULL, old_dentry);\n\trelease_dentry_name_snapshot(&old_name);\n\tunlock_rename(new_dir, old_dir);\n\tdput(dentry);\n\treturn old_dentry;\nexit:\n\tif (dentry && !IS_ERR(dentry))\n\t\tdput(dentry);\n\tunlock_rename(new_dir, old_dir);\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n {\n \tint error;\n \tstruct dentry *dentry = NULL, *trap;\n-\tconst char *old_name;\n+\tstruct name_snapshot old_name;\n \n \ttrap = lock_rename(new_dir, old_dir);\n \t/* Source or destination directories don't exist? */\n@@ -18,19 +18,19 @@\n \tif (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))\n \t\tgoto exit;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \n \terror = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),\n \t\t\t      dentry, 0);\n \tif (error) {\n-\t\tfsnotify_oldname_free(old_name);\n+\t\trelease_dentry_name_snapshot(&old_name);\n \t\tgoto exit;\n \t}\n \td_move(old_dentry, dentry);\n-\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,\n+\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,\n \t\td_is_dir(old_dentry),\n \t\tNULL, old_dentry);\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \tunlock_rename(new_dir, old_dir);\n \tdput(dentry);\n \treturn old_dentry;",
        "diff_line_info": {
            "deleted_lines": [
                "\tconst char *old_name;",
                "\told_name = fsnotify_oldname_init(old_dentry->d_name.name);",
                "\t\tfsnotify_oldname_free(old_name);",
                "\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,",
                "\tfsnotify_oldname_free(old_name);"
            ],
            "added_lines": [
                "\tstruct name_snapshot old_name;",
                "\ttake_dentry_name_snapshot(&old_name, old_dentry);",
                "\t\trelease_dentry_name_snapshot(&old_name);",
                "\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,",
                "\trelease_dentry_name_snapshot(&old_name);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-7533",
        "func_name": "torvalds/linux/vfs_rename",
        "description": "Race condition in the fsnotify implementation in the Linux kernel through 4.12.4 allows local users to gain privileges or cause a denial of service (memory corruption) via a crafted application that leverages simultaneous execution of the inotify_handle_event and vfs_rename functions.",
        "git_url": "https://github.com/torvalds/linux/commit/49d31c2f389acfe83417083e1208422b4091cd9e",
        "commit_title": "dentry name snapshots",
        "commit_text": " take_dentry_name_snapshot() takes a safe snapshot of dentry name; if the name is a short one, it gets copied into caller-supplied structure, otherwise an extra reference to external name is grabbed (those are never modified).  In either case the pointer to stable string is stored into the same structure.  dentry must be held by the caller of take_dentry_name_snapshot(), but may be freely dropped afterwards - the snapshot will stay until destroyed by release_dentry_name_snapshot().  Intended use: \tstruct name_snapshot s;  \ttake_dentry_name_snapshot(&s, dentry); \t... \taccess s.name \t... \trelease_dentry_name_snapshot(&s);  Replaces fsnotify_oldname_...(), gets used in fsnotify to obtain the name to pass down with event. ",
        "func_before": "int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n\t       struct inode *new_dir, struct dentry *new_dentry,\n\t       struct inode **delegated_inode, unsigned int flags)\n{\n\tint error;\n\tbool is_dir = d_is_dir(old_dentry);\n\tconst unsigned char *old_name;\n\tstruct inode *source = old_dentry->d_inode;\n\tstruct inode *target = new_dentry->d_inode;\n\tbool new_is_dir = false;\n\tunsigned max_links = new_dir->i_sb->s_max_links;\n\n\tif (source == target)\n\t\treturn 0;\n\n\terror = may_delete(old_dir, old_dentry, is_dir);\n\tif (error)\n\t\treturn error;\n\n\tif (!target) {\n\t\terror = may_create(new_dir, new_dentry);\n\t} else {\n\t\tnew_is_dir = d_is_dir(new_dentry);\n\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\terror = may_delete(new_dir, new_dentry, is_dir);\n\t\telse\n\t\t\terror = may_delete(new_dir, new_dentry, new_is_dir);\n\t}\n\tif (error)\n\t\treturn error;\n\n\tif (!old_dir->i_op->rename)\n\t\treturn -EPERM;\n\n\t/*\n\t * If we are going to change the parent - check write permissions,\n\t * we'll need to flip '..'.\n\t */\n\tif (new_dir != old_dir) {\n\t\tif (is_dir) {\n\t\t\terror = inode_permission(source, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t\tif ((flags & RENAME_EXCHANGE) && new_is_dir) {\n\t\t\terror = inode_permission(target, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\n\n\terror = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,\n\t\t\t\t      flags);\n\tif (error)\n\t\treturn error;\n\n\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n\tdget(new_dentry);\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_lock(target);\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))\n\t\tgoto out;\n\n\tif (max_links && new_dir != old_dir) {\n\t\terror = -EMLINK;\n\t\tif (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t\tif ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&\n\t\t    old_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t}\n\tif (is_dir && !(flags & RENAME_EXCHANGE) && target)\n\t\tshrink_dcache_parent(new_dentry);\n\tif (!is_dir) {\n\t\terror = try_break_deleg(source, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tif (target && !new_is_dir) {\n\t\terror = try_break_deleg(target, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\terror = old_dir->i_op->rename(old_dir, old_dentry,\n\t\t\t\t       new_dir, new_dentry, flags);\n\tif (error)\n\t\tgoto out;\n\n\tif (!(flags & RENAME_EXCHANGE) && target) {\n\t\tif (is_dir)\n\t\t\ttarget->i_flags |= S_DEAD;\n\t\tdont_mount(new_dentry);\n\t\tdetach_mounts(new_dentry);\n\t}\n\tif (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\td_move(old_dentry, new_dentry);\n\t\telse\n\t\t\td_exchange(old_dentry, new_dentry);\n\t}\nout:\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tunlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_unlock(target);\n\tdput(new_dentry);\n\tif (!error) {\n\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,\n\t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n\t\tif (flags & RENAME_EXCHANGE) {\n\t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n\t\t\t\t      new_is_dir, NULL, new_dentry);\n\t\t}\n\t}\n\tfsnotify_oldname_free(old_name);\n\n\treturn error;\n}",
        "func": "int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n\t       struct inode *new_dir, struct dentry *new_dentry,\n\t       struct inode **delegated_inode, unsigned int flags)\n{\n\tint error;\n\tbool is_dir = d_is_dir(old_dentry);\n\tstruct inode *source = old_dentry->d_inode;\n\tstruct inode *target = new_dentry->d_inode;\n\tbool new_is_dir = false;\n\tunsigned max_links = new_dir->i_sb->s_max_links;\n\tstruct name_snapshot old_name;\n\n\tif (source == target)\n\t\treturn 0;\n\n\terror = may_delete(old_dir, old_dentry, is_dir);\n\tif (error)\n\t\treturn error;\n\n\tif (!target) {\n\t\terror = may_create(new_dir, new_dentry);\n\t} else {\n\t\tnew_is_dir = d_is_dir(new_dentry);\n\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\terror = may_delete(new_dir, new_dentry, is_dir);\n\t\telse\n\t\t\terror = may_delete(new_dir, new_dentry, new_is_dir);\n\t}\n\tif (error)\n\t\treturn error;\n\n\tif (!old_dir->i_op->rename)\n\t\treturn -EPERM;\n\n\t/*\n\t * If we are going to change the parent - check write permissions,\n\t * we'll need to flip '..'.\n\t */\n\tif (new_dir != old_dir) {\n\t\tif (is_dir) {\n\t\t\terror = inode_permission(source, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t\tif ((flags & RENAME_EXCHANGE) && new_is_dir) {\n\t\t\terror = inode_permission(target, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\n\n\terror = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,\n\t\t\t\t      flags);\n\tif (error)\n\t\treturn error;\n\n\ttake_dentry_name_snapshot(&old_name, old_dentry);\n\tdget(new_dentry);\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_lock(target);\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))\n\t\tgoto out;\n\n\tif (max_links && new_dir != old_dir) {\n\t\terror = -EMLINK;\n\t\tif (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t\tif ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&\n\t\t    old_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t}\n\tif (is_dir && !(flags & RENAME_EXCHANGE) && target)\n\t\tshrink_dcache_parent(new_dentry);\n\tif (!is_dir) {\n\t\terror = try_break_deleg(source, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tif (target && !new_is_dir) {\n\t\terror = try_break_deleg(target, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\terror = old_dir->i_op->rename(old_dir, old_dentry,\n\t\t\t\t       new_dir, new_dentry, flags);\n\tif (error)\n\t\tgoto out;\n\n\tif (!(flags & RENAME_EXCHANGE) && target) {\n\t\tif (is_dir)\n\t\t\ttarget->i_flags |= S_DEAD;\n\t\tdont_mount(new_dentry);\n\t\tdetach_mounts(new_dentry);\n\t}\n\tif (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\td_move(old_dentry, new_dentry);\n\t\telse\n\t\t\td_exchange(old_dentry, new_dentry);\n\t}\nout:\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tunlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_unlock(target);\n\tdput(new_dentry);\n\tif (!error) {\n\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,\n\t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n\t\tif (flags & RENAME_EXCHANGE) {\n\t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n\t\t\t\t      new_is_dir, NULL, new_dentry);\n\t\t}\n\t}\n\trelease_dentry_name_snapshot(&old_name);\n\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,11 +4,11 @@\n {\n \tint error;\n \tbool is_dir = d_is_dir(old_dentry);\n-\tconst unsigned char *old_name;\n \tstruct inode *source = old_dentry->d_inode;\n \tstruct inode *target = new_dentry->d_inode;\n \tbool new_is_dir = false;\n \tunsigned max_links = new_dir->i_sb->s_max_links;\n+\tstruct name_snapshot old_name;\n \n \tif (source == target)\n \t\treturn 0;\n@@ -55,7 +55,7 @@\n \tif (error)\n \t\treturn error;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \tdget(new_dentry);\n \tif (!is_dir || (flags & RENAME_EXCHANGE))\n \t\tlock_two_nondirectories(source, target);\n@@ -110,14 +110,14 @@\n \t\tinode_unlock(target);\n \tdput(new_dentry);\n \tif (!error) {\n-\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,\n+\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,\n \t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n \t\tif (flags & RENAME_EXCHANGE) {\n \t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n \t\t\t\t      new_is_dir, NULL, new_dentry);\n \t\t}\n \t}\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \n \treturn error;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tconst unsigned char *old_name;",
                "\told_name = fsnotify_oldname_init(old_dentry->d_name.name);",
                "\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,",
                "\tfsnotify_oldname_free(old_name);"
            ],
            "added_lines": [
                "\tstruct name_snapshot old_name;",
                "\ttake_dentry_name_snapshot(&old_name, old_dentry);",
                "\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,",
                "\trelease_dentry_name_snapshot(&old_name);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-7533",
        "func_name": "torvalds/linux/__fsnotify_parent",
        "description": "Race condition in the fsnotify implementation in the Linux kernel through 4.12.4 allows local users to gain privileges or cause a denial of service (memory corruption) via a crafted application that leverages simultaneous execution of the inotify_handle_event and vfs_rename functions.",
        "git_url": "https://github.com/torvalds/linux/commit/49d31c2f389acfe83417083e1208422b4091cd9e",
        "commit_title": "dentry name snapshots",
        "commit_text": " take_dentry_name_snapshot() takes a safe snapshot of dentry name; if the name is a short one, it gets copied into caller-supplied structure, otherwise an extra reference to external name is grabbed (those are never modified).  In either case the pointer to stable string is stored into the same structure.  dentry must be held by the caller of take_dentry_name_snapshot(), but may be freely dropped afterwards - the snapshot will stay until destroyed by release_dentry_name_snapshot().  Intended use: \tstruct name_snapshot s;  \ttake_dentry_name_snapshot(&s, dentry); \t... \taccess s.name \t... \trelease_dentry_name_snapshot(&s);  Replaces fsnotify_oldname_...(), gets used in fsnotify to obtain the name to pass down with event. ",
        "func_before": "int __fsnotify_parent(const struct path *path, struct dentry *dentry, __u32 mask)\n{\n\tstruct dentry *parent;\n\tstruct inode *p_inode;\n\tint ret = 0;\n\n\tif (!dentry)\n\t\tdentry = path->dentry;\n\n\tif (!(dentry->d_flags & DCACHE_FSNOTIFY_PARENT_WATCHED))\n\t\treturn 0;\n\n\tparent = dget_parent(dentry);\n\tp_inode = parent->d_inode;\n\n\tif (unlikely(!fsnotify_inode_watches_children(p_inode)))\n\t\t__fsnotify_update_child_dentry_flags(p_inode);\n\telse if (p_inode->i_fsnotify_mask & mask) {\n\t\t/* we are notifying a parent so come up with the new mask which\n\t\t * specifies these are events which came from a child. */\n\t\tmask |= FS_EVENT_ON_CHILD;\n\n\t\tif (path)\n\t\t\tret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,\n\t\t\t\t       dentry->d_name.name, 0);\n\t\telse\n\t\t\tret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,\n\t\t\t\t       dentry->d_name.name, 0);\n\t}\n\n\tdput(parent);\n\n\treturn ret;\n}",
        "func": "int __fsnotify_parent(const struct path *path, struct dentry *dentry, __u32 mask)\n{\n\tstruct dentry *parent;\n\tstruct inode *p_inode;\n\tint ret = 0;\n\n\tif (!dentry)\n\t\tdentry = path->dentry;\n\n\tif (!(dentry->d_flags & DCACHE_FSNOTIFY_PARENT_WATCHED))\n\t\treturn 0;\n\n\tparent = dget_parent(dentry);\n\tp_inode = parent->d_inode;\n\n\tif (unlikely(!fsnotify_inode_watches_children(p_inode)))\n\t\t__fsnotify_update_child_dentry_flags(p_inode);\n\telse if (p_inode->i_fsnotify_mask & mask) {\n\t\tstruct name_snapshot name;\n\n\t\t/* we are notifying a parent so come up with the new mask which\n\t\t * specifies these are events which came from a child. */\n\t\tmask |= FS_EVENT_ON_CHILD;\n\n\t\ttake_dentry_name_snapshot(&name, dentry);\n\t\tif (path)\n\t\t\tret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,\n\t\t\t\t       name.name, 0);\n\t\telse\n\t\t\tret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,\n\t\t\t\t       name.name, 0);\n\t\trelease_dentry_name_snapshot(&name);\n\t}\n\n\tdput(parent);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,16 +16,20 @@\n \tif (unlikely(!fsnotify_inode_watches_children(p_inode)))\n \t\t__fsnotify_update_child_dentry_flags(p_inode);\n \telse if (p_inode->i_fsnotify_mask & mask) {\n+\t\tstruct name_snapshot name;\n+\n \t\t/* we are notifying a parent so come up with the new mask which\n \t\t * specifies these are events which came from a child. */\n \t\tmask |= FS_EVENT_ON_CHILD;\n \n+\t\ttake_dentry_name_snapshot(&name, dentry);\n \t\tif (path)\n \t\t\tret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n \t\telse\n \t\t\tret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n+\t\trelease_dentry_name_snapshot(&name);\n \t}\n \n \tdput(parent);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t       dentry->d_name.name, 0);",
                "\t\t\t\t       dentry->d_name.name, 0);"
            ],
            "added_lines": [
                "\t\tstruct name_snapshot name;",
                "",
                "\t\ttake_dentry_name_snapshot(&name, dentry);",
                "\t\t\t\t       name.name, 0);",
                "\t\t\t\t       name.name, 0);",
                "\t\trelease_dentry_name_snapshot(&name);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-12136",
        "func_name": "xen-project/xen/__get_maptrack_handle",
        "description": "Race condition in the grant table code in Xen 4.6.x through 4.9.x allows local guest OS administrators to cause a denial of service (free list corruption and host crash) or gain privileges on the host via vectors involving maptrack free list handling.",
        "git_url": "https://github.com/xen-project/xen/commit/02cbeeb6207508b0f04a2c6181445c8eb3f1e117",
        "commit_title": "gnttab: split maptrack lock to make it fulfill its purpose again",
        "commit_text": " The way the lock is currently being used in get_maptrack_handle(), it protects only the maptrack limit: The function acts on current's list only, so races on list accesses are impossible even without the lock.  Otoh list access races are possible between __get_maptrack_handle() and put_maptrack_handle(), due to the invocation of the former for other than current from steal_maptrack_handle(). Introduce a per-vCPU lock for list accesses to become race free again. This lock will be uncontended except when it becomes necessary to take the steal path, i.e. in the common case there should be no meaningful performance impact.  When in get_maptrack_handle adds a stolen entry to a fresh, empty, freelist, we think that there is probably no concurrency.  However, this is not a fast path and adding the locking there makes the code clearly correct.  Also, while we are here: the stolen maptrack_entry's tail pointer was not properly set.  Set it.  This is CVE-2017-12136 / XSA-228. ",
        "func_before": "static inline int\n__get_maptrack_handle(\n    struct grant_table *t,\n    struct vcpu *v)\n{\n    unsigned int head, next, prev_head;\n\n    do {\n        /* No maptrack pages allocated for this VCPU yet? */\n        head = read_atomic(&v->maptrack_head);\n        if ( unlikely(head == MAPTRACK_TAIL) )\n            return -1;\n\n        /*\n         * Always keep one entry in the free list to make it easier to\n         * add free entries to the tail.\n         */\n        next = read_atomic(&maptrack_entry(t, head).ref);\n        if ( unlikely(next == MAPTRACK_TAIL) )\n            return -1;\n\n        prev_head = head;\n        head = cmpxchg(&v->maptrack_head, prev_head, next);\n    } while ( head != prev_head );\n\n    return head;\n}",
        "func": "static inline int\n__get_maptrack_handle(\n    struct grant_table *t,\n    struct vcpu *v)\n{\n    unsigned int head, next, prev_head;\n\n    spin_lock(&v->maptrack_freelist_lock);\n\n    do {\n        /* No maptrack pages allocated for this VCPU yet? */\n        head = read_atomic(&v->maptrack_head);\n        if ( unlikely(head == MAPTRACK_TAIL) )\n        {\n            spin_unlock(&v->maptrack_freelist_lock);\n            return -1;\n        }\n\n        /*\n         * Always keep one entry in the free list to make it easier to\n         * add free entries to the tail.\n         */\n        next = read_atomic(&maptrack_entry(t, head).ref);\n        if ( unlikely(next == MAPTRACK_TAIL) )\n        {\n            spin_unlock(&v->maptrack_freelist_lock);\n            return -1;\n        }\n\n        prev_head = head;\n        head = cmpxchg(&v->maptrack_head, prev_head, next);\n    } while ( head != prev_head );\n\n    spin_unlock(&v->maptrack_freelist_lock);\n\n    return head;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,11 +5,16 @@\n {\n     unsigned int head, next, prev_head;\n \n+    spin_lock(&v->maptrack_freelist_lock);\n+\n     do {\n         /* No maptrack pages allocated for this VCPU yet? */\n         head = read_atomic(&v->maptrack_head);\n         if ( unlikely(head == MAPTRACK_TAIL) )\n+        {\n+            spin_unlock(&v->maptrack_freelist_lock);\n             return -1;\n+        }\n \n         /*\n          * Always keep one entry in the free list to make it easier to\n@@ -17,11 +22,16 @@\n          */\n         next = read_atomic(&maptrack_entry(t, head).ref);\n         if ( unlikely(next == MAPTRACK_TAIL) )\n+        {\n+            spin_unlock(&v->maptrack_freelist_lock);\n             return -1;\n+        }\n \n         prev_head = head;\n         head = cmpxchg(&v->maptrack_head, prev_head, next);\n     } while ( head != prev_head );\n \n+    spin_unlock(&v->maptrack_freelist_lock);\n+\n     return head;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    spin_lock(&v->maptrack_freelist_lock);",
                "",
                "        {",
                "            spin_unlock(&v->maptrack_freelist_lock);",
                "        }",
                "        {",
                "            spin_unlock(&v->maptrack_freelist_lock);",
                "        }",
                "    spin_unlock(&v->maptrack_freelist_lock);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-12136",
        "func_name": "xen-project/xen/grant_table_init_vcpu",
        "description": "Race condition in the grant table code in Xen 4.6.x through 4.9.x allows local guest OS administrators to cause a denial of service (free list corruption and host crash) or gain privileges on the host via vectors involving maptrack free list handling.",
        "git_url": "https://github.com/xen-project/xen/commit/02cbeeb6207508b0f04a2c6181445c8eb3f1e117",
        "commit_title": "gnttab: split maptrack lock to make it fulfill its purpose again",
        "commit_text": " The way the lock is currently being used in get_maptrack_handle(), it protects only the maptrack limit: The function acts on current's list only, so races on list accesses are impossible even without the lock.  Otoh list access races are possible between __get_maptrack_handle() and put_maptrack_handle(), due to the invocation of the former for other than current from steal_maptrack_handle(). Introduce a per-vCPU lock for list accesses to become race free again. This lock will be uncontended except when it becomes necessary to take the steal path, i.e. in the common case there should be no meaningful performance impact.  When in get_maptrack_handle adds a stolen entry to a fresh, empty, freelist, we think that there is probably no concurrency.  However, this is not a fast path and adding the locking there makes the code clearly correct.  Also, while we are here: the stolen maptrack_entry's tail pointer was not properly set.  Set it.  This is CVE-2017-12136 / XSA-228. ",
        "func_before": "void grant_table_init_vcpu(struct vcpu *v)\n{\n    v->maptrack_head = MAPTRACK_TAIL;\n    v->maptrack_tail = MAPTRACK_TAIL;\n}",
        "func": "void grant_table_init_vcpu(struct vcpu *v)\n{\n    spin_lock_init(&v->maptrack_freelist_lock);\n    v->maptrack_head = MAPTRACK_TAIL;\n    v->maptrack_tail = MAPTRACK_TAIL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n void grant_table_init_vcpu(struct vcpu *v)\n {\n+    spin_lock_init(&v->maptrack_freelist_lock);\n     v->maptrack_head = MAPTRACK_TAIL;\n     v->maptrack_tail = MAPTRACK_TAIL;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    spin_lock_init(&v->maptrack_freelist_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-12136",
        "func_name": "xen-project/xen/put_maptrack_handle",
        "description": "Race condition in the grant table code in Xen 4.6.x through 4.9.x allows local guest OS administrators to cause a denial of service (free list corruption and host crash) or gain privileges on the host via vectors involving maptrack free list handling.",
        "git_url": "https://github.com/xen-project/xen/commit/02cbeeb6207508b0f04a2c6181445c8eb3f1e117",
        "commit_title": "gnttab: split maptrack lock to make it fulfill its purpose again",
        "commit_text": " The way the lock is currently being used in get_maptrack_handle(), it protects only the maptrack limit: The function acts on current's list only, so races on list accesses are impossible even without the lock.  Otoh list access races are possible between __get_maptrack_handle() and put_maptrack_handle(), due to the invocation of the former for other than current from steal_maptrack_handle(). Introduce a per-vCPU lock for list accesses to become race free again. This lock will be uncontended except when it becomes necessary to take the steal path, i.e. in the common case there should be no meaningful performance impact.  When in get_maptrack_handle adds a stolen entry to a fresh, empty, freelist, we think that there is probably no concurrency.  However, this is not a fast path and adding the locking there makes the code clearly correct.  Also, while we are here: the stolen maptrack_entry's tail pointer was not properly set.  Set it.  This is CVE-2017-12136 / XSA-228. ",
        "func_before": "static inline void\nput_maptrack_handle(\n    struct grant_table *t, int handle)\n{\n    struct domain *currd = current->domain;\n    struct vcpu *v;\n    unsigned int prev_tail, cur_tail;\n\n    /* 1. Set entry to be a tail. */\n    maptrack_entry(t, handle).ref = MAPTRACK_TAIL;\n\n    /* 2. Add entry to the tail of the list on the original VCPU. */\n    v = currd->vcpu[maptrack_entry(t, handle).vcpu];\n\n    cur_tail = read_atomic(&v->maptrack_tail);\n    do {\n        prev_tail = cur_tail;\n        cur_tail = cmpxchg(&v->maptrack_tail, prev_tail, handle);\n    } while ( cur_tail != prev_tail );\n\n    /* 3. Update the old tail entry to point to the new entry. */\n    write_atomic(&maptrack_entry(t, prev_tail).ref, handle);\n}",
        "func": "static inline void\nput_maptrack_handle(\n    struct grant_table *t, int handle)\n{\n    struct domain *currd = current->domain;\n    struct vcpu *v;\n    unsigned int prev_tail, cur_tail;\n\n    /* 1. Set entry to be a tail. */\n    maptrack_entry(t, handle).ref = MAPTRACK_TAIL;\n\n    /* 2. Add entry to the tail of the list on the original VCPU. */\n    v = currd->vcpu[maptrack_entry(t, handle).vcpu];\n\n    spin_lock(&v->maptrack_freelist_lock);\n\n    cur_tail = read_atomic(&v->maptrack_tail);\n    do {\n        prev_tail = cur_tail;\n        cur_tail = cmpxchg(&v->maptrack_tail, prev_tail, handle);\n    } while ( cur_tail != prev_tail );\n\n    /* 3. Update the old tail entry to point to the new entry. */\n    write_atomic(&maptrack_entry(t, prev_tail).ref, handle);\n\n    spin_unlock(&v->maptrack_freelist_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,8 @@\n     /* 2. Add entry to the tail of the list on the original VCPU. */\n     v = currd->vcpu[maptrack_entry(t, handle).vcpu];\n \n+    spin_lock(&v->maptrack_freelist_lock);\n+\n     cur_tail = read_atomic(&v->maptrack_tail);\n     do {\n         prev_tail = cur_tail;\n@@ -20,4 +22,6 @@\n \n     /* 3. Update the old tail entry to point to the new entry. */\n     write_atomic(&maptrack_entry(t, prev_tail).ref, handle);\n+\n+    spin_unlock(&v->maptrack_freelist_lock);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    spin_lock(&v->maptrack_freelist_lock);",
                "",
                "",
                "    spin_unlock(&v->maptrack_freelist_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-12136",
        "func_name": "xen-project/xen/get_maptrack_handle",
        "description": "Race condition in the grant table code in Xen 4.6.x through 4.9.x allows local guest OS administrators to cause a denial of service (free list corruption and host crash) or gain privileges on the host via vectors involving maptrack free list handling.",
        "git_url": "https://github.com/xen-project/xen/commit/02cbeeb6207508b0f04a2c6181445c8eb3f1e117",
        "commit_title": "gnttab: split maptrack lock to make it fulfill its purpose again",
        "commit_text": " The way the lock is currently being used in get_maptrack_handle(), it protects only the maptrack limit: The function acts on current's list only, so races on list accesses are impossible even without the lock.  Otoh list access races are possible between __get_maptrack_handle() and put_maptrack_handle(), due to the invocation of the former for other than current from steal_maptrack_handle(). Introduce a per-vCPU lock for list accesses to become race free again. This lock will be uncontended except when it becomes necessary to take the steal path, i.e. in the common case there should be no meaningful performance impact.  When in get_maptrack_handle adds a stolen entry to a fresh, empty, freelist, we think that there is probably no concurrency.  However, this is not a fast path and adding the locking there makes the code clearly correct.  Also, while we are here: the stolen maptrack_entry's tail pointer was not properly set.  Set it.  This is CVE-2017-12136 / XSA-228. ",
        "func_before": "static inline int\nget_maptrack_handle(\n    struct grant_table *lgt)\n{\n    struct vcpu          *curr = current;\n    unsigned int          i, head;\n    grant_handle_t        handle;\n    struct grant_mapping *new_mt;\n\n    handle = __get_maptrack_handle(lgt, curr);\n    if ( likely(handle != -1) )\n        return handle;\n\n    spin_lock(&lgt->maptrack_lock);\n\n    /*\n     * If we've run out of frames, try stealing an entry from another\n     * VCPU (in case the guest isn't mapping across its VCPUs evenly).\n     */\n    if ( nr_maptrack_frames(lgt) >= max_maptrack_frames )\n    {\n        /*\n         * Can drop the lock since no other VCPU can be adding a new\n         * frame once they've run out.\n         */\n        spin_unlock(&lgt->maptrack_lock);\n\n        /*\n         * Uninitialized free list? Steal an extra entry for the tail\n         * sentinel.\n         */\n        if ( curr->maptrack_tail == MAPTRACK_TAIL )\n        {\n            handle = steal_maptrack_handle(lgt, curr);\n            if ( handle == -1 )\n                return -1;\n            curr->maptrack_tail = handle;\n            write_atomic(&curr->maptrack_head, handle);\n        }\n        return steal_maptrack_handle(lgt, curr);\n    }\n\n    new_mt = alloc_xenheap_page();\n    if ( !new_mt )\n    {\n        spin_unlock(&lgt->maptrack_lock);\n        return -1;\n    }\n    clear_page(new_mt);\n\n    /*\n     * Use the first new entry and add the remaining entries to the\n     * head of the free list.\n     */\n    handle = lgt->maptrack_limit;\n\n    for ( i = 0; i < MAPTRACK_PER_PAGE; i++ )\n    {\n        new_mt[i].ref = handle + i + 1;\n        new_mt[i].vcpu = curr->vcpu_id;\n    }\n\n    /* Set tail directly if this is the first page for this VCPU. */\n    if ( curr->maptrack_tail == MAPTRACK_TAIL )\n        curr->maptrack_tail = handle + MAPTRACK_PER_PAGE - 1;\n\n    lgt->maptrack[nr_maptrack_frames(lgt)] = new_mt;\n    smp_wmb();\n    lgt->maptrack_limit += MAPTRACK_PER_PAGE;\n\n    do {\n        new_mt[i - 1].ref = read_atomic(&curr->maptrack_head);\n        head = cmpxchg(&curr->maptrack_head, new_mt[i - 1].ref, handle + 1);\n    } while ( head != new_mt[i - 1].ref );\n\n    spin_unlock(&lgt->maptrack_lock);\n\n    return handle;\n}",
        "func": "static inline int\nget_maptrack_handle(\n    struct grant_table *lgt)\n{\n    struct vcpu          *curr = current;\n    unsigned int          i, head;\n    grant_handle_t        handle;\n    struct grant_mapping *new_mt;\n\n    handle = __get_maptrack_handle(lgt, curr);\n    if ( likely(handle != -1) )\n        return handle;\n\n    spin_lock(&lgt->maptrack_lock);\n\n    /*\n     * If we've run out of frames, try stealing an entry from another\n     * VCPU (in case the guest isn't mapping across its VCPUs evenly).\n     */\n    if ( nr_maptrack_frames(lgt) >= max_maptrack_frames )\n    {\n        spin_unlock(&lgt->maptrack_lock);\n\n        /*\n         * Uninitialized free list? Steal an extra entry for the tail\n         * sentinel.\n         */\n        if ( curr->maptrack_tail == MAPTRACK_TAIL )\n        {\n            handle = steal_maptrack_handle(lgt, curr);\n            if ( handle == -1 )\n                return -1;\n            spin_lock(&curr->maptrack_freelist_lock);\n            maptrack_entry(lgt, handle).ref = MAPTRACK_TAIL;\n            curr->maptrack_tail = handle;\n            if ( curr->maptrack_head == MAPTRACK_TAIL )\n                write_atomic(&curr->maptrack_head, handle);\n            spin_unlock(&curr->maptrack_freelist_lock);\n        }\n        return steal_maptrack_handle(lgt, curr);\n    }\n\n    new_mt = alloc_xenheap_page();\n    if ( !new_mt )\n    {\n        spin_unlock(&lgt->maptrack_lock);\n        return -1;\n    }\n    clear_page(new_mt);\n\n    /*\n     * Use the first new entry and add the remaining entries to the\n     * head of the free list.\n     */\n    handle = lgt->maptrack_limit;\n\n    for ( i = 0; i < MAPTRACK_PER_PAGE; i++ )\n    {\n        new_mt[i].ref = handle + i + 1;\n        new_mt[i].vcpu = curr->vcpu_id;\n    }\n\n    /* Set tail directly if this is the first page for this VCPU. */\n    if ( curr->maptrack_tail == MAPTRACK_TAIL )\n        curr->maptrack_tail = handle + MAPTRACK_PER_PAGE - 1;\n\n    lgt->maptrack[nr_maptrack_frames(lgt)] = new_mt;\n    smp_wmb();\n    lgt->maptrack_limit += MAPTRACK_PER_PAGE;\n\n    spin_unlock(&lgt->maptrack_lock);\n    spin_lock(&curr->maptrack_freelist_lock);\n\n    do {\n        new_mt[i - 1].ref = read_atomic(&curr->maptrack_head);\n        head = cmpxchg(&curr->maptrack_head, new_mt[i - 1].ref, handle + 1);\n    } while ( head != new_mt[i - 1].ref );\n\n    spin_unlock(&curr->maptrack_freelist_lock);\n\n    return handle;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,10 +19,6 @@\n      */\n     if ( nr_maptrack_frames(lgt) >= max_maptrack_frames )\n     {\n-        /*\n-         * Can drop the lock since no other VCPU can be adding a new\n-         * frame once they've run out.\n-         */\n         spin_unlock(&lgt->maptrack_lock);\n \n         /*\n@@ -34,8 +30,12 @@\n             handle = steal_maptrack_handle(lgt, curr);\n             if ( handle == -1 )\n                 return -1;\n+            spin_lock(&curr->maptrack_freelist_lock);\n+            maptrack_entry(lgt, handle).ref = MAPTRACK_TAIL;\n             curr->maptrack_tail = handle;\n-            write_atomic(&curr->maptrack_head, handle);\n+            if ( curr->maptrack_head == MAPTRACK_TAIL )\n+                write_atomic(&curr->maptrack_head, handle);\n+            spin_unlock(&curr->maptrack_freelist_lock);\n         }\n         return steal_maptrack_handle(lgt, curr);\n     }\n@@ -68,12 +68,15 @@\n     smp_wmb();\n     lgt->maptrack_limit += MAPTRACK_PER_PAGE;\n \n+    spin_unlock(&lgt->maptrack_lock);\n+    spin_lock(&curr->maptrack_freelist_lock);\n+\n     do {\n         new_mt[i - 1].ref = read_atomic(&curr->maptrack_head);\n         head = cmpxchg(&curr->maptrack_head, new_mt[i - 1].ref, handle + 1);\n     } while ( head != new_mt[i - 1].ref );\n \n-    spin_unlock(&lgt->maptrack_lock);\n+    spin_unlock(&curr->maptrack_freelist_lock);\n \n     return handle;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "        /*",
                "         * Can drop the lock since no other VCPU can be adding a new",
                "         * frame once they've run out.",
                "         */",
                "            write_atomic(&curr->maptrack_head, handle);",
                "    spin_unlock(&lgt->maptrack_lock);"
            ],
            "added_lines": [
                "            spin_lock(&curr->maptrack_freelist_lock);",
                "            maptrack_entry(lgt, handle).ref = MAPTRACK_TAIL;",
                "            if ( curr->maptrack_head == MAPTRACK_TAIL )",
                "                write_atomic(&curr->maptrack_head, handle);",
                "            spin_unlock(&curr->maptrack_freelist_lock);",
                "    spin_unlock(&lgt->maptrack_lock);",
                "    spin_lock(&curr->maptrack_freelist_lock);",
                "",
                "    spin_unlock(&curr->maptrack_freelist_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44512",
        "func_name": "tmate-io/tmate-ssh-server/main",
        "description": "World-writable permissions on the /tmp/tmate/sessions directory in tmate-ssh-server 2.3.0 allow a local attacker to compromise the integrity of session handling, or obtain the read-write session ID from a read-only session symlink in this directory.",
        "git_url": "https://github.com/tmate-io/tmate-ssh-server/commit/1c020d1f5ca462f5b150b46a027aaa1bbe3c9596",
        "commit_title": "Harden /tmp/tmate directory",
        "commit_text": " Suggested by Matthias Gerstner",
        "func_before": "int main(int argc, char **argv, char **envp)\n{\n\tint opt;\n\n\twhile ((opt = getopt(argc, argv, \"b:h:k:p:q:w:z:xv\")) != -1) {\n\t\tswitch (opt) {\n\t\tcase 'b':\n\t\t\ttmate_settings->bind_addr = xstrdup(optarg);\n\t\t\tbreak;\n\t\tcase 'h':\n\t\t\ttmate_settings->tmate_host = xstrdup(optarg);\n\t\t\tbreak;\n\t\tcase 'k':\n\t\t\ttmate_settings->keys_dir = xstrdup(optarg);\n\t\t\tbreak;\n\t\tcase 'p':\n\t\t\ttmate_settings->ssh_port = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'q':\n\t\t\ttmate_settings->ssh_port_advertized = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'w':\n\t\t\ttmate_settings->websocket_hostname = xstrdup(optarg);\n\t\t\tbreak;\n\t\tcase 'z':\n\t\t\ttmate_settings->websocket_port = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'x':\n\t\t\ttmate_settings->use_proxy_protocol = true;\n\t\t\tbreak;\n\t\tcase 'v':\n\t\t\ttmate_settings->log_level++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tinit_logging(tmate_settings->log_level);\n\n\tsetup_locale();\n\n\tif (!tmate_settings->tmate_host)\n\t\ttmate_settings->tmate_host = get_full_hostname();\n\n\tcmdline = *argv;\n\tcmdline_end = *envp;\n\n\ttmate_preload_trace_lib();\n\ttmate_catch_sigsegv();\n\ttmate_init_rand();\n\n\tif ((mkdir(TMATE_WORKDIR, 0701)             < 0 && errno != EEXIST) ||\n\t    (mkdir(TMATE_WORKDIR \"/sessions\", 0703) < 0 && errno != EEXIST) ||\n\t    (mkdir(TMATE_WORKDIR \"/jail\", 0700)     < 0 && errno != EEXIST))\n\t\ttmate_fatal(\"Cannot prepare session in \" TMATE_WORKDIR);\n\n\t/* The websocket server needs to access the /session dir to rename sockets */\n\tif ((chmod(TMATE_WORKDIR, 0701)             < 0) ||\n\t    (chmod(TMATE_WORKDIR \"/sessions\", 0703) < 0) ||\n\t    (chmod(TMATE_WORKDIR \"/jail\", 0700)     < 0))\n\t\ttmate_fatal(\"Cannot prepare session in \" TMATE_WORKDIR);\n\n\ttmate_ssh_server_main(tmate_session,\n\t\t\t      tmate_settings->keys_dir, tmate_settings->bind_addr, tmate_settings->ssh_port);\n\treturn 0;\n}",
        "func": "int main(int argc, char **argv, char **envp)\n{\n\tint opt;\n\n\twhile ((opt = getopt(argc, argv, \"b:h:k:p:q:w:z:xv\")) != -1) {\n\t\tswitch (opt) {\n\t\tcase 'b':\n\t\t\ttmate_settings->bind_addr = xstrdup(optarg);\n\t\t\tbreak;\n\t\tcase 'h':\n\t\t\ttmate_settings->tmate_host = xstrdup(optarg);\n\t\t\tbreak;\n\t\tcase 'k':\n\t\t\ttmate_settings->keys_dir = xstrdup(optarg);\n\t\t\tbreak;\n\t\tcase 'p':\n\t\t\ttmate_settings->ssh_port = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'q':\n\t\t\ttmate_settings->ssh_port_advertized = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'w':\n\t\t\ttmate_settings->websocket_hostname = xstrdup(optarg);\n\t\t\tbreak;\n\t\tcase 'z':\n\t\t\ttmate_settings->websocket_port = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'x':\n\t\t\ttmate_settings->use_proxy_protocol = true;\n\t\t\tbreak;\n\t\tcase 'v':\n\t\t\ttmate_settings->log_level++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tinit_logging(tmate_settings->log_level);\n\n\tsetup_locale();\n\n\tif (!tmate_settings->tmate_host)\n\t\ttmate_settings->tmate_host = get_full_hostname();\n\n\tcmdline = *argv;\n\tcmdline_end = *envp;\n\n\ttmate_preload_trace_lib();\n\ttmate_catch_sigsegv();\n\ttmate_init_rand();\n\n\tif ((mkdir(TMATE_WORKDIR, 0700)             < 0 && errno != EEXIST) ||\n\t    (mkdir(TMATE_WORKDIR \"/sessions\", 0700) < 0 && errno != EEXIST) ||\n\t    (mkdir(TMATE_WORKDIR \"/jail\", 0700)     < 0 && errno != EEXIST))\n\t\ttmate_fatal(\"Cannot prepare session in \" TMATE_WORKDIR);\n\n\tif ((chmod(TMATE_WORKDIR, 0700)             < 0) ||\n\t    (chmod(TMATE_WORKDIR \"/sessions\", 0700) < 0) ||\n\t    (chmod(TMATE_WORKDIR \"/jail\", 0700)     < 0))\n\t\ttmate_fatal(\"Cannot prepare session in \" TMATE_WORKDIR);\n\n\tif (check_owned_directory_mode(TMATE_WORKDIR, 0700) ||\n\t    check_owned_directory_mode(TMATE_WORKDIR \"/sessions\", 0700) ||\n\t    check_owned_directory_mode(TMATE_WORKDIR \"/jail\", 0700))\n\t\ttmate_fatal(TMATE_WORKDIR \" and subdirectories has incorrect ownership/mode. \"\n\t\t\t    \"Try deleting \" TMATE_WORKDIR \" and try again\");\n\n\ttmate_ssh_server_main(tmate_session,\n\t\t\t      tmate_settings->keys_dir, tmate_settings->bind_addr, tmate_settings->ssh_port);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -51,16 +51,21 @@\n \ttmate_catch_sigsegv();\n \ttmate_init_rand();\n \n-\tif ((mkdir(TMATE_WORKDIR, 0701)             < 0 && errno != EEXIST) ||\n-\t    (mkdir(TMATE_WORKDIR \"/sessions\", 0703) < 0 && errno != EEXIST) ||\n+\tif ((mkdir(TMATE_WORKDIR, 0700)             < 0 && errno != EEXIST) ||\n+\t    (mkdir(TMATE_WORKDIR \"/sessions\", 0700) < 0 && errno != EEXIST) ||\n \t    (mkdir(TMATE_WORKDIR \"/jail\", 0700)     < 0 && errno != EEXIST))\n \t\ttmate_fatal(\"Cannot prepare session in \" TMATE_WORKDIR);\n \n-\t/* The websocket server needs to access the /session dir to rename sockets */\n-\tif ((chmod(TMATE_WORKDIR, 0701)             < 0) ||\n-\t    (chmod(TMATE_WORKDIR \"/sessions\", 0703) < 0) ||\n+\tif ((chmod(TMATE_WORKDIR, 0700)             < 0) ||\n+\t    (chmod(TMATE_WORKDIR \"/sessions\", 0700) < 0) ||\n \t    (chmod(TMATE_WORKDIR \"/jail\", 0700)     < 0))\n \t\ttmate_fatal(\"Cannot prepare session in \" TMATE_WORKDIR);\n+\n+\tif (check_owned_directory_mode(TMATE_WORKDIR, 0700) ||\n+\t    check_owned_directory_mode(TMATE_WORKDIR \"/sessions\", 0700) ||\n+\t    check_owned_directory_mode(TMATE_WORKDIR \"/jail\", 0700))\n+\t\ttmate_fatal(TMATE_WORKDIR \" and subdirectories has incorrect ownership/mode. \"\n+\t\t\t    \"Try deleting \" TMATE_WORKDIR \" and try again\");\n \n \ttmate_ssh_server_main(tmate_session,\n \t\t\t      tmate_settings->keys_dir, tmate_settings->bind_addr, tmate_settings->ssh_port);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif ((mkdir(TMATE_WORKDIR, 0701)             < 0 && errno != EEXIST) ||",
                "\t    (mkdir(TMATE_WORKDIR \"/sessions\", 0703) < 0 && errno != EEXIST) ||",
                "\t/* The websocket server needs to access the /session dir to rename sockets */",
                "\tif ((chmod(TMATE_WORKDIR, 0701)             < 0) ||",
                "\t    (chmod(TMATE_WORKDIR \"/sessions\", 0703) < 0) ||"
            ],
            "added_lines": [
                "\tif ((mkdir(TMATE_WORKDIR, 0700)             < 0 && errno != EEXIST) ||",
                "\t    (mkdir(TMATE_WORKDIR \"/sessions\", 0700) < 0 && errno != EEXIST) ||",
                "\tif ((chmod(TMATE_WORKDIR, 0700)             < 0) ||",
                "\t    (chmod(TMATE_WORKDIR \"/sessions\", 0700) < 0) ||",
                "",
                "\tif (check_owned_directory_mode(TMATE_WORKDIR, 0700) ||",
                "\t    check_owned_directory_mode(TMATE_WORKDIR \"/sessions\", 0700) ||",
                "\t    check_owned_directory_mode(TMATE_WORKDIR \"/jail\", 0700))",
                "\t\ttmate_fatal(TMATE_WORKDIR \" and subdirectories has incorrect ownership/mode. \"",
                "\t\t\t    \"Try deleting \" TMATE_WORKDIR \" and try again\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44733",
        "func_name": "torvalds/linux/tee_shm_put",
        "description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=dfd0743f1d9ea76931510ed150334d571fbab49d",
        "commit_title": "Since the tee subsystem does not keep a strong reference to its idle",
        "commit_text": "shared memory buffers, it races with other threads that try to destroy a shared memory through a close of its dma-buf fd or by unmapping the memory.  In tee_shm_get_from_id() when a lookup in teedev->idr has been successful, it is possible that the tee_shm is in the dma-buf teardown path, but that path is blocked by the teedev mutex. Since we don't have an API to tell if the tee_shm is in the dma-buf teardown path or not we must find another way of detecting this condition.  Fix this by doing the reference counting directly on the tee_shm using a new refcount_t refcount field. dma-buf is replaced by using anon_inode_getfd() instead, this separates the life-cycle of the underlying file from the tee_shm. tee_shm_put() is updated to hold the mutex when decreasing the refcount to 0 and then remove the tee_shm from teedev->idr before releasing the mutex. This means that the tee_shm can never be found unless it has a refcount larger than 0.  Cc: stable@vger.kernel.org ",
        "func_before": "void tee_shm_put(struct tee_shm *shm)\n{\n\tif (shm->flags & TEE_SHM_DMA_BUF)\n\t\tdma_buf_put(shm->dmabuf);\n}",
        "func": "void tee_shm_put(struct tee_shm *shm)\n{\n\tstruct tee_device *teedev = shm->ctx->teedev;\n\tbool do_release = false;\n\n\tmutex_lock(&teedev->mutex);\n\tif (refcount_dec_and_test(&shm->refcount)) {\n\t\t/*\n\t\t * refcount has reached 0, we must now remove it from the\n\t\t * IDR before releasing the mutex. This will guarantee that\n\t\t * the refcount_inc() in tee_shm_get_from_id() never starts\n\t\t * from 0.\n\t\t */\n\t\tif (shm->flags & TEE_SHM_DMA_BUF)\n\t\t\tidr_remove(&teedev->idr, shm->id);\n\t\tdo_release = true;\n\t}\n\tmutex_unlock(&teedev->mutex);\n\n\tif (do_release)\n\t\ttee_shm_release(teedev, shm);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,22 @@\n void tee_shm_put(struct tee_shm *shm)\n {\n-\tif (shm->flags & TEE_SHM_DMA_BUF)\n-\t\tdma_buf_put(shm->dmabuf);\n+\tstruct tee_device *teedev = shm->ctx->teedev;\n+\tbool do_release = false;\n+\n+\tmutex_lock(&teedev->mutex);\n+\tif (refcount_dec_and_test(&shm->refcount)) {\n+\t\t/*\n+\t\t * refcount has reached 0, we must now remove it from the\n+\t\t * IDR before releasing the mutex. This will guarantee that\n+\t\t * the refcount_inc() in tee_shm_get_from_id() never starts\n+\t\t * from 0.\n+\t\t */\n+\t\tif (shm->flags & TEE_SHM_DMA_BUF)\n+\t\t\tidr_remove(&teedev->idr, shm->id);\n+\t\tdo_release = true;\n+\t}\n+\tmutex_unlock(&teedev->mutex);\n+\n+\tif (do_release)\n+\t\ttee_shm_release(teedev, shm);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (shm->flags & TEE_SHM_DMA_BUF)",
                "\t\tdma_buf_put(shm->dmabuf);"
            ],
            "added_lines": [
                "\tstruct tee_device *teedev = shm->ctx->teedev;",
                "\tbool do_release = false;",
                "",
                "\tmutex_lock(&teedev->mutex);",
                "\tif (refcount_dec_and_test(&shm->refcount)) {",
                "\t\t/*",
                "\t\t * refcount has reached 0, we must now remove it from the",
                "\t\t * IDR before releasing the mutex. This will guarantee that",
                "\t\t * the refcount_inc() in tee_shm_get_from_id() never starts",
                "\t\t * from 0.",
                "\t\t */",
                "\t\tif (shm->flags & TEE_SHM_DMA_BUF)",
                "\t\t\tidr_remove(&teedev->idr, shm->id);",
                "\t\tdo_release = true;",
                "\t}",
                "\tmutex_unlock(&teedev->mutex);",
                "",
                "\tif (do_release)",
                "\t\ttee_shm_release(teedev, shm);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44733",
        "func_name": "torvalds/linux/tee_shm_release",
        "description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=dfd0743f1d9ea76931510ed150334d571fbab49d",
        "commit_title": "Since the tee subsystem does not keep a strong reference to its idle",
        "commit_text": "shared memory buffers, it races with other threads that try to destroy a shared memory through a close of its dma-buf fd or by unmapping the memory.  In tee_shm_get_from_id() when a lookup in teedev->idr has been successful, it is possible that the tee_shm is in the dma-buf teardown path, but that path is blocked by the teedev mutex. Since we don't have an API to tell if the tee_shm is in the dma-buf teardown path or not we must find another way of detecting this condition.  Fix this by doing the reference counting directly on the tee_shm using a new refcount_t refcount field. dma-buf is replaced by using anon_inode_getfd() instead, this separates the life-cycle of the underlying file from the tee_shm. tee_shm_put() is updated to hold the mutex when decreasing the refcount to 0 and then remove the tee_shm from teedev->idr before releasing the mutex. This means that the tee_shm can never be found unless it has a refcount larger than 0.  Cc: stable@vger.kernel.org ",
        "func_before": "static void tee_shm_release(struct tee_shm *shm)\n{\n\tstruct tee_device *teedev = shm->ctx->teedev;\n\n\tif (shm->flags & TEE_SHM_DMA_BUF) {\n\t\tmutex_lock(&teedev->mutex);\n\t\tidr_remove(&teedev->idr, shm->id);\n\t\tmutex_unlock(&teedev->mutex);\n\t}\n\n\tif (shm->flags & TEE_SHM_POOL) {\n\t\tstruct tee_shm_pool_mgr *poolm;\n\n\t\tif (shm->flags & TEE_SHM_DMA_BUF)\n\t\t\tpoolm = teedev->pool->dma_buf_mgr;\n\t\telse\n\t\t\tpoolm = teedev->pool->private_mgr;\n\n\t\tpoolm->ops->free(poolm, shm);\n\t} else if (shm->flags & TEE_SHM_REGISTER) {\n\t\tint rc = teedev->desc->ops->shm_unregister(shm->ctx, shm);\n\n\t\tif (rc)\n\t\t\tdev_err(teedev->dev.parent,\n\t\t\t\t\"unregister shm %p failed: %d\", shm, rc);\n\n\t\trelease_registered_pages(shm);\n\t}\n\n\tteedev_ctx_put(shm->ctx);\n\n\tkfree(shm);\n\n\ttee_device_put(teedev);\n}",
        "func": "static void tee_shm_release(struct tee_device *teedev, struct tee_shm *shm)\n{\n\tif (shm->flags & TEE_SHM_POOL) {\n\t\tstruct tee_shm_pool_mgr *poolm;\n\n\t\tif (shm->flags & TEE_SHM_DMA_BUF)\n\t\t\tpoolm = teedev->pool->dma_buf_mgr;\n\t\telse\n\t\t\tpoolm = teedev->pool->private_mgr;\n\n\t\tpoolm->ops->free(poolm, shm);\n\t} else if (shm->flags & TEE_SHM_REGISTER) {\n\t\tint rc = teedev->desc->ops->shm_unregister(shm->ctx, shm);\n\n\t\tif (rc)\n\t\t\tdev_err(teedev->dev.parent,\n\t\t\t\t\"unregister shm %p failed: %d\", shm, rc);\n\n\t\trelease_registered_pages(shm);\n\t}\n\n\tteedev_ctx_put(shm->ctx);\n\n\tkfree(shm);\n\n\ttee_device_put(teedev);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,13 +1,5 @@\n-static void tee_shm_release(struct tee_shm *shm)\n+static void tee_shm_release(struct tee_device *teedev, struct tee_shm *shm)\n {\n-\tstruct tee_device *teedev = shm->ctx->teedev;\n-\n-\tif (shm->flags & TEE_SHM_DMA_BUF) {\n-\t\tmutex_lock(&teedev->mutex);\n-\t\tidr_remove(&teedev->idr, shm->id);\n-\t\tmutex_unlock(&teedev->mutex);\n-\t}\n-\n \tif (shm->flags & TEE_SHM_POOL) {\n \t\tstruct tee_shm_pool_mgr *poolm;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "static void tee_shm_release(struct tee_shm *shm)",
                "\tstruct tee_device *teedev = shm->ctx->teedev;",
                "",
                "\tif (shm->flags & TEE_SHM_DMA_BUF) {",
                "\t\tmutex_lock(&teedev->mutex);",
                "\t\tidr_remove(&teedev->idr, shm->id);",
                "\t\tmutex_unlock(&teedev->mutex);",
                "\t}",
                ""
            ],
            "added_lines": [
                "static void tee_shm_release(struct tee_device *teedev, struct tee_shm *shm)"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44733",
        "func_name": "torvalds/linux/tee_shm_register",
        "description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=dfd0743f1d9ea76931510ed150334d571fbab49d",
        "commit_title": "Since the tee subsystem does not keep a strong reference to its idle",
        "commit_text": "shared memory buffers, it races with other threads that try to destroy a shared memory through a close of its dma-buf fd or by unmapping the memory.  In tee_shm_get_from_id() when a lookup in teedev->idr has been successful, it is possible that the tee_shm is in the dma-buf teardown path, but that path is blocked by the teedev mutex. Since we don't have an API to tell if the tee_shm is in the dma-buf teardown path or not we must find another way of detecting this condition.  Fix this by doing the reference counting directly on the tee_shm using a new refcount_t refcount field. dma-buf is replaced by using anon_inode_getfd() instead, this separates the life-cycle of the underlying file from the tee_shm. tee_shm_put() is updated to hold the mutex when decreasing the refcount to 0 and then remove the tee_shm from teedev->idr before releasing the mutex. This means that the tee_shm can never be found unless it has a refcount larger than 0.  Cc: stable@vger.kernel.org ",
        "func_before": "struct tee_shm *tee_shm_register(struct tee_context *ctx, unsigned long addr,\n\t\t\t\t size_t length, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tconst u32 req_user_flags = TEE_SHM_DMA_BUF | TEE_SHM_USER_MAPPED;\n\tconst u32 req_kernel_flags = TEE_SHM_DMA_BUF | TEE_SHM_KERNEL_MAPPED;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\tint num_pages;\n\tunsigned long start;\n\n\tif (flags != req_user_flags && flags != req_kernel_flags)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->desc->ops->shm_register ||\n\t    !teedev->desc->ops->shm_unregister) {\n\t\ttee_device_put(teedev);\n\t\treturn ERR_PTR(-ENOTSUPP);\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\tshm->flags = flags | TEE_SHM_REGISTER;\n\tshm->ctx = ctx;\n\tshm->id = -1;\n\taddr = untagged_addr(addr);\n\tstart = rounddown(addr, PAGE_SIZE);\n\tshm->offset = addr - start;\n\tshm->size = length;\n\tnum_pages = (roundup(addr + length, PAGE_SIZE) - start) / PAGE_SIZE;\n\tshm->pages = kcalloc(num_pages, sizeof(*shm->pages), GFP_KERNEL);\n\tif (!shm->pages) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\tif (flags & TEE_SHM_USER_MAPPED) {\n\t\trc = pin_user_pages_fast(start, num_pages, FOLL_WRITE,\n\t\t\t\t\t shm->pages);\n\t} else {\n\t\tstruct kvec *kiov;\n\t\tint i;\n\n\t\tkiov = kcalloc(num_pages, sizeof(*kiov), GFP_KERNEL);\n\t\tif (!kiov) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor (i = 0; i < num_pages; i++) {\n\t\t\tkiov[i].iov_base = (void *)(start + i * PAGE_SIZE);\n\t\t\tkiov[i].iov_len = PAGE_SIZE;\n\t\t}\n\n\t\trc = get_kernel_pages(kiov, num_pages, 0, shm->pages);\n\t\tkfree(kiov);\n\t}\n\tif (rc > 0)\n\t\tshm->num_pages = rc;\n\tif (rc != num_pages) {\n\t\tif (rc >= 0)\n\t\t\trc = -ENOMEM;\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\tmutex_lock(&teedev->mutex);\n\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\tmutex_unlock(&teedev->mutex);\n\n\tif (shm->id < 0) {\n\t\tret = ERR_PTR(shm->id);\n\t\tgoto err;\n\t}\n\n\trc = teedev->desc->ops->shm_register(ctx, shm, shm->pages,\n\t\t\t\t\t     shm->num_pages, start);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\n\t\texp_info.ops = &tee_shm_dma_buf_ops;\n\t\texp_info.size = shm->size;\n\t\texp_info.flags = O_RDWR;\n\t\texp_info.priv = shm;\n\n\t\tshm->dmabuf = dma_buf_export(&exp_info);\n\t\tif (IS_ERR(shm->dmabuf)) {\n\t\t\tret = ERR_CAST(shm->dmabuf);\n\t\t\tteedev->desc->ops->shm_unregister(ctx, shm);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn shm;\nerr:\n\tif (shm) {\n\t\tif (shm->id >= 0) {\n\t\t\tmutex_lock(&teedev->mutex);\n\t\t\tidr_remove(&teedev->idr, shm->id);\n\t\t\tmutex_unlock(&teedev->mutex);\n\t\t}\n\t\trelease_registered_pages(shm);\n\t}\n\tkfree(shm);\n\tteedev_ctx_put(ctx);\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "func": "struct tee_shm *tee_shm_register(struct tee_context *ctx, unsigned long addr,\n\t\t\t\t size_t length, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tconst u32 req_user_flags = TEE_SHM_DMA_BUF | TEE_SHM_USER_MAPPED;\n\tconst u32 req_kernel_flags = TEE_SHM_DMA_BUF | TEE_SHM_KERNEL_MAPPED;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\tint num_pages;\n\tunsigned long start;\n\n\tif (flags != req_user_flags && flags != req_kernel_flags)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->desc->ops->shm_register ||\n\t    !teedev->desc->ops->shm_unregister) {\n\t\ttee_device_put(teedev);\n\t\treturn ERR_PTR(-ENOTSUPP);\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\trefcount_set(&shm->refcount, 1);\n\tshm->flags = flags | TEE_SHM_REGISTER;\n\tshm->ctx = ctx;\n\tshm->id = -1;\n\taddr = untagged_addr(addr);\n\tstart = rounddown(addr, PAGE_SIZE);\n\tshm->offset = addr - start;\n\tshm->size = length;\n\tnum_pages = (roundup(addr + length, PAGE_SIZE) - start) / PAGE_SIZE;\n\tshm->pages = kcalloc(num_pages, sizeof(*shm->pages), GFP_KERNEL);\n\tif (!shm->pages) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\tif (flags & TEE_SHM_USER_MAPPED) {\n\t\trc = pin_user_pages_fast(start, num_pages, FOLL_WRITE,\n\t\t\t\t\t shm->pages);\n\t} else {\n\t\tstruct kvec *kiov;\n\t\tint i;\n\n\t\tkiov = kcalloc(num_pages, sizeof(*kiov), GFP_KERNEL);\n\t\tif (!kiov) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor (i = 0; i < num_pages; i++) {\n\t\t\tkiov[i].iov_base = (void *)(start + i * PAGE_SIZE);\n\t\t\tkiov[i].iov_len = PAGE_SIZE;\n\t\t}\n\n\t\trc = get_kernel_pages(kiov, num_pages, 0, shm->pages);\n\t\tkfree(kiov);\n\t}\n\tif (rc > 0)\n\t\tshm->num_pages = rc;\n\tif (rc != num_pages) {\n\t\tif (rc >= 0)\n\t\t\trc = -ENOMEM;\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\tmutex_lock(&teedev->mutex);\n\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\tmutex_unlock(&teedev->mutex);\n\n\tif (shm->id < 0) {\n\t\tret = ERR_PTR(shm->id);\n\t\tgoto err;\n\t}\n\n\trc = teedev->desc->ops->shm_register(ctx, shm, shm->pages,\n\t\t\t\t\t     shm->num_pages, start);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\treturn shm;\nerr:\n\tif (shm) {\n\t\tif (shm->id >= 0) {\n\t\t\tmutex_lock(&teedev->mutex);\n\t\t\tidr_remove(&teedev->idr, shm->id);\n\t\t\tmutex_unlock(&teedev->mutex);\n\t\t}\n\t\trelease_registered_pages(shm);\n\t}\n\tkfree(shm);\n\tteedev_ctx_put(ctx);\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -30,6 +30,7 @@\n \t\tgoto err;\n \t}\n \n+\trefcount_set(&shm->refcount, 1);\n \tshm->flags = flags | TEE_SHM_REGISTER;\n \tshm->ctx = ctx;\n \tshm->id = -1;\n@@ -90,22 +91,6 @@\n \t\tgoto err;\n \t}\n \n-\tif (flags & TEE_SHM_DMA_BUF) {\n-\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n-\n-\t\texp_info.ops = &tee_shm_dma_buf_ops;\n-\t\texp_info.size = shm->size;\n-\t\texp_info.flags = O_RDWR;\n-\t\texp_info.priv = shm;\n-\n-\t\tshm->dmabuf = dma_buf_export(&exp_info);\n-\t\tif (IS_ERR(shm->dmabuf)) {\n-\t\t\tret = ERR_CAST(shm->dmabuf);\n-\t\t\tteedev->desc->ops->shm_unregister(ctx, shm);\n-\t\t\tgoto err;\n-\t\t}\n-\t}\n-\n \treturn shm;\n err:\n \tif (shm) {",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (flags & TEE_SHM_DMA_BUF) {",
                "\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);",
                "",
                "\t\texp_info.ops = &tee_shm_dma_buf_ops;",
                "\t\texp_info.size = shm->size;",
                "\t\texp_info.flags = O_RDWR;",
                "\t\texp_info.priv = shm;",
                "",
                "\t\tshm->dmabuf = dma_buf_export(&exp_info);",
                "\t\tif (IS_ERR(shm->dmabuf)) {",
                "\t\t\tret = ERR_CAST(shm->dmabuf);",
                "\t\t\tteedev->desc->ops->shm_unregister(ctx, shm);",
                "\t\t\tgoto err;",
                "\t\t}",
                "\t}",
                ""
            ],
            "added_lines": [
                "\trefcount_set(&shm->refcount, 1);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44733",
        "func_name": "torvalds/linux/tee_shm_alloc",
        "description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=dfd0743f1d9ea76931510ed150334d571fbab49d",
        "commit_title": "Since the tee subsystem does not keep a strong reference to its idle",
        "commit_text": "shared memory buffers, it races with other threads that try to destroy a shared memory through a close of its dma-buf fd or by unmapping the memory.  In tee_shm_get_from_id() when a lookup in teedev->idr has been successful, it is possible that the tee_shm is in the dma-buf teardown path, but that path is blocked by the teedev mutex. Since we don't have an API to tell if the tee_shm is in the dma-buf teardown path or not we must find another way of detecting this condition.  Fix this by doing the reference counting directly on the tee_shm using a new refcount_t refcount field. dma-buf is replaced by using anon_inode_getfd() instead, this separates the life-cycle of the underlying file from the tee_shm. tee_shm_put() is updated to hold the mutex when decreasing the refcount to 0 and then remove the tee_shm from teedev->idr before releasing the mutex. This means that the tee_shm can never be found unless it has a refcount larger than 0.  Cc: stable@vger.kernel.org ",
        "func_before": "struct tee_shm *tee_shm_alloc(struct tee_context *ctx, size_t size, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tstruct tee_shm_pool_mgr *poolm = NULL;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\n\tif (!(flags & TEE_SHM_MAPPED)) {\n\t\tdev_err(teedev->dev.parent,\n\t\t\t\"only mapped allocations supported\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif ((flags & ~(TEE_SHM_MAPPED | TEE_SHM_DMA_BUF | TEE_SHM_PRIV))) {\n\t\tdev_err(teedev->dev.parent, \"invalid shm flags 0x%x\", flags);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->pool) {\n\t\t/* teedev has been detached from driver */\n\t\tret = ERR_PTR(-EINVAL);\n\t\tgoto err_dev_put;\n\t}\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err_dev_put;\n\t}\n\n\tshm->flags = flags | TEE_SHM_POOL;\n\tshm->ctx = ctx;\n\tif (flags & TEE_SHM_DMA_BUF)\n\t\tpoolm = teedev->pool->dma_buf_mgr;\n\telse\n\t\tpoolm = teedev->pool->private_mgr;\n\n\trc = poolm->ops->alloc(poolm, shm, size);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err_kfree;\n\t}\n\n\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\n\t\tmutex_lock(&teedev->mutex);\n\t\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\t\tmutex_unlock(&teedev->mutex);\n\t\tif (shm->id < 0) {\n\t\t\tret = ERR_PTR(shm->id);\n\t\t\tgoto err_pool_free;\n\t\t}\n\n\t\texp_info.ops = &tee_shm_dma_buf_ops;\n\t\texp_info.size = shm->size;\n\t\texp_info.flags = O_RDWR;\n\t\texp_info.priv = shm;\n\n\t\tshm->dmabuf = dma_buf_export(&exp_info);\n\t\tif (IS_ERR(shm->dmabuf)) {\n\t\t\tret = ERR_CAST(shm->dmabuf);\n\t\t\tgoto err_rem;\n\t\t}\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\treturn shm;\nerr_rem:\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tmutex_lock(&teedev->mutex);\n\t\tidr_remove(&teedev->idr, shm->id);\n\t\tmutex_unlock(&teedev->mutex);\n\t}\nerr_pool_free:\n\tpoolm->ops->free(poolm, shm);\nerr_kfree:\n\tkfree(shm);\nerr_dev_put:\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "func": "struct tee_shm *tee_shm_alloc(struct tee_context *ctx, size_t size, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tstruct tee_shm_pool_mgr *poolm = NULL;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\n\tif (!(flags & TEE_SHM_MAPPED)) {\n\t\tdev_err(teedev->dev.parent,\n\t\t\t\"only mapped allocations supported\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif ((flags & ~(TEE_SHM_MAPPED | TEE_SHM_DMA_BUF | TEE_SHM_PRIV))) {\n\t\tdev_err(teedev->dev.parent, \"invalid shm flags 0x%x\", flags);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->pool) {\n\t\t/* teedev has been detached from driver */\n\t\tret = ERR_PTR(-EINVAL);\n\t\tgoto err_dev_put;\n\t}\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err_dev_put;\n\t}\n\n\trefcount_set(&shm->refcount, 1);\n\tshm->flags = flags | TEE_SHM_POOL;\n\tshm->ctx = ctx;\n\tif (flags & TEE_SHM_DMA_BUF)\n\t\tpoolm = teedev->pool->dma_buf_mgr;\n\telse\n\t\tpoolm = teedev->pool->private_mgr;\n\n\trc = poolm->ops->alloc(poolm, shm, size);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err_kfree;\n\t}\n\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tmutex_lock(&teedev->mutex);\n\t\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\t\tmutex_unlock(&teedev->mutex);\n\t\tif (shm->id < 0) {\n\t\t\tret = ERR_PTR(shm->id);\n\t\t\tgoto err_pool_free;\n\t\t}\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\treturn shm;\nerr_pool_free:\n\tpoolm->ops->free(poolm, shm);\nerr_kfree:\n\tkfree(shm);\nerr_dev_put:\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -32,6 +32,7 @@\n \t\tgoto err_dev_put;\n \t}\n \n+\trefcount_set(&shm->refcount, 1);\n \tshm->flags = flags | TEE_SHM_POOL;\n \tshm->ctx = ctx;\n \tif (flags & TEE_SHM_DMA_BUF)\n@@ -45,10 +46,7 @@\n \t\tgoto err_kfree;\n \t}\n \n-\n \tif (flags & TEE_SHM_DMA_BUF) {\n-\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n-\n \t\tmutex_lock(&teedev->mutex);\n \t\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n \t\tmutex_unlock(&teedev->mutex);\n@@ -56,28 +54,11 @@\n \t\t\tret = ERR_PTR(shm->id);\n \t\t\tgoto err_pool_free;\n \t\t}\n-\n-\t\texp_info.ops = &tee_shm_dma_buf_ops;\n-\t\texp_info.size = shm->size;\n-\t\texp_info.flags = O_RDWR;\n-\t\texp_info.priv = shm;\n-\n-\t\tshm->dmabuf = dma_buf_export(&exp_info);\n-\t\tif (IS_ERR(shm->dmabuf)) {\n-\t\t\tret = ERR_CAST(shm->dmabuf);\n-\t\t\tgoto err_rem;\n-\t\t}\n \t}\n \n \tteedev_ctx_get(ctx);\n \n \treturn shm;\n-err_rem:\n-\tif (flags & TEE_SHM_DMA_BUF) {\n-\t\tmutex_lock(&teedev->mutex);\n-\t\tidr_remove(&teedev->idr, shm->id);\n-\t\tmutex_unlock(&teedev->mutex);\n-\t}\n err_pool_free:\n \tpoolm->ops->free(poolm, shm);\n err_kfree:",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);",
                "",
                "",
                "\t\texp_info.ops = &tee_shm_dma_buf_ops;",
                "\t\texp_info.size = shm->size;",
                "\t\texp_info.flags = O_RDWR;",
                "\t\texp_info.priv = shm;",
                "",
                "\t\tshm->dmabuf = dma_buf_export(&exp_info);",
                "\t\tif (IS_ERR(shm->dmabuf)) {",
                "\t\t\tret = ERR_CAST(shm->dmabuf);",
                "\t\t\tgoto err_rem;",
                "\t\t}",
                "err_rem:",
                "\tif (flags & TEE_SHM_DMA_BUF) {",
                "\t\tmutex_lock(&teedev->mutex);",
                "\t\tidr_remove(&teedev->idr, shm->id);",
                "\t\tmutex_unlock(&teedev->mutex);",
                "\t}"
            ],
            "added_lines": [
                "\trefcount_set(&shm->refcount, 1);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44733",
        "func_name": "torvalds/linux/tee_shm_free",
        "description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=dfd0743f1d9ea76931510ed150334d571fbab49d",
        "commit_title": "Since the tee subsystem does not keep a strong reference to its idle",
        "commit_text": "shared memory buffers, it races with other threads that try to destroy a shared memory through a close of its dma-buf fd or by unmapping the memory.  In tee_shm_get_from_id() when a lookup in teedev->idr has been successful, it is possible that the tee_shm is in the dma-buf teardown path, but that path is blocked by the teedev mutex. Since we don't have an API to tell if the tee_shm is in the dma-buf teardown path or not we must find another way of detecting this condition.  Fix this by doing the reference counting directly on the tee_shm using a new refcount_t refcount field. dma-buf is replaced by using anon_inode_getfd() instead, this separates the life-cycle of the underlying file from the tee_shm. tee_shm_put() is updated to hold the mutex when decreasing the refcount to 0 and then remove the tee_shm from teedev->idr before releasing the mutex. This means that the tee_shm can never be found unless it has a refcount larger than 0.  Cc: stable@vger.kernel.org ",
        "func_before": "void tee_shm_free(struct tee_shm *shm)\n{\n\t/*\n\t * dma_buf_put() decreases the dmabuf reference counter and will\n\t * call tee_shm_release() when the last reference is gone.\n\t *\n\t * In the case of driver private memory we call tee_shm_release\n\t * directly instead as it doesn't have a reference counter.\n\t */\n\tif (shm->flags & TEE_SHM_DMA_BUF)\n\t\tdma_buf_put(shm->dmabuf);\n\telse\n\t\ttee_shm_release(shm);\n}",
        "func": "void tee_shm_free(struct tee_shm *shm)\n{\n\ttee_shm_put(shm);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,14 +1,4 @@\n void tee_shm_free(struct tee_shm *shm)\n {\n-\t/*\n-\t * dma_buf_put() decreases the dmabuf reference counter and will\n-\t * call tee_shm_release() when the last reference is gone.\n-\t *\n-\t * In the case of driver private memory we call tee_shm_release\n-\t * directly instead as it doesn't have a reference counter.\n-\t */\n-\tif (shm->flags & TEE_SHM_DMA_BUF)\n-\t\tdma_buf_put(shm->dmabuf);\n-\telse\n-\t\ttee_shm_release(shm);\n+\ttee_shm_put(shm);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t/*",
                "\t * dma_buf_put() decreases the dmabuf reference counter and will",
                "\t * call tee_shm_release() when the last reference is gone.",
                "\t *",
                "\t * In the case of driver private memory we call tee_shm_release",
                "\t * directly instead as it doesn't have a reference counter.",
                "\t */",
                "\tif (shm->flags & TEE_SHM_DMA_BUF)",
                "\t\tdma_buf_put(shm->dmabuf);",
                "\telse",
                "\t\ttee_shm_release(shm);"
            ],
            "added_lines": [
                "\ttee_shm_put(shm);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44733",
        "func_name": "torvalds/linux/tee_shm_get_from_id",
        "description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=dfd0743f1d9ea76931510ed150334d571fbab49d",
        "commit_title": "Since the tee subsystem does not keep a strong reference to its idle",
        "commit_text": "shared memory buffers, it races with other threads that try to destroy a shared memory through a close of its dma-buf fd or by unmapping the memory.  In tee_shm_get_from_id() when a lookup in teedev->idr has been successful, it is possible that the tee_shm is in the dma-buf teardown path, but that path is blocked by the teedev mutex. Since we don't have an API to tell if the tee_shm is in the dma-buf teardown path or not we must find another way of detecting this condition.  Fix this by doing the reference counting directly on the tee_shm using a new refcount_t refcount field. dma-buf is replaced by using anon_inode_getfd() instead, this separates the life-cycle of the underlying file from the tee_shm. tee_shm_put() is updated to hold the mutex when decreasing the refcount to 0 and then remove the tee_shm from teedev->idr before releasing the mutex. This means that the tee_shm can never be found unless it has a refcount larger than 0.  Cc: stable@vger.kernel.org ",
        "func_before": "struct tee_shm *tee_shm_get_from_id(struct tee_context *ctx, int id)\n{\n\tstruct tee_device *teedev;\n\tstruct tee_shm *shm;\n\n\tif (!ctx)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tteedev = ctx->teedev;\n\tmutex_lock(&teedev->mutex);\n\tshm = idr_find(&teedev->idr, id);\n\tif (!shm || shm->ctx != ctx)\n\t\tshm = ERR_PTR(-EINVAL);\n\telse if (shm->flags & TEE_SHM_DMA_BUF)\n\t\tget_dma_buf(shm->dmabuf);\n\tmutex_unlock(&teedev->mutex);\n\treturn shm;\n}",
        "func": "struct tee_shm *tee_shm_get_from_id(struct tee_context *ctx, int id)\n{\n\tstruct tee_device *teedev;\n\tstruct tee_shm *shm;\n\n\tif (!ctx)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tteedev = ctx->teedev;\n\tmutex_lock(&teedev->mutex);\n\tshm = idr_find(&teedev->idr, id);\n\t/*\n\t * If the tee_shm was found in the IDR it must have a refcount\n\t * larger than 0 due to the guarantee in tee_shm_put() below. So\n\t * it's safe to use refcount_inc().\n\t */\n\tif (!shm || shm->ctx != ctx)\n\t\tshm = ERR_PTR(-EINVAL);\n\telse\n\t\trefcount_inc(&shm->refcount);\n\tmutex_unlock(&teedev->mutex);\n\treturn shm;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,10 +9,15 @@\n \tteedev = ctx->teedev;\n \tmutex_lock(&teedev->mutex);\n \tshm = idr_find(&teedev->idr, id);\n+\t/*\n+\t * If the tee_shm was found in the IDR it must have a refcount\n+\t * larger than 0 due to the guarantee in tee_shm_put() below. So\n+\t * it's safe to use refcount_inc().\n+\t */\n \tif (!shm || shm->ctx != ctx)\n \t\tshm = ERR_PTR(-EINVAL);\n-\telse if (shm->flags & TEE_SHM_DMA_BUF)\n-\t\tget_dma_buf(shm->dmabuf);\n+\telse\n+\t\trefcount_inc(&shm->refcount);\n \tmutex_unlock(&teedev->mutex);\n \treturn shm;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\telse if (shm->flags & TEE_SHM_DMA_BUF)",
                "\t\tget_dma_buf(shm->dmabuf);"
            ],
            "added_lines": [
                "\t/*",
                "\t * If the tee_shm was found in the IDR it must have a refcount",
                "\t * larger than 0 due to the guarantee in tee_shm_put() below. So",
                "\t * it's safe to use refcount_inc().",
                "\t */",
                "\telse",
                "\t\trefcount_inc(&shm->refcount);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44733",
        "func_name": "torvalds/linux/tee_shm_get_fd",
        "description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=dfd0743f1d9ea76931510ed150334d571fbab49d",
        "commit_title": "Since the tee subsystem does not keep a strong reference to its idle",
        "commit_text": "shared memory buffers, it races with other threads that try to destroy a shared memory through a close of its dma-buf fd or by unmapping the memory.  In tee_shm_get_from_id() when a lookup in teedev->idr has been successful, it is possible that the tee_shm is in the dma-buf teardown path, but that path is blocked by the teedev mutex. Since we don't have an API to tell if the tee_shm is in the dma-buf teardown path or not we must find another way of detecting this condition.  Fix this by doing the reference counting directly on the tee_shm using a new refcount_t refcount field. dma-buf is replaced by using anon_inode_getfd() instead, this separates the life-cycle of the underlying file from the tee_shm. tee_shm_put() is updated to hold the mutex when decreasing the refcount to 0 and then remove the tee_shm from teedev->idr before releasing the mutex. This means that the tee_shm can never be found unless it has a refcount larger than 0.  Cc: stable@vger.kernel.org ",
        "func_before": "int tee_shm_get_fd(struct tee_shm *shm)\n{\n\tint fd;\n\n\tif (!(shm->flags & TEE_SHM_DMA_BUF))\n\t\treturn -EINVAL;\n\n\tget_dma_buf(shm->dmabuf);\n\tfd = dma_buf_fd(shm->dmabuf, O_CLOEXEC);\n\tif (fd < 0)\n\t\tdma_buf_put(shm->dmabuf);\n\treturn fd;\n}",
        "func": "int tee_shm_get_fd(struct tee_shm *shm)\n{\n\tint fd;\n\n\tif (!(shm->flags & TEE_SHM_DMA_BUF))\n\t\treturn -EINVAL;\n\n\t/* matched by tee_shm_put() in tee_shm_op_release() */\n\trefcount_inc(&shm->refcount);\n\tfd = anon_inode_getfd(\"tee_shm\", &tee_shm_fops, shm, O_RDWR);\n\tif (fd < 0)\n\t\ttee_shm_put(shm);\n\treturn fd;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,9 +5,10 @@\n \tif (!(shm->flags & TEE_SHM_DMA_BUF))\n \t\treturn -EINVAL;\n \n-\tget_dma_buf(shm->dmabuf);\n-\tfd = dma_buf_fd(shm->dmabuf, O_CLOEXEC);\n+\t/* matched by tee_shm_put() in tee_shm_op_release() */\n+\trefcount_inc(&shm->refcount);\n+\tfd = anon_inode_getfd(\"tee_shm\", &tee_shm_fops, shm, O_RDWR);\n \tif (fd < 0)\n-\t\tdma_buf_put(shm->dmabuf);\n+\t\ttee_shm_put(shm);\n \treturn fd;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tget_dma_buf(shm->dmabuf);",
                "\tfd = dma_buf_fd(shm->dmabuf, O_CLOEXEC);",
                "\t\tdma_buf_put(shm->dmabuf);"
            ],
            "added_lines": [
                "\t/* matched by tee_shm_put() in tee_shm_op_release() */",
                "\trefcount_inc(&shm->refcount);",
                "\tfd = anon_inode_getfd(\"tee_shm\", &tee_shm_fops, shm, O_RDWR);",
                "\t\ttee_shm_put(shm);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4083",
        "func_name": "torvalds/linux/__fget_files",
        "description": "A read-after-free memory flaw was found in the Linux kernel's garbage collection for Unix domain socket file handlers in the way users call close() and fget() simultaneously and can potentially trigger a race condition. This flaw allows a local user to crash the system or escalate their privileges on the system. This flaw affects Linux kernel versions prior to 5.16-rc4.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=054aa8d439b9",
        "commit_title": "Jann Horn points out that there is another possible race wrt Unix domain",
        "commit_text": "socket garbage collection, somewhat reminiscent of the one fixed in commit cbcf01128d0a (\"af_unix: fix garbage collect vs MSG_PEEK\").  See the extended comment about the garbage collection requirements added to unix_peek_fds() by that commit for details.  The race comes from how we can locklessly look up a file descriptor just as it is in the process of being closed, and with the right artificial timing (Jann added a few strategic 'mdelay(500)' calls to do that), the Unix domain socket garbage collector could see the reference count decrement of the close() happen before fget() took its reference to the file and the file was attached onto a new file descriptor.  This is all (intentionally) correct on the 'struct file *' side, with RCU lookups and lockless reference counting very much part of the design.  Getting that reference count out of order isn't a problem per se.  But the garbage collector can get confused by seeing this situation of having seen a file not having any remaining external references and then seeing it being attached to an fd.  In commit cbcf01128d0a (\"af_unix: fix garbage collect vs MSG_PEEK\") the fix was to serialize the file descriptor install with the garbage collector by taking and releasing the unix_gc_lock.  That's not really an option here, but since this all happens when we are in the process of looking up a file descriptor, we can instead simply just re-check that the file hasn't been closed in the meantime, and just re-do the lookup if we raced with a concurrent close() of the same file descriptor.  Reported-and-tested-by: Jann Horn <jannh@google.com> ",
        "func_before": "static struct file *__fget_files(struct files_struct *files, unsigned int fd,\n\t\t\t\t fmode_t mask, unsigned int refs)\n{\n\tstruct file *file;\n\n\trcu_read_lock();\nloop:\n\tfile = files_lookup_fd_rcu(files, fd);\n\tif (file) {\n\t\t/* File object ref couldn't be taken.\n\t\t * dup2() atomicity guarantee is the reason\n\t\t * we loop to catch the new file (or NULL pointer)\n\t\t */\n\t\tif (file->f_mode & mask)\n\t\t\tfile = NULL;\n\t\telse if (!get_file_rcu_many(file, refs))\n\t\t\tgoto loop;\n\t}\n\trcu_read_unlock();\n\n\treturn file;\n}",
        "func": "static struct file *__fget_files(struct files_struct *files, unsigned int fd,\n\t\t\t\t fmode_t mask, unsigned int refs)\n{\n\tstruct file *file;\n\n\trcu_read_lock();\nloop:\n\tfile = files_lookup_fd_rcu(files, fd);\n\tif (file) {\n\t\t/* File object ref couldn't be taken.\n\t\t * dup2() atomicity guarantee is the reason\n\t\t * we loop to catch the new file (or NULL pointer)\n\t\t */\n\t\tif (file->f_mode & mask)\n\t\t\tfile = NULL;\n\t\telse if (!get_file_rcu_many(file, refs))\n\t\t\tgoto loop;\n\t\telse if (files_lookup_fd_raw(files, fd) != file) {\n\t\t\tfput_many(file, refs);\n\t\t\tgoto loop;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn file;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,6 +15,10 @@\n \t\t\tfile = NULL;\n \t\telse if (!get_file_rcu_many(file, refs))\n \t\t\tgoto loop;\n+\t\telse if (files_lookup_fd_raw(files, fd) != file) {\n+\t\t\tfput_many(file, refs);\n+\t\t\tgoto loop;\n+\t\t}\n \t}\n \trcu_read_unlock();\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\telse if (files_lookup_fd_raw(files, fd) != file) {",
                "\t\t\tfput_many(file, refs);",
                "\t\t\tgoto loop;",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-6761",
        "func_name": "ffmpeg/update_dimensions",
        "description": "The update_dimensions function in libavcodec/vp8.c in FFmpeg through 2.8.1, as used in Google Chrome before 46.0.2490.71 and other products, relies on a coefficient-partition count during multi-threaded operation, which allows remote attackers to cause a denial of service (race condition and memory corruption) or possibly have unspecified other impact via a crafted WebM file.",
        "git_url": "http://git.videolan.org/?p=ffmpeg.git;a=commit;h=dabea74d0e82ea80cd344f630497cafcb3ef872c",
        "commit_title": "",
        "commit_text": "avcodec/vp8: Do not use num_coeff_partitions in thread/buffer setup  The variable is not a constant and can lead to race conditions   ",
        "func_before": "static av_always_inline\nint update_dimensions(VP8Context *s, int width, int height, int is_vp7)\n{\n    AVCodecContext *avctx = s->avctx;\n    int i, ret;\n\n    if (width  != s->avctx->width || ((width+15)/16 != s->mb_width || (height+15)/16 != s->mb_height) && s->macroblocks_base ||\n        height != s->avctx->height) {\n        vp8_decode_flush_impl(s->avctx, 1);\n\n        ret = ff_set_dimensions(s->avctx, width, height);\n        if (ret < 0)\n            return ret;\n    }\n\n    s->mb_width  = (s->avctx->coded_width  + 15) / 16;\n    s->mb_height = (s->avctx->coded_height + 15) / 16;\n\n    s->mb_layout = is_vp7 || avctx->active_thread_type == FF_THREAD_SLICE &&\n                   FFMIN(s->num_coeff_partitions, avctx->thread_count) > 1;\n    if (!s->mb_layout) { // Frame threading and one thread\n        s->macroblocks_base       = av_mallocz((s->mb_width + s->mb_height * 2 + 1) *\n                                               sizeof(*s->macroblocks));\n        s->intra4x4_pred_mode_top = av_mallocz(s->mb_width * 4);\n    } else // Sliced threading\n        s->macroblocks_base = av_mallocz((s->mb_width + 2) * (s->mb_height + 2) *\n                                         sizeof(*s->macroblocks));\n    s->top_nnz     = av_mallocz(s->mb_width * sizeof(*s->top_nnz));\n    s->top_border  = av_mallocz((s->mb_width + 1) * sizeof(*s->top_border));\n    s->thread_data = av_mallocz(MAX_THREADS * sizeof(VP8ThreadData));\n\n    if (!s->macroblocks_base || !s->top_nnz || !s->top_border ||\n        !s->thread_data || (!s->intra4x4_pred_mode_top && !s->mb_layout)) {\n        free_buffers(s);\n        return AVERROR(ENOMEM);\n    }\n\n    for (i = 0; i < MAX_THREADS; i++) {\n        s->thread_data[i].filter_strength =\n            av_mallocz(s->mb_width * sizeof(*s->thread_data[0].filter_strength));\n        if (!s->thread_data[i].filter_strength) {\n            free_buffers(s);\n            return AVERROR(ENOMEM);\n        }\n#if HAVE_THREADS\n        pthread_mutex_init(&s->thread_data[i].lock, NULL);\n        pthread_cond_init(&s->thread_data[i].cond, NULL);\n#endif\n    }\n\n    s->macroblocks = s->macroblocks_base + 1;\n\n    return 0;\n}",
        "func": "static av_always_inline\nint update_dimensions(VP8Context *s, int width, int height, int is_vp7)\n{\n    AVCodecContext *avctx = s->avctx;\n    int i, ret;\n\n    if (width  != s->avctx->width || ((width+15)/16 != s->mb_width || (height+15)/16 != s->mb_height) && s->macroblocks_base ||\n        height != s->avctx->height) {\n        vp8_decode_flush_impl(s->avctx, 1);\n\n        ret = ff_set_dimensions(s->avctx, width, height);\n        if (ret < 0)\n            return ret;\n    }\n\n    s->mb_width  = (s->avctx->coded_width  + 15) / 16;\n    s->mb_height = (s->avctx->coded_height + 15) / 16;\n\n    s->mb_layout = is_vp7 || avctx->active_thread_type == FF_THREAD_SLICE &&\n                   avctx->thread_count > 1;\n    if (!s->mb_layout) { // Frame threading and one thread\n        s->macroblocks_base       = av_mallocz((s->mb_width + s->mb_height * 2 + 1) *\n                                               sizeof(*s->macroblocks));\n        s->intra4x4_pred_mode_top = av_mallocz(s->mb_width * 4);\n    } else // Sliced threading\n        s->macroblocks_base = av_mallocz((s->mb_width + 2) * (s->mb_height + 2) *\n                                         sizeof(*s->macroblocks));\n    s->top_nnz     = av_mallocz(s->mb_width * sizeof(*s->top_nnz));\n    s->top_border  = av_mallocz((s->mb_width + 1) * sizeof(*s->top_border));\n    s->thread_data = av_mallocz(MAX_THREADS * sizeof(VP8ThreadData));\n\n    if (!s->macroblocks_base || !s->top_nnz || !s->top_border ||\n        !s->thread_data || (!s->intra4x4_pred_mode_top && !s->mb_layout)) {\n        free_buffers(s);\n        return AVERROR(ENOMEM);\n    }\n\n    for (i = 0; i < MAX_THREADS; i++) {\n        s->thread_data[i].filter_strength =\n            av_mallocz(s->mb_width * sizeof(*s->thread_data[0].filter_strength));\n        if (!s->thread_data[i].filter_strength) {\n            free_buffers(s);\n            return AVERROR(ENOMEM);\n        }\n#if HAVE_THREADS\n        pthread_mutex_init(&s->thread_data[i].lock, NULL);\n        pthread_cond_init(&s->thread_data[i].cond, NULL);\n#endif\n    }\n\n    s->macroblocks = s->macroblocks_base + 1;\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,7 +17,7 @@\n     s->mb_height = (s->avctx->coded_height + 15) / 16;\n \n     s->mb_layout = is_vp7 || avctx->active_thread_type == FF_THREAD_SLICE &&\n-                   FFMIN(s->num_coeff_partitions, avctx->thread_count) > 1;\n+                   avctx->thread_count > 1;\n     if (!s->mb_layout) { // Frame threading and one thread\n         s->macroblocks_base       = av_mallocz((s->mb_width + s->mb_height * 2 + 1) *\n                                                sizeof(*s->macroblocks));",
        "diff_line_info": {
            "deleted_lines": [
                "                   FFMIN(s->num_coeff_partitions, avctx->thread_count) > 1;"
            ],
            "added_lines": [
                "                   avctx->thread_count > 1;"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-7613",
        "func_name": "torvalds/linux/newque",
        "description": "Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",
        "git_url": "https://github.com/torvalds/linux/commit/b9a532277938798b53178d5a66af6e2915cb27cf",
        "commit_title": "Initialize msg/shm IPC objects before doing ipc_addid()",
        "commit_text": " As reported by Dmitry Vyukov, we really shouldn't do ipc_addid() before having initialized the IPC object state.  Yes, we initialize the IPC object in a locked state, but with all the lockless RCU lookup work, that IPC object lock no longer means that the state cannot be seen.  We already did this for the IPC semaphore code (see commit e8577d1f0329: \"ipc/sem.c: fully initialize sem_array before making it visible\") but we clearly forgot about msg and shm.  Cc: Manfred Spraul <manfred@colorfullife.com> Cc: Davidlohr Bueso <dbueso@suse.de> Cc: stable@vger.kernel.org",
        "func_before": "static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tstruct msg_queue *msq;\n\tint id, retval;\n\tkey_t key = params->key;\n\tint msgflg = params->flg;\n\n\tmsq = ipc_rcu_alloc(sizeof(*msq));\n\tif (!msq)\n\t\treturn -ENOMEM;\n\n\tmsq->q_perm.mode = msgflg & S_IRWXUGO;\n\tmsq->q_perm.key = key;\n\n\tmsq->q_perm.security = NULL;\n\tretval = security_msg_queue_alloc(msq);\n\tif (retval) {\n\t\tipc_rcu_putref(msq, ipc_rcu_free);\n\t\treturn retval;\n\t}\n\n\t/* ipc_addid() locks msq upon success. */\n\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n\tif (id < 0) {\n\t\tipc_rcu_putref(msq, msg_rcu_free);\n\t\treturn id;\n\t}\n\n\tmsq->q_stime = msq->q_rtime = 0;\n\tmsq->q_ctime = get_seconds();\n\tmsq->q_cbytes = msq->q_qnum = 0;\n\tmsq->q_qbytes = ns->msg_ctlmnb;\n\tmsq->q_lspid = msq->q_lrpid = 0;\n\tINIT_LIST_HEAD(&msq->q_messages);\n\tINIT_LIST_HEAD(&msq->q_receivers);\n\tINIT_LIST_HEAD(&msq->q_senders);\n\n\tipc_unlock_object(&msq->q_perm);\n\trcu_read_unlock();\n\n\treturn msq->q_perm.id;\n}",
        "func": "static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tstruct msg_queue *msq;\n\tint id, retval;\n\tkey_t key = params->key;\n\tint msgflg = params->flg;\n\n\tmsq = ipc_rcu_alloc(sizeof(*msq));\n\tif (!msq)\n\t\treturn -ENOMEM;\n\n\tmsq->q_perm.mode = msgflg & S_IRWXUGO;\n\tmsq->q_perm.key = key;\n\n\tmsq->q_perm.security = NULL;\n\tretval = security_msg_queue_alloc(msq);\n\tif (retval) {\n\t\tipc_rcu_putref(msq, ipc_rcu_free);\n\t\treturn retval;\n\t}\n\n\tmsq->q_stime = msq->q_rtime = 0;\n\tmsq->q_ctime = get_seconds();\n\tmsq->q_cbytes = msq->q_qnum = 0;\n\tmsq->q_qbytes = ns->msg_ctlmnb;\n\tmsq->q_lspid = msq->q_lrpid = 0;\n\tINIT_LIST_HEAD(&msq->q_messages);\n\tINIT_LIST_HEAD(&msq->q_receivers);\n\tINIT_LIST_HEAD(&msq->q_senders);\n\n\t/* ipc_addid() locks msq upon success. */\n\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n\tif (id < 0) {\n\t\tipc_rcu_putref(msq, msg_rcu_free);\n\t\treturn id;\n\t}\n\n\tipc_unlock_object(&msq->q_perm);\n\trcu_read_unlock();\n\n\treturn msq->q_perm.id;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,13 +19,6 @@\n \t\treturn retval;\n \t}\n \n-\t/* ipc_addid() locks msq upon success. */\n-\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n-\tif (id < 0) {\n-\t\tipc_rcu_putref(msq, msg_rcu_free);\n-\t\treturn id;\n-\t}\n-\n \tmsq->q_stime = msq->q_rtime = 0;\n \tmsq->q_ctime = get_seconds();\n \tmsq->q_cbytes = msq->q_qnum = 0;\n@@ -35,6 +28,13 @@\n \tINIT_LIST_HEAD(&msq->q_receivers);\n \tINIT_LIST_HEAD(&msq->q_senders);\n \n+\t/* ipc_addid() locks msq upon success. */\n+\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n+\tif (id < 0) {\n+\t\tipc_rcu_putref(msq, msg_rcu_free);\n+\t\treturn id;\n+\t}\n+\n \tipc_unlock_object(&msq->q_perm);\n \trcu_read_unlock();\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* ipc_addid() locks msq upon success. */",
                "\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);",
                "\tif (id < 0) {",
                "\t\tipc_rcu_putref(msq, msg_rcu_free);",
                "\t\treturn id;",
                "\t}",
                ""
            ],
            "added_lines": [
                "\t/* ipc_addid() locks msq upon success. */",
                "\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);",
                "\tif (id < 0) {",
                "\t\tipc_rcu_putref(msq, msg_rcu_free);",
                "\t\treturn id;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2015-7613",
        "func_name": "torvalds/linux/newseg",
        "description": "Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",
        "git_url": "https://github.com/torvalds/linux/commit/b9a532277938798b53178d5a66af6e2915cb27cf",
        "commit_title": "Initialize msg/shm IPC objects before doing ipc_addid()",
        "commit_text": " As reported by Dmitry Vyukov, we really shouldn't do ipc_addid() before having initialized the IPC object state.  Yes, we initialize the IPC object in a locked state, but with all the lockless RCU lookup work, that IPC object lock no longer means that the state cannot be seen.  We already did this for the IPC semaphore code (see commit e8577d1f0329: \"ipc/sem.c: fully initialize sem_array before making it visible\") but we clearly forgot about msg and shm.  Cc: Manfred Spraul <manfred@colorfullife.com> Cc: Davidlohr Bueso <dbueso@suse.de> Cc: stable@vger.kernel.org",
        "func_before": "static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tkey_t key = params->key;\n\tint shmflg = params->flg;\n\tsize_t size = params->u.size;\n\tint error;\n\tstruct shmid_kernel *shp;\n\tsize_t numpages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstruct file *file;\n\tchar name[13];\n\tint id;\n\tvm_flags_t acctflag = 0;\n\n\tif (size < SHMMIN || size > ns->shm_ctlmax)\n\t\treturn -EINVAL;\n\n\tif (numpages << PAGE_SHIFT < size)\n\t\treturn -ENOSPC;\n\n\tif (ns->shm_tot + numpages < ns->shm_tot ||\n\t\t\tns->shm_tot + numpages > ns->shm_ctlall)\n\t\treturn -ENOSPC;\n\n\tshp = ipc_rcu_alloc(sizeof(*shp));\n\tif (!shp)\n\t\treturn -ENOMEM;\n\n\tshp->shm_perm.key = key;\n\tshp->shm_perm.mode = (shmflg & S_IRWXUGO);\n\tshp->mlock_user = NULL;\n\n\tshp->shm_perm.security = NULL;\n\terror = security_shm_alloc(shp);\n\tif (error) {\n\t\tipc_rcu_putref(shp, ipc_rcu_free);\n\t\treturn error;\n\t}\n\n\tsprintf(name, \"SYSV%08x\", key);\n\tif (shmflg & SHM_HUGETLB) {\n\t\tstruct hstate *hs;\n\t\tsize_t hugesize;\n\n\t\ths = hstate_sizelog((shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t\tif (!hs) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto no_file;\n\t\t}\n\t\thugesize = ALIGN(size, huge_page_size(hs));\n\n\t\t/* hugetlb_file_setup applies strict accounting */\n\t\tif (shmflg & SHM_NORESERVE)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = hugetlb_file_setup(name, hugesize, acctflag,\n\t\t\t\t  &shp->mlock_user, HUGETLB_SHMFS_INODE,\n\t\t\t\t(shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t} else {\n\t\t/*\n\t\t * Do not allow no accounting for OVERCOMMIT_NEVER, even\n\t\t * if it's asked for.\n\t\t */\n\t\tif  ((shmflg & SHM_NORESERVE) &&\n\t\t\t\tsysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = shmem_kernel_file_setup(name, size, acctflag);\n\t}\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto no_file;\n\n\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n\tif (id < 0) {\n\t\terror = id;\n\t\tgoto no_id;\n\t}\n\n\tshp->shm_cprid = task_tgid_vnr(current);\n\tshp->shm_lprid = 0;\n\tshp->shm_atim = shp->shm_dtim = 0;\n\tshp->shm_ctim = get_seconds();\n\tshp->shm_segsz = size;\n\tshp->shm_nattch = 0;\n\tshp->shm_file = file;\n\tshp->shm_creator = current;\n\tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n\n\t/*\n\t * shmid gets reported as \"inode#\" in /proc/pid/maps.\n\t * proc-ps tools use this. Changing this will break them.\n\t */\n\tfile_inode(file)->i_ino = shp->shm_perm.id;\n\n\tns->shm_tot += numpages;\n\terror = shp->shm_perm.id;\n\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\treturn error;\n\nno_id:\n\tif (is_file_hugepages(file) && shp->mlock_user)\n\t\tuser_shm_unlock(size, shp->mlock_user);\n\tfput(file);\nno_file:\n\tipc_rcu_putref(shp, shm_rcu_free);\n\treturn error;\n}",
        "func": "static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tkey_t key = params->key;\n\tint shmflg = params->flg;\n\tsize_t size = params->u.size;\n\tint error;\n\tstruct shmid_kernel *shp;\n\tsize_t numpages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstruct file *file;\n\tchar name[13];\n\tint id;\n\tvm_flags_t acctflag = 0;\n\n\tif (size < SHMMIN || size > ns->shm_ctlmax)\n\t\treturn -EINVAL;\n\n\tif (numpages << PAGE_SHIFT < size)\n\t\treturn -ENOSPC;\n\n\tif (ns->shm_tot + numpages < ns->shm_tot ||\n\t\t\tns->shm_tot + numpages > ns->shm_ctlall)\n\t\treturn -ENOSPC;\n\n\tshp = ipc_rcu_alloc(sizeof(*shp));\n\tif (!shp)\n\t\treturn -ENOMEM;\n\n\tshp->shm_perm.key = key;\n\tshp->shm_perm.mode = (shmflg & S_IRWXUGO);\n\tshp->mlock_user = NULL;\n\n\tshp->shm_perm.security = NULL;\n\terror = security_shm_alloc(shp);\n\tif (error) {\n\t\tipc_rcu_putref(shp, ipc_rcu_free);\n\t\treturn error;\n\t}\n\n\tsprintf(name, \"SYSV%08x\", key);\n\tif (shmflg & SHM_HUGETLB) {\n\t\tstruct hstate *hs;\n\t\tsize_t hugesize;\n\n\t\ths = hstate_sizelog((shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t\tif (!hs) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto no_file;\n\t\t}\n\t\thugesize = ALIGN(size, huge_page_size(hs));\n\n\t\t/* hugetlb_file_setup applies strict accounting */\n\t\tif (shmflg & SHM_NORESERVE)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = hugetlb_file_setup(name, hugesize, acctflag,\n\t\t\t\t  &shp->mlock_user, HUGETLB_SHMFS_INODE,\n\t\t\t\t(shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t} else {\n\t\t/*\n\t\t * Do not allow no accounting for OVERCOMMIT_NEVER, even\n\t\t * if it's asked for.\n\t\t */\n\t\tif  ((shmflg & SHM_NORESERVE) &&\n\t\t\t\tsysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = shmem_kernel_file_setup(name, size, acctflag);\n\t}\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto no_file;\n\n\tshp->shm_cprid = task_tgid_vnr(current);\n\tshp->shm_lprid = 0;\n\tshp->shm_atim = shp->shm_dtim = 0;\n\tshp->shm_ctim = get_seconds();\n\tshp->shm_segsz = size;\n\tshp->shm_nattch = 0;\n\tshp->shm_file = file;\n\tshp->shm_creator = current;\n\n\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n\tif (id < 0) {\n\t\terror = id;\n\t\tgoto no_id;\n\t}\n\n\tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n\n\t/*\n\t * shmid gets reported as \"inode#\" in /proc/pid/maps.\n\t * proc-ps tools use this. Changing this will break them.\n\t */\n\tfile_inode(file)->i_ino = shp->shm_perm.id;\n\n\tns->shm_tot += numpages;\n\terror = shp->shm_perm.id;\n\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\treturn error;\n\nno_id:\n\tif (is_file_hugepages(file) && shp->mlock_user)\n\t\tuser_shm_unlock(size, shp->mlock_user);\n\tfput(file);\nno_file:\n\tipc_rcu_putref(shp, shm_rcu_free);\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -68,12 +68,6 @@\n \tif (IS_ERR(file))\n \t\tgoto no_file;\n \n-\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n-\tif (id < 0) {\n-\t\terror = id;\n-\t\tgoto no_id;\n-\t}\n-\n \tshp->shm_cprid = task_tgid_vnr(current);\n \tshp->shm_lprid = 0;\n \tshp->shm_atim = shp->shm_dtim = 0;\n@@ -82,6 +76,13 @@\n \tshp->shm_nattch = 0;\n \tshp->shm_file = file;\n \tshp->shm_creator = current;\n+\n+\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n+\tif (id < 0) {\n+\t\terror = id;\n+\t\tgoto no_id;\n+\t}\n+\n \tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n \n \t/*",
        "diff_line_info": {
            "deleted_lines": [
                "\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);",
                "\tif (id < 0) {",
                "\t\terror = id;",
                "\t\tgoto no_id;",
                "\t}",
                ""
            ],
            "added_lines": [
                "",
                "\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);",
                "\tif (id < 0) {",
                "\t\terror = id;",
                "\t\tgoto no_id;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2015-7613",
        "func_name": "torvalds/linux/ipc_addid",
        "description": "Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",
        "git_url": "https://github.com/torvalds/linux/commit/b9a532277938798b53178d5a66af6e2915cb27cf",
        "commit_title": "Initialize msg/shm IPC objects before doing ipc_addid()",
        "commit_text": " As reported by Dmitry Vyukov, we really shouldn't do ipc_addid() before having initialized the IPC object state.  Yes, we initialize the IPC object in a locked state, but with all the lockless RCU lookup work, that IPC object lock no longer means that the state cannot be seen.  We already did this for the IPC semaphore code (see commit e8577d1f0329: \"ipc/sem.c: fully initialize sem_array before making it visible\") but we clearly forgot about msg and shm.  Cc: Manfred Spraul <manfred@colorfullife.com> Cc: Davidlohr Bueso <dbueso@suse.de> Cc: stable@vger.kernel.org",
        "func_before": "int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n{\n\tkuid_t euid;\n\tkgid_t egid;\n\tint id;\n\tint next_id = ids->next_id;\n\n\tif (size > IPCMNI)\n\t\tsize = IPCMNI;\n\n\tif (ids->in_use >= size)\n\t\treturn -ENOSPC;\n\n\tidr_preload(GFP_KERNEL);\n\n\tspin_lock_init(&new->lock);\n\tnew->deleted = false;\n\trcu_read_lock();\n\tspin_lock(&new->lock);\n\n\tid = idr_alloc(&ids->ipcs_idr, new,\n\t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n\t\t       GFP_NOWAIT);\n\tidr_preload_end();\n\tif (id < 0) {\n\t\tspin_unlock(&new->lock);\n\t\trcu_read_unlock();\n\t\treturn id;\n\t}\n\n\tids->in_use++;\n\n\tcurrent_euid_egid(&euid, &egid);\n\tnew->cuid = new->uid = euid;\n\tnew->gid = new->cgid = egid;\n\n\tif (next_id < 0) {\n\t\tnew->seq = ids->seq++;\n\t\tif (ids->seq > IPCID_SEQ_MAX)\n\t\t\tids->seq = 0;\n\t} else {\n\t\tnew->seq = ipcid_to_seqx(next_id);\n\t\tids->next_id = -1;\n\t}\n\n\tnew->id = ipc_buildid(id, new->seq);\n\treturn id;\n}",
        "func": "int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n{\n\tkuid_t euid;\n\tkgid_t egid;\n\tint id;\n\tint next_id = ids->next_id;\n\n\tif (size > IPCMNI)\n\t\tsize = IPCMNI;\n\n\tif (ids->in_use >= size)\n\t\treturn -ENOSPC;\n\n\tidr_preload(GFP_KERNEL);\n\n\tspin_lock_init(&new->lock);\n\tnew->deleted = false;\n\trcu_read_lock();\n\tspin_lock(&new->lock);\n\n\tcurrent_euid_egid(&euid, &egid);\n\tnew->cuid = new->uid = euid;\n\tnew->gid = new->cgid = egid;\n\n\tid = idr_alloc(&ids->ipcs_idr, new,\n\t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n\t\t       GFP_NOWAIT);\n\tidr_preload_end();\n\tif (id < 0) {\n\t\tspin_unlock(&new->lock);\n\t\trcu_read_unlock();\n\t\treturn id;\n\t}\n\n\tids->in_use++;\n\n\tif (next_id < 0) {\n\t\tnew->seq = ids->seq++;\n\t\tif (ids->seq > IPCID_SEQ_MAX)\n\t\t\tids->seq = 0;\n\t} else {\n\t\tnew->seq = ipcid_to_seqx(next_id);\n\t\tids->next_id = -1;\n\t}\n\n\tnew->id = ipc_buildid(id, new->seq);\n\treturn id;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,6 +18,10 @@\n \trcu_read_lock();\n \tspin_lock(&new->lock);\n \n+\tcurrent_euid_egid(&euid, &egid);\n+\tnew->cuid = new->uid = euid;\n+\tnew->gid = new->cgid = egid;\n+\n \tid = idr_alloc(&ids->ipcs_idr, new,\n \t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n \t\t       GFP_NOWAIT);\n@@ -29,10 +33,6 @@\n \t}\n \n \tids->in_use++;\n-\n-\tcurrent_euid_egid(&euid, &egid);\n-\tnew->cuid = new->uid = euid;\n-\tnew->gid = new->cgid = egid;\n \n \tif (next_id < 0) {\n \t\tnew->seq = ids->seq++;",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\tcurrent_euid_egid(&euid, &egid);",
                "\tnew->cuid = new->uid = euid;",
                "\tnew->gid = new->cgid = egid;"
            ],
            "added_lines": [
                "\tcurrent_euid_egid(&euid, &egid);",
                "\tnew->cuid = new->uid = euid;",
                "\tnew->gid = new->cgid = egid;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2015-7814",
        "func_name": "xen-project/xen/relinquish_memory",
        "description": "Race condition in the relinquish_memory function in arch/arm/domain.c in Xen 4.6.x and earlier allows local domains with partial management control to cause a denial of service (host crash) via vectors involving the destruction of a domain and using XENMEM_decrease_reservation to reduce the memory of the domain.",
        "git_url": "https://github.com/xen-project/xen/commit/1ef01396fdff88b1c3331a09ca5c69619b90f4ea",
        "commit_title": "arm: handle races between relinquish_memory and free_domheap_pages",
        "commit_text": " Primarily this means XENMEM_decrease_reservation from a toolstack domain.  Unlike x86 we have no requirement right now to queue such pages onto a separate list, if we hit this race then the other code has already fully accepted responsibility for freeing this page and therefore there is no more for relinquish_memory to do.  This is CVE-2015-7814 / XSA-147. ",
        "func_before": "static int relinquish_memory(struct domain *d, struct page_list_head *list)\n{\n    struct page_info *page, *tmp;\n    int               ret = 0;\n\n    /* Use a recursive lock, as we may enter 'free_domheap_page'. */\n    spin_lock_recursive(&d->page_alloc_lock);\n\n    page_list_for_each_safe( page, tmp, list )\n    {\n        /* Grab a reference to the page so it won't disappear from under us. */\n        if ( unlikely(!get_page(page, d)) )\n            /* Couldn't get a reference -- someone is freeing this page. */\n            BUG();\n\n        if ( test_and_clear_bit(_PGC_allocated, &page->count_info) )\n            put_page(page);\n\n        put_page(page);\n\n        if ( hypercall_preempt_check() )\n        {\n            ret = -ERESTART;\n            goto out;\n        }\n    }\n\n  out:\n    spin_unlock_recursive(&d->page_alloc_lock);\n    return ret;\n}",
        "func": "static int relinquish_memory(struct domain *d, struct page_list_head *list)\n{\n    struct page_info *page, *tmp;\n    int               ret = 0;\n\n    /* Use a recursive lock, as we may enter 'free_domheap_page'. */\n    spin_lock_recursive(&d->page_alloc_lock);\n\n    page_list_for_each_safe( page, tmp, list )\n    {\n        /* Grab a reference to the page so it won't disappear from under us. */\n        if ( unlikely(!get_page(page, d)) )\n            /*\n             * Couldn't get a reference -- someone is freeing this page and\n             * has already committed to doing so, so no more to do here.\n             *\n             * Note that the page must be left on the list, a list_del\n             * here will clash with the list_del done by the other\n             * party in the race and corrupt the list head.\n             */\n            continue;\n\n        if ( test_and_clear_bit(_PGC_allocated, &page->count_info) )\n            put_page(page);\n\n        put_page(page);\n\n        if ( hypercall_preempt_check() )\n        {\n            ret = -ERESTART;\n            goto out;\n        }\n    }\n\n  out:\n    spin_unlock_recursive(&d->page_alloc_lock);\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,8 +10,15 @@\n     {\n         /* Grab a reference to the page so it won't disappear from under us. */\n         if ( unlikely(!get_page(page, d)) )\n-            /* Couldn't get a reference -- someone is freeing this page. */\n-            BUG();\n+            /*\n+             * Couldn't get a reference -- someone is freeing this page and\n+             * has already committed to doing so, so no more to do here.\n+             *\n+             * Note that the page must be left on the list, a list_del\n+             * here will clash with the list_del done by the other\n+             * party in the race and corrupt the list head.\n+             */\n+            continue;\n \n         if ( test_and_clear_bit(_PGC_allocated, &page->count_info) )\n             put_page(page);",
        "diff_line_info": {
            "deleted_lines": [
                "            /* Couldn't get a reference -- someone is freeing this page. */",
                "            BUG();"
            ],
            "added_lines": [
                "            /*",
                "             * Couldn't get a reference -- someone is freeing this page and",
                "             * has already committed to doing so, so no more to do here.",
                "             *",
                "             * Note that the page must be left on the list, a list_del",
                "             * here will clash with the list_del done by the other",
                "             * party in the race and corrupt the list head.",
                "             */",
                "            continue;"
            ]
        }
    },
    {
        "cve_id": "CVE-2008-2365",
        "func_name": "torvalds/linux/ptrace_attach",
        "description": "Race condition in the ptrace and utrace support in the Linux kernel 2.6.9 through 2.6.25, as used in Red Hat Enterprise Linux (RHEL) 4, allows local users to cause a denial of service (oops) via a long series of PTRACE_ATTACH ptrace calls to another user's process that trigger a conflict between utrace_detach and report_quiescent, related to \"late ptrace_may_attach() check\" and \"race around &dead_engine_ops setting,\" a different vulnerability than CVE-2007-0771 and CVE-2008-1514.  NOTE: this issue might only affect kernel versions before 2.6.16.x.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=f5b40e363ad6041a96e3da32281d8faa191597b9",
        "commit_title": "This holds the task lock (and, for ptrace_attach, the tasklist_lock)",
        "commit_text": "over the actual attach event, which closes a race between attacking to a thread that is either doing a PTRACE_TRACEME or getting de-threaded.  Thanks to Oleg Nesterov for reminding me about this, and Chris Wright for noticing a lost return value in my first version.  ",
        "func_before": "int ptrace_attach(struct task_struct *task)\n{\n\tint retval;\n\ttask_lock(task);\n\tretval = -EPERM;\n\tif (task->pid <= 1)\n\t\tgoto bad;\n\tif (task->tgid == current->tgid)\n\t\tgoto bad;\n\t/* the same process cannot be attached many times */\n\tif (task->ptrace & PT_PTRACED)\n\t\tgoto bad;\n\tretval = may_attach(task);\n\tif (retval)\n\t\tgoto bad;\n\n\t/* Go */\n\ttask->ptrace |= PT_PTRACED | ((task->real_parent != current)\n\t\t\t\t      ? PT_ATTACHED : 0);\n\tif (capable(CAP_SYS_PTRACE))\n\t\ttask->ptrace |= PT_PTRACE_CAP;\n\ttask_unlock(task);\n\n\twrite_lock_irq(&tasklist_lock);\n\t__ptrace_link(task, current);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tforce_sig_specific(SIGSTOP, task);\n\treturn 0;\n\nbad:\n\ttask_unlock(task);\n\treturn retval;\n}",
        "func": "int ptrace_attach(struct task_struct *task)\n{\n\tint retval;\n\n\tretval = -EPERM;\n\tif (task->pid <= 1)\n\t\tgoto out;\n\tif (task->tgid == current->tgid)\n\t\tgoto out;\n\n\twrite_lock_irq(&tasklist_lock);\n\ttask_lock(task);\n\n\t/* the same process cannot be attached many times */\n\tif (task->ptrace & PT_PTRACED)\n\t\tgoto bad;\n\tretval = may_attach(task);\n\tif (retval)\n\t\tgoto bad;\n\n\t/* Go */\n\ttask->ptrace |= PT_PTRACED | ((task->real_parent != current)\n\t\t\t\t      ? PT_ATTACHED : 0);\n\tif (capable(CAP_SYS_PTRACE))\n\t\ttask->ptrace |= PT_PTRACE_CAP;\n\n\t__ptrace_link(task, current);\n\n\tforce_sig_specific(SIGSTOP, task);\n\nbad:\n\twrite_unlock_irq(&tasklist_lock);\n\ttask_unlock(task);\nout:\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,12 +1,16 @@\n int ptrace_attach(struct task_struct *task)\n {\n \tint retval;\n-\ttask_lock(task);\n+\n \tretval = -EPERM;\n \tif (task->pid <= 1)\n-\t\tgoto bad;\n+\t\tgoto out;\n \tif (task->tgid == current->tgid)\n-\t\tgoto bad;\n+\t\tgoto out;\n+\n+\twrite_lock_irq(&tasklist_lock);\n+\ttask_lock(task);\n+\n \t/* the same process cannot be attached many times */\n \tif (task->ptrace & PT_PTRACED)\n \t\tgoto bad;\n@@ -19,16 +23,14 @@\n \t\t\t\t      ? PT_ATTACHED : 0);\n \tif (capable(CAP_SYS_PTRACE))\n \t\ttask->ptrace |= PT_PTRACE_CAP;\n-\ttask_unlock(task);\n \n-\twrite_lock_irq(&tasklist_lock);\n \t__ptrace_link(task, current);\n-\twrite_unlock_irq(&tasklist_lock);\n \n \tforce_sig_specific(SIGSTOP, task);\n-\treturn 0;\n \n bad:\n+\twrite_unlock_irq(&tasklist_lock);\n \ttask_unlock(task);\n+out:\n \treturn retval;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\ttask_lock(task);",
                "\t\tgoto bad;",
                "\t\tgoto bad;",
                "\ttask_unlock(task);",
                "\twrite_lock_irq(&tasklist_lock);",
                "\twrite_unlock_irq(&tasklist_lock);",
                "\treturn 0;"
            ],
            "added_lines": [
                "",
                "\t\tgoto out;",
                "\t\tgoto out;",
                "",
                "\twrite_lock_irq(&tasklist_lock);",
                "\ttask_lock(task);",
                "",
                "\twrite_unlock_irq(&tasklist_lock);",
                "out:"
            ]
        }
    },
    {
        "cve_id": "CVE-2008-2365",
        "func_name": "torvalds/linux/ptrace_traceme",
        "description": "Race condition in the ptrace and utrace support in the Linux kernel 2.6.9 through 2.6.25, as used in Red Hat Enterprise Linux (RHEL) 4, allows local users to cause a denial of service (oops) via a long series of PTRACE_ATTACH ptrace calls to another user's process that trigger a conflict between utrace_detach and report_quiescent, related to \"late ptrace_may_attach() check\" and \"race around &dead_engine_ops setting,\" a different vulnerability than CVE-2007-0771 and CVE-2008-1514.  NOTE: this issue might only affect kernel versions before 2.6.16.x.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=f5b40e363ad6041a96e3da32281d8faa191597b9",
        "commit_title": "This holds the task lock (and, for ptrace_attach, the tasklist_lock)",
        "commit_text": "over the actual attach event, which closes a race between attacking to a thread that is either doing a PTRACE_TRACEME or getting de-threaded.  Thanks to Oleg Nesterov for reminding me about this, and Chris Wright for noticing a lost return value in my first version.  ",
        "func_before": "int ptrace_traceme(void)\n{\n\tint ret;\n\n\t/*\n\t * Are we already being traced?\n\t */\n\tif (current->ptrace & PT_PTRACED)\n\t\treturn -EPERM;\n\tret = security_ptrace(current->parent, current);\n\tif (ret)\n\t\treturn -EPERM;\n\t/*\n\t * Set the ptrace bit in the process ptrace flags.\n\t */\n\tcurrent->ptrace |= PT_PTRACED;\n\treturn 0;\n}",
        "func": "int ptrace_traceme(void)\n{\n\tint ret = -EPERM;\n\n\t/*\n\t * Are we already being traced?\n\t */\n\ttask_lock(current);\n\tif (!(current->ptrace & PT_PTRACED)) {\n\t\tret = security_ptrace(current->parent, current);\n\t\t/*\n\t\t * Set the ptrace bit in the process ptrace flags.\n\t\t */\n\t\tif (!ret)\n\t\t\tcurrent->ptrace |= PT_PTRACED;\n\t}\n\ttask_unlock(current);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,18 +1,19 @@\n int ptrace_traceme(void)\n {\n-\tint ret;\n+\tint ret = -EPERM;\n \n \t/*\n \t * Are we already being traced?\n \t */\n-\tif (current->ptrace & PT_PTRACED)\n-\t\treturn -EPERM;\n-\tret = security_ptrace(current->parent, current);\n-\tif (ret)\n-\t\treturn -EPERM;\n-\t/*\n-\t * Set the ptrace bit in the process ptrace flags.\n-\t */\n-\tcurrent->ptrace |= PT_PTRACED;\n-\treturn 0;\n+\ttask_lock(current);\n+\tif (!(current->ptrace & PT_PTRACED)) {\n+\t\tret = security_ptrace(current->parent, current);\n+\t\t/*\n+\t\t * Set the ptrace bit in the process ptrace flags.\n+\t\t */\n+\t\tif (!ret)\n+\t\t\tcurrent->ptrace |= PT_PTRACED;\n+\t}\n+\ttask_unlock(current);\n+\treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tint ret;",
                "\tif (current->ptrace & PT_PTRACED)",
                "\t\treturn -EPERM;",
                "\tret = security_ptrace(current->parent, current);",
                "\tif (ret)",
                "\t\treturn -EPERM;",
                "\t/*",
                "\t * Set the ptrace bit in the process ptrace flags.",
                "\t */",
                "\tcurrent->ptrace |= PT_PTRACED;",
                "\treturn 0;"
            ],
            "added_lines": [
                "\tint ret = -EPERM;",
                "\ttask_lock(current);",
                "\tif (!(current->ptrace & PT_PTRACED)) {",
                "\t\tret = security_ptrace(current->parent, current);",
                "\t\t/*",
                "\t\t * Set the ptrace bit in the process ptrace flags.",
                "\t\t */",
                "\t\tif (!ret)",
                "\t\t\tcurrent->ptrace |= PT_PTRACED;",
                "\t}",
                "\ttask_unlock(current);",
                "\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2008-2365",
        "func_name": "torvalds/linux/ptrace_attach",
        "description": "Race condition in the ptrace and utrace support in the Linux kernel 2.6.9 through 2.6.25, as used in Red Hat Enterprise Linux (RHEL) 4, allows local users to cause a denial of service (oops) via a long series of PTRACE_ATTACH ptrace calls to another user's process that trigger a conflict between utrace_detach and report_quiescent, related to \"late ptrace_may_attach() check\" and \"race around &dead_engine_ops setting,\" a different vulnerability than CVE-2007-0771 and CVE-2008-1514.  NOTE: this issue might only affect kernel versions before 2.6.16.x.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=f358166a9405e4f1d8e50d8f415c26d95505b6de",
        "commit_title": "Eric Biederman points out that we can't take the task_lock while holding",
        "commit_text": "tasklist_lock for writing, because another CPU that holds the task lock might take an interrupt that then tries to take tasklist_lock for writing.  Which would be a nasty deadlock, with one CPU spinning forever in an interrupt handler (although admittedly you need to really work at triggering it ;)  Since the ptrace_attach() code is special and very unusual, just make it be extra careful, and use trylock+repeat to avoid the possible deadlock.  Cc: Oleg Nesterov <oleg@tv-sign.ru> Cc: Eric W. Biederman <ebiederm@xmission.com> Cc: Roland McGrath <roland@redhat.com> ",
        "func_before": "int ptrace_attach(struct task_struct *task)\n{\n\tint retval;\n\n\tretval = -EPERM;\n\tif (task->pid <= 1)\n\t\tgoto out;\n\tif (task->tgid == current->tgid)\n\t\tgoto out;\n\n\twrite_lock_irq(&tasklist_lock);\n\ttask_lock(task);\n\n\t/* the same process cannot be attached many times */\n\tif (task->ptrace & PT_PTRACED)\n\t\tgoto bad;\n\tretval = may_attach(task);\n\tif (retval)\n\t\tgoto bad;\n\n\t/* Go */\n\ttask->ptrace |= PT_PTRACED | ((task->real_parent != current)\n\t\t\t\t      ? PT_ATTACHED : 0);\n\tif (capable(CAP_SYS_PTRACE))\n\t\ttask->ptrace |= PT_PTRACE_CAP;\n\n\t__ptrace_link(task, current);\n\n\tforce_sig_specific(SIGSTOP, task);\n\nbad:\n\twrite_unlock_irq(&tasklist_lock);\n\ttask_unlock(task);\nout:\n\treturn retval;\n}",
        "func": "int ptrace_attach(struct task_struct *task)\n{\n\tint retval;\n\n\tretval = -EPERM;\n\tif (task->pid <= 1)\n\t\tgoto out;\n\tif (task->tgid == current->tgid)\n\t\tgoto out;\n\nrepeat:\n\t/*\n\t * Nasty, nasty.\n\t *\n\t * We want to hold both the task-lock and the\n\t * tasklist_lock for writing at the same time.\n\t * But that's against the rules (tasklist_lock\n\t * is taken for reading by interrupts on other\n\t * cpu's that may have task_lock).\n\t */\n\ttask_lock(task);\n\tlocal_irq_disable();\n\tif (!write_trylock(&tasklist_lock)) {\n\t\tlocal_irq_enable();\n\t\ttask_unlock(task);\n\t\tdo {\n\t\t\tcpu_relax();\n\t\t} while (!write_can_lock(&tasklist_lock));\n\t\tgoto repeat;\n\t}\n\n\t/* the same process cannot be attached many times */\n\tif (task->ptrace & PT_PTRACED)\n\t\tgoto bad;\n\tretval = may_attach(task);\n\tif (retval)\n\t\tgoto bad;\n\n\t/* Go */\n\ttask->ptrace |= PT_PTRACED | ((task->real_parent != current)\n\t\t\t\t      ? PT_ATTACHED : 0);\n\tif (capable(CAP_SYS_PTRACE))\n\t\ttask->ptrace |= PT_PTRACE_CAP;\n\n\t__ptrace_link(task, current);\n\n\tforce_sig_specific(SIGSTOP, task);\n\nbad:\n\twrite_unlock_irq(&tasklist_lock);\n\ttask_unlock(task);\nout:\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,8 +8,26 @@\n \tif (task->tgid == current->tgid)\n \t\tgoto out;\n \n-\twrite_lock_irq(&tasklist_lock);\n+repeat:\n+\t/*\n+\t * Nasty, nasty.\n+\t *\n+\t * We want to hold both the task-lock and the\n+\t * tasklist_lock for writing at the same time.\n+\t * But that's against the rules (tasklist_lock\n+\t * is taken for reading by interrupts on other\n+\t * cpu's that may have task_lock).\n+\t */\n \ttask_lock(task);\n+\tlocal_irq_disable();\n+\tif (!write_trylock(&tasklist_lock)) {\n+\t\tlocal_irq_enable();\n+\t\ttask_unlock(task);\n+\t\tdo {\n+\t\t\tcpu_relax();\n+\t\t} while (!write_can_lock(&tasklist_lock));\n+\t\tgoto repeat;\n+\t}\n \n \t/* the same process cannot be attached many times */\n \tif (task->ptrace & PT_PTRACED)",
        "diff_line_info": {
            "deleted_lines": [
                "\twrite_lock_irq(&tasklist_lock);"
            ],
            "added_lines": [
                "repeat:",
                "\t/*",
                "\t * Nasty, nasty.",
                "\t *",
                "\t * We want to hold both the task-lock and the",
                "\t * tasklist_lock for writing at the same time.",
                "\t * But that's against the rules (tasklist_lock",
                "\t * is taken for reading by interrupts on other",
                "\t * cpu's that may have task_lock).",
                "\t */",
                "\tlocal_irq_disable();",
                "\tif (!write_trylock(&tasklist_lock)) {",
                "\t\tlocal_irq_enable();",
                "\t\ttask_unlock(task);",
                "\t\tdo {",
                "\t\t\tcpu_relax();",
                "\t\t} while (!write_can_lock(&tasklist_lock));",
                "\t\tgoto repeat;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2008-2365",
        "func_name": "torvalds/linux/ptrace_detach",
        "description": "Race condition in the ptrace and utrace support in the Linux kernel 2.6.9 through 2.6.25, as used in Red Hat Enterprise Linux (RHEL) 4, allows local users to cause a denial of service (oops) via a long series of PTRACE_ATTACH ptrace calls to another user's process that trigger a conflict between utrace_detach and report_quiescent, related to \"late ptrace_may_attach() check\" and \"race around &dead_engine_ops setting,\" a different vulnerability than CVE-2007-0771 and CVE-2008-1514.  NOTE: this issue might only affect kernel versions before 2.6.16.x.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=5ecfbae093f0c37311e89b29bfc0c9d586eace87",
        "commit_title": "1. The tracee can go from ptrace_stop() to do_signal_stop()",
        "commit_text": "   after __ptrace_unlink(p).  2. It is unsafe to __ptrace_unlink(p) while p->parent may wait    for tasklist_lock in ptrace_detach().  Cc: Roland McGrath <roland@redhat.com> Cc: Ingo Molnar <mingo@elte.hu> Cc: Christoph Hellwig <hch@lst.de> Cc: Eric W. Biederman <ebiederm@xmission.com> ",
        "func_before": "int ptrace_detach(struct task_struct *child, unsigned int data)\n{\n\tif (!valid_signal(data))\n\t\treturn\t-EIO;\n\n\t/* Architecture-specific hardware disable .. */\n\tptrace_disable(child);\n\n\t/* .. re-parent .. */\n\tchild->exit_code = data;\n\n\twrite_lock_irq(&tasklist_lock);\n\t__ptrace_unlink(child);\n\t/* .. and wake it up. */\n\tif (child->exit_state != EXIT_ZOMBIE)\n\t\twake_up_process(child);\n\twrite_unlock_irq(&tasklist_lock);\n\n\treturn 0;\n}",
        "func": "int ptrace_detach(struct task_struct *child, unsigned int data)\n{\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\t/* Architecture-specific hardware disable .. */\n\tptrace_disable(child);\n\n\twrite_lock_irq(&tasklist_lock);\n\tif (child->ptrace)\n\t\t__ptrace_detach(child, data);\n\twrite_unlock_irq(&tasklist_lock);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,19 +1,14 @@\n int ptrace_detach(struct task_struct *child, unsigned int data)\n {\n \tif (!valid_signal(data))\n-\t\treturn\t-EIO;\n+\t\treturn -EIO;\n \n \t/* Architecture-specific hardware disable .. */\n \tptrace_disable(child);\n \n-\t/* .. re-parent .. */\n-\tchild->exit_code = data;\n-\n \twrite_lock_irq(&tasklist_lock);\n-\t__ptrace_unlink(child);\n-\t/* .. and wake it up. */\n-\tif (child->exit_state != EXIT_ZOMBIE)\n-\t\twake_up_process(child);\n+\tif (child->ptrace)\n+\t\t__ptrace_detach(child, data);\n \twrite_unlock_irq(&tasklist_lock);\n \n \treturn 0;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn\t-EIO;",
                "\t/* .. re-parent .. */",
                "\tchild->exit_code = data;",
                "",
                "\t__ptrace_unlink(child);",
                "\t/* .. and wake it up. */",
                "\tif (child->exit_state != EXIT_ZOMBIE)",
                "\t\twake_up_process(child);"
            ],
            "added_lines": [
                "\t\treturn -EIO;",
                "\tif (child->ptrace)",
                "\t\t__ptrace_detach(child, data);"
            ]
        }
    },
    {
        "cve_id": "CVE-2008-2365",
        "func_name": "torvalds/linux/__ptrace_unlink",
        "description": "Race condition in the ptrace and utrace support in the Linux kernel 2.6.9 through 2.6.25, as used in Red Hat Enterprise Linux (RHEL) 4, allows local users to cause a denial of service (oops) via a long series of PTRACE_ATTACH ptrace calls to another user's process that trigger a conflict between utrace_detach and report_quiescent, related to \"late ptrace_may_attach() check\" and \"race around &dead_engine_ops setting,\" a different vulnerability than CVE-2007-0771 and CVE-2008-1514.  NOTE: this issue might only affect kernel versions before 2.6.16.x.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=5ecfbae093f0c37311e89b29bfc0c9d586eace87",
        "commit_title": "1. The tracee can go from ptrace_stop() to do_signal_stop()",
        "commit_text": "   after __ptrace_unlink(p).  2. It is unsafe to __ptrace_unlink(p) while p->parent may wait    for tasklist_lock in ptrace_detach().  Cc: Roland McGrath <roland@redhat.com> Cc: Ingo Molnar <mingo@elte.hu> Cc: Christoph Hellwig <hch@lst.de> Cc: Eric W. Biederman <ebiederm@xmission.com> ",
        "func_before": "void __ptrace_unlink(task_t *child)\n{\n\tif (!child->ptrace)\n\t\tBUG();\n\tchild->ptrace = 0;\n\tif (!list_empty(&child->ptrace_list)) {\n\t\tlist_del_init(&child->ptrace_list);\n\t\tREMOVE_LINKS(child);\n\t\tchild->parent = child->real_parent;\n\t\tSET_LINKS(child);\n\t}\n\n\tptrace_untrace(child);\n}",
        "func": "void __ptrace_unlink(task_t *child)\n{\n\tBUG_ON(!child->ptrace);\n\n\tchild->ptrace = 0;\n\tif (!list_empty(&child->ptrace_list)) {\n\t\tlist_del_init(&child->ptrace_list);\n\t\tREMOVE_LINKS(child);\n\t\tchild->parent = child->real_parent;\n\t\tSET_LINKS(child);\n\t}\n\n\tptrace_untrace(child);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n void __ptrace_unlink(task_t *child)\n {\n-\tif (!child->ptrace)\n-\t\tBUG();\n+\tBUG_ON(!child->ptrace);\n+\n \tchild->ptrace = 0;\n \tif (!list_empty(&child->ptrace_list)) {\n \t\tlist_del_init(&child->ptrace_list);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!child->ptrace)",
                "\t\tBUG();"
            ],
            "added_lines": [
                "\tBUG_ON(!child->ptrace);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2008-2365",
        "func_name": "torvalds/linux/zap_threads",
        "description": "Race condition in the ptrace and utrace support in the Linux kernel 2.6.9 through 2.6.25, as used in Red Hat Enterprise Linux (RHEL) 4, allows local users to cause a denial of service (oops) via a long series of PTRACE_ATTACH ptrace calls to another user's process that trigger a conflict between utrace_detach and report_quiescent, related to \"late ptrace_may_attach() check\" and \"race around &dead_engine_ops setting,\" a different vulnerability than CVE-2007-0771 and CVE-2008-1514.  NOTE: this issue might only affect kernel versions before 2.6.16.x.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=5ecfbae093f0c37311e89b29bfc0c9d586eace87",
        "commit_title": "1. The tracee can go from ptrace_stop() to do_signal_stop()",
        "commit_text": "   after __ptrace_unlink(p).  2. It is unsafe to __ptrace_unlink(p) while p->parent may wait    for tasklist_lock in ptrace_detach().  Cc: Roland McGrath <roland@redhat.com> Cc: Ingo Molnar <mingo@elte.hu> Cc: Christoph Hellwig <hch@lst.de> Cc: Eric W. Biederman <ebiederm@xmission.com> ",
        "func_before": "static void zap_threads (struct mm_struct *mm)\n{\n\tstruct task_struct *g, *p;\n\tstruct task_struct *tsk = current;\n\tstruct completion *vfork_done = tsk->vfork_done;\n\tint traced = 0;\n\n\t/*\n\t * Make sure nobody is waiting for us to release the VM,\n\t * otherwise we can deadlock when we wait on each other\n\t */\n\tif (vfork_done) {\n\t\ttsk->vfork_done = NULL;\n\t\tcomplete(vfork_done);\n\t}\n\n\tread_lock(&tasklist_lock);\n\tdo_each_thread(g,p)\n\t\tif (mm == p->mm && p != tsk) {\n\t\t\tforce_sig_specific(SIGKILL, p);\n\t\t\tmm->core_waiters++;\n\t\t\tif (unlikely(p->ptrace) &&\n\t\t\t    unlikely(p->parent->mm == mm))\n\t\t\t\ttraced = 1;\n\t\t}\n\twhile_each_thread(g,p);\n\n\tread_unlock(&tasklist_lock);\n\n\tif (unlikely(traced)) {\n\t\t/*\n\t\t * We are zapping a thread and the thread it ptraces.\n\t\t * If the tracee went into a ptrace stop for exit tracing,\n\t\t * we could deadlock since the tracer is waiting for this\n\t\t * coredump to finish.  Detach them so they can both die.\n\t\t */\n\t\twrite_lock_irq(&tasklist_lock);\n\t\tdo_each_thread(g,p) {\n\t\t\tif (mm == p->mm && p != tsk &&\n\t\t\t    p->ptrace && p->parent->mm == mm) {\n\t\t\t\t__ptrace_unlink(p);\n\t\t\t}\n\t\t} while_each_thread(g,p);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t}\n}",
        "func": "static void zap_threads (struct mm_struct *mm)\n{\n\tstruct task_struct *g, *p;\n\tstruct task_struct *tsk = current;\n\tstruct completion *vfork_done = tsk->vfork_done;\n\tint traced = 0;\n\n\t/*\n\t * Make sure nobody is waiting for us to release the VM,\n\t * otherwise we can deadlock when we wait on each other\n\t */\n\tif (vfork_done) {\n\t\ttsk->vfork_done = NULL;\n\t\tcomplete(vfork_done);\n\t}\n\n\tread_lock(&tasklist_lock);\n\tdo_each_thread(g,p)\n\t\tif (mm == p->mm && p != tsk) {\n\t\t\tforce_sig_specific(SIGKILL, p);\n\t\t\tmm->core_waiters++;\n\t\t\tif (unlikely(p->ptrace) &&\n\t\t\t    unlikely(p->parent->mm == mm))\n\t\t\t\ttraced = 1;\n\t\t}\n\twhile_each_thread(g,p);\n\n\tread_unlock(&tasklist_lock);\n\n\tif (unlikely(traced)) {\n\t\t/*\n\t\t * We are zapping a thread and the thread it ptraces.\n\t\t * If the tracee went into a ptrace stop for exit tracing,\n\t\t * we could deadlock since the tracer is waiting for this\n\t\t * coredump to finish.  Detach them so they can both die.\n\t\t */\n\t\twrite_lock_irq(&tasklist_lock);\n\t\tdo_each_thread(g,p) {\n\t\t\tif (mm == p->mm && p != tsk &&\n\t\t\t    p->ptrace && p->parent->mm == mm) {\n\t\t\t\t__ptrace_detach(p, 0);\n\t\t\t}\n\t\t} while_each_thread(g,p);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,7 +38,7 @@\n \t\tdo_each_thread(g,p) {\n \t\t\tif (mm == p->mm && p != tsk &&\n \t\t\t    p->ptrace && p->parent->mm == mm) {\n-\t\t\t\t__ptrace_unlink(p);\n+\t\t\t\t__ptrace_detach(p, 0);\n \t\t\t}\n \t\t} while_each_thread(g,p);\n \t\twrite_unlock_irq(&tasklist_lock);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t__ptrace_unlink(p);"
            ],
            "added_lines": [
                "\t\t\t\t__ptrace_detach(p, 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2008-4307",
        "func_name": "torvalds/linux/do_setlk",
        "description": "Race condition in the do_setlk function in fs/nfs/file.c in the Linux kernel before 2.6.26 allows local users to cause a denial of service (crash) via vectors resulting in an interrupted RPC call that leads to a stray FL_POSIX lock, related to improper handling of a race between fcntl and close in the EINTR case.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=c4d7c402b788b73dc24f1e54a57f89d3dc5eb7bc",
        "commit_title": "Both NLM and NFSv4 should be able to clean up adequately in the case where",
        "commit_text": "the user interrupts the RPC call...  ",
        "func_before": "static int do_setlk(struct file *filp, int cmd, struct file_lock *fl)\n{\n\tstruct inode *inode = filp->f_mapping->host;\n\tint status;\n\n\t/*\n\t * Flush all pending writes before doing anything\n\t * with locks..\n\t */\n\tstatus = nfs_sync_mapping(filp->f_mapping);\n\tif (status != 0)\n\t\tgoto out;\n\n\tlock_kernel();\n\t/* Use local locking if mounted with \"-onolock\" */\n\tif (!(NFS_SERVER(inode)->flags & NFS_MOUNT_NONLM)) {\n\t\tstatus = NFS_PROTO(inode)->lock(filp, cmd, fl);\n\t\t/* If we were signalled we still need to ensure that\n\t\t * we clean up any state on the server. We therefore\n\t\t * record the lock call as having succeeded in order to\n\t\t * ensure that locks_remove_posix() cleans it out when\n\t\t * the process exits.\n\t\t */\n\t\tif (status == -EINTR || status == -ERESTARTSYS)\n\t\t\tdo_vfs_lock(filp, fl);\n\t} else\n\t\tstatus = do_vfs_lock(filp, fl);\n\tunlock_kernel();\n\tif (status < 0)\n\t\tgoto out;\n\t/*\n\t * Make sure we clear the cache whenever we try to get the lock.\n\t * This makes locking act as a cache coherency point.\n\t */\n\tnfs_sync_mapping(filp->f_mapping);\n\tnfs_zap_caches(inode);\nout:\n\treturn status;\n}",
        "func": "static int do_setlk(struct file *filp, int cmd, struct file_lock *fl)\n{\n\tstruct inode *inode = filp->f_mapping->host;\n\tint status;\n\n\t/*\n\t * Flush all pending writes before doing anything\n\t * with locks..\n\t */\n\tstatus = nfs_sync_mapping(filp->f_mapping);\n\tif (status != 0)\n\t\tgoto out;\n\n\tlock_kernel();\n\t/* Use local locking if mounted with \"-onolock\" */\n\tif (!(NFS_SERVER(inode)->flags & NFS_MOUNT_NONLM))\n\t\tstatus = NFS_PROTO(inode)->lock(filp, cmd, fl);\n\telse\n\t\tstatus = do_vfs_lock(filp, fl);\n\tunlock_kernel();\n\tif (status < 0)\n\t\tgoto out;\n\t/*\n\t * Make sure we clear the cache whenever we try to get the lock.\n\t * This makes locking act as a cache coherency point.\n\t */\n\tnfs_sync_mapping(filp->f_mapping);\n\tnfs_zap_caches(inode);\nout:\n\treturn status;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,17 +13,9 @@\n \n \tlock_kernel();\n \t/* Use local locking if mounted with \"-onolock\" */\n-\tif (!(NFS_SERVER(inode)->flags & NFS_MOUNT_NONLM)) {\n+\tif (!(NFS_SERVER(inode)->flags & NFS_MOUNT_NONLM))\n \t\tstatus = NFS_PROTO(inode)->lock(filp, cmd, fl);\n-\t\t/* If we were signalled we still need to ensure that\n-\t\t * we clean up any state on the server. We therefore\n-\t\t * record the lock call as having succeeded in order to\n-\t\t * ensure that locks_remove_posix() cleans it out when\n-\t\t * the process exits.\n-\t\t */\n-\t\tif (status == -EINTR || status == -ERESTARTSYS)\n-\t\t\tdo_vfs_lock(filp, fl);\n-\t} else\n+\telse\n \t\tstatus = do_vfs_lock(filp, fl);\n \tunlock_kernel();\n \tif (status < 0)",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!(NFS_SERVER(inode)->flags & NFS_MOUNT_NONLM)) {",
                "\t\t/* If we were signalled we still need to ensure that",
                "\t\t * we clean up any state on the server. We therefore",
                "\t\t * record the lock call as having succeeded in order to",
                "\t\t * ensure that locks_remove_posix() cleans it out when",
                "\t\t * the process exits.",
                "\t\t */",
                "\t\tif (status == -EINTR || status == -ERESTARTSYS)",
                "\t\t\tdo_vfs_lock(filp, fl);",
                "\t} else"
            ],
            "added_lines": [
                "\tif (!(NFS_SERVER(inode)->flags & NFS_MOUNT_NONLM))",
                "\telse"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-1527",
        "func_name": "torvalds/linux/ptrace_attach",
        "description": "Race condition in the ptrace_attach function in kernel/ptrace.c in the Linux kernel before 2.6.30-rc4 allows local users to gain privileges via a PTRACE_ATTACH ptrace call during an exec system call that is launching a setuid application, related to locking an incorrect cred_exec_mutex object.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=cad81bc2529ab8c62b6fdc83a1c0c7f4a87209eb",
        "commit_title": "ptrace_attach() needs task->cred_exec_mutex, not current->cred_exec_mutex.",
        "commit_text": " ",
        "func_before": "int ptrace_attach(struct task_struct *task)\n{\n\tint retval;\n\tunsigned long flags;\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/* Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently under ptrace.\n\t */\n\tretval = mutex_lock_interruptible(&current->cred_exec_mutex);\n\tif (retval  < 0)\n\t\tgoto out;\n\n\tretval = -EPERM;\nrepeat:\n\t/*\n\t * Nasty, nasty.\n\t *\n\t * We want to hold both the task-lock and the\n\t * tasklist_lock for writing at the same time.\n\t * But that's against the rules (tasklist_lock\n\t * is taken for reading by interrupts on other\n\t * cpu's that may have task_lock).\n\t */\n\ttask_lock(task);\n\tif (!write_trylock_irqsave(&tasklist_lock, flags)) {\n\t\ttask_unlock(task);\n\t\tdo {\n\t\t\tcpu_relax();\n\t\t} while (!write_can_lock(&tasklist_lock));\n\t\tgoto repeat;\n\t}\n\n\tif (!task->mm)\n\t\tgoto bad;\n\t/* the same process cannot be attached many times */\n\tif (task->ptrace & PT_PTRACED)\n\t\tgoto bad;\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\tif (retval)\n\t\tgoto bad;\n\n\t/* Go */\n\ttask->ptrace |= PT_PTRACED;\n\tif (capable(CAP_SYS_PTRACE))\n\t\ttask->ptrace |= PT_PTRACE_CAP;\n\n\t__ptrace_link(task, current);\n\n\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\nbad:\n\twrite_unlock_irqrestore(&tasklist_lock, flags);\n\ttask_unlock(task);\n\tmutex_unlock(&current->cred_exec_mutex);\nout:\n\treturn retval;\n}",
        "func": "int ptrace_attach(struct task_struct *task)\n{\n\tint retval;\n\tunsigned long flags;\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/* Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently under ptrace.\n\t */\n\tretval = mutex_lock_interruptible(&task->cred_exec_mutex);\n\tif (retval  < 0)\n\t\tgoto out;\n\n\tretval = -EPERM;\nrepeat:\n\t/*\n\t * Nasty, nasty.\n\t *\n\t * We want to hold both the task-lock and the\n\t * tasklist_lock for writing at the same time.\n\t * But that's against the rules (tasklist_lock\n\t * is taken for reading by interrupts on other\n\t * cpu's that may have task_lock).\n\t */\n\ttask_lock(task);\n\tif (!write_trylock_irqsave(&tasklist_lock, flags)) {\n\t\ttask_unlock(task);\n\t\tdo {\n\t\t\tcpu_relax();\n\t\t} while (!write_can_lock(&tasklist_lock));\n\t\tgoto repeat;\n\t}\n\n\tif (!task->mm)\n\t\tgoto bad;\n\t/* the same process cannot be attached many times */\n\tif (task->ptrace & PT_PTRACED)\n\t\tgoto bad;\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\tif (retval)\n\t\tgoto bad;\n\n\t/* Go */\n\ttask->ptrace |= PT_PTRACED;\n\tif (capable(CAP_SYS_PTRACE))\n\t\ttask->ptrace |= PT_PTRACE_CAP;\n\n\t__ptrace_link(task, current);\n\n\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\nbad:\n\twrite_unlock_irqrestore(&tasklist_lock, flags);\n\ttask_unlock(task);\n\tmutex_unlock(&task->cred_exec_mutex);\nout:\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,7 @@\n \t/* Protect exec's credential calculations against our interference;\n \t * SUID, SGID and LSM creds get determined differently under ptrace.\n \t */\n-\tretval = mutex_lock_interruptible(&current->cred_exec_mutex);\n+\tretval = mutex_lock_interruptible(&task->cred_exec_mutex);\n \tif (retval  < 0)\n \t\tgoto out;\n \n@@ -56,7 +56,7 @@\n bad:\n \twrite_unlock_irqrestore(&tasklist_lock, flags);\n \ttask_unlock(task);\n-\tmutex_unlock(&current->cred_exec_mutex);\n+\tmutex_unlock(&task->cred_exec_mutex);\n out:\n \treturn retval;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tretval = mutex_lock_interruptible(&current->cred_exec_mutex);",
                "\tmutex_unlock(&current->cred_exec_mutex);"
            ],
            "added_lines": [
                "\tretval = mutex_lock_interruptible(&task->cred_exec_mutex);",
                "\tmutex_unlock(&task->cred_exec_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-3547",
        "func_name": "torvalds/linux/pipe_rdwr_open",
        "description": "Multiple race conditions in fs/pipe.c in the Linux kernel before 2.6.32-rc6 allow local users to cause a denial of service (NULL pointer dereference and system crash) or gain privileges by attempting to open an anonymous pipe via a /proc/*/fd/ pathname.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=ad3960243e55320d74195fb85c975e0a8cc4466c",
        "commit_title": "This patch fixes a null pointer exception in pipe_rdwr_open() which",
        "commit_text": "generates the stack trace:  > Unable to handle kernel NULL pointer dereference at 0000000000000028 RIP: >  [<ffffffff802899a5>] pipe_rdwr_open+0x35/0x70 >  [<ffffffff8028125c>] __dentry_open+0x13c/0x230 >  [<ffffffff8028143d>] do_filp_open+0x2d/0x40 >  [<ffffffff802814aa>] do_sys_open+0x5a/0x100 >  [<ffffffff8021faf3>] sysenter_do_call+0x1b/0x67  The failure mode is triggered by an attempt to open an anonymous pipe via /proc/pid/fd/* as exemplified by this script:  ============================================================= while : ; do    { echo y ; sleep 1 ; } | { while read ; do echo z$REPLY; done ; } &    PID=$!    OUT=$(ps -efl | grep 'sleep 1' | grep -v grep |         { read PID REST ; echo $PID; } )    OUT=\"${OUT%% *}\"    DELAY=$((RANDOM * 1000 / 32768))    usleep $((DELAY * 1000 + RANDOM % 1000 ))    echo n > /proc/$OUT/fd/1                 # Trigger defect done =============================================================  Note that the failure window is quite small and I could only reliably reproduce the defect by inserting a small delay in pipe_rdwr_open(). For example:   static int  pipe_rdwr_open(struct inode *inode, struct file *filp)  {        msleep(100);        mutex_lock(&inode->i_mutex);  Although the defect was observed in pipe_rdwr_open(), I think it makes sense to replicate the change through all the pipe_*_open() functions.  The core of the change is to verify that inode->i_pipe has not been released before attempting to manipulate it. If inode->i_pipe is no longer present, return ENOENT to indicate so.  The comment about potentially using atomic_t for i_pipe->readers and i_pipe->writers has also been removed because it is no longer relevant in this context. The inode->i_mutex lock must be used so that inode->i_pipe can be dealt with correctly.  Cc: stable@kernel.org ",
        "func_before": "static int\npipe_rdwr_open(struct inode *inode, struct file *filp)\n{\n\tmutex_lock(&inode->i_mutex);\n\tif (filp->f_mode & FMODE_READ)\n\t\tinode->i_pipe->readers++;\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tinode->i_pipe->writers++;\n\tmutex_unlock(&inode->i_mutex);\n\n\treturn 0;\n}",
        "func": "static int\npipe_rdwr_open(struct inode *inode, struct file *filp)\n{\n\tint ret = -ENOENT;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tif (inode->i_pipe) {\n\t\tret = 0;\n\t\tif (filp->f_mode & FMODE_READ)\n\t\t\tinode->i_pipe->readers++;\n\t\tif (filp->f_mode & FMODE_WRITE)\n\t\t\tinode->i_pipe->writers++;\n\t}\n\n\tmutex_unlock(&inode->i_mutex);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,12 +1,19 @@\n static int\n pipe_rdwr_open(struct inode *inode, struct file *filp)\n {\n+\tint ret = -ENOENT;\n+\n \tmutex_lock(&inode->i_mutex);\n-\tif (filp->f_mode & FMODE_READ)\n-\t\tinode->i_pipe->readers++;\n-\tif (filp->f_mode & FMODE_WRITE)\n-\t\tinode->i_pipe->writers++;\n+\n+\tif (inode->i_pipe) {\n+\t\tret = 0;\n+\t\tif (filp->f_mode & FMODE_READ)\n+\t\t\tinode->i_pipe->readers++;\n+\t\tif (filp->f_mode & FMODE_WRITE)\n+\t\t\tinode->i_pipe->writers++;\n+\t}\n+\n \tmutex_unlock(&inode->i_mutex);\n \n-\treturn 0;\n+\treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (filp->f_mode & FMODE_READ)",
                "\t\tinode->i_pipe->readers++;",
                "\tif (filp->f_mode & FMODE_WRITE)",
                "\t\tinode->i_pipe->writers++;",
                "\treturn 0;"
            ],
            "added_lines": [
                "\tint ret = -ENOENT;",
                "",
                "",
                "\tif (inode->i_pipe) {",
                "\t\tret = 0;",
                "\t\tif (filp->f_mode & FMODE_READ)",
                "\t\t\tinode->i_pipe->readers++;",
                "\t\tif (filp->f_mode & FMODE_WRITE)",
                "\t\t\tinode->i_pipe->writers++;",
                "\t}",
                "",
                "\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-3547",
        "func_name": "torvalds/linux/pipe_write_open",
        "description": "Multiple race conditions in fs/pipe.c in the Linux kernel before 2.6.32-rc6 allow local users to cause a denial of service (NULL pointer dereference and system crash) or gain privileges by attempting to open an anonymous pipe via a /proc/*/fd/ pathname.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=ad3960243e55320d74195fb85c975e0a8cc4466c",
        "commit_title": "This patch fixes a null pointer exception in pipe_rdwr_open() which",
        "commit_text": "generates the stack trace:  > Unable to handle kernel NULL pointer dereference at 0000000000000028 RIP: >  [<ffffffff802899a5>] pipe_rdwr_open+0x35/0x70 >  [<ffffffff8028125c>] __dentry_open+0x13c/0x230 >  [<ffffffff8028143d>] do_filp_open+0x2d/0x40 >  [<ffffffff802814aa>] do_sys_open+0x5a/0x100 >  [<ffffffff8021faf3>] sysenter_do_call+0x1b/0x67  The failure mode is triggered by an attempt to open an anonymous pipe via /proc/pid/fd/* as exemplified by this script:  ============================================================= while : ; do    { echo y ; sleep 1 ; } | { while read ; do echo z$REPLY; done ; } &    PID=$!    OUT=$(ps -efl | grep 'sleep 1' | grep -v grep |         { read PID REST ; echo $PID; } )    OUT=\"${OUT%% *}\"    DELAY=$((RANDOM * 1000 / 32768))    usleep $((DELAY * 1000 + RANDOM % 1000 ))    echo n > /proc/$OUT/fd/1                 # Trigger defect done =============================================================  Note that the failure window is quite small and I could only reliably reproduce the defect by inserting a small delay in pipe_rdwr_open(). For example:   static int  pipe_rdwr_open(struct inode *inode, struct file *filp)  {        msleep(100);        mutex_lock(&inode->i_mutex);  Although the defect was observed in pipe_rdwr_open(), I think it makes sense to replicate the change through all the pipe_*_open() functions.  The core of the change is to verify that inode->i_pipe has not been released before attempting to manipulate it. If inode->i_pipe is no longer present, return ENOENT to indicate so.  The comment about potentially using atomic_t for i_pipe->readers and i_pipe->writers has also been removed because it is no longer relevant in this context. The inode->i_mutex lock must be used so that inode->i_pipe can be dealt with correctly.  Cc: stable@kernel.org ",
        "func_before": "static int\npipe_write_open(struct inode *inode, struct file *filp)\n{\n\tmutex_lock(&inode->i_mutex);\n\tinode->i_pipe->writers++;\n\tmutex_unlock(&inode->i_mutex);\n\n\treturn 0;\n}",
        "func": "static int\npipe_write_open(struct inode *inode, struct file *filp)\n{\n\tint ret = -ENOENT;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tif (inode->i_pipe) {\n\t\tret = 0;\n\t\tinode->i_pipe->writers++;\n\t}\n\n\tmutex_unlock(&inode->i_mutex);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,16 @@\n static int\n pipe_write_open(struct inode *inode, struct file *filp)\n {\n+\tint ret = -ENOENT;\n+\n \tmutex_lock(&inode->i_mutex);\n-\tinode->i_pipe->writers++;\n+\n+\tif (inode->i_pipe) {\n+\t\tret = 0;\n+\t\tinode->i_pipe->writers++;\n+\t}\n+\n \tmutex_unlock(&inode->i_mutex);\n \n-\treturn 0;\n+\treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tinode->i_pipe->writers++;",
                "\treturn 0;"
            ],
            "added_lines": [
                "\tint ret = -ENOENT;",
                "",
                "",
                "\tif (inode->i_pipe) {",
                "\t\tret = 0;",
                "\t\tinode->i_pipe->writers++;",
                "\t}",
                "",
                "\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-3547",
        "func_name": "torvalds/linux/pipe_read_open",
        "description": "Multiple race conditions in fs/pipe.c in the Linux kernel before 2.6.32-rc6 allow local users to cause a denial of service (NULL pointer dereference and system crash) or gain privileges by attempting to open an anonymous pipe via a /proc/*/fd/ pathname.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=ad3960243e55320d74195fb85c975e0a8cc4466c",
        "commit_title": "This patch fixes a null pointer exception in pipe_rdwr_open() which",
        "commit_text": "generates the stack trace:  > Unable to handle kernel NULL pointer dereference at 0000000000000028 RIP: >  [<ffffffff802899a5>] pipe_rdwr_open+0x35/0x70 >  [<ffffffff8028125c>] __dentry_open+0x13c/0x230 >  [<ffffffff8028143d>] do_filp_open+0x2d/0x40 >  [<ffffffff802814aa>] do_sys_open+0x5a/0x100 >  [<ffffffff8021faf3>] sysenter_do_call+0x1b/0x67  The failure mode is triggered by an attempt to open an anonymous pipe via /proc/pid/fd/* as exemplified by this script:  ============================================================= while : ; do    { echo y ; sleep 1 ; } | { while read ; do echo z$REPLY; done ; } &    PID=$!    OUT=$(ps -efl | grep 'sleep 1' | grep -v grep |         { read PID REST ; echo $PID; } )    OUT=\"${OUT%% *}\"    DELAY=$((RANDOM * 1000 / 32768))    usleep $((DELAY * 1000 + RANDOM % 1000 ))    echo n > /proc/$OUT/fd/1                 # Trigger defect done =============================================================  Note that the failure window is quite small and I could only reliably reproduce the defect by inserting a small delay in pipe_rdwr_open(). For example:   static int  pipe_rdwr_open(struct inode *inode, struct file *filp)  {        msleep(100);        mutex_lock(&inode->i_mutex);  Although the defect was observed in pipe_rdwr_open(), I think it makes sense to replicate the change through all the pipe_*_open() functions.  The core of the change is to verify that inode->i_pipe has not been released before attempting to manipulate it. If inode->i_pipe is no longer present, return ENOENT to indicate so.  The comment about potentially using atomic_t for i_pipe->readers and i_pipe->writers has also been removed because it is no longer relevant in this context. The inode->i_mutex lock must be used so that inode->i_pipe can be dealt with correctly.  Cc: stable@kernel.org ",
        "func_before": "static int\npipe_read_open(struct inode *inode, struct file *filp)\n{\n\t/* We could have perhaps used atomic_t, but this and friends\n\t   below are the only places.  So it doesn't seem worthwhile.  */\n\tmutex_lock(&inode->i_mutex);\n\tinode->i_pipe->readers++;\n\tmutex_unlock(&inode->i_mutex);\n\n\treturn 0;\n}",
        "func": "static int\npipe_read_open(struct inode *inode, struct file *filp)\n{\n\tint ret = -ENOENT;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tif (inode->i_pipe) {\n\t\tret = 0;\n\t\tinode->i_pipe->readers++;\n\t}\n\n\tmutex_unlock(&inode->i_mutex);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,16 @@\n static int\n pipe_read_open(struct inode *inode, struct file *filp)\n {\n-\t/* We could have perhaps used atomic_t, but this and friends\n-\t   below are the only places.  So it doesn't seem worthwhile.  */\n+\tint ret = -ENOENT;\n+\n \tmutex_lock(&inode->i_mutex);\n-\tinode->i_pipe->readers++;\n+\n+\tif (inode->i_pipe) {\n+\t\tret = 0;\n+\t\tinode->i_pipe->readers++;\n+\t}\n+\n \tmutex_unlock(&inode->i_mutex);\n \n-\treturn 0;\n+\treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* We could have perhaps used atomic_t, but this and friends",
                "\t   below are the only places.  So it doesn't seem worthwhile.  */",
                "\tinode->i_pipe->readers++;",
                "\treturn 0;"
            ],
            "added_lines": [
                "\tint ret = -ENOENT;",
                "",
                "",
                "\tif (inode->i_pipe) {",
                "\t\tret = 0;",
                "\t\tinode->i_pipe->readers++;",
                "\t}",
                "",
                "\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-4026",
        "func_name": "torvalds/linux/ieee80211_process_delba",
        "description": "The mac80211 subsystem in the Linux kernel before 2.6.32-rc8-next-20091201 allows remote attackers to cause a denial of service (panic) via a crafted Delete Block ACK (aka DELBA) packet, related to an erroneous \"code shuffling patch.\"",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=827d42c9ac91ddd728e4f4a31fefb906ef2ceff7",
        "commit_title": "Lennert Buytenhek noticed that delBA handling in mac80211",
        "commit_text": "was broken and has remotely triggerable problems, some of which are due to some code shuffling I did that ended up changing the order in which things were done -- this was    commit d75636ef9c1af224f1097941879d5a8db7cd04e5   Author: Johannes Berg <johannes@sipsolutions.net>   Date:   Tue Feb 10 21:25:53 2009 +0100      mac80211: RX aggregation: clean up stop session  and other parts were already present in the original    commit d92684e66091c0f0101819619b315b4bb8b5bcc5   Author: Ron Rindjunsky <ron.rindjunsky@intel.com>   Date:   Mon Jan 28 14:07:22 2008 +0200        mac80211: A-MPDU Tx add delBA from recipient support  The first problem is that I moved a BUG_ON before various checks -- thereby making it possible to hit. As the comment indicates, the BUG_ON can be removed since the ampdu_action callback must already exist when the state is != IDLE.  The second problem isn't easily exploitable but there's a race condition due to unconditionally setting the state to OPERATIONAL when a delBA frame is received, even when no aggregation session was ever initiated. All the drivers accept stopping the session even then, but that opens a race window where crashes could happen before the driver accepts it. Right now, a WARN_ON may happen with non-HT drivers, while the race opens only for HT drivers.  For this case, there are two things necessary to fix it:  1) don't process spurious delBA frames, and be more careful     about the session state; don't drop the lock   2) HT drivers need to be prepared to handle a session stop     even before the session was really started -- this is     true for all drivers (that support aggregation) but     iwlwifi which can be fixed easily. The other HT drivers     (ath9k and ar9170) are behaving properly already.  Cc: stable@kernel.org ",
        "func_before": "void ieee80211_process_delba(struct ieee80211_sub_if_data *sdata,\n\t\t\t     struct sta_info *sta,\n\t\t\t     struct ieee80211_mgmt *mgmt, size_t len)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tu16 tid, params;\n\tu16 initiator;\n\n\tparams = le16_to_cpu(mgmt->u.action.u.delba.params);\n\ttid = (params & IEEE80211_DELBA_PARAM_TID_MASK) >> 12;\n\tinitiator = (params & IEEE80211_DELBA_PARAM_INITIATOR_MASK) >> 11;\n\n#ifdef CONFIG_MAC80211_HT_DEBUG\n\tif (net_ratelimit())\n\t\tprintk(KERN_DEBUG \"delba from %pM (%s) tid %d reason code %d\\n\",\n\t\t\tmgmt->sa, initiator ? \"initiator\" : \"recipient\", tid,\n\t\t\tle16_to_cpu(mgmt->u.action.u.delba.reason_code));\n#endif /* CONFIG_MAC80211_HT_DEBUG */\n\n\tif (initiator == WLAN_BACK_INITIATOR)\n\t\tieee80211_sta_stop_rx_ba_session(sdata, sta->sta.addr, tid,\n\t\t\t\t\t\t WLAN_BACK_INITIATOR, 0);\n\telse { /* WLAN_BACK_RECIPIENT */\n\t\tspin_lock_bh(&sta->lock);\n\t\tsta->ampdu_mlme.tid_state_tx[tid] =\n\t\t\t\tHT_AGG_STATE_OPERATIONAL;\n\t\tspin_unlock_bh(&sta->lock);\n\t\tieee80211_stop_tx_ba_session(&local->hw, sta->sta.addr, tid,\n\t\t\t\t\t     WLAN_BACK_RECIPIENT);\n\t}\n}",
        "func": "void ieee80211_process_delba(struct ieee80211_sub_if_data *sdata,\n\t\t\t     struct sta_info *sta,\n\t\t\t     struct ieee80211_mgmt *mgmt, size_t len)\n{\n\tu16 tid, params;\n\tu16 initiator;\n\n\tparams = le16_to_cpu(mgmt->u.action.u.delba.params);\n\ttid = (params & IEEE80211_DELBA_PARAM_TID_MASK) >> 12;\n\tinitiator = (params & IEEE80211_DELBA_PARAM_INITIATOR_MASK) >> 11;\n\n#ifdef CONFIG_MAC80211_HT_DEBUG\n\tif (net_ratelimit())\n\t\tprintk(KERN_DEBUG \"delba from %pM (%s) tid %d reason code %d\\n\",\n\t\t\tmgmt->sa, initiator ? \"initiator\" : \"recipient\", tid,\n\t\t\tle16_to_cpu(mgmt->u.action.u.delba.reason_code));\n#endif /* CONFIG_MAC80211_HT_DEBUG */\n\n\tif (initiator == WLAN_BACK_INITIATOR)\n\t\tieee80211_sta_stop_rx_ba_session(sdata, sta->sta.addr, tid,\n\t\t\t\t\t\t WLAN_BACK_INITIATOR, 0);\n\telse { /* WLAN_BACK_RECIPIENT */\n\t\tspin_lock_bh(&sta->lock);\n\t\tif (sta->ampdu_mlme.tid_state_tx[tid] & HT_ADDBA_REQUESTED_MSK)\n\t\t\t___ieee80211_stop_tx_ba_session(sta, tid,\n\t\t\t\t\t\t\tWLAN_BACK_RECIPIENT);\n\t\tspin_unlock_bh(&sta->lock);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,6 @@\n \t\t\t     struct sta_info *sta,\n \t\t\t     struct ieee80211_mgmt *mgmt, size_t len)\n {\n-\tstruct ieee80211_local *local = sdata->local;\n \tu16 tid, params;\n \tu16 initiator;\n \n@@ -22,10 +21,9 @@\n \t\t\t\t\t\t WLAN_BACK_INITIATOR, 0);\n \telse { /* WLAN_BACK_RECIPIENT */\n \t\tspin_lock_bh(&sta->lock);\n-\t\tsta->ampdu_mlme.tid_state_tx[tid] =\n-\t\t\t\tHT_AGG_STATE_OPERATIONAL;\n+\t\tif (sta->ampdu_mlme.tid_state_tx[tid] & HT_ADDBA_REQUESTED_MSK)\n+\t\t\t___ieee80211_stop_tx_ba_session(sta, tid,\n+\t\t\t\t\t\t\tWLAN_BACK_RECIPIENT);\n \t\tspin_unlock_bh(&sta->lock);\n-\t\tieee80211_stop_tx_ba_session(&local->hw, sta->sta.addr, tid,\n-\t\t\t\t\t     WLAN_BACK_RECIPIENT);\n \t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct ieee80211_local *local = sdata->local;",
                "\t\tsta->ampdu_mlme.tid_state_tx[tid] =",
                "\t\t\t\tHT_AGG_STATE_OPERATIONAL;",
                "\t\tieee80211_stop_tx_ba_session(&local->hw, sta->sta.addr, tid,",
                "\t\t\t\t\t     WLAN_BACK_RECIPIENT);"
            ],
            "added_lines": [
                "\t\tif (sta->ampdu_mlme.tid_state_tx[tid] & HT_ADDBA_REQUESTED_MSK)",
                "\t\t\t___ieee80211_stop_tx_ba_session(sta, tid,",
                "\t\t\t\t\t\t\tWLAN_BACK_RECIPIENT);"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-4026",
        "func_name": "torvalds/linux/__ieee80211_stop_tx_ba_session",
        "description": "The mac80211 subsystem in the Linux kernel before 2.6.32-rc8-next-20091201 allows remote attackers to cause a denial of service (panic) via a crafted Delete Block ACK (aka DELBA) packet, related to an erroneous \"code shuffling patch.\"",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=827d42c9ac91ddd728e4f4a31fefb906ef2ceff7",
        "commit_title": "Lennert Buytenhek noticed that delBA handling in mac80211",
        "commit_text": "was broken and has remotely triggerable problems, some of which are due to some code shuffling I did that ended up changing the order in which things were done -- this was    commit d75636ef9c1af224f1097941879d5a8db7cd04e5   Author: Johannes Berg <johannes@sipsolutions.net>   Date:   Tue Feb 10 21:25:53 2009 +0100      mac80211: RX aggregation: clean up stop session  and other parts were already present in the original    commit d92684e66091c0f0101819619b315b4bb8b5bcc5   Author: Ron Rindjunsky <ron.rindjunsky@intel.com>   Date:   Mon Jan 28 14:07:22 2008 +0200        mac80211: A-MPDU Tx add delBA from recipient support  The first problem is that I moved a BUG_ON before various checks -- thereby making it possible to hit. As the comment indicates, the BUG_ON can be removed since the ampdu_action callback must already exist when the state is != IDLE.  The second problem isn't easily exploitable but there's a race condition due to unconditionally setting the state to OPERATIONAL when a delBA frame is received, even when no aggregation session was ever initiated. All the drivers accept stopping the session even then, but that opens a race window where crashes could happen before the driver accepts it. Right now, a WARN_ON may happen with non-HT drivers, while the race opens only for HT drivers.  For this case, there are two things necessary to fix it:  1) don't process spurious delBA frames, and be more careful     about the session state; don't drop the lock   2) HT drivers need to be prepared to handle a session stop     even before the session was really started -- this is     true for all drivers (that support aggregation) but     iwlwifi which can be fixed easily. The other HT drivers     (ath9k and ar9170) are behaving properly already.  Cc: stable@kernel.org ",
        "func_before": "int __ieee80211_stop_tx_ba_session(struct sta_info *sta, u16 tid,\n\t\t\t\t   enum ieee80211_back_parties initiator)\n{\n\tu8 *state;\n\tint ret;\n\n\t/* check if the TID is in aggregation */\n\tstate = &sta->ampdu_mlme.tid_state_tx[tid];\n\tspin_lock_bh(&sta->lock);\n\n\tif (*state != HT_AGG_STATE_OPERATIONAL) {\n\t\tret = -ENOENT;\n\t\tgoto unlock;\n\t}\n\n#ifdef CONFIG_MAC80211_HT_DEBUG\n\tprintk(KERN_DEBUG \"Tx BA session stop requested for %pM tid %u\\n\",\n\t       sta->sta.addr, tid);\n#endif /* CONFIG_MAC80211_HT_DEBUG */\n\n\tret = ___ieee80211_stop_tx_ba_session(sta, tid, initiator);\n\n unlock:\n\tspin_unlock_bh(&sta->lock);\n\treturn ret;\n}",
        "func": "int __ieee80211_stop_tx_ba_session(struct sta_info *sta, u16 tid,\n\t\t\t\t   enum ieee80211_back_parties initiator)\n{\n\tu8 *state;\n\tint ret;\n\n\t/* check if the TID is in aggregation */\n\tstate = &sta->ampdu_mlme.tid_state_tx[tid];\n\tspin_lock_bh(&sta->lock);\n\n\tif (*state != HT_AGG_STATE_OPERATIONAL) {\n\t\tret = -ENOENT;\n\t\tgoto unlock;\n\t}\n\n\tret = ___ieee80211_stop_tx_ba_session(sta, tid, initiator);\n\n unlock:\n\tspin_unlock_bh(&sta->lock);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,11 +13,6 @@\n \t\tgoto unlock;\n \t}\n \n-#ifdef CONFIG_MAC80211_HT_DEBUG\n-\tprintk(KERN_DEBUG \"Tx BA session stop requested for %pM tid %u\\n\",\n-\t       sta->sta.addr, tid);\n-#endif /* CONFIG_MAC80211_HT_DEBUG */\n-\n \tret = ___ieee80211_stop_tx_ba_session(sta, tid, initiator);\n \n  unlock:",
        "diff_line_info": {
            "deleted_lines": [
                "#ifdef CONFIG_MAC80211_HT_DEBUG",
                "\tprintk(KERN_DEBUG \"Tx BA session stop requested for %pM tid %u\\n\",",
                "\t       sta->sta.addr, tid);",
                "#endif /* CONFIG_MAC80211_HT_DEBUG */",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2009-4026",
        "func_name": "torvalds/linux/___ieee80211_stop_tx_ba_session",
        "description": "The mac80211 subsystem in the Linux kernel before 2.6.32-rc8-next-20091201 allows remote attackers to cause a denial of service (panic) via a crafted Delete Block ACK (aka DELBA) packet, related to an erroneous \"code shuffling patch.\"",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=827d42c9ac91ddd728e4f4a31fefb906ef2ceff7",
        "commit_title": "Lennert Buytenhek noticed that delBA handling in mac80211",
        "commit_text": "was broken and has remotely triggerable problems, some of which are due to some code shuffling I did that ended up changing the order in which things were done -- this was    commit d75636ef9c1af224f1097941879d5a8db7cd04e5   Author: Johannes Berg <johannes@sipsolutions.net>   Date:   Tue Feb 10 21:25:53 2009 +0100      mac80211: RX aggregation: clean up stop session  and other parts were already present in the original    commit d92684e66091c0f0101819619b315b4bb8b5bcc5   Author: Ron Rindjunsky <ron.rindjunsky@intel.com>   Date:   Mon Jan 28 14:07:22 2008 +0200        mac80211: A-MPDU Tx add delBA from recipient support  The first problem is that I moved a BUG_ON before various checks -- thereby making it possible to hit. As the comment indicates, the BUG_ON can be removed since the ampdu_action callback must already exist when the state is != IDLE.  The second problem isn't easily exploitable but there's a race condition due to unconditionally setting the state to OPERATIONAL when a delBA frame is received, even when no aggregation session was ever initiated. All the drivers accept stopping the session even then, but that opens a race window where crashes could happen before the driver accepts it. Right now, a WARN_ON may happen with non-HT drivers, while the race opens only for HT drivers.  For this case, there are two things necessary to fix it:  1) don't process spurious delBA frames, and be more careful     about the session state; don't drop the lock   2) HT drivers need to be prepared to handle a session stop     even before the session was really started -- this is     true for all drivers (that support aggregation) but     iwlwifi which can be fixed easily. The other HT drivers     (ath9k and ar9170) are behaving properly already.  Cc: stable@kernel.org ",
        "func_before": "static int ___ieee80211_stop_tx_ba_session(struct sta_info *sta, u16 tid,\n\t\t\t\t\t   enum ieee80211_back_parties initiator)\n{\n\tstruct ieee80211_local *local = sta->local;\n\tint ret;\n\tu8 *state;\n\n\tstate = &sta->ampdu_mlme.tid_state_tx[tid];\n\n\tif (*state == HT_AGG_STATE_OPERATIONAL)\n\t\tsta->ampdu_mlme.addba_req_num[tid] = 0;\n\n\t*state = HT_AGG_STATE_REQ_STOP_BA_MSK |\n\t\t(initiator << HT_AGG_STATE_INITIATOR_SHIFT);\n\n\tret = drv_ampdu_action(local, IEEE80211_AMPDU_TX_STOP,\n\t\t\t       &sta->sta, tid, NULL);\n\n\t/* HW shall not deny going back to legacy */\n\tif (WARN_ON(ret)) {\n\t\t*state = HT_AGG_STATE_OPERATIONAL;\n\t\t/*\n\t\t * We may have pending packets get stuck in this case...\n\t\t * Not bothering with a workaround for now.\n\t\t */\n\t}\n\n\treturn ret;\n}",
        "func": "int ___ieee80211_stop_tx_ba_session(struct sta_info *sta, u16 tid,\n\t\t\t\t    enum ieee80211_back_parties initiator)\n{\n\tstruct ieee80211_local *local = sta->local;\n\tint ret;\n\tu8 *state;\n\n#ifdef CONFIG_MAC80211_HT_DEBUG\n\tprintk(KERN_DEBUG \"Tx BA session stop requested for %pM tid %u\\n\",\n\t       sta->sta.addr, tid);\n#endif /* CONFIG_MAC80211_HT_DEBUG */\n\n\tstate = &sta->ampdu_mlme.tid_state_tx[tid];\n\n\tif (*state == HT_AGG_STATE_OPERATIONAL)\n\t\tsta->ampdu_mlme.addba_req_num[tid] = 0;\n\n\t*state = HT_AGG_STATE_REQ_STOP_BA_MSK |\n\t\t(initiator << HT_AGG_STATE_INITIATOR_SHIFT);\n\n\tret = drv_ampdu_action(local, IEEE80211_AMPDU_TX_STOP,\n\t\t\t       &sta->sta, tid, NULL);\n\n\t/* HW shall not deny going back to legacy */\n\tif (WARN_ON(ret)) {\n\t\t/*\n\t\t * We may have pending packets get stuck in this case...\n\t\t * Not bothering with a workaround for now.\n\t\t */\n\t}\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,14 @@\n-static int ___ieee80211_stop_tx_ba_session(struct sta_info *sta, u16 tid,\n-\t\t\t\t\t   enum ieee80211_back_parties initiator)\n+int ___ieee80211_stop_tx_ba_session(struct sta_info *sta, u16 tid,\n+\t\t\t\t    enum ieee80211_back_parties initiator)\n {\n \tstruct ieee80211_local *local = sta->local;\n \tint ret;\n \tu8 *state;\n+\n+#ifdef CONFIG_MAC80211_HT_DEBUG\n+\tprintk(KERN_DEBUG \"Tx BA session stop requested for %pM tid %u\\n\",\n+\t       sta->sta.addr, tid);\n+#endif /* CONFIG_MAC80211_HT_DEBUG */\n \n \tstate = &sta->ampdu_mlme.tid_state_tx[tid];\n \n@@ -18,7 +23,6 @@\n \n \t/* HW shall not deny going back to legacy */\n \tif (WARN_ON(ret)) {\n-\t\t*state = HT_AGG_STATE_OPERATIONAL;\n \t\t/*\n \t\t * We may have pending packets get stuck in this case...\n \t\t * Not bothering with a workaround for now.",
        "diff_line_info": {
            "deleted_lines": [
                "static int ___ieee80211_stop_tx_ba_session(struct sta_info *sta, u16 tid,",
                "\t\t\t\t\t   enum ieee80211_back_parties initiator)",
                "\t\t*state = HT_AGG_STATE_OPERATIONAL;"
            ],
            "added_lines": [
                "int ___ieee80211_stop_tx_ba_session(struct sta_info *sta, u16 tid,",
                "\t\t\t\t    enum ieee80211_back_parties initiator)",
                "",
                "#ifdef CONFIG_MAC80211_HT_DEBUG",
                "\tprintk(KERN_DEBUG \"Tx BA session stop requested for %pM tid %u\\n\",",
                "\t       sta->sta.addr, tid);",
                "#endif /* CONFIG_MAC80211_HT_DEBUG */"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-4026",
        "func_name": "torvalds/linux/iwl_tx_agg_stop",
        "description": "The mac80211 subsystem in the Linux kernel before 2.6.32-rc8-next-20091201 allows remote attackers to cause a denial of service (panic) via a crafted Delete Block ACK (aka DELBA) packet, related to an erroneous \"code shuffling patch.\"",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=827d42c9ac91ddd728e4f4a31fefb906ef2ceff7",
        "commit_title": "Lennert Buytenhek noticed that delBA handling in mac80211",
        "commit_text": "was broken and has remotely triggerable problems, some of which are due to some code shuffling I did that ended up changing the order in which things were done -- this was    commit d75636ef9c1af224f1097941879d5a8db7cd04e5   Author: Johannes Berg <johannes@sipsolutions.net>   Date:   Tue Feb 10 21:25:53 2009 +0100      mac80211: RX aggregation: clean up stop session  and other parts were already present in the original    commit d92684e66091c0f0101819619b315b4bb8b5bcc5   Author: Ron Rindjunsky <ron.rindjunsky@intel.com>   Date:   Mon Jan 28 14:07:22 2008 +0200        mac80211: A-MPDU Tx add delBA from recipient support  The first problem is that I moved a BUG_ON before various checks -- thereby making it possible to hit. As the comment indicates, the BUG_ON can be removed since the ampdu_action callback must already exist when the state is != IDLE.  The second problem isn't easily exploitable but there's a race condition due to unconditionally setting the state to OPERATIONAL when a delBA frame is received, even when no aggregation session was ever initiated. All the drivers accept stopping the session even then, but that opens a race window where crashes could happen before the driver accepts it. Right now, a WARN_ON may happen with non-HT drivers, while the race opens only for HT drivers.  For this case, there are two things necessary to fix it:  1) don't process spurious delBA frames, and be more careful     about the session state; don't drop the lock   2) HT drivers need to be prepared to handle a session stop     even before the session was really started -- this is     true for all drivers (that support aggregation) but     iwlwifi which can be fixed easily. The other HT drivers     (ath9k and ar9170) are behaving properly already.  Cc: stable@kernel.org ",
        "func_before": "int iwl_tx_agg_stop(struct iwl_priv *priv , const u8 *ra, u16 tid)\n{\n\tint tx_fifo_id, txq_id, sta_id, ssn = -1;\n\tstruct iwl_tid_data *tid_data;\n\tint ret, write_ptr, read_ptr;\n\tunsigned long flags;\n\n\tif (!ra) {\n\t\tIWL_ERR(priv, \"ra = NULL\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(tid >= MAX_TID_COUNT))\n\t\treturn -EINVAL;\n\n\tif (likely(tid < ARRAY_SIZE(default_tid_to_tx_fifo)))\n\t\ttx_fifo_id = default_tid_to_tx_fifo[tid];\n\telse\n\t\treturn -EINVAL;\n\n\tsta_id = iwl_find_station(priv, ra);\n\n\tif (sta_id == IWL_INVALID_STATION) {\n\t\tIWL_ERR(priv, \"Invalid station for AGG tid %d\\n\", tid);\n\t\treturn -ENXIO;\n\t}\n\n\tif (priv->stations[sta_id].tid[tid].agg.state != IWL_AGG_ON)\n\t\tIWL_WARN(priv, \"Stopping AGG while state not IWL_AGG_ON\\n\");\n\n\ttid_data = &priv->stations[sta_id].tid[tid];\n\tssn = (tid_data->seq_number & IEEE80211_SCTL_SEQ) >> 4;\n\ttxq_id = tid_data->agg.txq_id;\n\twrite_ptr = priv->txq[txq_id].q.write_ptr;\n\tread_ptr = priv->txq[txq_id].q.read_ptr;\n\n\t/* The queue is not empty */\n\tif (write_ptr != read_ptr) {\n\t\tIWL_DEBUG_HT(priv, \"Stopping a non empty AGG HW QUEUE\\n\");\n\t\tpriv->stations[sta_id].tid[tid].agg.state =\n\t\t\t\tIWL_EMPTYING_HW_QUEUE_DELBA;\n\t\treturn 0;\n\t}\n\n\tIWL_DEBUG_HT(priv, \"HW queue is empty\\n\");\n\tpriv->stations[sta_id].tid[tid].agg.state = IWL_AGG_OFF;\n\n\tspin_lock_irqsave(&priv->lock, flags);\n\tret = priv->cfg->ops->lib->txq_agg_disable(priv, txq_id, ssn,\n\t\t\t\t\t\t   tx_fifo_id);\n\tspin_unlock_irqrestore(&priv->lock, flags);\n\n\tif (ret)\n\t\treturn ret;\n\n\tieee80211_stop_tx_ba_cb_irqsafe(priv->hw, ra, tid);\n\n\treturn 0;\n}",
        "func": "int iwl_tx_agg_stop(struct iwl_priv *priv , const u8 *ra, u16 tid)\n{\n\tint tx_fifo_id, txq_id, sta_id, ssn = -1;\n\tstruct iwl_tid_data *tid_data;\n\tint ret, write_ptr, read_ptr;\n\tunsigned long flags;\n\n\tif (!ra) {\n\t\tIWL_ERR(priv, \"ra = NULL\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(tid >= MAX_TID_COUNT))\n\t\treturn -EINVAL;\n\n\tif (likely(tid < ARRAY_SIZE(default_tid_to_tx_fifo)))\n\t\ttx_fifo_id = default_tid_to_tx_fifo[tid];\n\telse\n\t\treturn -EINVAL;\n\n\tsta_id = iwl_find_station(priv, ra);\n\n\tif (sta_id == IWL_INVALID_STATION) {\n\t\tIWL_ERR(priv, \"Invalid station for AGG tid %d\\n\", tid);\n\t\treturn -ENXIO;\n\t}\n\n\tif (priv->stations[sta_id].tid[tid].agg.state ==\n\t\t\t\tIWL_EMPTYING_HW_QUEUE_ADDBA) {\n\t\tIWL_DEBUG_HT(priv, \"AGG stop before setup done\\n\");\n\t\tieee80211_stop_tx_ba_cb_irqsafe(priv->hw, ra, tid);\n\t\tpriv->stations[sta_id].tid[tid].agg.state = IWL_AGG_OFF;\n\t\treturn 0;\n\t}\n\n\tif (priv->stations[sta_id].tid[tid].agg.state != IWL_AGG_ON)\n\t\tIWL_WARN(priv, \"Stopping AGG while state not ON or starting\\n\");\n\n\ttid_data = &priv->stations[sta_id].tid[tid];\n\tssn = (tid_data->seq_number & IEEE80211_SCTL_SEQ) >> 4;\n\ttxq_id = tid_data->agg.txq_id;\n\twrite_ptr = priv->txq[txq_id].q.write_ptr;\n\tread_ptr = priv->txq[txq_id].q.read_ptr;\n\n\t/* The queue is not empty */\n\tif (write_ptr != read_ptr) {\n\t\tIWL_DEBUG_HT(priv, \"Stopping a non empty AGG HW QUEUE\\n\");\n\t\tpriv->stations[sta_id].tid[tid].agg.state =\n\t\t\t\tIWL_EMPTYING_HW_QUEUE_DELBA;\n\t\treturn 0;\n\t}\n\n\tIWL_DEBUG_HT(priv, \"HW queue is empty\\n\");\n\tpriv->stations[sta_id].tid[tid].agg.state = IWL_AGG_OFF;\n\n\tspin_lock_irqsave(&priv->lock, flags);\n\tret = priv->cfg->ops->lib->txq_agg_disable(priv, txq_id, ssn,\n\t\t\t\t\t\t   tx_fifo_id);\n\tspin_unlock_irqrestore(&priv->lock, flags);\n\n\tif (ret)\n\t\treturn ret;\n\n\tieee80211_stop_tx_ba_cb_irqsafe(priv->hw, ra, tid);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,8 +25,16 @@\n \t\treturn -ENXIO;\n \t}\n \n+\tif (priv->stations[sta_id].tid[tid].agg.state ==\n+\t\t\t\tIWL_EMPTYING_HW_QUEUE_ADDBA) {\n+\t\tIWL_DEBUG_HT(priv, \"AGG stop before setup done\\n\");\n+\t\tieee80211_stop_tx_ba_cb_irqsafe(priv->hw, ra, tid);\n+\t\tpriv->stations[sta_id].tid[tid].agg.state = IWL_AGG_OFF;\n+\t\treturn 0;\n+\t}\n+\n \tif (priv->stations[sta_id].tid[tid].agg.state != IWL_AGG_ON)\n-\t\tIWL_WARN(priv, \"Stopping AGG while state not IWL_AGG_ON\\n\");\n+\t\tIWL_WARN(priv, \"Stopping AGG while state not ON or starting\\n\");\n \n \ttid_data = &priv->stations[sta_id].tid[tid];\n \tssn = (tid_data->seq_number & IEEE80211_SCTL_SEQ) >> 4;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tIWL_WARN(priv, \"Stopping AGG while state not IWL_AGG_ON\\n\");"
            ],
            "added_lines": [
                "\tif (priv->stations[sta_id].tid[tid].agg.state ==",
                "\t\t\t\tIWL_EMPTYING_HW_QUEUE_ADDBA) {",
                "\t\tIWL_DEBUG_HT(priv, \"AGG stop before setup done\\n\");",
                "\t\tieee80211_stop_tx_ba_cb_irqsafe(priv->hw, ra, tid);",
                "\t\tpriv->stations[sta_id].tid[tid].agg.state = IWL_AGG_OFF;",
                "\t\treturn 0;",
                "\t}",
                "",
                "\t\tIWL_WARN(priv, \"Stopping AGG while state not ON or starting\\n\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-40077",
        "func_name": "android/MetaDataBase::toString",
        "description": "In multiple functions of MetaDataBase.cpp, there is a possible UAF write due to a race condition. This could lead to remote escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.",
        "git_url": "https://android.googlesource.com/platform/frameworks/av/+/58fd993a89a3a22fa5a4a1a4548125c6783ec80c",
        "commit_title": "httplive: fix use-after-free",
        "commit_text": " Implement a mutex to ensure secure multi-threaded access to the KeyedVector in MetaDataBase. Concurrent access by different threads can lead to accessing the wrong memory location due to potential changes in the vector  Bug: 298057702 Test: HTTP Live Streaming test (cherry picked from https://partner-android-review.googlesource.com/q/commit:a2dfb31957a9d5358d0219a0eda7dcb5b0fff5fe) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:90fb4ca425444429ada6ce0de1c13d35829bc196) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:3c1d9613ef64e01d2e81c4aa44c90dcd8ca958b9) Merged-In: I46b05c85d9c39f4ce549efc160c08a0646c9fd0a ",
        "func_before": "String8 MetaDataBase::toString() const {\n    String8 s;\n    for (int i = mInternalData->mItems.size(); --i >= 0;) {\n        int32_t key = mInternalData->mItems.keyAt(i);\n        char cc[5];\n        MakeFourCCString(key, cc);\n        const typed_data &item = mInternalData->mItems.valueAt(i);\n        s.appendFormat(\"%s: %s\", cc, item.asString(false).string());\n        if (i != 0) {\n            s.append(\", \");\n        }\n    }\n    return s;\n}",
        "func": "String8 MetaDataBase::toString() const {\n    String8 s;\n    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n    for (int i = mInternalData->mItems.size(); --i >= 0;) {\n        int32_t key = mInternalData->mItems.keyAt(i);\n        char cc[5];\n        MakeFourCCString(key, cc);\n        const typed_data &item = mInternalData->mItems.valueAt(i);\n        s.appendFormat(\"%s: %s\", cc, item.asString(false).string());\n        if (i != 0) {\n            s.append(\", \");\n        }\n    }\n    return s;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n String8 MetaDataBase::toString() const {\n     String8 s;\n+    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n     for (int i = mInternalData->mItems.size(); --i >= 0;) {\n         int32_t key = mInternalData->mItems.keyAt(i);\n         char cc[5];",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    std::lock_guard<std::mutex> guard(mInternalData->mLock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-40077",
        "func_name": "android/MetaDataBase::writeToParcel",
        "description": "In multiple functions of MetaDataBase.cpp, there is a possible UAF write due to a race condition. This could lead to remote escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.",
        "git_url": "https://android.googlesource.com/platform/frameworks/av/+/58fd993a89a3a22fa5a4a1a4548125c6783ec80c",
        "commit_title": "httplive: fix use-after-free",
        "commit_text": " Implement a mutex to ensure secure multi-threaded access to the KeyedVector in MetaDataBase. Concurrent access by different threads can lead to accessing the wrong memory location due to potential changes in the vector  Bug: 298057702 Test: HTTP Live Streaming test (cherry picked from https://partner-android-review.googlesource.com/q/commit:a2dfb31957a9d5358d0219a0eda7dcb5b0fff5fe) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:90fb4ca425444429ada6ce0de1c13d35829bc196) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:3c1d9613ef64e01d2e81c4aa44c90dcd8ca958b9) Merged-In: I46b05c85d9c39f4ce549efc160c08a0646c9fd0a ",
        "func_before": "status_t MetaDataBase::writeToParcel(Parcel &parcel) {\n    status_t ret;\n    size_t numItems = mInternalData->mItems.size();\n    ret = parcel.writeUint32(uint32_t(numItems));\n    if (ret) {\n        return ret;\n    }\n    for (size_t i = 0; i < numItems; i++) {\n        int32_t key = mInternalData->mItems.keyAt(i);\n        const typed_data &item = mInternalData->mItems.valueAt(i);\n        uint32_t type;\n        const void *data;\n        size_t size;\n        item.getData(&type, &data, &size);\n        ret = parcel.writeInt32(key);\n        if (ret) {\n            return ret;\n        }\n        ret = parcel.writeUint32(type);\n        if (ret) {\n            return ret;\n        }\n        if (type == TYPE_NONE) {\n            android::Parcel::WritableBlob blob;\n            ret = parcel.writeUint32(static_cast<uint32_t>(size));\n            if (ret) {\n                return ret;\n            }\n            ret = parcel.writeBlob(size, false, &blob);\n            if (ret) {\n                return ret;\n            }\n            memcpy(blob.data(), data, size);\n            blob.release();\n        } else {\n            ret = parcel.writeByteArray(size, (uint8_t*)data);\n            if (ret) {\n                return ret;\n            }\n        }\n    }\n    return OK;\n}",
        "func": "status_t MetaDataBase::writeToParcel(Parcel &parcel) {\n    status_t ret;\n    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n    size_t numItems = mInternalData->mItems.size();\n    ret = parcel.writeUint32(uint32_t(numItems));\n    if (ret) {\n        return ret;\n    }\n    for (size_t i = 0; i < numItems; i++) {\n        int32_t key = mInternalData->mItems.keyAt(i);\n        const typed_data &item = mInternalData->mItems.valueAt(i);\n        uint32_t type;\n        const void *data;\n        size_t size;\n        item.getData(&type, &data, &size);\n        ret = parcel.writeInt32(key);\n        if (ret) {\n            return ret;\n        }\n        ret = parcel.writeUint32(type);\n        if (ret) {\n            return ret;\n        }\n        if (type == TYPE_NONE) {\n            android::Parcel::WritableBlob blob;\n            ret = parcel.writeUint32(static_cast<uint32_t>(size));\n            if (ret) {\n                return ret;\n            }\n            ret = parcel.writeBlob(size, false, &blob);\n            if (ret) {\n                return ret;\n            }\n            memcpy(blob.data(), data, size);\n            blob.release();\n        } else {\n            ret = parcel.writeByteArray(size, (uint8_t*)data);\n            if (ret) {\n                return ret;\n            }\n        }\n    }\n    return OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n status_t MetaDataBase::writeToParcel(Parcel &parcel) {\n     status_t ret;\n+    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n     size_t numItems = mInternalData->mItems.size();\n     ret = parcel.writeUint32(uint32_t(numItems));\n     if (ret) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    std::lock_guard<std::mutex> guard(mInternalData->mLock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-40077",
        "func_name": "android/MetaDataBase::setData",
        "description": "In multiple functions of MetaDataBase.cpp, there is a possible UAF write due to a race condition. This could lead to remote escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.",
        "git_url": "https://android.googlesource.com/platform/frameworks/av/+/58fd993a89a3a22fa5a4a1a4548125c6783ec80c",
        "commit_title": "httplive: fix use-after-free",
        "commit_text": " Implement a mutex to ensure secure multi-threaded access to the KeyedVector in MetaDataBase. Concurrent access by different threads can lead to accessing the wrong memory location due to potential changes in the vector  Bug: 298057702 Test: HTTP Live Streaming test (cherry picked from https://partner-android-review.googlesource.com/q/commit:a2dfb31957a9d5358d0219a0eda7dcb5b0fff5fe) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:90fb4ca425444429ada6ce0de1c13d35829bc196) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:3c1d9613ef64e01d2e81c4aa44c90dcd8ca958b9) Merged-In: I46b05c85d9c39f4ce549efc160c08a0646c9fd0a ",
        "func_before": "bool MetaDataBase::setData(\n        uint32_t key, uint32_t type, const void *data, size_t size) {\n    bool overwrote_existing = true;\n\n    ssize_t i = mInternalData->mItems.indexOfKey(key);\n    if (i < 0) {\n        typed_data item;\n        i = mInternalData->mItems.add(key, item);\n\n        overwrote_existing = false;\n    }\n\n    typed_data &item = mInternalData->mItems.editValueAt(i);\n\n    item.setData(type, data, size);\n\n    return overwrote_existing;\n}",
        "func": "bool MetaDataBase::setData(\n        uint32_t key, uint32_t type, const void *data, size_t size) {\n    bool overwrote_existing = true;\n\n    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n    ssize_t i = mInternalData->mItems.indexOfKey(key);\n    if (i < 0) {\n        typed_data item;\n        i = mInternalData->mItems.add(key, item);\n\n        overwrote_existing = false;\n    }\n\n    typed_data &item = mInternalData->mItems.editValueAt(i);\n\n    item.setData(type, data, size);\n\n    return overwrote_existing;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n         uint32_t key, uint32_t type, const void *data, size_t size) {\n     bool overwrote_existing = true;\n \n+    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n     ssize_t i = mInternalData->mItems.indexOfKey(key);\n     if (i < 0) {\n         typed_data item;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    std::lock_guard<std::mutex> guard(mInternalData->mLock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-40077",
        "func_name": "android/MetaDataBase::hasData",
        "description": "In multiple functions of MetaDataBase.cpp, there is a possible UAF write due to a race condition. This could lead to remote escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.",
        "git_url": "https://android.googlesource.com/platform/frameworks/av/+/58fd993a89a3a22fa5a4a1a4548125c6783ec80c",
        "commit_title": "httplive: fix use-after-free",
        "commit_text": " Implement a mutex to ensure secure multi-threaded access to the KeyedVector in MetaDataBase. Concurrent access by different threads can lead to accessing the wrong memory location due to potential changes in the vector  Bug: 298057702 Test: HTTP Live Streaming test (cherry picked from https://partner-android-review.googlesource.com/q/commit:a2dfb31957a9d5358d0219a0eda7dcb5b0fff5fe) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:90fb4ca425444429ada6ce0de1c13d35829bc196) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:3c1d9613ef64e01d2e81c4aa44c90dcd8ca958b9) Merged-In: I46b05c85d9c39f4ce549efc160c08a0646c9fd0a ",
        "func_before": "bool MetaDataBase::hasData(uint32_t key) const {\n    ssize_t i = mInternalData->mItems.indexOfKey(key);\n\n    if (i < 0) {\n        return false;\n    }\n\n    return true;\n}",
        "func": "bool MetaDataBase::hasData(uint32_t key) const {\n    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n    ssize_t i = mInternalData->mItems.indexOfKey(key);\n\n    if (i < 0) {\n        return false;\n    }\n\n    return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n bool MetaDataBase::hasData(uint32_t key) const {\n+    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n     ssize_t i = mInternalData->mItems.indexOfKey(key);\n \n     if (i < 0) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    std::lock_guard<std::mutex> guard(mInternalData->mLock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-40077",
        "func_name": "android/MetaDataBase::remove",
        "description": "In multiple functions of MetaDataBase.cpp, there is a possible UAF write due to a race condition. This could lead to remote escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.",
        "git_url": "https://android.googlesource.com/platform/frameworks/av/+/58fd993a89a3a22fa5a4a1a4548125c6783ec80c",
        "commit_title": "httplive: fix use-after-free",
        "commit_text": " Implement a mutex to ensure secure multi-threaded access to the KeyedVector in MetaDataBase. Concurrent access by different threads can lead to accessing the wrong memory location due to potential changes in the vector  Bug: 298057702 Test: HTTP Live Streaming test (cherry picked from https://partner-android-review.googlesource.com/q/commit:a2dfb31957a9d5358d0219a0eda7dcb5b0fff5fe) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:90fb4ca425444429ada6ce0de1c13d35829bc196) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:3c1d9613ef64e01d2e81c4aa44c90dcd8ca958b9) Merged-In: I46b05c85d9c39f4ce549efc160c08a0646c9fd0a ",
        "func_before": "bool MetaDataBase::remove(uint32_t key) {\n    ssize_t i = mInternalData->mItems.indexOfKey(key);\n\n    if (i < 0) {\n        return false;\n    }\n\n    mInternalData->mItems.removeItemsAt(i);\n\n    return true;\n}",
        "func": "bool MetaDataBase::remove(uint32_t key) {\n    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n    ssize_t i = mInternalData->mItems.indexOfKey(key);\n\n    if (i < 0) {\n        return false;\n    }\n\n    mInternalData->mItems.removeItemsAt(i);\n\n    return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n bool MetaDataBase::remove(uint32_t key) {\n+    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n     ssize_t i = mInternalData->mItems.indexOfKey(key);\n \n     if (i < 0) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    std::lock_guard<std::mutex> guard(mInternalData->mLock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-40077",
        "func_name": "android/MetaDataBase::dumpToLog",
        "description": "In multiple functions of MetaDataBase.cpp, there is a possible UAF write due to a race condition. This could lead to remote escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.",
        "git_url": "https://android.googlesource.com/platform/frameworks/av/+/58fd993a89a3a22fa5a4a1a4548125c6783ec80c",
        "commit_title": "httplive: fix use-after-free",
        "commit_text": " Implement a mutex to ensure secure multi-threaded access to the KeyedVector in MetaDataBase. Concurrent access by different threads can lead to accessing the wrong memory location due to potential changes in the vector  Bug: 298057702 Test: HTTP Live Streaming test (cherry picked from https://partner-android-review.googlesource.com/q/commit:a2dfb31957a9d5358d0219a0eda7dcb5b0fff5fe) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:90fb4ca425444429ada6ce0de1c13d35829bc196) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:3c1d9613ef64e01d2e81c4aa44c90dcd8ca958b9) Merged-In: I46b05c85d9c39f4ce549efc160c08a0646c9fd0a ",
        "func_before": "void MetaDataBase::dumpToLog() const {\n    for (int i = mInternalData->mItems.size(); --i >= 0;) {\n        int32_t key = mInternalData->mItems.keyAt(i);\n        char cc[5];\n        MakeFourCCString(key, cc);\n        const typed_data &item = mInternalData->mItems.valueAt(i);\n        ALOGI(\"%s: %s\", cc, item.asString(true /* verbose */).string());\n    }\n}",
        "func": "void MetaDataBase::dumpToLog() const {\n    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n    for (int i = mInternalData->mItems.size(); --i >= 0;) {\n        int32_t key = mInternalData->mItems.keyAt(i);\n        char cc[5];\n        MakeFourCCString(key, cc);\n        const typed_data &item = mInternalData->mItems.valueAt(i);\n        ALOGI(\"%s: %s\", cc, item.asString(true /* verbose */).string());\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n void MetaDataBase::dumpToLog() const {\n+    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n     for (int i = mInternalData->mItems.size(); --i >= 0;) {\n         int32_t key = mInternalData->mItems.keyAt(i);\n         char cc[5];",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    std::lock_guard<std::mutex> guard(mInternalData->mLock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-40077",
        "func_name": "android/MetaDataBase::findData",
        "description": "In multiple functions of MetaDataBase.cpp, there is a possible UAF write due to a race condition. This could lead to remote escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.",
        "git_url": "https://android.googlesource.com/platform/frameworks/av/+/58fd993a89a3a22fa5a4a1a4548125c6783ec80c",
        "commit_title": "httplive: fix use-after-free",
        "commit_text": " Implement a mutex to ensure secure multi-threaded access to the KeyedVector in MetaDataBase. Concurrent access by different threads can lead to accessing the wrong memory location due to potential changes in the vector  Bug: 298057702 Test: HTTP Live Streaming test (cherry picked from https://partner-android-review.googlesource.com/q/commit:a2dfb31957a9d5358d0219a0eda7dcb5b0fff5fe) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:90fb4ca425444429ada6ce0de1c13d35829bc196) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:3c1d9613ef64e01d2e81c4aa44c90dcd8ca958b9) Merged-In: I46b05c85d9c39f4ce549efc160c08a0646c9fd0a ",
        "func_before": "bool MetaDataBase::findData(uint32_t key, uint32_t *type,\n                        const void **data, size_t *size) const {\n    ssize_t i = mInternalData->mItems.indexOfKey(key);\n\n    if (i < 0) {\n        return false;\n    }\n\n    const typed_data &item = mInternalData->mItems.valueAt(i);\n\n    item.getData(type, data, size);\n\n    return true;\n}",
        "func": "bool MetaDataBase::findData(uint32_t key, uint32_t *type,\n                        const void **data, size_t *size) const {\n    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n    ssize_t i = mInternalData->mItems.indexOfKey(key);\n\n    if (i < 0) {\n        return false;\n    }\n\n    const typed_data &item = mInternalData->mItems.valueAt(i);\n\n    item.getData(type, data, size);\n\n    return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n bool MetaDataBase::findData(uint32_t key, uint32_t *type,\n                         const void **data, size_t *size) const {\n+    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n     ssize_t i = mInternalData->mItems.indexOfKey(key);\n \n     if (i < 0) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    std::lock_guard<std::mutex> guard(mInternalData->mLock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-40077",
        "func_name": "android/MetaDataBase::clear",
        "description": "In multiple functions of MetaDataBase.cpp, there is a possible UAF write due to a race condition. This could lead to remote escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.",
        "git_url": "https://android.googlesource.com/platform/frameworks/av/+/58fd993a89a3a22fa5a4a1a4548125c6783ec80c",
        "commit_title": "httplive: fix use-after-free",
        "commit_text": " Implement a mutex to ensure secure multi-threaded access to the KeyedVector in MetaDataBase. Concurrent access by different threads can lead to accessing the wrong memory location due to potential changes in the vector  Bug: 298057702 Test: HTTP Live Streaming test (cherry picked from https://partner-android-review.googlesource.com/q/commit:a2dfb31957a9d5358d0219a0eda7dcb5b0fff5fe) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:90fb4ca425444429ada6ce0de1c13d35829bc196) (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:3c1d9613ef64e01d2e81c4aa44c90dcd8ca958b9) Merged-In: I46b05c85d9c39f4ce549efc160c08a0646c9fd0a ",
        "func_before": "void MetaDataBase::clear() {\n    mInternalData->mItems.clear();\n}",
        "func": "void MetaDataBase::clear() {\n    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n    mInternalData->mItems.clear();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,3 +1,4 @@\n void MetaDataBase::clear() {\n+    std::lock_guard<std::mutex> guard(mInternalData->mLock);\n     mInternalData->mItems.clear();\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    std::lock_guard<std::mutex> guard(mInternalData->mLock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-49786",
        "func_name": "asterisk/__rtp_recvfrom",
        "description": "Asterisk is an open source private branch exchange and telephony toolkit. In Asterisk prior to versions 18.20.1, 20.5.1, and 21.0.1; as well as certified-asterisk prior to 18.9-cert6; Asterisk is susceptible to a DoS due to a race condition in the hello handshake phase of the DTLS protocol when handling DTLS-SRTP for media setup. This attack can be done continuously, thus denying new DTLS-SRTP encrypted calls during the attack. Abuse of this vulnerability may lead to a massive Denial of Service on vulnerable Asterisk servers for calls that rely on DTLS-SRTP. Commit d7d7764cb07c8a1872804321302ef93bf62cba05 contains a fix, which is part of versions 18.20.1, 20.5.1, 21.0.1, amd 18.9-cert6.",
        "git_url": "https://github.com/asterisk/asterisk/commit/d7d7764cb07c8a1872804321302ef93bf62cba05",
        "commit_title": "res_rtp_asterisk.c: Check DTLS packets against ICE candidate list",
        "commit_text": " When ICE is in use, we can prevent a possible DOS attack by allowing DTLS protocol messages (client hello, etc) only from sources that are in the active remote candidates list.  Resolves: GHSA-hxj9-xwr8-w8pq",
        "func_before": "static int __rtp_recvfrom(struct ast_rtp_instance *instance, void *buf, size_t size, int flags, struct ast_sockaddr *sa, int rtcp)\n{\n\tint len;\n\tstruct ast_rtp *rtp = ast_rtp_instance_get_data(instance);\n#if defined(HAVE_OPENSSL) && (OPENSSL_VERSION_NUMBER >= 0x10001000L) && !defined(OPENSSL_NO_SRTP)\n\tchar *in = buf;\n#endif\n#ifdef HAVE_PJPROJECT\n\tstruct ast_sockaddr *loop = rtcp ? &rtp->rtcp_loop : &rtp->rtp_loop;\n#endif\n#ifdef TEST_FRAMEWORK\n\tstruct ast_rtp_engine_test *test = ast_rtp_instance_get_test(instance);\n#endif\n\n\tif ((len = ast_recvfrom(rtcp ? rtp->rtcp->s : rtp->s, buf, size, flags, sa)) < 0) {\n\t\treturn len;\n\t}\n\n#ifdef TEST_FRAMEWORK\n\tif (test && test->packets_to_drop > 0) {\n\t\ttest->packets_to_drop--;\n\t\treturn 0;\n\t}\n#endif\n\n#if defined(HAVE_OPENSSL) && (OPENSSL_VERSION_NUMBER >= 0x10001000L) && !defined(OPENSSL_NO_SRTP)\n\t/* If this is an SSL packet pass it to OpenSSL for processing. RFC section for first byte value:\n\t * https://tools.ietf.org/html/rfc5764#section-5.1.2 */\n\tif ((*in >= 20) && (*in <= 63)) {\n\t\tstruct dtls_details *dtls = !rtcp ? &rtp->dtls : &rtp->rtcp->dtls;\n\t\tint res = 0;\n\n\t\t/* If no SSL session actually exists terminate things */\n\t\tif (!dtls->ssl) {\n\t\t\tast_log(LOG_ERROR, \"Received SSL traffic on RTP instance '%p' without an SSL session\\n\",\n\t\t\t\tinstance);\n\t\t\treturn -1;\n\t\t}\n\n\t\tast_debug_dtls(3, \"(%p) DTLS - __rtp_recvfrom rtp=%p - Got SSL packet '%d'\\n\", instance, rtp, *in);\n\n\t\t/*\n\t\t * A race condition is prevented between dtls_perform_handshake()\n\t\t * and this function because both functions have to get the\n\t\t * instance lock before they can do anything.  The\n\t\t * dtls_perform_handshake() function needs to start the timer\n\t\t * before we stop it below.\n\t\t */\n\n\t\t/* Before we feed data into OpenSSL ensure that the timeout timer is either stopped or completed */\n\t\tao2_unlock(instance);\n\t\tdtls_srtp_stop_timeout_timer(instance, rtp, rtcp);\n\t\tao2_lock(instance);\n\n\t\t/* If we don't yet know if we are active or passive and we receive a packet... we are obviously passive */\n\t\tif (dtls->dtls_setup == AST_RTP_DTLS_SETUP_ACTPASS) {\n\t\t\tdtls->dtls_setup = AST_RTP_DTLS_SETUP_PASSIVE;\n\t\t\tSSL_set_accept_state(dtls->ssl);\n\t\t}\n\n\t\tBIO_write(dtls->read_bio, buf, len);\n\n\t\tlen = SSL_read(dtls->ssl, buf, len);\n\n\t\tif ((len < 0) && (SSL_get_error(dtls->ssl, len) == SSL_ERROR_SSL)) {\n\t\t\tunsigned long error = ERR_get_error();\n\t\t\tast_log(LOG_ERROR, \"DTLS failure occurred on RTP instance '%p' due to reason '%s', terminating\\n\",\n\t\t\t\tinstance, ERR_reason_error_string(error));\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (SSL_is_init_finished(dtls->ssl)) {\n\t\t\t/* Any further connections will be existing since this is now established */\n\t\t\tdtls->connection = AST_RTP_DTLS_CONNECTION_EXISTING;\n\t\t\t/* Use the keying material to set up key/salt information */\n\t\t\tif ((res = dtls_srtp_setup(rtp, instance, rtcp))) {\n\t\t\t\treturn res;\n\t\t\t}\n\t\t\t/* Notify that dtls has been established */\n\t\t\tres = RTP_DTLS_ESTABLISHED;\n\n\t\t\tast_debug_dtls(3, \"(%p) DTLS - __rtp_recvfrom rtp=%p - established'\\n\", instance, rtp);\n\t\t} else {\n\t\t\t/* Since we've sent additional traffic start the timeout timer for retransmission */\n\t\t\tdtls_srtp_start_timeout_timer(instance, rtp, rtcp);\n\t\t}\n\n\t\treturn res;\n\t}\n#endif\n\n#ifdef HAVE_PJPROJECT\n\tif (!ast_sockaddr_isnull(loop) && !ast_sockaddr_cmp(loop, sa)) {\n\t\t/* ICE traffic will have been handled in the TURN callback, so skip it but update the address\n\t\t * so it reflects the actual source and not the loopback\n\t\t */\n\t\tif (rtcp) {\n\t\t\tast_sockaddr_copy(sa, &rtp->rtcp->them);\n\t\t} else {\n\t\t\tast_rtp_instance_get_remote_address(instance, sa);\n\t\t}\n\t} else if (rtp->ice) {\n\t\tpj_str_t combined = pj_str(ast_sockaddr_stringify(sa));\n\t\tpj_sockaddr address;\n\t\tpj_status_t status;\n\t\tstruct ice_wrap *ice;\n\n\t\tpj_thread_register_check();\n\n\t\tpj_sockaddr_parse(pj_AF_UNSPEC(), 0, &combined, &address);\n\n\t\t/* Release the instance lock to avoid deadlock with PJPROJECT group lock */\n\t\tice = rtp->ice;\n\t\tao2_ref(ice, +1);\n\t\tao2_unlock(instance);\n\t\tstatus = pj_ice_sess_on_rx_pkt(ice->real_ice,\n\t\t\trtcp ? AST_RTP_ICE_COMPONENT_RTCP : AST_RTP_ICE_COMPONENT_RTP,\n\t\t\trtcp ? TRANSPORT_SOCKET_RTCP : TRANSPORT_SOCKET_RTP, buf, len, &address,\n\t\t\tpj_sockaddr_get_len(&address));\n\t\tao2_ref(ice, -1);\n\t\tao2_lock(instance);\n\t\tif (status != PJ_SUCCESS) {\n\t\t\tchar err_buf[100];\n\n\t\t\tpj_strerror(status, err_buf, sizeof(err_buf));\n\t\t\tast_log(LOG_WARNING, \"PJ ICE Rx error status code: %d '%s'.\\n\",\n\t\t\t\t(int)status, err_buf);\n\t\t\treturn -1;\n\t\t}\n\t\tif (!rtp->passthrough) {\n\t\t\t/* If a unidirectional ICE negotiation occurs then lock on to the source of the\n\t\t\t * ICE traffic and use it as the target. This will occur if the remote side only\n\t\t\t * wants to receive media but never send to us.\n\t\t\t */\n\t\t\tif (!rtp->ice_active_remote_candidates && !rtp->ice_proposed_remote_candidates) {\n\t\t\t\tif (rtcp) {\n\t\t\t\t\tast_sockaddr_copy(&rtp->rtcp->them, sa);\n\t\t\t\t} else {\n\t\t\t\t\tast_rtp_instance_set_remote_address(instance, sa);\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t\trtp->passthrough = 0;\n\t}\n#endif\n\n\treturn len;\n}",
        "func": "static int __rtp_recvfrom(struct ast_rtp_instance *instance, void *buf, size_t size, int flags, struct ast_sockaddr *sa, int rtcp)\n{\n\tint len;\n\tstruct ast_rtp *rtp = ast_rtp_instance_get_data(instance);\n#if defined(HAVE_OPENSSL) && (OPENSSL_VERSION_NUMBER >= 0x10001000L) && !defined(OPENSSL_NO_SRTP)\n\tchar *in = buf;\n#endif\n#ifdef HAVE_PJPROJECT\n\tstruct ast_sockaddr *loop = rtcp ? &rtp->rtcp_loop : &rtp->rtp_loop;\n#endif\n#ifdef TEST_FRAMEWORK\n\tstruct ast_rtp_engine_test *test = ast_rtp_instance_get_test(instance);\n#endif\n\n\tif ((len = ast_recvfrom(rtcp ? rtp->rtcp->s : rtp->s, buf, size, flags, sa)) < 0) {\n\t\treturn len;\n\t}\n\n#ifdef TEST_FRAMEWORK\n\tif (test && test->packets_to_drop > 0) {\n\t\ttest->packets_to_drop--;\n\t\treturn 0;\n\t}\n#endif\n\n#if defined(HAVE_OPENSSL) && (OPENSSL_VERSION_NUMBER >= 0x10001000L) && !defined(OPENSSL_NO_SRTP)\n\t/* If this is an SSL packet pass it to OpenSSL for processing. RFC section for first byte value:\n\t * https://tools.ietf.org/html/rfc5764#section-5.1.2 */\n\tif ((*in >= 20) && (*in <= 63)) {\n\t\tstruct dtls_details *dtls = !rtcp ? &rtp->dtls : &rtp->rtcp->dtls;\n\t\tint res = 0;\n\n\t\t/* If no SSL session actually exists terminate things */\n\t\tif (!dtls->ssl) {\n\t\t\tast_log(LOG_ERROR, \"Received SSL traffic on RTP instance '%p' without an SSL session\\n\",\n\t\t\t\tinstance);\n\t\t\treturn -1;\n\t\t}\n\n\t\tast_debug_dtls(3, \"(%p) DTLS - __rtp_recvfrom rtp=%p - Got SSL packet '%d'\\n\", instance, rtp, *in);\n\n\t\t/*\n\t\t * If ICE is in use, we can prevent a possible DOS attack\n\t\t * by allowing DTLS protocol messages (client hello, etc)\n\t\t * only from sources that are in the active remote\n\t\t * candidates list.\n\t\t */\n\n\t\tif (rtp->ice) {\n\t\t\tint pass_src_check = 0;\n\t\t\tstruct ao2_iterator i;\n\t\t\tstruct ast_rtp_engine_ice_candidate *candidate;\n\t\t\tint cand_cnt = 0;\n\n\t\t\t/*\n\t\t\t * You'd think that this check would cause a \"deadlock\"\n\t\t\t * because ast_rtp_ice_start_media calls dtls_perform_handshake\n\t\t\t * before it sets ice_media_started = 1 so how can we do a\n\t\t\t * handshake if we're dropping packets before we send them\n\t\t\t * to openssl.  Fortunately, dtls_perform_handshake just sets\n\t\t\t * up openssl to do the handshake and doesn't actually perform it\n\t\t\t * itself and the locking prevents __rtp_recvfrom from\n\t\t\t * running before the ice_media_started flag is set.  So only\n\t\t\t * unexpected DTLS packets can get dropped here.\n\t\t\t */\n\t\t\tif (!rtp->ice_media_started) {\n\t\t\t\tast_log(LOG_WARNING, \"%s: DTLS packet from %s dropped. ICE not completed yet.\\n\",\n\t\t\t\t\tast_rtp_instance_get_channel_id(instance),\n\t\t\t\t\tast_sockaddr_stringify(sa));\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If we got this far, then ice_active_remote_candidates\n\t\t\t * can't be NULL.\n\t\t\t */\n\t\t\ti = ao2_iterator_init(rtp->ice_active_remote_candidates, 0);\n\t\t\twhile ((candidate = ao2_iterator_next(&i)) && (cand_cnt < PJ_ICE_MAX_CAND)) {\n\t\t\t\tres = ast_sockaddr_cmp_addr(&candidate->address, sa);\n\t\t\t\tao2_ref(candidate, -1);\n\t\t\t\tif (res == 0) {\n\t\t\t\t\tpass_src_check = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcand_cnt++;\n\t\t\t}\n\t\t\tao2_iterator_destroy(&i);\n\n\t\t\tif (!pass_src_check) {\n\t\t\t\tast_log(LOG_WARNING, \"%s: DTLS packet from %s dropped. Source not in ICE active candidate list.\\n\",\n\t\t\t\t\tast_rtp_instance_get_channel_id(instance),\n\t\t\t\t\tast_sockaddr_stringify(sa));\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * A race condition is prevented between dtls_perform_handshake()\n\t\t * and this function because both functions have to get the\n\t\t * instance lock before they can do anything.  The\n\t\t * dtls_perform_handshake() function needs to start the timer\n\t\t * before we stop it below.\n\t\t */\n\n\t\t/* Before we feed data into OpenSSL ensure that the timeout timer is either stopped or completed */\n\t\tao2_unlock(instance);\n\t\tdtls_srtp_stop_timeout_timer(instance, rtp, rtcp);\n\t\tao2_lock(instance);\n\n\t\t/* If we don't yet know if we are active or passive and we receive a packet... we are obviously passive */\n\t\tif (dtls->dtls_setup == AST_RTP_DTLS_SETUP_ACTPASS) {\n\t\t\tdtls->dtls_setup = AST_RTP_DTLS_SETUP_PASSIVE;\n\t\t\tSSL_set_accept_state(dtls->ssl);\n\t\t}\n\n\t\tBIO_write(dtls->read_bio, buf, len);\n\n\t\tlen = SSL_read(dtls->ssl, buf, len);\n\n\t\tif ((len < 0) && (SSL_get_error(dtls->ssl, len) == SSL_ERROR_SSL)) {\n\t\t\tunsigned long error = ERR_get_error();\n\t\t\tast_log(LOG_ERROR, \"DTLS failure occurred on RTP instance '%p' due to reason '%s', terminating\\n\",\n\t\t\t\tinstance, ERR_reason_error_string(error));\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (SSL_is_init_finished(dtls->ssl)) {\n\t\t\t/* Any further connections will be existing since this is now established */\n\t\t\tdtls->connection = AST_RTP_DTLS_CONNECTION_EXISTING;\n\t\t\t/* Use the keying material to set up key/salt information */\n\t\t\tif ((res = dtls_srtp_setup(rtp, instance, rtcp))) {\n\t\t\t\treturn res;\n\t\t\t}\n\t\t\t/* Notify that dtls has been established */\n\t\t\tres = RTP_DTLS_ESTABLISHED;\n\n\t\t\tast_debug_dtls(3, \"(%p) DTLS - __rtp_recvfrom rtp=%p - established'\\n\", instance, rtp);\n\t\t} else {\n\t\t\t/* Since we've sent additional traffic start the timeout timer for retransmission */\n\t\t\tdtls_srtp_start_timeout_timer(instance, rtp, rtcp);\n\t\t}\n\n\t\treturn res;\n\t}\n#endif\n\n#ifdef HAVE_PJPROJECT\n\tif (!ast_sockaddr_isnull(loop) && !ast_sockaddr_cmp(loop, sa)) {\n\t\t/* ICE traffic will have been handled in the TURN callback, so skip it but update the address\n\t\t * so it reflects the actual source and not the loopback\n\t\t */\n\t\tif (rtcp) {\n\t\t\tast_sockaddr_copy(sa, &rtp->rtcp->them);\n\t\t} else {\n\t\t\tast_rtp_instance_get_remote_address(instance, sa);\n\t\t}\n\t} else if (rtp->ice) {\n\t\tpj_str_t combined = pj_str(ast_sockaddr_stringify(sa));\n\t\tpj_sockaddr address;\n\t\tpj_status_t status;\n\t\tstruct ice_wrap *ice;\n\n\t\tpj_thread_register_check();\n\n\t\tpj_sockaddr_parse(pj_AF_UNSPEC(), 0, &combined, &address);\n\n\t\t/* Release the instance lock to avoid deadlock with PJPROJECT group lock */\n\t\tice = rtp->ice;\n\t\tao2_ref(ice, +1);\n\t\tao2_unlock(instance);\n\t\tstatus = pj_ice_sess_on_rx_pkt(ice->real_ice,\n\t\t\trtcp ? AST_RTP_ICE_COMPONENT_RTCP : AST_RTP_ICE_COMPONENT_RTP,\n\t\t\trtcp ? TRANSPORT_SOCKET_RTCP : TRANSPORT_SOCKET_RTP, buf, len, &address,\n\t\t\tpj_sockaddr_get_len(&address));\n\t\tao2_ref(ice, -1);\n\t\tao2_lock(instance);\n\t\tif (status != PJ_SUCCESS) {\n\t\t\tchar err_buf[100];\n\n\t\t\tpj_strerror(status, err_buf, sizeof(err_buf));\n\t\t\tast_log(LOG_WARNING, \"PJ ICE Rx error status code: %d '%s'.\\n\",\n\t\t\t\t(int)status, err_buf);\n\t\t\treturn -1;\n\t\t}\n\t\tif (!rtp->passthrough) {\n\t\t\t/* If a unidirectional ICE negotiation occurs then lock on to the source of the\n\t\t\t * ICE traffic and use it as the target. This will occur if the remote side only\n\t\t\t * wants to receive media but never send to us.\n\t\t\t */\n\t\t\tif (!rtp->ice_active_remote_candidates && !rtp->ice_proposed_remote_candidates) {\n\t\t\t\tif (rtcp) {\n\t\t\t\t\tast_sockaddr_copy(&rtp->rtcp->them, sa);\n\t\t\t\t} else {\n\t\t\t\t\tast_rtp_instance_set_remote_address(instance, sa);\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t\trtp->passthrough = 0;\n\t}\n#endif\n\n\treturn len;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,6 +38,61 @@\n \t\t}\n \n \t\tast_debug_dtls(3, \"(%p) DTLS - __rtp_recvfrom rtp=%p - Got SSL packet '%d'\\n\", instance, rtp, *in);\n+\n+\t\t/*\n+\t\t * If ICE is in use, we can prevent a possible DOS attack\n+\t\t * by allowing DTLS protocol messages (client hello, etc)\n+\t\t * only from sources that are in the active remote\n+\t\t * candidates list.\n+\t\t */\n+\n+\t\tif (rtp->ice) {\n+\t\t\tint pass_src_check = 0;\n+\t\t\tstruct ao2_iterator i;\n+\t\t\tstruct ast_rtp_engine_ice_candidate *candidate;\n+\t\t\tint cand_cnt = 0;\n+\n+\t\t\t/*\n+\t\t\t * You'd think that this check would cause a \"deadlock\"\n+\t\t\t * because ast_rtp_ice_start_media calls dtls_perform_handshake\n+\t\t\t * before it sets ice_media_started = 1 so how can we do a\n+\t\t\t * handshake if we're dropping packets before we send them\n+\t\t\t * to openssl.  Fortunately, dtls_perform_handshake just sets\n+\t\t\t * up openssl to do the handshake and doesn't actually perform it\n+\t\t\t * itself and the locking prevents __rtp_recvfrom from\n+\t\t\t * running before the ice_media_started flag is set.  So only\n+\t\t\t * unexpected DTLS packets can get dropped here.\n+\t\t\t */\n+\t\t\tif (!rtp->ice_media_started) {\n+\t\t\t\tast_log(LOG_WARNING, \"%s: DTLS packet from %s dropped. ICE not completed yet.\\n\",\n+\t\t\t\t\tast_rtp_instance_get_channel_id(instance),\n+\t\t\t\t\tast_sockaddr_stringify(sa));\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\n+\t\t\t/*\n+\t\t\t * If we got this far, then ice_active_remote_candidates\n+\t\t\t * can't be NULL.\n+\t\t\t */\n+\t\t\ti = ao2_iterator_init(rtp->ice_active_remote_candidates, 0);\n+\t\t\twhile ((candidate = ao2_iterator_next(&i)) && (cand_cnt < PJ_ICE_MAX_CAND)) {\n+\t\t\t\tres = ast_sockaddr_cmp_addr(&candidate->address, sa);\n+\t\t\t\tao2_ref(candidate, -1);\n+\t\t\t\tif (res == 0) {\n+\t\t\t\t\tpass_src_check = 1;\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t\tcand_cnt++;\n+\t\t\t}\n+\t\t\tao2_iterator_destroy(&i);\n+\n+\t\t\tif (!pass_src_check) {\n+\t\t\t\tast_log(LOG_WARNING, \"%s: DTLS packet from %s dropped. Source not in ICE active candidate list.\\n\",\n+\t\t\t\t\tast_rtp_instance_get_channel_id(instance),\n+\t\t\t\t\tast_sockaddr_stringify(sa));\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t}\n \n \t\t/*\n \t\t * A race condition is prevented between dtls_perform_handshake()",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t\t/*",
                "\t\t * If ICE is in use, we can prevent a possible DOS attack",
                "\t\t * by allowing DTLS protocol messages (client hello, etc)",
                "\t\t * only from sources that are in the active remote",
                "\t\t * candidates list.",
                "\t\t */",
                "",
                "\t\tif (rtp->ice) {",
                "\t\t\tint pass_src_check = 0;",
                "\t\t\tstruct ao2_iterator i;",
                "\t\t\tstruct ast_rtp_engine_ice_candidate *candidate;",
                "\t\t\tint cand_cnt = 0;",
                "",
                "\t\t\t/*",
                "\t\t\t * You'd think that this check would cause a \"deadlock\"",
                "\t\t\t * because ast_rtp_ice_start_media calls dtls_perform_handshake",
                "\t\t\t * before it sets ice_media_started = 1 so how can we do a",
                "\t\t\t * handshake if we're dropping packets before we send them",
                "\t\t\t * to openssl.  Fortunately, dtls_perform_handshake just sets",
                "\t\t\t * up openssl to do the handshake and doesn't actually perform it",
                "\t\t\t * itself and the locking prevents __rtp_recvfrom from",
                "\t\t\t * running before the ice_media_started flag is set.  So only",
                "\t\t\t * unexpected DTLS packets can get dropped here.",
                "\t\t\t */",
                "\t\t\tif (!rtp->ice_media_started) {",
                "\t\t\t\tast_log(LOG_WARNING, \"%s: DTLS packet from %s dropped. ICE not completed yet.\\n\",",
                "\t\t\t\t\tast_rtp_instance_get_channel_id(instance),",
                "\t\t\t\t\tast_sockaddr_stringify(sa));",
                "\t\t\t\treturn 0;",
                "\t\t\t}",
                "",
                "\t\t\t/*",
                "\t\t\t * If we got this far, then ice_active_remote_candidates",
                "\t\t\t * can't be NULL.",
                "\t\t\t */",
                "\t\t\ti = ao2_iterator_init(rtp->ice_active_remote_candidates, 0);",
                "\t\t\twhile ((candidate = ao2_iterator_next(&i)) && (cand_cnt < PJ_ICE_MAX_CAND)) {",
                "\t\t\t\tres = ast_sockaddr_cmp_addr(&candidate->address, sa);",
                "\t\t\t\tao2_ref(candidate, -1);",
                "\t\t\t\tif (res == 0) {",
                "\t\t\t\t\tpass_src_check = 1;",
                "\t\t\t\t\tbreak;",
                "\t\t\t\t}",
                "\t\t\t\tcand_cnt++;",
                "\t\t\t}",
                "\t\t\tao2_iterator_destroy(&i);",
                "",
                "\t\t\tif (!pass_src_check) {",
                "\t\t\t\tast_log(LOG_WARNING, \"%s: DTLS packet from %s dropped. Source not in ICE active candidate list.\\n\",",
                "\t\t\t\t\tast_rtp_instance_get_channel_id(instance),",
                "\t\t\t\t\tast_sockaddr_stringify(sa));",
                "\t\t\t\treturn 0;",
                "\t\t\t}",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6546",
        "func_name": "torvalds/linux/gsm_cleanup_mux",
        "description": "A race condition was found in the GSM 0710 tty multiplexor in the Linux kernel. This issue occurs when two threads execute the GSMIOC_SETCONF ioctl on the same tty file descriptor with the gsm line discipline enabled, and can lead to a use-after-free problem on a struct gsm_dlci while restarting the gsm mux. This could allow a local unprivileged user to escalate their privileges on the system.",
        "git_url": "https://github.com/torvalds/linux/commit/3c4f8333b582487a2d1e02171f1465531cde53e3",
        "commit_title": "tty: n_gsm: fix the UAF caused by race condition in gsm_cleanup_mux",
        "commit_text": " In commit 9b9c8195f3f0 (\"tty: n_gsm: fix UAF in gsm_cleanup_mux\"), the UAF problem is not completely fixed. There is a race condition in gsm_cleanup_mux(), which caused this UAF.  The UAF problem is triggered by the following race: task[5046]                     task[5054] -----------------------        ----------------------- gsm_cleanup_mux(); dlci = gsm->dlci[0]; mutex_lock(&gsm->mutex);                                gsm_cleanup_mux(); \t\t\t       dlci = gsm->dlci[0]; //Didn't take the lock gsm_dlci_release(gsm->dlci[i]); gsm->dlci[i] = NULL; mutex_unlock(&gsm->mutex);                                mutex_lock(&gsm->mutex); \t\t\t       dlci->dead = true; //UAF  Fix it by assigning values after mutex_lock().  Link: https://syzkaller.appspot.com/text?tag=CrashReport&x=176188b5a80000 Cc: stable <stable@kernel.org> Co-developed-by: Qiumiao Zhang <zhangqiumiao1@huawei.com> Link: https://lore.kernel.org/r/20230811031121.153237-1-yiyang13@huawei.com",
        "func_before": "static void gsm_cleanup_mux(struct gsm_mux *gsm, bool disc)\n{\n\tint i;\n\tstruct gsm_dlci *dlci = gsm->dlci[0];\n\tstruct gsm_msg *txq, *ntxq;\n\n\tgsm->dead = true;\n\tmutex_lock(&gsm->mutex);\n\n\tif (dlci) {\n\t\tif (disc && dlci->state != DLCI_CLOSED) {\n\t\t\tgsm_dlci_begin_close(dlci);\n\t\t\twait_event(gsm->event, dlci->state == DLCI_CLOSED);\n\t\t}\n\t\tdlci->dead = true;\n\t}\n\n\t/* Finish outstanding timers, making sure they are done */\n\tdel_timer_sync(&gsm->kick_timer);\n\tdel_timer_sync(&gsm->t2_timer);\n\tdel_timer_sync(&gsm->ka_timer);\n\n\t/* Finish writing to ldisc */\n\tflush_work(&gsm->tx_work);\n\n\t/* Free up any link layer users and finally the control channel */\n\tif (gsm->has_devices) {\n\t\tgsm_unregister_devices(gsm_tty_driver, gsm->num);\n\t\tgsm->has_devices = false;\n\t}\n\tfor (i = NUM_DLCI - 1; i >= 0; i--)\n\t\tif (gsm->dlci[i]) {\n\t\t\tgsm_dlci_release(gsm->dlci[i]);\n\t\t\tgsm->dlci[i] = NULL;\n\t\t}\n\tmutex_unlock(&gsm->mutex);\n\t/* Now wipe the queues */\n\ttty_ldisc_flush(gsm->tty);\n\tlist_for_each_entry_safe(txq, ntxq, &gsm->tx_ctrl_list, list)\n\t\tkfree(txq);\n\tINIT_LIST_HEAD(&gsm->tx_ctrl_list);\n\tlist_for_each_entry_safe(txq, ntxq, &gsm->tx_data_list, list)\n\t\tkfree(txq);\n\tINIT_LIST_HEAD(&gsm->tx_data_list);\n}",
        "func": "static void gsm_cleanup_mux(struct gsm_mux *gsm, bool disc)\n{\n\tint i;\n\tstruct gsm_dlci *dlci;\n\tstruct gsm_msg *txq, *ntxq;\n\n\tgsm->dead = true;\n\tmutex_lock(&gsm->mutex);\n\n\tdlci = gsm->dlci[0];\n\tif (dlci) {\n\t\tif (disc && dlci->state != DLCI_CLOSED) {\n\t\t\tgsm_dlci_begin_close(dlci);\n\t\t\twait_event(gsm->event, dlci->state == DLCI_CLOSED);\n\t\t}\n\t\tdlci->dead = true;\n\t}\n\n\t/* Finish outstanding timers, making sure they are done */\n\tdel_timer_sync(&gsm->kick_timer);\n\tdel_timer_sync(&gsm->t2_timer);\n\tdel_timer_sync(&gsm->ka_timer);\n\n\t/* Finish writing to ldisc */\n\tflush_work(&gsm->tx_work);\n\n\t/* Free up any link layer users and finally the control channel */\n\tif (gsm->has_devices) {\n\t\tgsm_unregister_devices(gsm_tty_driver, gsm->num);\n\t\tgsm->has_devices = false;\n\t}\n\tfor (i = NUM_DLCI - 1; i >= 0; i--)\n\t\tif (gsm->dlci[i]) {\n\t\t\tgsm_dlci_release(gsm->dlci[i]);\n\t\t\tgsm->dlci[i] = NULL;\n\t\t}\n\tmutex_unlock(&gsm->mutex);\n\t/* Now wipe the queues */\n\ttty_ldisc_flush(gsm->tty);\n\tlist_for_each_entry_safe(txq, ntxq, &gsm->tx_ctrl_list, list)\n\t\tkfree(txq);\n\tINIT_LIST_HEAD(&gsm->tx_ctrl_list);\n\tlist_for_each_entry_safe(txq, ntxq, &gsm->tx_data_list, list)\n\t\tkfree(txq);\n\tINIT_LIST_HEAD(&gsm->tx_data_list);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,12 +1,13 @@\n static void gsm_cleanup_mux(struct gsm_mux *gsm, bool disc)\n {\n \tint i;\n-\tstruct gsm_dlci *dlci = gsm->dlci[0];\n+\tstruct gsm_dlci *dlci;\n \tstruct gsm_msg *txq, *ntxq;\n \n \tgsm->dead = true;\n \tmutex_lock(&gsm->mutex);\n \n+\tdlci = gsm->dlci[0];\n \tif (dlci) {\n \t\tif (disc && dlci->state != DLCI_CLOSED) {\n \t\t\tgsm_dlci_begin_close(dlci);",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct gsm_dlci *dlci = gsm->dlci[0];"
            ],
            "added_lines": [
                "\tstruct gsm_dlci *dlci;",
                "\tdlci = gsm->dlci[0];"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/fib6_alloc_table",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "static struct fib6_table *fib6_alloc_table(struct net *net, u32 id)\n{\n\tstruct fib6_table *table;\n\n\ttable = kzalloc(sizeof(*table), GFP_ATOMIC);\n\tif (table) {\n\t\ttable->tb6_id = id;\n\t\trcu_assign_pointer(table->tb6_root.leaf,\n\t\t\t\t   net->ipv6.fib6_null_entry);\n\t\ttable->tb6_root.fn_flags = RTN_ROOT | RTN_TL_ROOT | RTN_RTINFO;\n\t\tinet_peer_base_init(&table->tb6_peers);\n\t\tINIT_HLIST_HEAD(&table->tb6_gc_hlist);\n\t}\n\n\treturn table;\n}",
        "func": "static struct fib6_table *fib6_alloc_table(struct net *net, u32 id)\n{\n\tstruct fib6_table *table;\n\n\ttable = kzalloc(sizeof(*table), GFP_ATOMIC);\n\tif (table) {\n\t\ttable->tb6_id = id;\n\t\trcu_assign_pointer(table->tb6_root.leaf,\n\t\t\t\t   net->ipv6.fib6_null_entry);\n\t\ttable->tb6_root.fn_flags = RTN_ROOT | RTN_TL_ROOT | RTN_RTINFO;\n\t\tinet_peer_base_init(&table->tb6_peers);\n\t}\n\n\treturn table;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,6 @@\n \t\t\t\t   net->ipv6.fib6_null_entry);\n \t\ttable->tb6_root.fn_flags = RTN_ROOT | RTN_TL_ROOT | RTN_RTINFO;\n \t\tinet_peer_base_init(&table->tb6_peers);\n-\t\tINIT_HLIST_HEAD(&table->tb6_gc_hlist);\n \t}\n \n \treturn table;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tINIT_HLIST_HEAD(&table->tb6_gc_hlist);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/fib6_add_rt2node",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "static int fib6_add_rt2node(struct fib6_node *fn, struct fib6_info *rt,\n\t\t\t    struct nl_info *info,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct fib6_info *leaf = rcu_dereference_protected(fn->leaf,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\tstruct fib6_info *iter = NULL;\n\tstruct fib6_info __rcu **ins;\n\tstruct fib6_info __rcu **fallback_ins = NULL;\n\tint replace = (info->nlh &&\n\t\t       (info->nlh->nlmsg_flags & NLM_F_REPLACE));\n\tint add = (!info->nlh ||\n\t\t   (info->nlh->nlmsg_flags & NLM_F_CREATE));\n\tint found = 0;\n\tbool rt_can_ecmp = rt6_qualify_for_ecmp(rt);\n\tbool notify_sibling_rt = false;\n\tu16 nlflags = NLM_F_EXCL;\n\tint err;\n\n\tif (info->nlh && (info->nlh->nlmsg_flags & NLM_F_APPEND))\n\t\tnlflags |= NLM_F_APPEND;\n\n\tins = &fn->leaf;\n\n\tfor (iter = leaf; iter;\n\t     iter = rcu_dereference_protected(iter->fib6_next,\n\t\t\t\tlockdep_is_held(&rt->fib6_table->tb6_lock))) {\n\t\t/*\n\t\t *\tSearch for duplicates\n\t\t */\n\n\t\tif (iter->fib6_metric == rt->fib6_metric) {\n\t\t\t/*\n\t\t\t *\tSame priority level\n\t\t\t */\n\t\t\tif (info->nlh &&\n\t\t\t    (info->nlh->nlmsg_flags & NLM_F_EXCL))\n\t\t\t\treturn -EEXIST;\n\n\t\t\tnlflags &= ~NLM_F_EXCL;\n\t\t\tif (replace) {\n\t\t\t\tif (rt_can_ecmp == rt6_qualify_for_ecmp(iter)) {\n\t\t\t\t\tfound++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tfallback_ins = fallback_ins ?: ins;\n\t\t\t\tgoto next_iter;\n\t\t\t}\n\n\t\t\tif (rt6_duplicate_nexthop(iter, rt)) {\n\t\t\t\tif (rt->fib6_nsiblings)\n\t\t\t\t\trt->fib6_nsiblings = 0;\n\t\t\t\tif (!(iter->fib6_flags & RTF_EXPIRES))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (!(rt->fib6_flags & RTF_EXPIRES))\n\t\t\t\t\tfib6_clean_expires_locked(iter);\n\t\t\t\telse\n\t\t\t\t\tfib6_set_expires_locked(iter,\n\t\t\t\t\t\t\t\trt->expires);\n\n\t\t\t\tif (rt->fib6_pmtu)\n\t\t\t\t\tfib6_metric_set(iter, RTAX_MTU,\n\t\t\t\t\t\t\trt->fib6_pmtu);\n\t\t\t\treturn -EEXIST;\n\t\t\t}\n\t\t\t/* If we have the same destination and the same metric,\n\t\t\t * but not the same gateway, then the route we try to\n\t\t\t * add is sibling to this route, increment our counter\n\t\t\t * of siblings, and later we will add our route to the\n\t\t\t * list.\n\t\t\t * Only static routes (which don't have flag\n\t\t\t * RTF_EXPIRES) are used for ECMPv6.\n\t\t\t *\n\t\t\t * To avoid long list, we only had siblings if the\n\t\t\t * route have a gateway.\n\t\t\t */\n\t\t\tif (rt_can_ecmp &&\n\t\t\t    rt6_qualify_for_ecmp(iter))\n\t\t\t\trt->fib6_nsiblings++;\n\t\t}\n\n\t\tif (iter->fib6_metric > rt->fib6_metric)\n\t\t\tbreak;\n\nnext_iter:\n\t\tins = &iter->fib6_next;\n\t}\n\n\tif (fallback_ins && !found) {\n\t\t/* No matching route with same ecmp-able-ness found, replace\n\t\t * first matching route\n\t\t */\n\t\tins = fallback_ins;\n\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\tfound++;\n\t}\n\n\t/* Reset round-robin state, if necessary */\n\tif (ins == &fn->leaf)\n\t\tfn->rr_ptr = NULL;\n\n\t/* Link this route to others same route. */\n\tif (rt->fib6_nsiblings) {\n\t\tunsigned int fib6_nsiblings;\n\t\tstruct fib6_info *sibling, *temp_sibling;\n\n\t\t/* Find the first route that have the same metric */\n\t\tsibling = leaf;\n\t\tnotify_sibling_rt = true;\n\t\twhile (sibling) {\n\t\t\tif (sibling->fib6_metric == rt->fib6_metric &&\n\t\t\t    rt6_qualify_for_ecmp(sibling)) {\n\t\t\t\tlist_add_tail(&rt->fib6_siblings,\n\t\t\t\t\t      &sibling->fib6_siblings);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsibling = rcu_dereference_protected(sibling->fib6_next,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\tnotify_sibling_rt = false;\n\t\t}\n\t\t/* For each sibling in the list, increment the counter of\n\t\t * siblings. BUG() if counters does not match, list of siblings\n\t\t * is broken!\n\t\t */\n\t\tfib6_nsiblings = 0;\n\t\tlist_for_each_entry_safe(sibling, temp_sibling,\n\t\t\t\t\t &rt->fib6_siblings, fib6_siblings) {\n\t\t\tsibling->fib6_nsiblings++;\n\t\t\tBUG_ON(sibling->fib6_nsiblings != rt->fib6_nsiblings);\n\t\t\tfib6_nsiblings++;\n\t\t}\n\t\tBUG_ON(fib6_nsiblings != rt->fib6_nsiblings);\n\t\trt6_multipath_rebalance(temp_sibling);\n\t}\n\n\t/*\n\t *\tinsert node\n\t */\n\tif (!replace) {\n\t\tif (!add)\n\t\t\tpr_warn(\"NLM_F_CREATE should be set when creating new route\\n\");\n\nadd:\n\t\tnlflags |= NLM_F_CREATE;\n\n\t\t/* The route should only be notified if it is the first\n\t\t * route in the node or if it is added as a sibling\n\t\t * route to the first route in the node.\n\t\t */\n\t\tif (!info->skip_notify_kernel &&\n\t\t    (notify_sibling_rt || ins == &fn->leaf)) {\n\t\t\tenum fib_event_type fib_event;\n\n\t\t\tif (notify_sibling_rt)\n\t\t\t\tfib_event = FIB_EVENT_ENTRY_APPEND;\n\t\t\telse\n\t\t\t\tfib_event = FIB_EVENT_ENTRY_REPLACE;\n\t\t\terr = call_fib6_entry_notifiers(info->nl_net,\n\t\t\t\t\t\t\tfib_event, rt,\n\t\t\t\t\t\t\textack);\n\t\t\tif (err) {\n\t\t\t\tstruct fib6_info *sibling, *next_sibling;\n\n\t\t\t\t/* If the route has siblings, then it first\n\t\t\t\t * needs to be unlinked from them.\n\t\t\t\t */\n\t\t\t\tif (!rt->fib6_nsiblings)\n\t\t\t\t\treturn err;\n\n\t\t\t\tlist_for_each_entry_safe(sibling, next_sibling,\n\t\t\t\t\t\t\t &rt->fib6_siblings,\n\t\t\t\t\t\t\t fib6_siblings)\n\t\t\t\t\tsibling->fib6_nsiblings--;\n\t\t\t\trt->fib6_nsiblings = 0;\n\t\t\t\tlist_del_init(&rt->fib6_siblings);\n\t\t\t\trt6_multipath_rebalance(next_sibling);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\trcu_assign_pointer(rt->fib6_next, iter);\n\t\tfib6_info_hold(rt);\n\t\trcu_assign_pointer(rt->fib6_node, fn);\n\t\trcu_assign_pointer(*ins, rt);\n\t\tif (!info->skip_notify)\n\t\t\tinet6_rt_notify(RTM_NEWROUTE, rt, info, nlflags);\n\t\tinfo->nl_net->ipv6.rt6_stats->fib_rt_entries++;\n\n\t\tif (!(fn->fn_flags & RTN_RTINFO)) {\n\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_route_nodes++;\n\t\t\tfn->fn_flags |= RTN_RTINFO;\n\t\t}\n\n\t} else {\n\t\tint nsiblings;\n\n\t\tif (!found) {\n\t\t\tif (add)\n\t\t\t\tgoto add;\n\t\t\tpr_warn(\"NLM_F_REPLACE set, but no existing node found!\\n\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tif (!info->skip_notify_kernel && ins == &fn->leaf) {\n\t\t\terr = call_fib6_entry_notifiers(info->nl_net,\n\t\t\t\t\t\t\tFIB_EVENT_ENTRY_REPLACE,\n\t\t\t\t\t\t\trt, extack);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tfib6_info_hold(rt);\n\t\trcu_assign_pointer(rt->fib6_node, fn);\n\t\trt->fib6_next = iter->fib6_next;\n\t\trcu_assign_pointer(*ins, rt);\n\t\tif (!info->skip_notify)\n\t\t\tinet6_rt_notify(RTM_NEWROUTE, rt, info, NLM_F_REPLACE);\n\t\tif (!(fn->fn_flags & RTN_RTINFO)) {\n\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_route_nodes++;\n\t\t\tfn->fn_flags |= RTN_RTINFO;\n\t\t}\n\t\tnsiblings = iter->fib6_nsiblings;\n\t\titer->fib6_node = NULL;\n\t\tfib6_purge_rt(iter, fn, info->nl_net);\n\t\tif (rcu_access_pointer(fn->rr_ptr) == iter)\n\t\t\tfn->rr_ptr = NULL;\n\t\tfib6_info_release(iter);\n\n\t\tif (nsiblings) {\n\t\t\t/* Replacing an ECMP route, remove all siblings */\n\t\t\tins = &rt->fib6_next;\n\t\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\twhile (iter) {\n\t\t\t\tif (iter->fib6_metric > rt->fib6_metric)\n\t\t\t\t\tbreak;\n\t\t\t\tif (rt6_qualify_for_ecmp(iter)) {\n\t\t\t\t\t*ins = iter->fib6_next;\n\t\t\t\t\titer->fib6_node = NULL;\n\t\t\t\t\tfib6_purge_rt(iter, fn, info->nl_net);\n\t\t\t\t\tif (rcu_access_pointer(fn->rr_ptr) == iter)\n\t\t\t\t\t\tfn->rr_ptr = NULL;\n\t\t\t\t\tfib6_info_release(iter);\n\t\t\t\t\tnsiblings--;\n\t\t\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_rt_entries--;\n\t\t\t\t} else {\n\t\t\t\t\tins = &iter->fib6_next;\n\t\t\t\t}\n\t\t\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t\tlockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\t}\n\t\t\tWARN_ON(nsiblings != 0);\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "func": "static int fib6_add_rt2node(struct fib6_node *fn, struct fib6_info *rt,\n\t\t\t    struct nl_info *info,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct fib6_info *leaf = rcu_dereference_protected(fn->leaf,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\tstruct fib6_info *iter = NULL;\n\tstruct fib6_info __rcu **ins;\n\tstruct fib6_info __rcu **fallback_ins = NULL;\n\tint replace = (info->nlh &&\n\t\t       (info->nlh->nlmsg_flags & NLM_F_REPLACE));\n\tint add = (!info->nlh ||\n\t\t   (info->nlh->nlmsg_flags & NLM_F_CREATE));\n\tint found = 0;\n\tbool rt_can_ecmp = rt6_qualify_for_ecmp(rt);\n\tbool notify_sibling_rt = false;\n\tu16 nlflags = NLM_F_EXCL;\n\tint err;\n\n\tif (info->nlh && (info->nlh->nlmsg_flags & NLM_F_APPEND))\n\t\tnlflags |= NLM_F_APPEND;\n\n\tins = &fn->leaf;\n\n\tfor (iter = leaf; iter;\n\t     iter = rcu_dereference_protected(iter->fib6_next,\n\t\t\t\tlockdep_is_held(&rt->fib6_table->tb6_lock))) {\n\t\t/*\n\t\t *\tSearch for duplicates\n\t\t */\n\n\t\tif (iter->fib6_metric == rt->fib6_metric) {\n\t\t\t/*\n\t\t\t *\tSame priority level\n\t\t\t */\n\t\t\tif (info->nlh &&\n\t\t\t    (info->nlh->nlmsg_flags & NLM_F_EXCL))\n\t\t\t\treturn -EEXIST;\n\n\t\t\tnlflags &= ~NLM_F_EXCL;\n\t\t\tif (replace) {\n\t\t\t\tif (rt_can_ecmp == rt6_qualify_for_ecmp(iter)) {\n\t\t\t\t\tfound++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tfallback_ins = fallback_ins ?: ins;\n\t\t\t\tgoto next_iter;\n\t\t\t}\n\n\t\t\tif (rt6_duplicate_nexthop(iter, rt)) {\n\t\t\t\tif (rt->fib6_nsiblings)\n\t\t\t\t\trt->fib6_nsiblings = 0;\n\t\t\t\tif (!(iter->fib6_flags & RTF_EXPIRES))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (!(rt->fib6_flags & RTF_EXPIRES))\n\t\t\t\t\tfib6_clean_expires(iter);\n\t\t\t\telse\n\t\t\t\t\tfib6_set_expires(iter, rt->expires);\n\n\t\t\t\tif (rt->fib6_pmtu)\n\t\t\t\t\tfib6_metric_set(iter, RTAX_MTU,\n\t\t\t\t\t\t\trt->fib6_pmtu);\n\t\t\t\treturn -EEXIST;\n\t\t\t}\n\t\t\t/* If we have the same destination and the same metric,\n\t\t\t * but not the same gateway, then the route we try to\n\t\t\t * add is sibling to this route, increment our counter\n\t\t\t * of siblings, and later we will add our route to the\n\t\t\t * list.\n\t\t\t * Only static routes (which don't have flag\n\t\t\t * RTF_EXPIRES) are used for ECMPv6.\n\t\t\t *\n\t\t\t * To avoid long list, we only had siblings if the\n\t\t\t * route have a gateway.\n\t\t\t */\n\t\t\tif (rt_can_ecmp &&\n\t\t\t    rt6_qualify_for_ecmp(iter))\n\t\t\t\trt->fib6_nsiblings++;\n\t\t}\n\n\t\tif (iter->fib6_metric > rt->fib6_metric)\n\t\t\tbreak;\n\nnext_iter:\n\t\tins = &iter->fib6_next;\n\t}\n\n\tif (fallback_ins && !found) {\n\t\t/* No matching route with same ecmp-able-ness found, replace\n\t\t * first matching route\n\t\t */\n\t\tins = fallback_ins;\n\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\tfound++;\n\t}\n\n\t/* Reset round-robin state, if necessary */\n\tif (ins == &fn->leaf)\n\t\tfn->rr_ptr = NULL;\n\n\t/* Link this route to others same route. */\n\tif (rt->fib6_nsiblings) {\n\t\tunsigned int fib6_nsiblings;\n\t\tstruct fib6_info *sibling, *temp_sibling;\n\n\t\t/* Find the first route that have the same metric */\n\t\tsibling = leaf;\n\t\tnotify_sibling_rt = true;\n\t\twhile (sibling) {\n\t\t\tif (sibling->fib6_metric == rt->fib6_metric &&\n\t\t\t    rt6_qualify_for_ecmp(sibling)) {\n\t\t\t\tlist_add_tail(&rt->fib6_siblings,\n\t\t\t\t\t      &sibling->fib6_siblings);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsibling = rcu_dereference_protected(sibling->fib6_next,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\tnotify_sibling_rt = false;\n\t\t}\n\t\t/* For each sibling in the list, increment the counter of\n\t\t * siblings. BUG() if counters does not match, list of siblings\n\t\t * is broken!\n\t\t */\n\t\tfib6_nsiblings = 0;\n\t\tlist_for_each_entry_safe(sibling, temp_sibling,\n\t\t\t\t\t &rt->fib6_siblings, fib6_siblings) {\n\t\t\tsibling->fib6_nsiblings++;\n\t\t\tBUG_ON(sibling->fib6_nsiblings != rt->fib6_nsiblings);\n\t\t\tfib6_nsiblings++;\n\t\t}\n\t\tBUG_ON(fib6_nsiblings != rt->fib6_nsiblings);\n\t\trt6_multipath_rebalance(temp_sibling);\n\t}\n\n\t/*\n\t *\tinsert node\n\t */\n\tif (!replace) {\n\t\tif (!add)\n\t\t\tpr_warn(\"NLM_F_CREATE should be set when creating new route\\n\");\n\nadd:\n\t\tnlflags |= NLM_F_CREATE;\n\n\t\t/* The route should only be notified if it is the first\n\t\t * route in the node or if it is added as a sibling\n\t\t * route to the first route in the node.\n\t\t */\n\t\tif (!info->skip_notify_kernel &&\n\t\t    (notify_sibling_rt || ins == &fn->leaf)) {\n\t\t\tenum fib_event_type fib_event;\n\n\t\t\tif (notify_sibling_rt)\n\t\t\t\tfib_event = FIB_EVENT_ENTRY_APPEND;\n\t\t\telse\n\t\t\t\tfib_event = FIB_EVENT_ENTRY_REPLACE;\n\t\t\terr = call_fib6_entry_notifiers(info->nl_net,\n\t\t\t\t\t\t\tfib_event, rt,\n\t\t\t\t\t\t\textack);\n\t\t\tif (err) {\n\t\t\t\tstruct fib6_info *sibling, *next_sibling;\n\n\t\t\t\t/* If the route has siblings, then it first\n\t\t\t\t * needs to be unlinked from them.\n\t\t\t\t */\n\t\t\t\tif (!rt->fib6_nsiblings)\n\t\t\t\t\treturn err;\n\n\t\t\t\tlist_for_each_entry_safe(sibling, next_sibling,\n\t\t\t\t\t\t\t &rt->fib6_siblings,\n\t\t\t\t\t\t\t fib6_siblings)\n\t\t\t\t\tsibling->fib6_nsiblings--;\n\t\t\t\trt->fib6_nsiblings = 0;\n\t\t\t\tlist_del_init(&rt->fib6_siblings);\n\t\t\t\trt6_multipath_rebalance(next_sibling);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\trcu_assign_pointer(rt->fib6_next, iter);\n\t\tfib6_info_hold(rt);\n\t\trcu_assign_pointer(rt->fib6_node, fn);\n\t\trcu_assign_pointer(*ins, rt);\n\t\tif (!info->skip_notify)\n\t\t\tinet6_rt_notify(RTM_NEWROUTE, rt, info, nlflags);\n\t\tinfo->nl_net->ipv6.rt6_stats->fib_rt_entries++;\n\n\t\tif (!(fn->fn_flags & RTN_RTINFO)) {\n\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_route_nodes++;\n\t\t\tfn->fn_flags |= RTN_RTINFO;\n\t\t}\n\n\t} else {\n\t\tint nsiblings;\n\n\t\tif (!found) {\n\t\t\tif (add)\n\t\t\t\tgoto add;\n\t\t\tpr_warn(\"NLM_F_REPLACE set, but no existing node found!\\n\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tif (!info->skip_notify_kernel && ins == &fn->leaf) {\n\t\t\terr = call_fib6_entry_notifiers(info->nl_net,\n\t\t\t\t\t\t\tFIB_EVENT_ENTRY_REPLACE,\n\t\t\t\t\t\t\trt, extack);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tfib6_info_hold(rt);\n\t\trcu_assign_pointer(rt->fib6_node, fn);\n\t\trt->fib6_next = iter->fib6_next;\n\t\trcu_assign_pointer(*ins, rt);\n\t\tif (!info->skip_notify)\n\t\t\tinet6_rt_notify(RTM_NEWROUTE, rt, info, NLM_F_REPLACE);\n\t\tif (!(fn->fn_flags & RTN_RTINFO)) {\n\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_route_nodes++;\n\t\t\tfn->fn_flags |= RTN_RTINFO;\n\t\t}\n\t\tnsiblings = iter->fib6_nsiblings;\n\t\titer->fib6_node = NULL;\n\t\tfib6_purge_rt(iter, fn, info->nl_net);\n\t\tif (rcu_access_pointer(fn->rr_ptr) == iter)\n\t\t\tfn->rr_ptr = NULL;\n\t\tfib6_info_release(iter);\n\n\t\tif (nsiblings) {\n\t\t\t/* Replacing an ECMP route, remove all siblings */\n\t\t\tins = &rt->fib6_next;\n\t\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\twhile (iter) {\n\t\t\t\tif (iter->fib6_metric > rt->fib6_metric)\n\t\t\t\t\tbreak;\n\t\t\t\tif (rt6_qualify_for_ecmp(iter)) {\n\t\t\t\t\t*ins = iter->fib6_next;\n\t\t\t\t\titer->fib6_node = NULL;\n\t\t\t\t\tfib6_purge_rt(iter, fn, info->nl_net);\n\t\t\t\t\tif (rcu_access_pointer(fn->rr_ptr) == iter)\n\t\t\t\t\t\tfn->rr_ptr = NULL;\n\t\t\t\t\tfib6_info_release(iter);\n\t\t\t\t\tnsiblings--;\n\t\t\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_rt_entries--;\n\t\t\t\t} else {\n\t\t\t\t\tins = &iter->fib6_next;\n\t\t\t\t}\n\t\t\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t\tlockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\t}\n\t\t\tWARN_ON(nsiblings != 0);\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -53,10 +53,9 @@\n \t\t\t\tif (!(iter->fib6_flags & RTF_EXPIRES))\n \t\t\t\t\treturn -EEXIST;\n \t\t\t\tif (!(rt->fib6_flags & RTF_EXPIRES))\n-\t\t\t\t\tfib6_clean_expires_locked(iter);\n+\t\t\t\t\tfib6_clean_expires(iter);\n \t\t\t\telse\n-\t\t\t\t\tfib6_set_expires_locked(iter,\n-\t\t\t\t\t\t\t\trt->expires);\n+\t\t\t\t\tfib6_set_expires(iter, rt->expires);\n \n \t\t\t\tif (rt->fib6_pmtu)\n \t\t\t\t\tfib6_metric_set(iter, RTAX_MTU,",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t\tfib6_clean_expires_locked(iter);",
                "\t\t\t\t\tfib6_set_expires_locked(iter,",
                "\t\t\t\t\t\t\t\trt->expires);"
            ],
            "added_lines": [
                "\t\t\t\t\tfib6_clean_expires(iter);",
                "\t\t\t\t\tfib6_set_expires(iter, rt->expires);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/fib6_purge_rt",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "static void fib6_purge_rt(struct fib6_info *rt, struct fib6_node *fn,\n\t\t\t  struct net *net)\n{\n\tstruct fib6_table *table = rt->fib6_table;\n\n\t/* Flush all cached dst in exception table */\n\trt6_flush_exceptions(rt);\n\tfib6_drop_pcpu_from(rt, table);\n\n\tif (rt->nh && !list_empty(&rt->nh_list))\n\t\tlist_del_init(&rt->nh_list);\n\n\tif (refcount_read(&rt->fib6_ref) != 1) {\n\t\t/* This route is used as dummy address holder in some split\n\t\t * nodes. It is not leaked, but it still holds other resources,\n\t\t * which must be released in time. So, scan ascendant nodes\n\t\t * and replace dummy references to this route with references\n\t\t * to still alive ones.\n\t\t */\n\t\twhile (fn) {\n\t\t\tstruct fib6_info *leaf = rcu_dereference_protected(fn->leaf,\n\t\t\t\t\t    lockdep_is_held(&table->tb6_lock));\n\t\t\tstruct fib6_info *new_leaf;\n\t\t\tif (!(fn->fn_flags & RTN_RTINFO) && leaf == rt) {\n\t\t\t\tnew_leaf = fib6_find_prefix(net, table, fn);\n\t\t\t\tfib6_info_hold(new_leaf);\n\n\t\t\t\trcu_assign_pointer(fn->leaf, new_leaf);\n\t\t\t\tfib6_info_release(rt);\n\t\t\t}\n\t\t\tfn = rcu_dereference_protected(fn->parent,\n\t\t\t\t    lockdep_is_held(&table->tb6_lock));\n\t\t}\n\t}\n\n\tfib6_clean_expires_locked(rt);\n}",
        "func": "static void fib6_purge_rt(struct fib6_info *rt, struct fib6_node *fn,\n\t\t\t  struct net *net)\n{\n\tstruct fib6_table *table = rt->fib6_table;\n\n\t/* Flush all cached dst in exception table */\n\trt6_flush_exceptions(rt);\n\tfib6_drop_pcpu_from(rt, table);\n\n\tif (rt->nh && !list_empty(&rt->nh_list))\n\t\tlist_del_init(&rt->nh_list);\n\n\tif (refcount_read(&rt->fib6_ref) != 1) {\n\t\t/* This route is used as dummy address holder in some split\n\t\t * nodes. It is not leaked, but it still holds other resources,\n\t\t * which must be released in time. So, scan ascendant nodes\n\t\t * and replace dummy references to this route with references\n\t\t * to still alive ones.\n\t\t */\n\t\twhile (fn) {\n\t\t\tstruct fib6_info *leaf = rcu_dereference_protected(fn->leaf,\n\t\t\t\t\t    lockdep_is_held(&table->tb6_lock));\n\t\t\tstruct fib6_info *new_leaf;\n\t\t\tif (!(fn->fn_flags & RTN_RTINFO) && leaf == rt) {\n\t\t\t\tnew_leaf = fib6_find_prefix(net, table, fn);\n\t\t\t\tfib6_info_hold(new_leaf);\n\n\t\t\t\trcu_assign_pointer(fn->leaf, new_leaf);\n\t\t\t\tfib6_info_release(rt);\n\t\t\t}\n\t\t\tfn = rcu_dereference_protected(fn->parent,\n\t\t\t\t    lockdep_is_held(&table->tb6_lock));\n\t\t}\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -32,6 +32,4 @@\n \t\t\t\t    lockdep_is_held(&table->tb6_lock));\n \t\t}\n \t}\n-\n-\tfib6_clean_expires_locked(rt);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\tfib6_clean_expires_locked(rt);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/fib6_info_alloc",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "struct fib6_info *fib6_info_alloc(gfp_t gfp_flags, bool with_fib6_nh)\n{\n\tstruct fib6_info *f6i;\n\tsize_t sz = sizeof(*f6i);\n\n\tif (with_fib6_nh)\n\t\tsz += sizeof(struct fib6_nh);\n\n\tf6i = kzalloc(sz, gfp_flags);\n\tif (!f6i)\n\t\treturn NULL;\n\n\t/* fib6_siblings is a union with nh_list, so this initializes both */\n\tINIT_LIST_HEAD(&f6i->fib6_siblings);\n\trefcount_set(&f6i->fib6_ref, 1);\n\n\tINIT_HLIST_NODE(&f6i->gc_link);\n\n\treturn f6i;\n}",
        "func": "struct fib6_info *fib6_info_alloc(gfp_t gfp_flags, bool with_fib6_nh)\n{\n\tstruct fib6_info *f6i;\n\tsize_t sz = sizeof(*f6i);\n\n\tif (with_fib6_nh)\n\t\tsz += sizeof(struct fib6_nh);\n\n\tf6i = kzalloc(sz, gfp_flags);\n\tif (!f6i)\n\t\treturn NULL;\n\n\t/* fib6_siblings is a union with nh_list, so this initializes both */\n\tINIT_LIST_HEAD(&f6i->fib6_siblings);\n\trefcount_set(&f6i->fib6_ref, 1);\n\n\treturn f6i;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,5 @@\n \tINIT_LIST_HEAD(&f6i->fib6_siblings);\n \trefcount_set(&f6i->fib6_ref, 1);\n \n-\tINIT_HLIST_NODE(&f6i->gc_link);\n-\n \treturn f6i;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tINIT_HLIST_NODE(&f6i->gc_link);",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/fib6_add",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "int fib6_add(struct fib6_node *root, struct fib6_info *rt,\n\t     struct nl_info *info, struct netlink_ext_ack *extack)\n{\n\tstruct fib6_table *table = rt->fib6_table;\n\tstruct fib6_node *fn, *pn = NULL;\n\tint err = -ENOMEM;\n\tint allow_create = 1;\n\tint replace_required = 0;\n\n\tif (info->nlh) {\n\t\tif (!(info->nlh->nlmsg_flags & NLM_F_CREATE))\n\t\t\tallow_create = 0;\n\t\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\treplace_required = 1;\n\t}\n\tif (!allow_create && !replace_required)\n\t\tpr_warn(\"RTM_NEWROUTE with no NLM_F_CREATE or NLM_F_REPLACE\\n\");\n\n\tfn = fib6_add_1(info->nl_net, table, root,\n\t\t\t&rt->fib6_dst.addr, rt->fib6_dst.plen,\n\t\t\toffsetof(struct fib6_info, fib6_dst), allow_create,\n\t\t\treplace_required, extack);\n\tif (IS_ERR(fn)) {\n\t\terr = PTR_ERR(fn);\n\t\tfn = NULL;\n\t\tgoto out;\n\t}\n\n\tpn = fn;\n\n#ifdef CONFIG_IPV6_SUBTREES\n\tif (rt->fib6_src.plen) {\n\t\tstruct fib6_node *sn;\n\n\t\tif (!rcu_access_pointer(fn->subtree)) {\n\t\t\tstruct fib6_node *sfn;\n\n\t\t\t/*\n\t\t\t * Create subtree.\n\t\t\t *\n\t\t\t *\t\tfn[main tree]\n\t\t\t *\t\t|\n\t\t\t *\t\tsfn[subtree root]\n\t\t\t *\t\t   \\\n\t\t\t *\t\t    sn[new leaf node]\n\t\t\t */\n\n\t\t\t/* Create subtree root node */\n\t\t\tsfn = node_alloc(info->nl_net);\n\t\t\tif (!sfn)\n\t\t\t\tgoto failure;\n\n\t\t\tfib6_info_hold(info->nl_net->ipv6.fib6_null_entry);\n\t\t\trcu_assign_pointer(sfn->leaf,\n\t\t\t\t\t   info->nl_net->ipv6.fib6_null_entry);\n\t\t\tsfn->fn_flags = RTN_ROOT;\n\n\t\t\t/* Now add the first leaf node to new subtree */\n\n\t\t\tsn = fib6_add_1(info->nl_net, table, sfn,\n\t\t\t\t\t&rt->fib6_src.addr, rt->fib6_src.plen,\n\t\t\t\t\toffsetof(struct fib6_info, fib6_src),\n\t\t\t\t\tallow_create, replace_required, extack);\n\n\t\t\tif (IS_ERR(sn)) {\n\t\t\t\t/* If it is failed, discard just allocated\n\t\t\t\t   root, and then (in failure) stale node\n\t\t\t\t   in main tree.\n\t\t\t\t */\n\t\t\t\tnode_free_immediate(info->nl_net, sfn);\n\t\t\t\terr = PTR_ERR(sn);\n\t\t\t\tgoto failure;\n\t\t\t}\n\n\t\t\t/* Now link new subtree to main tree */\n\t\t\trcu_assign_pointer(sfn->parent, fn);\n\t\t\trcu_assign_pointer(fn->subtree, sfn);\n\t\t} else {\n\t\t\tsn = fib6_add_1(info->nl_net, table, FIB6_SUBTREE(fn),\n\t\t\t\t\t&rt->fib6_src.addr, rt->fib6_src.plen,\n\t\t\t\t\toffsetof(struct fib6_info, fib6_src),\n\t\t\t\t\tallow_create, replace_required, extack);\n\n\t\t\tif (IS_ERR(sn)) {\n\t\t\t\terr = PTR_ERR(sn);\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t}\n\n\t\tif (!rcu_access_pointer(fn->leaf)) {\n\t\t\tif (fn->fn_flags & RTN_TL_ROOT) {\n\t\t\t\t/* put back null_entry for root node */\n\t\t\t\trcu_assign_pointer(fn->leaf,\n\t\t\t\t\t    info->nl_net->ipv6.fib6_null_entry);\n\t\t\t} else {\n\t\t\t\tfib6_info_hold(rt);\n\t\t\t\trcu_assign_pointer(fn->leaf, rt);\n\t\t\t}\n\t\t}\n\t\tfn = sn;\n\t}\n#endif\n\n\terr = fib6_add_rt2node(fn, rt, info, extack);\n\tif (!err) {\n\t\tif (rt->nh)\n\t\t\tlist_add(&rt->nh_list, &rt->nh->f6i_list);\n\t\t__fib6_update_sernum_upto_root(rt, fib6_new_sernum(info->nl_net));\n\n\t\tif (fib6_has_expires(rt))\n\t\t\thlist_add_head(&rt->gc_link, &table->tb6_gc_hlist);\n\n\t\tfib6_start_gc(info->nl_net, rt);\n\t}\n\nout:\n\tif (err) {\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t/*\n\t\t * If fib6_add_1 has cleared the old leaf pointer in the\n\t\t * super-tree leaf node we have to find a new one for it.\n\t\t */\n\t\tif (pn != fn) {\n\t\t\tstruct fib6_info *pn_leaf =\n\t\t\t\trcu_dereference_protected(pn->leaf,\n\t\t\t\t    lockdep_is_held(&table->tb6_lock));\n\t\t\tif (pn_leaf == rt) {\n\t\t\t\tpn_leaf = NULL;\n\t\t\t\tRCU_INIT_POINTER(pn->leaf, NULL);\n\t\t\t\tfib6_info_release(rt);\n\t\t\t}\n\t\t\tif (!pn_leaf && !(pn->fn_flags & RTN_RTINFO)) {\n\t\t\t\tpn_leaf = fib6_find_prefix(info->nl_net, table,\n\t\t\t\t\t\t\t   pn);\n\t\t\t\tif (!pn_leaf)\n\t\t\t\t\tpn_leaf =\n\t\t\t\t\t    info->nl_net->ipv6.fib6_null_entry;\n\t\t\t\tfib6_info_hold(pn_leaf);\n\t\t\t\trcu_assign_pointer(pn->leaf, pn_leaf);\n\t\t\t}\n\t\t}\n#endif\n\t\tgoto failure;\n\t} else if (fib6_requires_src(rt)) {\n\t\tfib6_routes_require_src_inc(info->nl_net);\n\t}\n\treturn err;\n\nfailure:\n\t/* fn->leaf could be NULL and fib6_repair_tree() needs to be called if:\n\t * 1. fn is an intermediate node and we failed to add the new\n\t * route to it in both subtree creation failure and fib6_add_rt2node()\n\t * failure case.\n\t * 2. fn is the root node in the table and we fail to add the first\n\t * default route to it.\n\t */\n\tif (fn &&\n\t    (!(fn->fn_flags & (RTN_RTINFO|RTN_ROOT)) ||\n\t     (fn->fn_flags & RTN_TL_ROOT &&\n\t      !rcu_access_pointer(fn->leaf))))\n\t\tfib6_repair_tree(info->nl_net, table, fn);\n\treturn err;\n}",
        "func": "int fib6_add(struct fib6_node *root, struct fib6_info *rt,\n\t     struct nl_info *info, struct netlink_ext_ack *extack)\n{\n\tstruct fib6_table *table = rt->fib6_table;\n\tstruct fib6_node *fn, *pn = NULL;\n\tint err = -ENOMEM;\n\tint allow_create = 1;\n\tint replace_required = 0;\n\n\tif (info->nlh) {\n\t\tif (!(info->nlh->nlmsg_flags & NLM_F_CREATE))\n\t\t\tallow_create = 0;\n\t\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\treplace_required = 1;\n\t}\n\tif (!allow_create && !replace_required)\n\t\tpr_warn(\"RTM_NEWROUTE with no NLM_F_CREATE or NLM_F_REPLACE\\n\");\n\n\tfn = fib6_add_1(info->nl_net, table, root,\n\t\t\t&rt->fib6_dst.addr, rt->fib6_dst.plen,\n\t\t\toffsetof(struct fib6_info, fib6_dst), allow_create,\n\t\t\treplace_required, extack);\n\tif (IS_ERR(fn)) {\n\t\terr = PTR_ERR(fn);\n\t\tfn = NULL;\n\t\tgoto out;\n\t}\n\n\tpn = fn;\n\n#ifdef CONFIG_IPV6_SUBTREES\n\tif (rt->fib6_src.plen) {\n\t\tstruct fib6_node *sn;\n\n\t\tif (!rcu_access_pointer(fn->subtree)) {\n\t\t\tstruct fib6_node *sfn;\n\n\t\t\t/*\n\t\t\t * Create subtree.\n\t\t\t *\n\t\t\t *\t\tfn[main tree]\n\t\t\t *\t\t|\n\t\t\t *\t\tsfn[subtree root]\n\t\t\t *\t\t   \\\n\t\t\t *\t\t    sn[new leaf node]\n\t\t\t */\n\n\t\t\t/* Create subtree root node */\n\t\t\tsfn = node_alloc(info->nl_net);\n\t\t\tif (!sfn)\n\t\t\t\tgoto failure;\n\n\t\t\tfib6_info_hold(info->nl_net->ipv6.fib6_null_entry);\n\t\t\trcu_assign_pointer(sfn->leaf,\n\t\t\t\t\t   info->nl_net->ipv6.fib6_null_entry);\n\t\t\tsfn->fn_flags = RTN_ROOT;\n\n\t\t\t/* Now add the first leaf node to new subtree */\n\n\t\t\tsn = fib6_add_1(info->nl_net, table, sfn,\n\t\t\t\t\t&rt->fib6_src.addr, rt->fib6_src.plen,\n\t\t\t\t\toffsetof(struct fib6_info, fib6_src),\n\t\t\t\t\tallow_create, replace_required, extack);\n\n\t\t\tif (IS_ERR(sn)) {\n\t\t\t\t/* If it is failed, discard just allocated\n\t\t\t\t   root, and then (in failure) stale node\n\t\t\t\t   in main tree.\n\t\t\t\t */\n\t\t\t\tnode_free_immediate(info->nl_net, sfn);\n\t\t\t\terr = PTR_ERR(sn);\n\t\t\t\tgoto failure;\n\t\t\t}\n\n\t\t\t/* Now link new subtree to main tree */\n\t\t\trcu_assign_pointer(sfn->parent, fn);\n\t\t\trcu_assign_pointer(fn->subtree, sfn);\n\t\t} else {\n\t\t\tsn = fib6_add_1(info->nl_net, table, FIB6_SUBTREE(fn),\n\t\t\t\t\t&rt->fib6_src.addr, rt->fib6_src.plen,\n\t\t\t\t\toffsetof(struct fib6_info, fib6_src),\n\t\t\t\t\tallow_create, replace_required, extack);\n\n\t\t\tif (IS_ERR(sn)) {\n\t\t\t\terr = PTR_ERR(sn);\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t}\n\n\t\tif (!rcu_access_pointer(fn->leaf)) {\n\t\t\tif (fn->fn_flags & RTN_TL_ROOT) {\n\t\t\t\t/* put back null_entry for root node */\n\t\t\t\trcu_assign_pointer(fn->leaf,\n\t\t\t\t\t    info->nl_net->ipv6.fib6_null_entry);\n\t\t\t} else {\n\t\t\t\tfib6_info_hold(rt);\n\t\t\t\trcu_assign_pointer(fn->leaf, rt);\n\t\t\t}\n\t\t}\n\t\tfn = sn;\n\t}\n#endif\n\n\terr = fib6_add_rt2node(fn, rt, info, extack);\n\tif (!err) {\n\t\tif (rt->nh)\n\t\t\tlist_add(&rt->nh_list, &rt->nh->f6i_list);\n\t\t__fib6_update_sernum_upto_root(rt, fib6_new_sernum(info->nl_net));\n\t\tfib6_start_gc(info->nl_net, rt);\n\t}\n\nout:\n\tif (err) {\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t/*\n\t\t * If fib6_add_1 has cleared the old leaf pointer in the\n\t\t * super-tree leaf node we have to find a new one for it.\n\t\t */\n\t\tif (pn != fn) {\n\t\t\tstruct fib6_info *pn_leaf =\n\t\t\t\trcu_dereference_protected(pn->leaf,\n\t\t\t\t    lockdep_is_held(&table->tb6_lock));\n\t\t\tif (pn_leaf == rt) {\n\t\t\t\tpn_leaf = NULL;\n\t\t\t\tRCU_INIT_POINTER(pn->leaf, NULL);\n\t\t\t\tfib6_info_release(rt);\n\t\t\t}\n\t\t\tif (!pn_leaf && !(pn->fn_flags & RTN_RTINFO)) {\n\t\t\t\tpn_leaf = fib6_find_prefix(info->nl_net, table,\n\t\t\t\t\t\t\t   pn);\n\t\t\t\tif (!pn_leaf)\n\t\t\t\t\tpn_leaf =\n\t\t\t\t\t    info->nl_net->ipv6.fib6_null_entry;\n\t\t\t\tfib6_info_hold(pn_leaf);\n\t\t\t\trcu_assign_pointer(pn->leaf, pn_leaf);\n\t\t\t}\n\t\t}\n#endif\n\t\tgoto failure;\n\t} else if (fib6_requires_src(rt)) {\n\t\tfib6_routes_require_src_inc(info->nl_net);\n\t}\n\treturn err;\n\nfailure:\n\t/* fn->leaf could be NULL and fib6_repair_tree() needs to be called if:\n\t * 1. fn is an intermediate node and we failed to add the new\n\t * route to it in both subtree creation failure and fib6_add_rt2node()\n\t * failure case.\n\t * 2. fn is the root node in the table and we fail to add the first\n\t * default route to it.\n\t */\n\tif (fn &&\n\t    (!(fn->fn_flags & (RTN_RTINFO|RTN_ROOT)) ||\n\t     (fn->fn_flags & RTN_TL_ROOT &&\n\t      !rcu_access_pointer(fn->leaf))))\n\t\tfib6_repair_tree(info->nl_net, table, fn);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -106,10 +106,6 @@\n \t\tif (rt->nh)\n \t\t\tlist_add(&rt->nh_list, &rt->nh->f6i_list);\n \t\t__fib6_update_sernum_upto_root(rt, fib6_new_sernum(info->nl_net));\n-\n-\t\tif (fib6_has_expires(rt))\n-\t\t\thlist_add_head(&rt->gc_link, &table->tb6_gc_hlist);\n-\n \t\tfib6_start_gc(info->nl_net, rt);\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\t\tif (fib6_has_expires(rt))",
                "\t\t\thlist_add_head(&rt->gc_link, &table->tb6_gc_hlist);",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/fib6_run_gc",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "void fib6_run_gc(unsigned long expires, struct net *net, bool force)\n{\n\tstruct fib6_gc_args gc_args;\n\tunsigned long now;\n\n\tif (force) {\n\t\tspin_lock_bh(&net->ipv6.fib6_gc_lock);\n\t} else if (!spin_trylock_bh(&net->ipv6.fib6_gc_lock)) {\n\t\tmod_timer(&net->ipv6.ip6_fib_timer, jiffies + HZ);\n\t\treturn;\n\t}\n\tgc_args.timeout = expires ? (int)expires :\n\t\t\t  net->ipv6.sysctl.ip6_rt_gc_interval;\n\tgc_args.more = 0;\n\n\tfib6_gc_all(net, &gc_args);\n\tnow = jiffies;\n\tnet->ipv6.ip6_rt_last_gc = now;\n\n\tif (gc_args.more)\n\t\tmod_timer(&net->ipv6.ip6_fib_timer,\n\t\t\t  round_jiffies(now\n\t\t\t\t\t+ net->ipv6.sysctl.ip6_rt_gc_interval));\n\telse\n\t\tdel_timer(&net->ipv6.ip6_fib_timer);\n\tspin_unlock_bh(&net->ipv6.fib6_gc_lock);\n}",
        "func": "void fib6_run_gc(unsigned long expires, struct net *net, bool force)\n{\n\tstruct fib6_gc_args gc_args;\n\tunsigned long now;\n\n\tif (force) {\n\t\tspin_lock_bh(&net->ipv6.fib6_gc_lock);\n\t} else if (!spin_trylock_bh(&net->ipv6.fib6_gc_lock)) {\n\t\tmod_timer(&net->ipv6.ip6_fib_timer, jiffies + HZ);\n\t\treturn;\n\t}\n\tgc_args.timeout = expires ? (int)expires :\n\t\t\t  net->ipv6.sysctl.ip6_rt_gc_interval;\n\tgc_args.more = 0;\n\n\tfib6_clean_all(net, fib6_age, &gc_args);\n\tnow = jiffies;\n\tnet->ipv6.ip6_rt_last_gc = now;\n\n\tif (gc_args.more)\n\t\tmod_timer(&net->ipv6.ip6_fib_timer,\n\t\t\t  round_jiffies(now\n\t\t\t\t\t+ net->ipv6.sysctl.ip6_rt_gc_interval));\n\telse\n\t\tdel_timer(&net->ipv6.ip6_fib_timer);\n\tspin_unlock_bh(&net->ipv6.fib6_gc_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n \t\t\t  net->ipv6.sysctl.ip6_rt_gc_interval;\n \tgc_args.more = 0;\n \n-\tfib6_gc_all(net, &gc_args);\n+\tfib6_clean_all(net, fib6_age, &gc_args);\n \tnow = jiffies;\n \tnet->ipv6.ip6_rt_last_gc = now;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tfib6_gc_all(net, &gc_args);"
            ],
            "added_lines": [
                "\tfib6_clean_all(net, fib6_age, &gc_args);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/fib6_age",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "static int fib6_age(struct fib6_info *rt, struct fib6_gc_args *gc_args)\n{\n\tunsigned long now = jiffies;\n\n\t/*\n\t *\tcheck addrconf expiration here.\n\t *\tRoutes are expired even if they are in use.\n\t */\n\n\tif (fib6_has_expires(rt) && rt->expires) {\n\t\tif (time_after(now, rt->expires)) {\n\t\t\tRT6_TRACE(\"expiring %p\\n\", rt);\n\t\t\treturn -1;\n\t\t}\n\t\tgc_args->more++;\n\t}\n\n\t/*\tAlso age clones in the exception table.\n\t *\tNote, that clones are aged out\n\t *\tonly if they are not in use now.\n\t */\n\trt6_age_exceptions(rt, gc_args, now);\n\n\treturn 0;\n}",
        "func": "static int fib6_age(struct fib6_info *rt, void *arg)\n{\n\tstruct fib6_gc_args *gc_args = arg;\n\tunsigned long now = jiffies;\n\n\t/*\n\t *\tcheck addrconf expiration here.\n\t *\tRoutes are expired even if they are in use.\n\t */\n\n\tif (rt->fib6_flags & RTF_EXPIRES && rt->expires) {\n\t\tif (time_after(now, rt->expires)) {\n\t\t\tRT6_TRACE(\"expiring %p\\n\", rt);\n\t\t\treturn -1;\n\t\t}\n\t\tgc_args->more++;\n\t}\n\n\t/*\tAlso age clones in the exception table.\n\t *\tNote, that clones are aged out\n\t *\tonly if they are not in use now.\n\t */\n\trt6_age_exceptions(rt, gc_args, now);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n-static int fib6_age(struct fib6_info *rt, struct fib6_gc_args *gc_args)\n+static int fib6_age(struct fib6_info *rt, void *arg)\n {\n+\tstruct fib6_gc_args *gc_args = arg;\n \tunsigned long now = jiffies;\n \n \t/*\n@@ -7,7 +8,7 @@\n \t *\tRoutes are expired even if they are in use.\n \t */\n \n-\tif (fib6_has_expires(rt) && rt->expires) {\n+\tif (rt->fib6_flags & RTF_EXPIRES && rt->expires) {\n \t\tif (time_after(now, rt->expires)) {\n \t\t\tRT6_TRACE(\"expiring %p\\n\", rt);\n \t\t\treturn -1;",
        "diff_line_info": {
            "deleted_lines": [
                "static int fib6_age(struct fib6_info *rt, struct fib6_gc_args *gc_args)",
                "\tif (fib6_has_expires(rt) && rt->expires) {"
            ],
            "added_lines": [
                "static int fib6_age(struct fib6_info *rt, void *arg)",
                "\tstruct fib6_gc_args *gc_args = arg;",
                "\tif (rt->fib6_flags & RTF_EXPIRES && rt->expires) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/ip6_route_info_create",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "static struct fib6_info *ip6_route_info_create(struct fib6_config *cfg,\n\t\t\t\t\t      gfp_t gfp_flags,\n\t\t\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct net *net = cfg->fc_nlinfo.nl_net;\n\tstruct fib6_info *rt = NULL;\n\tstruct nexthop *nh = NULL;\n\tstruct fib6_table *table;\n\tstruct fib6_nh *fib6_nh;\n\tint err = -EINVAL;\n\tint addr_type;\n\n\t/* RTF_PCPU is an internal flag; can not be set by userspace */\n\tif (cfg->fc_flags & RTF_PCPU) {\n\t\tNL_SET_ERR_MSG(extack, \"Userspace can not set RTF_PCPU\");\n\t\tgoto out;\n\t}\n\n\t/* RTF_CACHE is an internal flag; can not be set by userspace */\n\tif (cfg->fc_flags & RTF_CACHE) {\n\t\tNL_SET_ERR_MSG(extack, \"Userspace can not set RTF_CACHE\");\n\t\tgoto out;\n\t}\n\n\tif (cfg->fc_type > RTN_MAX) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid route type\");\n\t\tgoto out;\n\t}\n\n\tif (cfg->fc_dst_len > 128) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid prefix length\");\n\t\tgoto out;\n\t}\n\tif (cfg->fc_src_len > 128) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid source address length\");\n\t\tgoto out;\n\t}\n#ifndef CONFIG_IPV6_SUBTREES\n\tif (cfg->fc_src_len) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Specifying source address requires IPV6_SUBTREES to be enabled\");\n\t\tgoto out;\n\t}\n#endif\n\tif (cfg->fc_nh_id) {\n\t\tnh = nexthop_find_by_id(net, cfg->fc_nh_id);\n\t\tif (!nh) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop id does not exist\");\n\t\t\tgoto out;\n\t\t}\n\t\terr = fib6_check_nexthop(nh, cfg, extack);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = -ENOBUFS;\n\tif (cfg->fc_nlinfo.nlh &&\n\t    !(cfg->fc_nlinfo.nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\ttable = fib6_get_table(net, cfg->fc_table);\n\t\tif (!table) {\n\t\t\tpr_warn(\"NLM_F_CREATE should be specified when creating new route\\n\");\n\t\t\ttable = fib6_new_table(net, cfg->fc_table);\n\t\t}\n\t} else {\n\t\ttable = fib6_new_table(net, cfg->fc_table);\n\t}\n\n\tif (!table)\n\t\tgoto out;\n\n\terr = -ENOMEM;\n\trt = fib6_info_alloc(gfp_flags, !nh);\n\tif (!rt)\n\t\tgoto out;\n\n\trt->fib6_metrics = ip_fib_metrics_init(net, cfg->fc_mx, cfg->fc_mx_len,\n\t\t\t\t\t       extack);\n\tif (IS_ERR(rt->fib6_metrics)) {\n\t\terr = PTR_ERR(rt->fib6_metrics);\n\t\t/* Do not leave garbage there. */\n\t\trt->fib6_metrics = (struct dst_metrics *)&dst_default_metrics;\n\t\tgoto out_free;\n\t}\n\n\tif (cfg->fc_flags & RTF_ADDRCONF)\n\t\trt->dst_nocount = true;\n\n\tif (cfg->fc_flags & RTF_EXPIRES)\n\t\tfib6_set_expires_locked(rt, jiffies +\n\t\t\t\t\tclock_t_to_jiffies(cfg->fc_expires));\n\telse\n\t\tfib6_clean_expires_locked(rt);\n\n\tif (cfg->fc_protocol == RTPROT_UNSPEC)\n\t\tcfg->fc_protocol = RTPROT_BOOT;\n\trt->fib6_protocol = cfg->fc_protocol;\n\n\trt->fib6_table = table;\n\trt->fib6_metric = cfg->fc_metric;\n\trt->fib6_type = cfg->fc_type ? : RTN_UNICAST;\n\trt->fib6_flags = cfg->fc_flags & ~RTF_GATEWAY;\n\n\tipv6_addr_prefix(&rt->fib6_dst.addr, &cfg->fc_dst, cfg->fc_dst_len);\n\trt->fib6_dst.plen = cfg->fc_dst_len;\n\n#ifdef CONFIG_IPV6_SUBTREES\n\tipv6_addr_prefix(&rt->fib6_src.addr, &cfg->fc_src, cfg->fc_src_len);\n\trt->fib6_src.plen = cfg->fc_src_len;\n#endif\n\tif (nh) {\n\t\tif (rt->fib6_src.plen) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthops can not be used with source routing\");\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (!nexthop_get(nh)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop has been deleted\");\n\t\t\tgoto out_free;\n\t\t}\n\t\trt->nh = nh;\n\t\tfib6_nh = nexthop_fib6_nh(rt->nh);\n\t} else {\n\t\terr = fib6_nh_init(net, rt->fib6_nh, cfg, gfp_flags, extack);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tfib6_nh = rt->fib6_nh;\n\n\t\t/* We cannot add true routes via loopback here, they would\n\t\t * result in kernel looping; promote them to reject routes\n\t\t */\n\t\taddr_type = ipv6_addr_type(&cfg->fc_dst);\n\t\tif (fib6_is_reject(cfg->fc_flags, rt->fib6_nh->fib_nh_dev,\n\t\t\t\t   addr_type))\n\t\t\trt->fib6_flags = RTF_REJECT | RTF_NONEXTHOP;\n\t}\n\n\tif (!ipv6_addr_any(&cfg->fc_prefsrc)) {\n\t\tstruct net_device *dev = fib6_nh->fib_nh_dev;\n\n\t\tif (!ipv6_chk_addr(net, &cfg->fc_prefsrc, dev, 0)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid source address\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\trt->fib6_prefsrc.addr = cfg->fc_prefsrc;\n\t\trt->fib6_prefsrc.plen = 128;\n\t} else\n\t\trt->fib6_prefsrc.plen = 0;\n\n\treturn rt;\nout:\n\tfib6_info_release(rt);\n\treturn ERR_PTR(err);\nout_free:\n\tip_fib_metrics_put(rt->fib6_metrics);\n\tkfree(rt);\n\treturn ERR_PTR(err);\n}",
        "func": "static struct fib6_info *ip6_route_info_create(struct fib6_config *cfg,\n\t\t\t\t\t      gfp_t gfp_flags,\n\t\t\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct net *net = cfg->fc_nlinfo.nl_net;\n\tstruct fib6_info *rt = NULL;\n\tstruct nexthop *nh = NULL;\n\tstruct fib6_table *table;\n\tstruct fib6_nh *fib6_nh;\n\tint err = -EINVAL;\n\tint addr_type;\n\n\t/* RTF_PCPU is an internal flag; can not be set by userspace */\n\tif (cfg->fc_flags & RTF_PCPU) {\n\t\tNL_SET_ERR_MSG(extack, \"Userspace can not set RTF_PCPU\");\n\t\tgoto out;\n\t}\n\n\t/* RTF_CACHE is an internal flag; can not be set by userspace */\n\tif (cfg->fc_flags & RTF_CACHE) {\n\t\tNL_SET_ERR_MSG(extack, \"Userspace can not set RTF_CACHE\");\n\t\tgoto out;\n\t}\n\n\tif (cfg->fc_type > RTN_MAX) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid route type\");\n\t\tgoto out;\n\t}\n\n\tif (cfg->fc_dst_len > 128) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid prefix length\");\n\t\tgoto out;\n\t}\n\tif (cfg->fc_src_len > 128) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid source address length\");\n\t\tgoto out;\n\t}\n#ifndef CONFIG_IPV6_SUBTREES\n\tif (cfg->fc_src_len) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Specifying source address requires IPV6_SUBTREES to be enabled\");\n\t\tgoto out;\n\t}\n#endif\n\tif (cfg->fc_nh_id) {\n\t\tnh = nexthop_find_by_id(net, cfg->fc_nh_id);\n\t\tif (!nh) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop id does not exist\");\n\t\t\tgoto out;\n\t\t}\n\t\terr = fib6_check_nexthop(nh, cfg, extack);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = -ENOBUFS;\n\tif (cfg->fc_nlinfo.nlh &&\n\t    !(cfg->fc_nlinfo.nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\ttable = fib6_get_table(net, cfg->fc_table);\n\t\tif (!table) {\n\t\t\tpr_warn(\"NLM_F_CREATE should be specified when creating new route\\n\");\n\t\t\ttable = fib6_new_table(net, cfg->fc_table);\n\t\t}\n\t} else {\n\t\ttable = fib6_new_table(net, cfg->fc_table);\n\t}\n\n\tif (!table)\n\t\tgoto out;\n\n\terr = -ENOMEM;\n\trt = fib6_info_alloc(gfp_flags, !nh);\n\tif (!rt)\n\t\tgoto out;\n\n\trt->fib6_metrics = ip_fib_metrics_init(net, cfg->fc_mx, cfg->fc_mx_len,\n\t\t\t\t\t       extack);\n\tif (IS_ERR(rt->fib6_metrics)) {\n\t\terr = PTR_ERR(rt->fib6_metrics);\n\t\t/* Do not leave garbage there. */\n\t\trt->fib6_metrics = (struct dst_metrics *)&dst_default_metrics;\n\t\tgoto out_free;\n\t}\n\n\tif (cfg->fc_flags & RTF_ADDRCONF)\n\t\trt->dst_nocount = true;\n\n\tif (cfg->fc_flags & RTF_EXPIRES)\n\t\tfib6_set_expires(rt, jiffies +\n\t\t\t\tclock_t_to_jiffies(cfg->fc_expires));\n\telse\n\t\tfib6_clean_expires(rt);\n\n\tif (cfg->fc_protocol == RTPROT_UNSPEC)\n\t\tcfg->fc_protocol = RTPROT_BOOT;\n\trt->fib6_protocol = cfg->fc_protocol;\n\n\trt->fib6_table = table;\n\trt->fib6_metric = cfg->fc_metric;\n\trt->fib6_type = cfg->fc_type ? : RTN_UNICAST;\n\trt->fib6_flags = cfg->fc_flags & ~RTF_GATEWAY;\n\n\tipv6_addr_prefix(&rt->fib6_dst.addr, &cfg->fc_dst, cfg->fc_dst_len);\n\trt->fib6_dst.plen = cfg->fc_dst_len;\n\n#ifdef CONFIG_IPV6_SUBTREES\n\tipv6_addr_prefix(&rt->fib6_src.addr, &cfg->fc_src, cfg->fc_src_len);\n\trt->fib6_src.plen = cfg->fc_src_len;\n#endif\n\tif (nh) {\n\t\tif (rt->fib6_src.plen) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthops can not be used with source routing\");\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (!nexthop_get(nh)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop has been deleted\");\n\t\t\tgoto out_free;\n\t\t}\n\t\trt->nh = nh;\n\t\tfib6_nh = nexthop_fib6_nh(rt->nh);\n\t} else {\n\t\terr = fib6_nh_init(net, rt->fib6_nh, cfg, gfp_flags, extack);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tfib6_nh = rt->fib6_nh;\n\n\t\t/* We cannot add true routes via loopback here, they would\n\t\t * result in kernel looping; promote them to reject routes\n\t\t */\n\t\taddr_type = ipv6_addr_type(&cfg->fc_dst);\n\t\tif (fib6_is_reject(cfg->fc_flags, rt->fib6_nh->fib_nh_dev,\n\t\t\t\t   addr_type))\n\t\t\trt->fib6_flags = RTF_REJECT | RTF_NONEXTHOP;\n\t}\n\n\tif (!ipv6_addr_any(&cfg->fc_prefsrc)) {\n\t\tstruct net_device *dev = fib6_nh->fib_nh_dev;\n\n\t\tif (!ipv6_chk_addr(net, &cfg->fc_prefsrc, dev, 0)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid source address\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\trt->fib6_prefsrc.addr = cfg->fc_prefsrc;\n\t\trt->fib6_prefsrc.plen = 128;\n\t} else\n\t\trt->fib6_prefsrc.plen = 0;\n\n\treturn rt;\nout:\n\tfib6_info_release(rt);\n\treturn ERR_PTR(err);\nout_free:\n\tip_fib_metrics_put(rt->fib6_metrics);\n\tkfree(rt);\n\treturn ERR_PTR(err);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -86,10 +86,10 @@\n \t\trt->dst_nocount = true;\n \n \tif (cfg->fc_flags & RTF_EXPIRES)\n-\t\tfib6_set_expires_locked(rt, jiffies +\n-\t\t\t\t\tclock_t_to_jiffies(cfg->fc_expires));\n+\t\tfib6_set_expires(rt, jiffies +\n+\t\t\t\tclock_t_to_jiffies(cfg->fc_expires));\n \telse\n-\t\tfib6_clean_expires_locked(rt);\n+\t\tfib6_clean_expires(rt);\n \n \tif (cfg->fc_protocol == RTPROT_UNSPEC)\n \t\tcfg->fc_protocol = RTPROT_BOOT;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tfib6_set_expires_locked(rt, jiffies +",
                "\t\t\t\t\tclock_t_to_jiffies(cfg->fc_expires));",
                "\t\tfib6_clean_expires_locked(rt);"
            ],
            "added_lines": [
                "\t\tfib6_set_expires(rt, jiffies +",
                "\t\t\t\tclock_t_to_jiffies(cfg->fc_expires));",
                "\t\tfib6_clean_expires(rt);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/fib6_clean_expires",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "static inline void fib6_clean_expires(struct fib6_info *f6i)\n{\n\tspin_lock_bh(&f6i->fib6_table->tb6_lock);\n\tfib6_clean_expires_locked(f6i);\n\tspin_unlock_bh(&f6i->fib6_table->tb6_lock);\n}",
        "func": "static inline void fib6_clean_expires(struct fib6_info *f6i)\n{\n\tf6i->fib6_flags &= ~RTF_EXPIRES;\n\tf6i->expires = 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,5 @@\n static inline void fib6_clean_expires(struct fib6_info *f6i)\n {\n-\tspin_lock_bh(&f6i->fib6_table->tb6_lock);\n-\tfib6_clean_expires_locked(f6i);\n-\tspin_unlock_bh(&f6i->fib6_table->tb6_lock);\n+\tf6i->fib6_flags &= ~RTF_EXPIRES;\n+\tf6i->expires = 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tspin_lock_bh(&f6i->fib6_table->tb6_lock);",
                "\tfib6_clean_expires_locked(f6i);",
                "\tspin_unlock_bh(&f6i->fib6_table->tb6_lock);"
            ],
            "added_lines": [
                "\tf6i->fib6_flags &= ~RTF_EXPIRES;",
                "\tf6i->expires = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6200",
        "func_name": "torvalds/linux/fib6_set_expires",
        "description": "A race condition was found in the Linux Kernel. Under certain conditions, an unauthenticated attacker from an adjacent network could send an ICMPv6 router advertisement packet, causing arbitrary code execution.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dade3f6a1e4e",
        "commit_title": "This reverts commit 3dec89b14d37ee635e772636dad3f09f78f1ab87.",
        "commit_text": " The commit has some race conditions given how expires is managed on a fib6_info in relation to gc start, adding the entry to the gc list and setting the timer value leading to UAF. Revert the commit and try again in a later release.  Cc: Kui-Feng Lee <thinker.li@gmail.com> Link: https://lore.kernel.org/r/20231219030243.25687-1-dsahern@kernel.org ",
        "func_before": "static inline void fib6_set_expires(struct fib6_info *f6i,\n\t\t\t\t    unsigned long expires)\n{\n\tspin_lock_bh(&f6i->fib6_table->tb6_lock);\n\tfib6_set_expires_locked(f6i, expires);\n\tspin_unlock_bh(&f6i->fib6_table->tb6_lock);\n}",
        "func": "static inline void fib6_set_expires(struct fib6_info *f6i,\n\t\t\t\t    unsigned long expires)\n{\n\tf6i->expires = expires;\n\tf6i->fib6_flags |= RTF_EXPIRES;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,6 @@\n static inline void fib6_set_expires(struct fib6_info *f6i,\n \t\t\t\t    unsigned long expires)\n {\n-\tspin_lock_bh(&f6i->fib6_table->tb6_lock);\n-\tfib6_set_expires_locked(f6i, expires);\n-\tspin_unlock_bh(&f6i->fib6_table->tb6_lock);\n+\tf6i->expires = expires;\n+\tf6i->fib6_flags |= RTF_EXPIRES;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tspin_lock_bh(&f6i->fib6_table->tb6_lock);",
                "\tfib6_set_expires_locked(f6i, expires);",
                "\tspin_unlock_bh(&f6i->fib6_table->tb6_lock);"
            ],
            "added_lines": [
                "\tf6i->expires = expires;",
                "\tf6i->fib6_flags |= RTF_EXPIRES;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smcr_link_down",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "static void smcr_link_down(struct smc_link *lnk)\n{\n\tstruct smc_link_group *lgr = lnk->lgr;\n\tstruct smc_link *to_lnk;\n\tint del_link_id;\n\n\tif (!lgr || lnk->state == SMC_LNK_UNUSED || list_empty(&lgr->list))\n\t\treturn;\n\n\tsmc_ib_modify_qp_reset(lnk);\n\tto_lnk = smc_switch_conns(lgr, lnk, true);\n\tif (!to_lnk) { /* no backup link available */\n\t\tsmcr_link_clear(lnk, true);\n\t\treturn;\n\t}\n\tsmcr_lgr_set_type(lgr, SMC_LGR_SINGLE);\n\tdel_link_id = lnk->link_id;\n\n\tif (lgr->role == SMC_SERV) {\n\t\t/* trigger local delete link processing */\n\t\tsmc_llc_srv_delete_link_local(to_lnk, del_link_id);\n\t} else {\n\t\tif (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {\n\t\t\t/* another llc task is ongoing */\n\t\t\tmutex_unlock(&lgr->llc_conf_mutex);\n\t\t\twait_event_timeout(lgr->llc_flow_waiter,\n\t\t\t\t(list_empty(&lgr->list) ||\n\t\t\t\t lgr->llc_flow_lcl.type == SMC_LLC_FLOW_NONE),\n\t\t\t\tSMC_LLC_WAIT_TIME);\n\t\t\tmutex_lock(&lgr->llc_conf_mutex);\n\t\t}\n\t\tif (!list_empty(&lgr->list)) {\n\t\t\tsmc_llc_send_delete_link(to_lnk, del_link_id,\n\t\t\t\t\t\t SMC_LLC_REQ, true,\n\t\t\t\t\t\t SMC_LLC_DEL_LOST_PATH);\n\t\t\tsmcr_link_clear(lnk, true);\n\t\t}\n\t\twake_up(&lgr->llc_flow_waiter);\t/* wake up next waiter */\n\t}\n}",
        "func": "static void smcr_link_down(struct smc_link *lnk)\n{\n\tstruct smc_link_group *lgr = lnk->lgr;\n\tstruct smc_link *to_lnk;\n\tint del_link_id;\n\n\tif (!lgr || lnk->state == SMC_LNK_UNUSED || list_empty(&lgr->list))\n\t\treturn;\n\n\tto_lnk = smc_switch_conns(lgr, lnk, true);\n\tif (!to_lnk) { /* no backup link available */\n\t\tsmcr_link_clear(lnk, true);\n\t\treturn;\n\t}\n\tsmcr_lgr_set_type(lgr, SMC_LGR_SINGLE);\n\tdel_link_id = lnk->link_id;\n\n\tif (lgr->role == SMC_SERV) {\n\t\t/* trigger local delete link processing */\n\t\tsmc_llc_srv_delete_link_local(to_lnk, del_link_id);\n\t} else {\n\t\tif (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {\n\t\t\t/* another llc task is ongoing */\n\t\t\tmutex_unlock(&lgr->llc_conf_mutex);\n\t\t\twait_event_timeout(lgr->llc_flow_waiter,\n\t\t\t\t(list_empty(&lgr->list) ||\n\t\t\t\t lgr->llc_flow_lcl.type == SMC_LLC_FLOW_NONE),\n\t\t\t\tSMC_LLC_WAIT_TIME);\n\t\t\tmutex_lock(&lgr->llc_conf_mutex);\n\t\t}\n\t\tif (!list_empty(&lgr->list)) {\n\t\t\tsmc_llc_send_delete_link(to_lnk, del_link_id,\n\t\t\t\t\t\t SMC_LLC_REQ, true,\n\t\t\t\t\t\t SMC_LLC_DEL_LOST_PATH);\n\t\t\tsmcr_link_clear(lnk, true);\n\t\t}\n\t\twake_up(&lgr->llc_flow_waiter);\t/* wake up next waiter */\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,6 @@\n \tif (!lgr || lnk->state == SMC_LNK_UNUSED || list_empty(&lgr->list))\n \t\treturn;\n \n-\tsmc_ib_modify_qp_reset(lnk);\n \tto_lnk = smc_switch_conns(lgr, lnk, true);\n \tif (!to_lnk) { /* no backup link available */\n \t\tsmcr_link_clear(lnk, true);",
        "diff_line_info": {
            "deleted_lines": [
                "\tsmc_ib_modify_qp_reset(lnk);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smc_conn_kill",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "static void smc_conn_kill(struct smc_connection *conn, bool soft)\n{\n\tstruct smc_sock *smc = container_of(conn, struct smc_sock, conn);\n\n\tif (conn->lgr->is_smcd && conn->lgr->peer_shutdown)\n\t\tconn->local_tx_ctrl.conn_state_flags.peer_conn_abort = 1;\n\telse\n\t\tsmc_close_abort(conn);\n\tconn->killed = 1;\n\tsmc->sk.sk_err = ECONNABORTED;\n\tsmc_sk_wake_ups(smc);\n\tif (conn->lgr->is_smcd) {\n\t\tsmc_ism_unset_conn(conn);\n\t\tif (soft)\n\t\t\ttasklet_kill(&conn->rx_tsklet);\n\t\telse\n\t\t\ttasklet_unlock_wait(&conn->rx_tsklet);\n\t} else {\n\t\tsmc_cdc_tx_dismiss_slots(conn);\n\t}\n\tsmc_lgr_unregister_conn(conn);\n\tsmc_close_active_abort(smc);\n}",
        "func": "static void smc_conn_kill(struct smc_connection *conn, bool soft)\n{\n\tstruct smc_sock *smc = container_of(conn, struct smc_sock, conn);\n\n\tif (conn->lgr->is_smcd && conn->lgr->peer_shutdown)\n\t\tconn->local_tx_ctrl.conn_state_flags.peer_conn_abort = 1;\n\telse\n\t\tsmc_close_abort(conn);\n\tconn->killed = 1;\n\tsmc->sk.sk_err = ECONNABORTED;\n\tsmc_sk_wake_ups(smc);\n\tif (conn->lgr->is_smcd) {\n\t\tsmc_ism_unset_conn(conn);\n\t\tif (soft)\n\t\t\ttasklet_kill(&conn->rx_tsklet);\n\t\telse\n\t\t\ttasklet_unlock_wait(&conn->rx_tsklet);\n\t} else {\n\t\tsmc_cdc_wait_pend_tx_wr(conn);\n\t}\n\tsmc_lgr_unregister_conn(conn);\n\tsmc_close_active_abort(smc);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,7 +16,7 @@\n \t\telse\n \t\t\ttasklet_unlock_wait(&conn->rx_tsklet);\n \t} else {\n-\t\tsmc_cdc_tx_dismiss_slots(conn);\n+\t\tsmc_cdc_wait_pend_tx_wr(conn);\n \t}\n \tsmc_lgr_unregister_conn(conn);\n \tsmc_close_active_abort(smc);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tsmc_cdc_tx_dismiss_slots(conn);"
            ],
            "added_lines": [
                "\t\tsmc_cdc_wait_pend_tx_wr(conn);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smc_conn_create",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "int smc_conn_create(struct smc_sock *smc, struct smc_init_info *ini)\n{\n\tstruct smc_connection *conn = &smc->conn;\n\tstruct list_head *lgr_list;\n\tstruct smc_link_group *lgr;\n\tenum smc_lgr_role role;\n\tspinlock_t *lgr_lock;\n\tint rc = 0;\n\n\tlgr_list = ini->is_smcd ? &ini->ism_dev[ini->ism_selected]->lgr_list :\n\t\t\t\t  &smc_lgr_list.list;\n\tlgr_lock = ini->is_smcd ? &ini->ism_dev[ini->ism_selected]->lgr_lock :\n\t\t\t\t  &smc_lgr_list.lock;\n\tini->first_contact_local = 1;\n\trole = smc->listen_smc ? SMC_SERV : SMC_CLNT;\n\tif (role == SMC_CLNT && ini->first_contact_peer)\n\t\t/* create new link group as well */\n\t\tgoto create;\n\n\t/* determine if an existing link group can be reused */\n\tspin_lock_bh(lgr_lock);\n\tlist_for_each_entry(lgr, lgr_list, list) {\n\t\twrite_lock_bh(&lgr->conns_lock);\n\t\tif ((ini->is_smcd ?\n\t\t     smcd_lgr_match(lgr, ini->ism_dev[ini->ism_selected],\n\t\t\t\t    ini->ism_peer_gid[ini->ism_selected]) :\n\t\t     smcr_lgr_match(lgr, ini->ib_lcl, role, ini->ib_clcqpn)) &&\n\t\t    !lgr->sync_err &&\n\t\t    (ini->smcd_version == SMC_V2 ||\n\t\t     lgr->vlan_id == ini->vlan_id) &&\n\t\t    (role == SMC_CLNT || ini->is_smcd ||\n\t\t     lgr->conns_num < SMC_RMBS_PER_LGR_MAX)) {\n\t\t\t/* link group found */\n\t\t\tini->first_contact_local = 0;\n\t\t\tconn->lgr = lgr;\n\t\t\trc = smc_lgr_register_conn(conn, false);\n\t\t\twrite_unlock_bh(&lgr->conns_lock);\n\t\t\tif (!rc && delayed_work_pending(&lgr->free_work))\n\t\t\t\tcancel_delayed_work(&lgr->free_work);\n\t\t\tbreak;\n\t\t}\n\t\twrite_unlock_bh(&lgr->conns_lock);\n\t}\n\tspin_unlock_bh(lgr_lock);\n\tif (rc)\n\t\treturn rc;\n\n\tif (role == SMC_CLNT && !ini->first_contact_peer &&\n\t    ini->first_contact_local) {\n\t\t/* Server reuses a link group, but Client wants to start\n\t\t * a new one\n\t\t * send out_of_sync decline, reason synchr. error\n\t\t */\n\t\treturn SMC_CLC_DECL_SYNCERR;\n\t}\n\ncreate:\n\tif (ini->first_contact_local) {\n\t\trc = smc_lgr_create(smc, ini);\n\t\tif (rc)\n\t\t\tgoto out;\n\t\tlgr = conn->lgr;\n\t\twrite_lock_bh(&lgr->conns_lock);\n\t\trc = smc_lgr_register_conn(conn, true);\n\t\twrite_unlock_bh(&lgr->conns_lock);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\tconn->local_tx_ctrl.common.type = SMC_CDC_MSG_TYPE;\n\tconn->local_tx_ctrl.len = SMC_WR_TX_SIZE;\n\tconn->urg_state = SMC_URG_READ;\n\tINIT_WORK(&smc->conn.abort_work, smc_conn_abort_work);\n\tif (ini->is_smcd) {\n\t\tconn->rx_off = sizeof(struct smcd_cdc_msg);\n\t\tsmcd_cdc_rx_init(conn); /* init tasklet for this conn */\n\t} else {\n\t\tconn->rx_off = 0;\n\t}\n#ifndef KERNEL_HAS_ATOMIC64\n\tspin_lock_init(&conn->acurs_lock);\n#endif\n\nout:\n\treturn rc;\n}",
        "func": "int smc_conn_create(struct smc_sock *smc, struct smc_init_info *ini)\n{\n\tstruct smc_connection *conn = &smc->conn;\n\tstruct list_head *lgr_list;\n\tstruct smc_link_group *lgr;\n\tenum smc_lgr_role role;\n\tspinlock_t *lgr_lock;\n\tint rc = 0;\n\n\tlgr_list = ini->is_smcd ? &ini->ism_dev[ini->ism_selected]->lgr_list :\n\t\t\t\t  &smc_lgr_list.list;\n\tlgr_lock = ini->is_smcd ? &ini->ism_dev[ini->ism_selected]->lgr_lock :\n\t\t\t\t  &smc_lgr_list.lock;\n\tini->first_contact_local = 1;\n\trole = smc->listen_smc ? SMC_SERV : SMC_CLNT;\n\tif (role == SMC_CLNT && ini->first_contact_peer)\n\t\t/* create new link group as well */\n\t\tgoto create;\n\n\t/* determine if an existing link group can be reused */\n\tspin_lock_bh(lgr_lock);\n\tlist_for_each_entry(lgr, lgr_list, list) {\n\t\twrite_lock_bh(&lgr->conns_lock);\n\t\tif ((ini->is_smcd ?\n\t\t     smcd_lgr_match(lgr, ini->ism_dev[ini->ism_selected],\n\t\t\t\t    ini->ism_peer_gid[ini->ism_selected]) :\n\t\t     smcr_lgr_match(lgr, ini->ib_lcl, role, ini->ib_clcqpn)) &&\n\t\t    !lgr->sync_err &&\n\t\t    (ini->smcd_version == SMC_V2 ||\n\t\t     lgr->vlan_id == ini->vlan_id) &&\n\t\t    (role == SMC_CLNT || ini->is_smcd ||\n\t\t     lgr->conns_num < SMC_RMBS_PER_LGR_MAX)) {\n\t\t\t/* link group found */\n\t\t\tini->first_contact_local = 0;\n\t\t\tconn->lgr = lgr;\n\t\t\trc = smc_lgr_register_conn(conn, false);\n\t\t\twrite_unlock_bh(&lgr->conns_lock);\n\t\t\tif (!rc && delayed_work_pending(&lgr->free_work))\n\t\t\t\tcancel_delayed_work(&lgr->free_work);\n\t\t\tbreak;\n\t\t}\n\t\twrite_unlock_bh(&lgr->conns_lock);\n\t}\n\tspin_unlock_bh(lgr_lock);\n\tif (rc)\n\t\treturn rc;\n\n\tif (role == SMC_CLNT && !ini->first_contact_peer &&\n\t    ini->first_contact_local) {\n\t\t/* Server reuses a link group, but Client wants to start\n\t\t * a new one\n\t\t * send out_of_sync decline, reason synchr. error\n\t\t */\n\t\treturn SMC_CLC_DECL_SYNCERR;\n\t}\n\ncreate:\n\tif (ini->first_contact_local) {\n\t\trc = smc_lgr_create(smc, ini);\n\t\tif (rc)\n\t\t\tgoto out;\n\t\tlgr = conn->lgr;\n\t\twrite_lock_bh(&lgr->conns_lock);\n\t\trc = smc_lgr_register_conn(conn, true);\n\t\twrite_unlock_bh(&lgr->conns_lock);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\tconn->local_tx_ctrl.common.type = SMC_CDC_MSG_TYPE;\n\tconn->local_tx_ctrl.len = SMC_WR_TX_SIZE;\n\tconn->urg_state = SMC_URG_READ;\n\tinit_waitqueue_head(&conn->cdc_pend_tx_wq);\n\tINIT_WORK(&smc->conn.abort_work, smc_conn_abort_work);\n\tif (ini->is_smcd) {\n\t\tconn->rx_off = sizeof(struct smcd_cdc_msg);\n\t\tsmcd_cdc_rx_init(conn); /* init tasklet for this conn */\n\t} else {\n\t\tconn->rx_off = 0;\n\t}\n#ifndef KERNEL_HAS_ATOMIC64\n\tspin_lock_init(&conn->acurs_lock);\n#endif\n\nout:\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -69,6 +69,7 @@\n \tconn->local_tx_ctrl.common.type = SMC_CDC_MSG_TYPE;\n \tconn->local_tx_ctrl.len = SMC_WR_TX_SIZE;\n \tconn->urg_state = SMC_URG_READ;\n+\tinit_waitqueue_head(&conn->cdc_pend_tx_wq);\n \tINIT_WORK(&smc->conn.abort_work, smc_conn_abort_work);\n \tif (ini->is_smcd) {\n \t\tconn->rx_off = sizeof(struct smcd_cdc_msg);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tinit_waitqueue_head(&conn->cdc_pend_tx_wq);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smcr_link_clear",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "void smcr_link_clear(struct smc_link *lnk, bool log)\n{\n\tstruct smc_ib_device *smcibdev;\n\n\tif (!lnk->lgr || lnk->state == SMC_LNK_UNUSED)\n\t\treturn;\n\tlnk->peer_qpn = 0;\n\tsmc_llc_link_clear(lnk, log);\n\tsmcr_buf_unmap_lgr(lnk);\n\tsmcr_rtoken_clear_link(lnk);\n\tsmc_ib_modify_qp_reset(lnk);\n\tsmc_wr_free_link(lnk);\n\tsmc_ib_destroy_queue_pair(lnk);\n\tsmc_ib_dealloc_protection_domain(lnk);\n\tsmc_wr_free_link_mem(lnk);\n\tsmc_ibdev_cnt_dec(lnk);\n\tput_device(&lnk->smcibdev->ibdev->dev);\n\tsmcibdev = lnk->smcibdev;\n\tmemset(lnk, 0, sizeof(struct smc_link));\n\tlnk->state = SMC_LNK_UNUSED;\n\tif (!atomic_dec_return(&smcibdev->lnk_cnt))\n\t\twake_up(&smcibdev->lnks_deleted);\n}",
        "func": "void smcr_link_clear(struct smc_link *lnk, bool log)\n{\n\tstruct smc_ib_device *smcibdev;\n\n\tif (!lnk->lgr || lnk->state == SMC_LNK_UNUSED)\n\t\treturn;\n\tlnk->peer_qpn = 0;\n\tsmc_llc_link_clear(lnk, log);\n\tsmcr_buf_unmap_lgr(lnk);\n\tsmcr_rtoken_clear_link(lnk);\n\tsmc_ib_modify_qp_error(lnk);\n\tsmc_wr_free_link(lnk);\n\tsmc_ib_destroy_queue_pair(lnk);\n\tsmc_ib_dealloc_protection_domain(lnk);\n\tsmc_wr_free_link_mem(lnk);\n\tsmc_ibdev_cnt_dec(lnk);\n\tput_device(&lnk->smcibdev->ibdev->dev);\n\tsmcibdev = lnk->smcibdev;\n\tmemset(lnk, 0, sizeof(struct smc_link));\n\tlnk->state = SMC_LNK_UNUSED;\n\tif (!atomic_dec_return(&smcibdev->lnk_cnt))\n\t\twake_up(&smcibdev->lnks_deleted);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,7 @@\n \tsmc_llc_link_clear(lnk, log);\n \tsmcr_buf_unmap_lgr(lnk);\n \tsmcr_rtoken_clear_link(lnk);\n-\tsmc_ib_modify_qp_reset(lnk);\n+\tsmc_ib_modify_qp_error(lnk);\n \tsmc_wr_free_link(lnk);\n \tsmc_ib_destroy_queue_pair(lnk);\n \tsmc_ib_dealloc_protection_domain(lnk);",
        "diff_line_info": {
            "deleted_lines": [
                "\tsmc_ib_modify_qp_reset(lnk);"
            ],
            "added_lines": [
                "\tsmc_ib_modify_qp_error(lnk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smc_conn_free",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "void smc_conn_free(struct smc_connection *conn)\n{\n\tstruct smc_link_group *lgr = conn->lgr;\n\n\tif (!lgr)\n\t\treturn;\n\tif (lgr->is_smcd) {\n\t\tif (!list_empty(&lgr->list))\n\t\t\tsmc_ism_unset_conn(conn);\n\t\ttasklet_kill(&conn->rx_tsklet);\n\t} else {\n\t\tsmc_cdc_tx_dismiss_slots(conn);\n\t\tif (current_work() != &conn->abort_work)\n\t\t\tcancel_work_sync(&conn->abort_work);\n\t}\n\tif (!list_empty(&lgr->list)) {\n\t\tsmc_lgr_unregister_conn(conn);\n\t\tsmc_buf_unuse(conn, lgr); /* allow buffer reuse */\n\t}\n\n\tif (!lgr->conns_num)\n\t\tsmc_lgr_schedule_free_work(lgr);\n}",
        "func": "void smc_conn_free(struct smc_connection *conn)\n{\n\tstruct smc_link_group *lgr = conn->lgr;\n\n\tif (!lgr)\n\t\treturn;\n\tif (lgr->is_smcd) {\n\t\tif (!list_empty(&lgr->list))\n\t\t\tsmc_ism_unset_conn(conn);\n\t\ttasklet_kill(&conn->rx_tsklet);\n\t} else {\n\t\tsmc_cdc_wait_pend_tx_wr(conn);\n\t\tif (current_work() != &conn->abort_work)\n\t\t\tcancel_work_sync(&conn->abort_work);\n\t}\n\tif (!list_empty(&lgr->list)) {\n\t\tsmc_lgr_unregister_conn(conn);\n\t\tsmc_buf_unuse(conn, lgr); /* allow buffer reuse */\n\t}\n\n\tif (!lgr->conns_num)\n\t\tsmc_lgr_schedule_free_work(lgr);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n \t\t\tsmc_ism_unset_conn(conn);\n \t\ttasklet_kill(&conn->rx_tsklet);\n \t} else {\n-\t\tsmc_cdc_tx_dismiss_slots(conn);\n+\t\tsmc_cdc_wait_pend_tx_wr(conn);\n \t\tif (current_work() != &conn->abort_work)\n \t\t\tcancel_work_sync(&conn->abort_work);\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tsmc_cdc_tx_dismiss_slots(conn);"
            ],
            "added_lines": [
                "\t\tsmc_cdc_wait_pend_tx_wr(conn);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smc_smcr_terminate_all",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "void smc_smcr_terminate_all(struct smc_ib_device *smcibdev)\n{\n\tstruct smc_link_group *lgr, *lg;\n\tLIST_HEAD(lgr_free_list);\n\tint i;\n\n\tspin_lock_bh(&smc_lgr_list.lock);\n\tif (!smcibdev) {\n\t\tlist_splice_init(&smc_lgr_list.list, &lgr_free_list);\n\t\tlist_for_each_entry(lgr, &lgr_free_list, list)\n\t\t\tlgr->freeing = 1;\n\t} else {\n\t\tlist_for_each_entry_safe(lgr, lg, &smc_lgr_list.list, list) {\n\t\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\t\t\tif (lgr->lnk[i].smcibdev == smcibdev)\n\t\t\t\t\tsmcr_link_down_cond_sched(&lgr->lnk[i]);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_bh(&smc_lgr_list.lock);\n\n\tlist_for_each_entry_safe(lgr, lg, &lgr_free_list, list) {\n\t\tlist_del_init(&lgr->list);\n\t\tsmc_llc_set_termination_rsn(lgr, SMC_LLC_DEL_OP_INIT_TERM);\n\t\t__smc_lgr_terminate(lgr, false);\n\t}\n\n\tif (smcibdev) {\n\t\tif (atomic_read(&smcibdev->lnk_cnt))\n\t\t\twait_event(smcibdev->lnks_deleted,\n\t\t\t\t   !atomic_read(&smcibdev->lnk_cnt));\n\t} else {\n\t\tif (atomic_read(&lgr_cnt))\n\t\t\twait_event(lgrs_deleted, !atomic_read(&lgr_cnt));\n\t}\n}",
        "func": "void smc_smcr_terminate_all(struct smc_ib_device *smcibdev)\n{\n\tstruct smc_link_group *lgr, *lg;\n\tLIST_HEAD(lgr_free_list);\n\tLIST_HEAD(lgr_linkdown_list);\n\tint i;\n\n\tspin_lock_bh(&smc_lgr_list.lock);\n\tif (!smcibdev) {\n\t\tlist_splice_init(&smc_lgr_list.list, &lgr_free_list);\n\t\tlist_for_each_entry(lgr, &lgr_free_list, list)\n\t\t\tlgr->freeing = 1;\n\t} else {\n\t\tlist_for_each_entry_safe(lgr, lg, &smc_lgr_list.list, list) {\n\t\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\t\t\tif (lgr->lnk[i].smcibdev == smcibdev)\n\t\t\t\t\tlist_move_tail(&lgr->list, &lgr_linkdown_list);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_bh(&smc_lgr_list.lock);\n\n\tlist_for_each_entry_safe(lgr, lg, &lgr_free_list, list) {\n\t\tlist_del_init(&lgr->list);\n\t\tsmc_llc_set_termination_rsn(lgr, SMC_LLC_DEL_OP_INIT_TERM);\n\t\t__smc_lgr_terminate(lgr, false);\n\t}\n\n\tlist_for_each_entry_safe(lgr, lg, &lgr_linkdown_list, list) {\n\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\t\tif (lgr->lnk[i].smcibdev == smcibdev) {\n\t\t\t\tmutex_lock(&lgr->llc_conf_mutex);\n\t\t\t\tsmcr_link_down_cond(&lgr->lnk[i]);\n\t\t\t\tmutex_unlock(&lgr->llc_conf_mutex);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (smcibdev) {\n\t\tif (atomic_read(&smcibdev->lnk_cnt))\n\t\t\twait_event(smcibdev->lnks_deleted,\n\t\t\t\t   !atomic_read(&smcibdev->lnk_cnt));\n\t} else {\n\t\tif (atomic_read(&lgr_cnt))\n\t\t\twait_event(lgrs_deleted, !atomic_read(&lgr_cnt));\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n {\n \tstruct smc_link_group *lgr, *lg;\n \tLIST_HEAD(lgr_free_list);\n+\tLIST_HEAD(lgr_linkdown_list);\n \tint i;\n \n \tspin_lock_bh(&smc_lgr_list.lock);\n@@ -13,7 +14,7 @@\n \t\tlist_for_each_entry_safe(lgr, lg, &smc_lgr_list.list, list) {\n \t\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n \t\t\t\tif (lgr->lnk[i].smcibdev == smcibdev)\n-\t\t\t\t\tsmcr_link_down_cond_sched(&lgr->lnk[i]);\n+\t\t\t\t\tlist_move_tail(&lgr->list, &lgr_linkdown_list);\n \t\t\t}\n \t\t}\n \t}\n@@ -25,6 +26,16 @@\n \t\t__smc_lgr_terminate(lgr, false);\n \t}\n \n+\tlist_for_each_entry_safe(lgr, lg, &lgr_linkdown_list, list) {\n+\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n+\t\t\tif (lgr->lnk[i].smcibdev == smcibdev) {\n+\t\t\t\tmutex_lock(&lgr->llc_conf_mutex);\n+\t\t\t\tsmcr_link_down_cond(&lgr->lnk[i]);\n+\t\t\t\tmutex_unlock(&lgr->llc_conf_mutex);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n \tif (smcibdev) {\n \t\tif (atomic_read(&smcibdev->lnk_cnt))\n \t\t\twait_event(smcibdev->lnks_deleted,",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t\tsmcr_link_down_cond_sched(&lgr->lnk[i]);"
            ],
            "added_lines": [
                "\tLIST_HEAD(lgr_linkdown_list);",
                "\t\t\t\t\tlist_move_tail(&lgr->list, &lgr_linkdown_list);",
                "\tlist_for_each_entry_safe(lgr, lg, &lgr_linkdown_list, list) {",
                "\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {",
                "\t\t\tif (lgr->lnk[i].smcibdev == smcibdev) {",
                "\t\t\t\tmutex_lock(&lgr->llc_conf_mutex);",
                "\t\t\t\tsmcr_link_down_cond(&lgr->lnk[i]);",
                "\t\t\t\tmutex_unlock(&lgr->llc_conf_mutex);",
                "\t\t\t}",
                "\t\t}",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smc_wr_tx_wait_no_pending_sends",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "int smc_wr_tx_wait_no_pending_sends(struct smc_link *link)\n{\n\tif (wait_event_timeout(link->wr_tx_wait, !smc_wr_is_tx_pend(link),\n\t\t\t       SMC_WR_TX_WAIT_PENDING_TIME))\n\t\treturn 0;\n\telse /* timeout */\n\t\treturn -EPIPE;\n}",
        "func": "void smc_wr_tx_wait_no_pending_sends(struct smc_link *link)\n{\n\twait_event(link->wr_tx_wait, !smc_wr_is_tx_pend(link));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,4 @@\n-int smc_wr_tx_wait_no_pending_sends(struct smc_link *link)\n+void smc_wr_tx_wait_no_pending_sends(struct smc_link *link)\n {\n-\tif (wait_event_timeout(link->wr_tx_wait, !smc_wr_is_tx_pend(link),\n-\t\t\t       SMC_WR_TX_WAIT_PENDING_TIME))\n-\t\treturn 0;\n-\telse /* timeout */\n-\t\treturn -EPIPE;\n+\twait_event(link->wr_tx_wait, !smc_wr_is_tx_pend(link));\n }",
        "diff_line_info": {
            "deleted_lines": [
                "int smc_wr_tx_wait_no_pending_sends(struct smc_link *link)",
                "\tif (wait_event_timeout(link->wr_tx_wait, !smc_wr_is_tx_pend(link),",
                "\t\t\t       SMC_WR_TX_WAIT_PENDING_TIME))",
                "\t\treturn 0;",
                "\telse /* timeout */",
                "\t\treturn -EPIPE;"
            ],
            "added_lines": [
                "void smc_wr_tx_wait_no_pending_sends(struct smc_link *link)",
                "\twait_event(link->wr_tx_wait, !smc_wr_is_tx_pend(link));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smc_wr_free_link",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "void smc_wr_free_link(struct smc_link *lnk)\n{\n\tstruct ib_device *ibdev;\n\n\tif (!lnk->smcibdev)\n\t\treturn;\n\tibdev = lnk->smcibdev->ibdev;\n\n\tsmc_wr_wakeup_reg_wait(lnk);\n\tsmc_wr_wakeup_tx_wait(lnk);\n\n\tif (smc_wr_tx_wait_no_pending_sends(lnk))\n\t\tmemset(lnk->wr_tx_mask, 0,\n\t\t       BITS_TO_LONGS(SMC_WR_BUF_CNT) *\n\t\t\t\t\t\tsizeof(*lnk->wr_tx_mask));\n\twait_event(lnk->wr_reg_wait, (!atomic_read(&lnk->wr_reg_refcnt)));\n\twait_event(lnk->wr_tx_wait, (!atomic_read(&lnk->wr_tx_refcnt)));\n\n\tif (lnk->wr_rx_dma_addr) {\n\t\tib_dma_unmap_single(ibdev, lnk->wr_rx_dma_addr,\n\t\t\t\t    SMC_WR_BUF_SIZE * lnk->wr_rx_cnt,\n\t\t\t\t    DMA_FROM_DEVICE);\n\t\tlnk->wr_rx_dma_addr = 0;\n\t}\n\tif (lnk->wr_tx_dma_addr) {\n\t\tib_dma_unmap_single(ibdev, lnk->wr_tx_dma_addr,\n\t\t\t\t    SMC_WR_BUF_SIZE * lnk->wr_tx_cnt,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tlnk->wr_tx_dma_addr = 0;\n\t}\n}",
        "func": "void smc_wr_free_link(struct smc_link *lnk)\n{\n\tstruct ib_device *ibdev;\n\n\tif (!lnk->smcibdev)\n\t\treturn;\n\tibdev = lnk->smcibdev->ibdev;\n\n\tsmc_wr_wakeup_reg_wait(lnk);\n\tsmc_wr_wakeup_tx_wait(lnk);\n\n\tsmc_wr_tx_wait_no_pending_sends(lnk);\n\twait_event(lnk->wr_reg_wait, (!atomic_read(&lnk->wr_reg_refcnt)));\n\twait_event(lnk->wr_tx_wait, (!atomic_read(&lnk->wr_tx_refcnt)));\n\n\tif (lnk->wr_rx_dma_addr) {\n\t\tib_dma_unmap_single(ibdev, lnk->wr_rx_dma_addr,\n\t\t\t\t    SMC_WR_BUF_SIZE * lnk->wr_rx_cnt,\n\t\t\t\t    DMA_FROM_DEVICE);\n\t\tlnk->wr_rx_dma_addr = 0;\n\t}\n\tif (lnk->wr_tx_dma_addr) {\n\t\tib_dma_unmap_single(ibdev, lnk->wr_tx_dma_addr,\n\t\t\t\t    SMC_WR_BUF_SIZE * lnk->wr_tx_cnt,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tlnk->wr_tx_dma_addr = 0;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,10 +9,7 @@\n \tsmc_wr_wakeup_reg_wait(lnk);\n \tsmc_wr_wakeup_tx_wait(lnk);\n \n-\tif (smc_wr_tx_wait_no_pending_sends(lnk))\n-\t\tmemset(lnk->wr_tx_mask, 0,\n-\t\t       BITS_TO_LONGS(SMC_WR_BUF_CNT) *\n-\t\t\t\t\t\tsizeof(*lnk->wr_tx_mask));\n+\tsmc_wr_tx_wait_no_pending_sends(lnk);\n \twait_event(lnk->wr_reg_wait, (!atomic_read(&lnk->wr_reg_refcnt)));\n \twait_event(lnk->wr_tx_wait, (!atomic_read(&lnk->wr_tx_refcnt)));\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (smc_wr_tx_wait_no_pending_sends(lnk))",
                "\t\tmemset(lnk->wr_tx_mask, 0,",
                "\t\t       BITS_TO_LONGS(SMC_WR_BUF_CNT) *",
                "\t\t\t\t\t\tsizeof(*lnk->wr_tx_mask));"
            ],
            "added_lines": [
                "\tsmc_wr_tx_wait_no_pending_sends(lnk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smc_wr_tx_process_cqe",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "static inline void smc_wr_tx_process_cqe(struct ib_wc *wc)\n{\n\tstruct smc_wr_tx_pend pnd_snd;\n\tstruct smc_link *link;\n\tu32 pnd_snd_idx;\n\tint i;\n\n\tlink = wc->qp->qp_context;\n\n\tif (wc->opcode == IB_WC_REG_MR) {\n\t\tif (wc->status)\n\t\t\tlink->wr_reg_state = FAILED;\n\t\telse\n\t\t\tlink->wr_reg_state = CONFIRMED;\n\t\tsmc_wr_wakeup_reg_wait(link);\n\t\treturn;\n\t}\n\n\tpnd_snd_idx = smc_wr_tx_find_pending_index(link, wc->wr_id);\n\tif (pnd_snd_idx == link->wr_tx_cnt)\n\t\treturn;\n\tlink->wr_tx_pends[pnd_snd_idx].wc_status = wc->status;\n\tif (link->wr_tx_pends[pnd_snd_idx].compl_requested)\n\t\tcomplete(&link->wr_tx_compl[pnd_snd_idx]);\n\tmemcpy(&pnd_snd, &link->wr_tx_pends[pnd_snd_idx], sizeof(pnd_snd));\n\t/* clear the full struct smc_wr_tx_pend including .priv */\n\tmemset(&link->wr_tx_pends[pnd_snd_idx], 0,\n\t       sizeof(link->wr_tx_pends[pnd_snd_idx]));\n\tmemset(&link->wr_tx_bufs[pnd_snd_idx], 0,\n\t       sizeof(link->wr_tx_bufs[pnd_snd_idx]));\n\tif (!test_and_clear_bit(pnd_snd_idx, link->wr_tx_mask))\n\t\treturn;\n\tif (wc->status) {\n\t\tfor_each_set_bit(i, link->wr_tx_mask, link->wr_tx_cnt) {\n\t\t\t/* clear full struct smc_wr_tx_pend including .priv */\n\t\t\tmemset(&link->wr_tx_pends[i], 0,\n\t\t\t       sizeof(link->wr_tx_pends[i]));\n\t\t\tmemset(&link->wr_tx_bufs[i], 0,\n\t\t\t       sizeof(link->wr_tx_bufs[i]));\n\t\t\tclear_bit(i, link->wr_tx_mask);\n\t\t}\n\t\t/* terminate link */\n\t\tsmcr_link_down_cond_sched(link);\n\t}\n\tif (pnd_snd.handler)\n\t\tpnd_snd.handler(&pnd_snd.priv, link, wc->status);\n\twake_up(&link->wr_tx_wait);\n}",
        "func": "static inline void smc_wr_tx_process_cqe(struct ib_wc *wc)\n{\n\tstruct smc_wr_tx_pend pnd_snd;\n\tstruct smc_link *link;\n\tu32 pnd_snd_idx;\n\n\tlink = wc->qp->qp_context;\n\n\tif (wc->opcode == IB_WC_REG_MR) {\n\t\tif (wc->status)\n\t\t\tlink->wr_reg_state = FAILED;\n\t\telse\n\t\t\tlink->wr_reg_state = CONFIRMED;\n\t\tsmc_wr_wakeup_reg_wait(link);\n\t\treturn;\n\t}\n\n\tpnd_snd_idx = smc_wr_tx_find_pending_index(link, wc->wr_id);\n\tif (pnd_snd_idx == link->wr_tx_cnt)\n\t\treturn;\n\tlink->wr_tx_pends[pnd_snd_idx].wc_status = wc->status;\n\tif (link->wr_tx_pends[pnd_snd_idx].compl_requested)\n\t\tcomplete(&link->wr_tx_compl[pnd_snd_idx]);\n\tmemcpy(&pnd_snd, &link->wr_tx_pends[pnd_snd_idx], sizeof(pnd_snd));\n\t/* clear the full struct smc_wr_tx_pend including .priv */\n\tmemset(&link->wr_tx_pends[pnd_snd_idx], 0,\n\t       sizeof(link->wr_tx_pends[pnd_snd_idx]));\n\tmemset(&link->wr_tx_bufs[pnd_snd_idx], 0,\n\t       sizeof(link->wr_tx_bufs[pnd_snd_idx]));\n\tif (!test_and_clear_bit(pnd_snd_idx, link->wr_tx_mask))\n\t\treturn;\n\tif (wc->status) {\n\t\t/* terminate link */\n\t\tsmcr_link_down_cond_sched(link);\n\t}\n\tif (pnd_snd.handler)\n\t\tpnd_snd.handler(&pnd_snd.priv, link, wc->status);\n\twake_up(&link->wr_tx_wait);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,6 @@\n \tstruct smc_wr_tx_pend pnd_snd;\n \tstruct smc_link *link;\n \tu32 pnd_snd_idx;\n-\tint i;\n \n \tlink = wc->qp->qp_context;\n \n@@ -31,14 +30,6 @@\n \tif (!test_and_clear_bit(pnd_snd_idx, link->wr_tx_mask))\n \t\treturn;\n \tif (wc->status) {\n-\t\tfor_each_set_bit(i, link->wr_tx_mask, link->wr_tx_cnt) {\n-\t\t\t/* clear full struct smc_wr_tx_pend including .priv */\n-\t\t\tmemset(&link->wr_tx_pends[i], 0,\n-\t\t\t       sizeof(link->wr_tx_pends[i]));\n-\t\t\tmemset(&link->wr_tx_bufs[i], 0,\n-\t\t\t       sizeof(link->wr_tx_bufs[i]));\n-\t\t\tclear_bit(i, link->wr_tx_mask);\n-\t\t}\n \t\t/* terminate link */\n \t\tsmcr_link_down_cond_sched(link);\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tint i;",
                "\t\tfor_each_set_bit(i, link->wr_tx_mask, link->wr_tx_cnt) {",
                "\t\t\t/* clear full struct smc_wr_tx_pend including .priv */",
                "\t\t\tmemset(&link->wr_tx_pends[i], 0,",
                "\t\t\t       sizeof(link->wr_tx_pends[i]));",
                "\t\t\tmemset(&link->wr_tx_bufs[i], 0,",
                "\t\t\t       sizeof(link->wr_tx_bufs[i]));",
                "\t\t\tclear_bit(i, link->wr_tx_mask);",
                "\t\t}"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smc_cdc_msg_send",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "int smc_cdc_msg_send(struct smc_connection *conn,\n\t\t     struct smc_wr_buf *wr_buf,\n\t\t     struct smc_cdc_tx_pend *pend)\n{\n\tstruct smc_link *link = conn->lnk;\n\tunion smc_host_cursor cfed;\n\tint rc;\n\n\tsmc_cdc_add_pending_send(conn, pend);\n\n\tconn->tx_cdc_seq++;\n\tconn->local_tx_ctrl.seqno = conn->tx_cdc_seq;\n\tsmc_host_msg_to_cdc((struct smc_cdc_msg *)wr_buf, conn, &cfed);\n\trc = smc_wr_tx_send(link, (struct smc_wr_tx_pend_priv *)pend);\n\tif (!rc) {\n\t\tsmc_curs_copy(&conn->rx_curs_confirmed, &cfed, conn);\n\t\tconn->local_rx_ctrl.prod_flags.cons_curs_upd_req = 0;\n\t} else {\n\t\tconn->tx_cdc_seq--;\n\t\tconn->local_tx_ctrl.seqno = conn->tx_cdc_seq;\n\t}\n\n\treturn rc;\n}",
        "func": "int smc_cdc_msg_send(struct smc_connection *conn,\n\t\t     struct smc_wr_buf *wr_buf,\n\t\t     struct smc_cdc_tx_pend *pend)\n{\n\tstruct smc_link *link = conn->lnk;\n\tunion smc_host_cursor cfed;\n\tint rc;\n\n\tsmc_cdc_add_pending_send(conn, pend);\n\n\tconn->tx_cdc_seq++;\n\tconn->local_tx_ctrl.seqno = conn->tx_cdc_seq;\n\tsmc_host_msg_to_cdc((struct smc_cdc_msg *)wr_buf, conn, &cfed);\n\n\tatomic_inc(&conn->cdc_pend_tx_wr);\n\tsmp_mb__after_atomic(); /* Make sure cdc_pend_tx_wr added before post */\n\n\trc = smc_wr_tx_send(link, (struct smc_wr_tx_pend_priv *)pend);\n\tif (!rc) {\n\t\tsmc_curs_copy(&conn->rx_curs_confirmed, &cfed, conn);\n\t\tconn->local_rx_ctrl.prod_flags.cons_curs_upd_req = 0;\n\t} else {\n\t\tconn->tx_cdc_seq--;\n\t\tconn->local_tx_ctrl.seqno = conn->tx_cdc_seq;\n\t\tatomic_dec(&conn->cdc_pend_tx_wr);\n\t}\n\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,6 +11,10 @@\n \tconn->tx_cdc_seq++;\n \tconn->local_tx_ctrl.seqno = conn->tx_cdc_seq;\n \tsmc_host_msg_to_cdc((struct smc_cdc_msg *)wr_buf, conn, &cfed);\n+\n+\tatomic_inc(&conn->cdc_pend_tx_wr);\n+\tsmp_mb__after_atomic(); /* Make sure cdc_pend_tx_wr added before post */\n+\n \trc = smc_wr_tx_send(link, (struct smc_wr_tx_pend_priv *)pend);\n \tif (!rc) {\n \t\tsmc_curs_copy(&conn->rx_curs_confirmed, &cfed, conn);\n@@ -18,6 +22,7 @@\n \t} else {\n \t\tconn->tx_cdc_seq--;\n \t\tconn->local_tx_ctrl.seqno = conn->tx_cdc_seq;\n+\t\tatomic_dec(&conn->cdc_pend_tx_wr);\n \t}\n \n \treturn rc;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tatomic_inc(&conn->cdc_pend_tx_wr);",
                "\tsmp_mb__after_atomic(); /* Make sure cdc_pend_tx_wr added before post */",
                "",
                "\t\tatomic_dec(&conn->cdc_pend_tx_wr);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smc_cdc_tx_handler",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "static void smc_cdc_tx_handler(struct smc_wr_tx_pend_priv *pnd_snd,\n\t\t\t       struct smc_link *link,\n\t\t\t       enum ib_wc_status wc_status)\n{\n\tstruct smc_cdc_tx_pend *cdcpend = (struct smc_cdc_tx_pend *)pnd_snd;\n\tstruct smc_connection *conn = cdcpend->conn;\n\tstruct smc_sock *smc;\n\tint diff;\n\n\tif (!conn)\n\t\t/* already dismissed */\n\t\treturn;\n\n\tsmc = container_of(conn, struct smc_sock, conn);\n\tbh_lock_sock(&smc->sk);\n\tif (!wc_status) {\n\t\tdiff = smc_curs_diff(cdcpend->conn->sndbuf_desc->len,\n\t\t\t\t     &cdcpend->conn->tx_curs_fin,\n\t\t\t\t     &cdcpend->cursor);\n\t\t/* sndbuf_space is decreased in smc_sendmsg */\n\t\tsmp_mb__before_atomic();\n\t\tatomic_add(diff, &cdcpend->conn->sndbuf_space);\n\t\t/* guarantee 0 <= sndbuf_space <= sndbuf_desc->len */\n\t\tsmp_mb__after_atomic();\n\t\tsmc_curs_copy(&conn->tx_curs_fin, &cdcpend->cursor, conn);\n\t\tsmc_curs_copy(&conn->local_tx_ctrl_fin, &cdcpend->p_cursor,\n\t\t\t      conn);\n\t\tconn->tx_cdc_seq_fin = cdcpend->ctrl_seq;\n\t}\n\tsmc_tx_sndbuf_nonfull(smc);\n\tbh_unlock_sock(&smc->sk);\n}",
        "func": "static void smc_cdc_tx_handler(struct smc_wr_tx_pend_priv *pnd_snd,\n\t\t\t       struct smc_link *link,\n\t\t\t       enum ib_wc_status wc_status)\n{\n\tstruct smc_cdc_tx_pend *cdcpend = (struct smc_cdc_tx_pend *)pnd_snd;\n\tstruct smc_connection *conn = cdcpend->conn;\n\tstruct smc_sock *smc;\n\tint diff;\n\n\tsmc = container_of(conn, struct smc_sock, conn);\n\tbh_lock_sock(&smc->sk);\n\tif (!wc_status) {\n\t\tdiff = smc_curs_diff(cdcpend->conn->sndbuf_desc->len,\n\t\t\t\t     &cdcpend->conn->tx_curs_fin,\n\t\t\t\t     &cdcpend->cursor);\n\t\t/* sndbuf_space is decreased in smc_sendmsg */\n\t\tsmp_mb__before_atomic();\n\t\tatomic_add(diff, &cdcpend->conn->sndbuf_space);\n\t\t/* guarantee 0 <= sndbuf_space <= sndbuf_desc->len */\n\t\tsmp_mb__after_atomic();\n\t\tsmc_curs_copy(&conn->tx_curs_fin, &cdcpend->cursor, conn);\n\t\tsmc_curs_copy(&conn->local_tx_ctrl_fin, &cdcpend->p_cursor,\n\t\t\t      conn);\n\t\tconn->tx_cdc_seq_fin = cdcpend->ctrl_seq;\n\t}\n\n\tif (atomic_dec_and_test(&conn->cdc_pend_tx_wr) &&\n\t    unlikely(wq_has_sleeper(&conn->cdc_pend_tx_wq)))\n\t\twake_up(&conn->cdc_pend_tx_wq);\n\tWARN_ON(atomic_read(&conn->cdc_pend_tx_wr) < 0);\n\n\tsmc_tx_sndbuf_nonfull(smc);\n\tbh_unlock_sock(&smc->sk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,10 +6,6 @@\n \tstruct smc_connection *conn = cdcpend->conn;\n \tstruct smc_sock *smc;\n \tint diff;\n-\n-\tif (!conn)\n-\t\t/* already dismissed */\n-\t\treturn;\n \n \tsmc = container_of(conn, struct smc_sock, conn);\n \tbh_lock_sock(&smc->sk);\n@@ -27,6 +23,12 @@\n \t\t\t      conn);\n \t\tconn->tx_cdc_seq_fin = cdcpend->ctrl_seq;\n \t}\n+\n+\tif (atomic_dec_and_test(&conn->cdc_pend_tx_wr) &&\n+\t    unlikely(wq_has_sleeper(&conn->cdc_pend_tx_wq)))\n+\t\twake_up(&conn->cdc_pend_tx_wq);\n+\tWARN_ON(atomic_read(&conn->cdc_pend_tx_wr) < 0);\n+\n \tsmc_tx_sndbuf_nonfull(smc);\n \tbh_unlock_sock(&smc->sk);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\tif (!conn)",
                "\t\t/* already dismissed */",
                "\t\treturn;"
            ],
            "added_lines": [
                "",
                "\tif (atomic_dec_and_test(&conn->cdc_pend_tx_wr) &&",
                "\t    unlikely(wq_has_sleeper(&conn->cdc_pend_tx_wq)))",
                "\t\twake_up(&conn->cdc_pend_tx_wq);",
                "\tWARN_ON(atomic_read(&conn->cdc_pend_tx_wr) < 0);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46925",
        "func_name": "torvalds/linux/smcr_cdc_msg_send_validation",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnet/smc: fix kernel panic caused by race of smc_sock\n\nA crash occurs when smc_cdc_tx_handler() tries to access smc_sock\nbut smc_release() has already freed it.\n\n[ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88\n[ 4570.696048] #PF: supervisor write access in kernel mode\n[ 4570.696728] #PF: error_code(0x0002) - not-present page\n[ 4570.697401] PGD 0 P4D 0\n[ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI\n[ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111\n[ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0\n[ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30\n<...>\n[ 4570.711446] Call Trace:\n[ 4570.711746]  <IRQ>\n[ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0\n[ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560\n[ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10\n[ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140\n[ 4570.714083]  __do_softirq+0x123/0x2f4\n[ 4570.714521]  irq_exit_rcu+0xc4/0xf0\n[ 4570.714934]  common_interrupt+0xba/0xe0\n\nThough smc_cdc_tx_handler() checked the existence of smc connection,\nsmc_release() may have already dismissed and released the smc socket\nbefore smc_cdc_tx_handler() further visits it.\n\nsmc_cdc_tx_handler()           |smc_release()\nif (!conn)                     |\n                               |\n                               |smc_cdc_tx_dismiss_slots()\n                               |      smc_cdc_tx_dismisser()\n                               |\n                               |sock_put(&smc->sk) <- last sock_put,\n                               |                      smc_sock freed\nbh_lock_sock(&smc->sk) (panic) |\n\nTo make sure we won't receive any CDC messages after we free the\nsmc_sock, add a refcount on the smc_connection for inflight CDC\nmessage(posted to the QP but haven't received related CQE), and\ndon't release the smc_connection until all the inflight CDC messages\nhaven been done, for both success or failed ones.\n\nUsing refcount on CDC messages brings another problem: when the link\nis going to be destroyed, smcr_link_clear() will reset the QP, which\nthen remove all the pending CQEs related to the QP in the CQ. To make\nsure all the CQEs will always come back so the refcount on the\nsmc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced\nby smc_ib_modify_qp_error().\nAnd remove the timeout in smc_wr_tx_wait_no_pending_sends() since we\nneed to wait for all pending WQEs done, or we may encounter use-after-\nfree when handling CQEs.\n\nFor IB device removal routine, we need to wait for all the QPs on that\ndevice been destroyed before we can destroy CQs on the device, or\nthe refcount on smc_connection won't reach 0 and smc_sock cannot be\nreleased.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=b85f751d71ae8e2a15e9bda98852ea9af35282eb",
        "commit_title": "[ Upstream commit 349d43127dac00c15231e8ffbcaabd70f7b0e544 ]",
        "commit_text": " A crash occurs when smc_cdc_tx_handler() tries to access smc_sock but smc_release() has already freed it.  [ 4570.695099] BUG: unable to handle page fault for address: 000000002eae9e88 [ 4570.696048] #PF: supervisor write access in kernel mode [ 4570.696728] #PF: error_code(0x0002) - not-present page [ 4570.697401] PGD 0 P4D 0 [ 4570.697716] Oops: 0002 [#1] PREEMPT SMP NOPTI [ 4570.698228] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.16.0-rc4+ #111 [ 4570.699013] Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/0 [ 4570.699933] RIP: 0010:_raw_spin_lock+0x1a/0x30 <...> [ 4570.711446] Call Trace: [ 4570.711746]  <IRQ> [ 4570.711992]  smc_cdc_tx_handler+0x41/0xc0 [ 4570.712470]  smc_wr_tx_tasklet_fn+0x213/0x560 [ 4570.712981]  ? smc_cdc_tx_dismisser+0x10/0x10 [ 4570.713489]  tasklet_action_common.isra.17+0x66/0x140 [ 4570.714083]  __do_softirq+0x123/0x2f4 [ 4570.714521]  irq_exit_rcu+0xc4/0xf0 [ 4570.714934]  common_interrupt+0xba/0xe0  Though smc_cdc_tx_handler() checked the existence of smc connection, smc_release() may have already dismissed and released the smc socket before smc_cdc_tx_handler() further visits it.  smc_cdc_tx_handler()           |smc_release() if (!conn)                     |                                |                                |smc_cdc_tx_dismiss_slots()                                |      smc_cdc_tx_dismisser()                                |                                |sock_put(&smc->sk) <- last sock_put,                                |                      smc_sock freed bh_lock_sock(&smc->sk) (panic) |  To make sure we won't receive any CDC messages after we free the smc_sock, add a refcount on the smc_connection for inflight CDC message(posted to the QP but haven't received related CQE), and don't release the smc_connection until all the inflight CDC messages haven been done, for both success or failed ones.  Using refcount on CDC messages brings another problem: when the link is going to be destroyed, smcr_link_clear() will reset the QP, which then remove all the pending CQEs related to the QP in the CQ. To make sure all the CQEs will always come back so the refcount on the smc_connection can always reach 0, smc_ib_modify_qp_reset() was replaced by smc_ib_modify_qp_error(). And remove the timeout in smc_wr_tx_wait_no_pending_sends() since we need to wait for all pending WQEs done, or we may encounter use-after- free when handling CQEs.  For IB device removal routine, we need to wait for all the QPs on that device been destroyed before we can destroy CQs on the device, or the refcount on smc_connection won't reach 0 and smc_sock cannot be released.  ",
        "func_before": "int smcr_cdc_msg_send_validation(struct smc_connection *conn,\n\t\t\t\t struct smc_cdc_tx_pend *pend,\n\t\t\t\t struct smc_wr_buf *wr_buf)\n{\n\tstruct smc_host_cdc_msg *local = &conn->local_tx_ctrl;\n\tstruct smc_link *link = conn->lnk;\n\tstruct smc_cdc_msg *peer;\n\tint rc;\n\n\tpeer = (struct smc_cdc_msg *)wr_buf;\n\tpeer->common.type = local->common.type;\n\tpeer->len = local->len;\n\tpeer->seqno = htons(conn->tx_cdc_seq_fin); /* seqno last compl. tx */\n\tpeer->token = htonl(local->token);\n\tpeer->prod_flags.failover_validation = 1;\n\n\trc = smc_wr_tx_send(link, (struct smc_wr_tx_pend_priv *)pend);\n\treturn rc;\n}",
        "func": "int smcr_cdc_msg_send_validation(struct smc_connection *conn,\n\t\t\t\t struct smc_cdc_tx_pend *pend,\n\t\t\t\t struct smc_wr_buf *wr_buf)\n{\n\tstruct smc_host_cdc_msg *local = &conn->local_tx_ctrl;\n\tstruct smc_link *link = conn->lnk;\n\tstruct smc_cdc_msg *peer;\n\tint rc;\n\n\tpeer = (struct smc_cdc_msg *)wr_buf;\n\tpeer->common.type = local->common.type;\n\tpeer->len = local->len;\n\tpeer->seqno = htons(conn->tx_cdc_seq_fin); /* seqno last compl. tx */\n\tpeer->token = htonl(local->token);\n\tpeer->prod_flags.failover_validation = 1;\n\n\t/* We need to set pend->conn here to make sure smc_cdc_tx_handler()\n\t * can handle properly\n\t */\n\tsmc_cdc_add_pending_send(conn, pend);\n\n\tatomic_inc(&conn->cdc_pend_tx_wr);\n\tsmp_mb__after_atomic(); /* Make sure cdc_pend_tx_wr added before post */\n\n\trc = smc_wr_tx_send(link, (struct smc_wr_tx_pend_priv *)pend);\n\tif (unlikely(rc))\n\t\tatomic_dec(&conn->cdc_pend_tx_wr);\n\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,6 +14,17 @@\n \tpeer->token = htonl(local->token);\n \tpeer->prod_flags.failover_validation = 1;\n \n+\t/* We need to set pend->conn here to make sure smc_cdc_tx_handler()\n+\t * can handle properly\n+\t */\n+\tsmc_cdc_add_pending_send(conn, pend);\n+\n+\tatomic_inc(&conn->cdc_pend_tx_wr);\n+\tsmp_mb__after_atomic(); /* Make sure cdc_pend_tx_wr added before post */\n+\n \trc = smc_wr_tx_send(link, (struct smc_wr_tx_pend_priv *)pend);\n+\tif (unlikely(rc))\n+\t\tatomic_dec(&conn->cdc_pend_tx_wr);\n+\n \treturn rc;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t/* We need to set pend->conn here to make sure smc_cdc_tx_handler()",
                "\t * can handle properly",
                "\t */",
                "\tsmc_cdc_add_pending_send(conn, pend);",
                "",
                "\tatomic_inc(&conn->cdc_pend_tx_wr);",
                "\tsmp_mb__after_atomic(); /* Make sure cdc_pend_tx_wr added before post */",
                "",
                "\tif (unlikely(rc))",
                "\t\tatomic_dec(&conn->cdc_pend_tx_wr);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv_mutex_trylock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/d4ff8fd5c175bc825258da3ffb01bd21bd5a6202",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  This is a back-port of commits 3eb6764, 1ad6ad7, 9a4fd26, 9823922 85adf43 and bd1777f from the v1.x branch.  Refs: https://github.com/libuv/libuv/pull/525 PR-URL: https://github.com/libuv/libuv/pull/903 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Bert Belder <bertbelder@gmail.com>",
        "func_before": "int uv_mutex_trylock(uv_mutex_t* mutex) {\n  int r;\n\n  r = pthread_mutex_trylock(mutex);\n\n  if (r && r != EBUSY && r != EAGAIN)\n    abort();\n\n  if (r)\n    return -1;\n  else\n    return 0;\n}",
        "func": "int uv_mutex_trylock(uv_mutex_t* mutex) {\n  int r;\n\n  r = pthread_mutex_trylock(mutex);\n\n  if (r) {\n    if (r != EBUSY && r != EAGAIN)\n      abort();\n    return -1;\n  }\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,11 +3,10 @@\n \n   r = pthread_mutex_trylock(mutex);\n \n-  if (r && r != EBUSY && r != EAGAIN)\n-    abort();\n-\n-  if (r)\n+  if (r) {\n+    if (r != EBUSY && r != EAGAIN)\n+      abort();\n     return -1;\n-  else\n-    return 0;\n+  }\n+  return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  if (r && r != EBUSY && r != EAGAIN)",
                "    abort();",
                "",
                "  if (r)",
                "  else",
                "    return 0;"
            ],
            "added_lines": [
                "  if (r) {",
                "    if (r != EBUSY && r != EAGAIN)",
                "      abort();",
                "  }",
                "  return 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv_rwlock_tryrdlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/d4ff8fd5c175bc825258da3ffb01bd21bd5a6202",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  This is a back-port of commits 3eb6764, 1ad6ad7, 9a4fd26, 9823922 85adf43 and bd1777f from the v1.x branch.  Refs: https://github.com/libuv/libuv/pull/525 PR-URL: https://github.com/libuv/libuv/pull/903 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Bert Belder <bertbelder@gmail.com>",
        "func_before": "int uv_rwlock_tryrdlock(uv_rwlock_t* rwlock) {\n  int r;\n\n  r = pthread_rwlock_tryrdlock(rwlock);\n\n  if (r && r != EBUSY && r != EAGAIN)\n    abort();\n\n  if (r)\n    return -1;\n  else\n    return 0;\n}",
        "func": "int uv_rwlock_tryrdlock(uv_rwlock_t* rwlock) {\n  int r;\n\n  r = pthread_rwlock_tryrdlock(rwlock);\n\n  if (r) {\n    if (r != EBUSY && r != EAGAIN)\n      abort();\n    return -1;\n  }\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,11 +3,10 @@\n \n   r = pthread_rwlock_tryrdlock(rwlock);\n \n-  if (r && r != EBUSY && r != EAGAIN)\n-    abort();\n-\n-  if (r)\n+  if (r) {\n+    if (r != EBUSY && r != EAGAIN)\n+      abort();\n     return -1;\n-  else\n-    return 0;\n+  }\n+  return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  if (r && r != EBUSY && r != EAGAIN)",
                "    abort();",
                "",
                "  if (r)",
                "  else",
                "    return 0;"
            ],
            "added_lines": [
                "  if (r) {",
                "    if (r != EBUSY && r != EAGAIN)",
                "      abort();",
                "  }",
                "  return 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv_rwlock_trywrlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/d4ff8fd5c175bc825258da3ffb01bd21bd5a6202",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  This is a back-port of commits 3eb6764, 1ad6ad7, 9a4fd26, 9823922 85adf43 and bd1777f from the v1.x branch.  Refs: https://github.com/libuv/libuv/pull/525 PR-URL: https://github.com/libuv/libuv/pull/903 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Bert Belder <bertbelder@gmail.com>",
        "func_before": "int uv_rwlock_trywrlock(uv_rwlock_t* rwlock) {\n  int r;\n\n  r = pthread_rwlock_trywrlock(rwlock);\n\n  if (r && r != EBUSY && r != EAGAIN)\n    abort();\n\n  if (r)\n    return -1;\n  else\n    return 0;\n}",
        "func": "int uv_rwlock_trywrlock(uv_rwlock_t* rwlock) {\n  int r;\n\n  r = pthread_rwlock_trywrlock(rwlock);\n\n  if (r) {\n    if (r != EBUSY && r != EAGAIN)\n      abort();\n    return -1;\n  }\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,11 +3,10 @@\n \n   r = pthread_rwlock_trywrlock(rwlock);\n \n-  if (r && r != EBUSY && r != EAGAIN)\n-    abort();\n-\n-  if (r)\n+  if (r) {\n+    if (r != EBUSY && r != EAGAIN)\n+      abort();\n     return -1;\n-  else\n-    return 0;\n+  }\n+  return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  if (r && r != EBUSY && r != EAGAIN)",
                "    abort();",
                "",
                "  if (r)",
                "  else",
                "    return 0;"
            ],
            "added_lines": [
                "  if (r) {",
                "    if (r != EBUSY && r != EAGAIN)",
                "      abort();",
                "  }",
                "  return 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv_mutex_trylock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/d4ff8fd5c175bc825258da3ffb01bd21bd5a6202",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  This is a back-port of commits 3eb6764, 1ad6ad7, 9a4fd26, 9823922 85adf43 and bd1777f from the v1.x branch.  Refs: https://github.com/libuv/libuv/pull/525 PR-URL: https://github.com/libuv/libuv/pull/903 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Bert Belder <bertbelder@gmail.com>",
        "func_before": "int uv_mutex_trylock(uv_mutex_t* mutex) {\n  int r;\n\n  r = pthread_mutex_trylock(mutex);\n\n  if (r && r != EBUSY && r != EAGAIN)\n    abort();\n\n  if (r)\n    return -1;\n  else\n    return 0;\n}",
        "func": "int uv_mutex_trylock(uv_mutex_t* mutex) {\n  int r;\n\n  r = pthread_mutex_trylock(mutex);\n\n  if (r) {\n    if (r != EBUSY && r != EAGAIN)\n      abort();\n    return -1;\n  }\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,11 +3,10 @@\n \n   r = pthread_mutex_trylock(mutex);\n \n-  if (r && r != EBUSY && r != EAGAIN)\n-    abort();\n-\n-  if (r)\n+  if (r) {\n+    if (r != EBUSY && r != EAGAIN)\n+      abort();\n     return -1;\n-  else\n-    return 0;\n+  }\n+  return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  if (r && r != EBUSY && r != EAGAIN)",
                "    abort();",
                "",
                "  if (r)",
                "  else",
                "    return 0;"
            ],
            "added_lines": [
                "  if (r) {",
                "    if (r != EBUSY && r != EAGAIN)",
                "      abort();",
                "  }",
                "  return 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv_rwlock_tryrdlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/d4ff8fd5c175bc825258da3ffb01bd21bd5a6202",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  This is a back-port of commits 3eb6764, 1ad6ad7, 9a4fd26, 9823922 85adf43 and bd1777f from the v1.x branch.  Refs: https://github.com/libuv/libuv/pull/525 PR-URL: https://github.com/libuv/libuv/pull/903 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Bert Belder <bertbelder@gmail.com>",
        "func_before": "int uv_rwlock_tryrdlock(uv_rwlock_t* rwlock) {\n  int r;\n\n  r = pthread_rwlock_tryrdlock(rwlock);\n\n  if (r && r != EBUSY && r != EAGAIN)\n    abort();\n\n  if (r)\n    return -1;\n  else\n    return 0;\n}",
        "func": "int uv_rwlock_tryrdlock(uv_rwlock_t* rwlock) {\n  int r;\n\n  r = pthread_rwlock_tryrdlock(rwlock);\n\n  if (r) {\n    if (r != EBUSY && r != EAGAIN)\n      abort();\n    return -1;\n  }\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,11 +3,10 @@\n \n   r = pthread_rwlock_tryrdlock(rwlock);\n \n-  if (r && r != EBUSY && r != EAGAIN)\n-    abort();\n-\n-  if (r)\n+  if (r) {\n+    if (r != EBUSY && r != EAGAIN)\n+      abort();\n     return -1;\n-  else\n-    return 0;\n+  }\n+  return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  if (r && r != EBUSY && r != EAGAIN)",
                "    abort();",
                "",
                "  if (r)",
                "  else",
                "    return 0;"
            ],
            "added_lines": [
                "  if (r) {",
                "    if (r != EBUSY && r != EAGAIN)",
                "      abort();",
                "  }",
                "  return 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv_rwlock_trywrlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/d4ff8fd5c175bc825258da3ffb01bd21bd5a6202",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  This is a back-port of commits 3eb6764, 1ad6ad7, 9a4fd26, 9823922 85adf43 and bd1777f from the v1.x branch.  Refs: https://github.com/libuv/libuv/pull/525 PR-URL: https://github.com/libuv/libuv/pull/903 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Bert Belder <bertbelder@gmail.com>",
        "func_before": "int uv_rwlock_trywrlock(uv_rwlock_t* rwlock) {\n  int r;\n\n  r = pthread_rwlock_trywrlock(rwlock);\n\n  if (r && r != EBUSY && r != EAGAIN)\n    abort();\n\n  if (r)\n    return -1;\n  else\n    return 0;\n}",
        "func": "int uv_rwlock_trywrlock(uv_rwlock_t* rwlock) {\n  int r;\n\n  r = pthread_rwlock_trywrlock(rwlock);\n\n  if (r) {\n    if (r != EBUSY && r != EAGAIN)\n      abort();\n    return -1;\n  }\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,11 +3,10 @@\n \n   r = pthread_rwlock_trywrlock(rwlock);\n \n-  if (r && r != EBUSY && r != EAGAIN)\n-    abort();\n-\n-  if (r)\n+  if (r) {\n+    if (r != EBUSY && r != EAGAIN)\n+      abort();\n     return -1;\n-  else\n-    return 0;\n+  }\n+  return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  if (r && r != EBUSY && r != EAGAIN)",
                "    abort();",
                "",
                "  if (r)",
                "  else",
                "    return 0;"
            ],
            "added_lines": [
                "  if (r) {",
                "    if (r != EBUSY && r != EAGAIN)",
                "      abort();",
                "  }",
                "  return 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv_winapi_init",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/d4ff8fd5c175bc825258da3ffb01bd21bd5a6202",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  This is a back-port of commits 3eb6764, 1ad6ad7, 9a4fd26, 9823922 85adf43 and bd1777f from the v1.x branch.  Refs: https://github.com/libuv/libuv/pull/525 PR-URL: https://github.com/libuv/libuv/pull/903 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Bert Belder <bertbelder@gmail.com>",
        "func_before": "void uv_winapi_init() {\n  HMODULE ntdll_module;\n  HMODULE kernel32_module;\n\n  ntdll_module = GetModuleHandleA(\"ntdll.dll\");\n  if (ntdll_module == NULL) {\n    uv_fatal_error(GetLastError(), \"GetModuleHandleA\");\n  }\n\n  pRtlNtStatusToDosError = (sRtlNtStatusToDosError) GetProcAddress(\n      ntdll_module,\n      \"RtlNtStatusToDosError\");\n  if (pRtlNtStatusToDosError == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  pNtQueryInformationFile = (sNtQueryInformationFile) GetProcAddress(\n      ntdll_module,\n      \"NtQueryInformationFile\");\n  if (pNtQueryInformationFile == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  pNtDeviceIoControlFile = (sNtDeviceIoControlFile) GetProcAddress(\n      ntdll_module,\n      \"NtDeviceIoControlFile\");\n  if (pNtDeviceIoControlFile == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  pNtSetInformationFile = (sNtSetInformationFile) GetProcAddress(\n      ntdll_module,\n      \"NtSetInformationFile\");\n  if (pNtSetInformationFile == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  pNtQuerySystemInformation = (sNtQuerySystemInformation) GetProcAddress(\n      ntdll_module,\n      \"NtQuerySystemInformation\");\n  if (pNtQuerySystemInformation == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  kernel32_module = GetModuleHandleA(\"kernel32.dll\");\n  if (kernel32_module == NULL) {\n    uv_fatal_error(GetLastError(), \"GetModuleHandleA\");\n  }\n\n  pGetQueuedCompletionStatusEx = (sGetQueuedCompletionStatusEx) GetProcAddress(\n      kernel32_module,\n      \"GetQueuedCompletionStatusEx\");\n\n  pSetFileCompletionNotificationModes = (sSetFileCompletionNotificationModes)\n    GetProcAddress(kernel32_module, \"SetFileCompletionNotificationModes\");\n\n  pCreateSymbolicLinkW = (sCreateSymbolicLinkW)\n    GetProcAddress(kernel32_module, \"CreateSymbolicLinkW\");\n\n  pCancelIoEx = (sCancelIoEx)\n    GetProcAddress(kernel32_module, \"CancelIoEx\");\n\n  pInitializeSRWLock = (sInitializeSRWLock)\n    GetProcAddress(kernel32_module, \"InitializeSRWLock\");\n\n  pAcquireSRWLockShared = (sAcquireSRWLockShared)\n    GetProcAddress(kernel32_module, \"AcquireSRWLockShared\");\n\n  pAcquireSRWLockExclusive = (sAcquireSRWLockExclusive)\n    GetProcAddress(kernel32_module, \"AcquireSRWLockExclusive\");\n\n  pTryAcquireSRWLockShared = (sTryAcquireSRWLockShared)\n    GetProcAddress(kernel32_module, \"TryAcquireSRWLockShared\");\n\n  pTryAcquireSRWLockExclusive = (sTryAcquireSRWLockExclusive)\n    GetProcAddress(kernel32_module, \"TryAcquireSRWLockExclusive\");\n\n  pReleaseSRWLockShared = (sReleaseSRWLockShared)\n    GetProcAddress(kernel32_module, \"ReleaseSRWLockShared\");\n\n  pReleaseSRWLockExclusive = (sReleaseSRWLockExclusive)\n    GetProcAddress(kernel32_module, \"ReleaseSRWLockExclusive\");\n\n  pInitializeConditionVariable = (sInitializeConditionVariable)\n    GetProcAddress(kernel32_module, \"InitializeConditionVariable\");\n\n  pSleepConditionVariableCS = (sSleepConditionVariableCS)\n    GetProcAddress(kernel32_module, \"SleepConditionVariableCS\");\n\n  pSleepConditionVariableSRW = (sSleepConditionVariableSRW)\n    GetProcAddress(kernel32_module, \"SleepConditionVariableSRW\");\n\n  pWakeAllConditionVariable = (sWakeAllConditionVariable)\n    GetProcAddress(kernel32_module, \"WakeAllConditionVariable\");\n\n  pWakeConditionVariable = (sWakeConditionVariable)\n    GetProcAddress(kernel32_module, \"WakeConditionVariable\");\n}",
        "func": "void uv_winapi_init() {\n  HMODULE ntdll_module;\n  HMODULE kernel32_module;\n\n  ntdll_module = GetModuleHandleA(\"ntdll.dll\");\n  if (ntdll_module == NULL) {\n    uv_fatal_error(GetLastError(), \"GetModuleHandleA\");\n  }\n\n  pRtlNtStatusToDosError = (sRtlNtStatusToDosError) GetProcAddress(\n      ntdll_module,\n      \"RtlNtStatusToDosError\");\n  if (pRtlNtStatusToDosError == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  pNtQueryInformationFile = (sNtQueryInformationFile) GetProcAddress(\n      ntdll_module,\n      \"NtQueryInformationFile\");\n  if (pNtQueryInformationFile == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  pNtDeviceIoControlFile = (sNtDeviceIoControlFile) GetProcAddress(\n      ntdll_module,\n      \"NtDeviceIoControlFile\");\n  if (pNtDeviceIoControlFile == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  pNtSetInformationFile = (sNtSetInformationFile) GetProcAddress(\n      ntdll_module,\n      \"NtSetInformationFile\");\n  if (pNtSetInformationFile == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  pNtQuerySystemInformation = (sNtQuerySystemInformation) GetProcAddress(\n      ntdll_module,\n      \"NtQuerySystemInformation\");\n  if (pNtQuerySystemInformation == NULL) {\n    uv_fatal_error(GetLastError(), \"GetProcAddress\");\n  }\n\n  kernel32_module = GetModuleHandleA(\"kernel32.dll\");\n  if (kernel32_module == NULL) {\n    uv_fatal_error(GetLastError(), \"GetModuleHandleA\");\n  }\n\n  pGetQueuedCompletionStatusEx = (sGetQueuedCompletionStatusEx) GetProcAddress(\n      kernel32_module,\n      \"GetQueuedCompletionStatusEx\");\n\n  pSetFileCompletionNotificationModes = (sSetFileCompletionNotificationModes)\n    GetProcAddress(kernel32_module, \"SetFileCompletionNotificationModes\");\n\n  pCreateSymbolicLinkW = (sCreateSymbolicLinkW)\n    GetProcAddress(kernel32_module, \"CreateSymbolicLinkW\");\n\n  pCancelIoEx = (sCancelIoEx)\n    GetProcAddress(kernel32_module, \"CancelIoEx\");\n\n  pInitializeConditionVariable = (sInitializeConditionVariable)\n    GetProcAddress(kernel32_module, \"InitializeConditionVariable\");\n\n  pSleepConditionVariableCS = (sSleepConditionVariableCS)\n    GetProcAddress(kernel32_module, \"SleepConditionVariableCS\");\n\n  pSleepConditionVariableSRW = (sSleepConditionVariableSRW)\n    GetProcAddress(kernel32_module, \"SleepConditionVariableSRW\");\n\n  pWakeAllConditionVariable = (sWakeAllConditionVariable)\n    GetProcAddress(kernel32_module, \"WakeAllConditionVariable\");\n\n  pWakeConditionVariable = (sWakeConditionVariable)\n    GetProcAddress(kernel32_module, \"WakeConditionVariable\");\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -60,27 +60,6 @@\n   pCancelIoEx = (sCancelIoEx)\n     GetProcAddress(kernel32_module, \"CancelIoEx\");\n \n-  pInitializeSRWLock = (sInitializeSRWLock)\n-    GetProcAddress(kernel32_module, \"InitializeSRWLock\");\n-\n-  pAcquireSRWLockShared = (sAcquireSRWLockShared)\n-    GetProcAddress(kernel32_module, \"AcquireSRWLockShared\");\n-\n-  pAcquireSRWLockExclusive = (sAcquireSRWLockExclusive)\n-    GetProcAddress(kernel32_module, \"AcquireSRWLockExclusive\");\n-\n-  pTryAcquireSRWLockShared = (sTryAcquireSRWLockShared)\n-    GetProcAddress(kernel32_module, \"TryAcquireSRWLockShared\");\n-\n-  pTryAcquireSRWLockExclusive = (sTryAcquireSRWLockExclusive)\n-    GetProcAddress(kernel32_module, \"TryAcquireSRWLockExclusive\");\n-\n-  pReleaseSRWLockShared = (sReleaseSRWLockShared)\n-    GetProcAddress(kernel32_module, \"ReleaseSRWLockShared\");\n-\n-  pReleaseSRWLockExclusive = (sReleaseSRWLockExclusive)\n-    GetProcAddress(kernel32_module, \"ReleaseSRWLockExclusive\");\n-\n   pInitializeConditionVariable = (sInitializeConditionVariable)\n     GetProcAddress(kernel32_module, \"InitializeConditionVariable\");\n ",
        "diff_line_info": {
            "deleted_lines": [
                "  pInitializeSRWLock = (sInitializeSRWLock)",
                "    GetProcAddress(kernel32_module, \"InitializeSRWLock\");",
                "",
                "  pAcquireSRWLockShared = (sAcquireSRWLockShared)",
                "    GetProcAddress(kernel32_module, \"AcquireSRWLockShared\");",
                "",
                "  pAcquireSRWLockExclusive = (sAcquireSRWLockExclusive)",
                "    GetProcAddress(kernel32_module, \"AcquireSRWLockExclusive\");",
                "",
                "  pTryAcquireSRWLockShared = (sTryAcquireSRWLockShared)",
                "    GetProcAddress(kernel32_module, \"TryAcquireSRWLockShared\");",
                "",
                "  pTryAcquireSRWLockExclusive = (sTryAcquireSRWLockExclusive)",
                "    GetProcAddress(kernel32_module, \"TryAcquireSRWLockExclusive\");",
                "",
                "  pReleaseSRWLockShared = (sReleaseSRWLockShared)",
                "    GetProcAddress(kernel32_module, \"ReleaseSRWLockShared\");",
                "",
                "  pReleaseSRWLockExclusive = (sReleaseSRWLockExclusive)",
                "    GetProcAddress(kernel32_module, \"ReleaseSRWLockExclusive\");",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "nodejs/node/crypto_lock_init",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/nodejs/node/commit/7ee58bebff8074610d83bdf75ce01e9833b58a0e",
        "commit_title": "crypto: replace rwlocks with simple mutexes",
        "commit_text": " It was pointed out by Zhou Ran that the Windows XP implementation of uv_rwlock_rdlock() and friends may unlock the inner write mutex on a different thread than the one that locked it, resulting in undefined behavior.  The only place that uses rwlocks is the crypto module.  Make that use normal (simple) mutexes instead.  OpenSSL's critical sections are generally very short, with exclusive access outnumbering shared access by a factor of three or more, so it's not as if using rwlocks gives a decisive performance advantage.  PR-URL: https://github.com/nodejs/node/pull/2723 Reviewed-By: Fedor Indutny <fedor@indutny.com>",
        "func_before": "static void crypto_lock_init(void) {\n  int i, n;\n\n  n = CRYPTO_num_locks();\n  locks = new uv_rwlock_t[n];\n\n  for (i = 0; i < n; i++)\n    if (uv_rwlock_init(locks + i))\n      abort();\n}",
        "func": "static void crypto_lock_init(void) {\n  int i, n;\n\n  n = CRYPTO_num_locks();\n  locks = new uv_mutex_t[n];\n\n  for (i = 0; i < n; i++)\n    if (uv_mutex_init(locks + i))\n      abort();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,9 +2,9 @@\n   int i, n;\n \n   n = CRYPTO_num_locks();\n-  locks = new uv_rwlock_t[n];\n+  locks = new uv_mutex_t[n];\n \n   for (i = 0; i < n; i++)\n-    if (uv_rwlock_init(locks + i))\n+    if (uv_mutex_init(locks + i))\n       abort();\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  locks = new uv_rwlock_t[n];",
                "    if (uv_rwlock_init(locks + i))"
            ],
            "added_lines": [
                "  locks = new uv_mutex_t[n];",
                "    if (uv_mutex_init(locks + i))"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "nodejs/node/crypto_lock_cb",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/nodejs/node/commit/7ee58bebff8074610d83bdf75ce01e9833b58a0e",
        "commit_title": "crypto: replace rwlocks with simple mutexes",
        "commit_text": " It was pointed out by Zhou Ran that the Windows XP implementation of uv_rwlock_rdlock() and friends may unlock the inner write mutex on a different thread than the one that locked it, resulting in undefined behavior.  The only place that uses rwlocks is the crypto module.  Make that use normal (simple) mutexes instead.  OpenSSL's critical sections are generally very short, with exclusive access outnumbering shared access by a factor of three or more, so it's not as if using rwlocks gives a decisive performance advantage.  PR-URL: https://github.com/nodejs/node/pull/2723 Reviewed-By: Fedor Indutny <fedor@indutny.com>",
        "func_before": "static void crypto_lock_cb(int mode, int n, const char* file, int line) {\n  CHECK((mode & CRYPTO_LOCK) || (mode & CRYPTO_UNLOCK));\n  CHECK((mode & CRYPTO_READ) || (mode & CRYPTO_WRITE));\n\n  if (mode & CRYPTO_LOCK) {\n    if (mode & CRYPTO_READ)\n      uv_rwlock_rdlock(locks + n);\n    else\n      uv_rwlock_wrlock(locks + n);\n  } else {\n    if (mode & CRYPTO_READ)\n      uv_rwlock_rdunlock(locks + n);\n    else\n      uv_rwlock_wrunlock(locks + n);\n  }\n}",
        "func": "static void crypto_lock_cb(int mode, int n, const char* file, int line) {\n  CHECK(!(mode & CRYPTO_LOCK) ^ !(mode & CRYPTO_UNLOCK));\n  CHECK(!(mode & CRYPTO_READ) ^ !(mode & CRYPTO_WRITE));\n\n  if (mode & CRYPTO_LOCK)\n    uv_mutex_lock(locks + n);\n  else\n    uv_mutex_unlock(locks + n);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,16 +1,9 @@\n static void crypto_lock_cb(int mode, int n, const char* file, int line) {\n-  CHECK((mode & CRYPTO_LOCK) || (mode & CRYPTO_UNLOCK));\n-  CHECK((mode & CRYPTO_READ) || (mode & CRYPTO_WRITE));\n+  CHECK(!(mode & CRYPTO_LOCK) ^ !(mode & CRYPTO_UNLOCK));\n+  CHECK(!(mode & CRYPTO_READ) ^ !(mode & CRYPTO_WRITE));\n \n-  if (mode & CRYPTO_LOCK) {\n-    if (mode & CRYPTO_READ)\n-      uv_rwlock_rdlock(locks + n);\n-    else\n-      uv_rwlock_wrlock(locks + n);\n-  } else {\n-    if (mode & CRYPTO_READ)\n-      uv_rwlock_rdunlock(locks + n);\n-    else\n-      uv_rwlock_wrunlock(locks + n);\n-  }\n+  if (mode & CRYPTO_LOCK)\n+    uv_mutex_lock(locks + n);\n+  else\n+    uv_mutex_unlock(locks + n);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  CHECK((mode & CRYPTO_LOCK) || (mode & CRYPTO_UNLOCK));",
                "  CHECK((mode & CRYPTO_READ) || (mode & CRYPTO_WRITE));",
                "  if (mode & CRYPTO_LOCK) {",
                "    if (mode & CRYPTO_READ)",
                "      uv_rwlock_rdlock(locks + n);",
                "    else",
                "      uv_rwlock_wrlock(locks + n);",
                "  } else {",
                "    if (mode & CRYPTO_READ)",
                "      uv_rwlock_rdunlock(locks + n);",
                "    else",
                "      uv_rwlock_wrunlock(locks + n);",
                "  }"
            ],
            "added_lines": [
                "  CHECK(!(mode & CRYPTO_LOCK) ^ !(mode & CRYPTO_UNLOCK));",
                "  CHECK(!(mode & CRYPTO_READ) ^ !(mode & CRYPTO_WRITE));",
                "  if (mode & CRYPTO_LOCK)",
                "    uv_mutex_lock(locks + n);",
                "  else",
                "    uv_mutex_unlock(locks + n);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv__rwlock_fallback_init",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/3eb6764acd2d708f6873c177a77f9bef3b266fa9",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  PR-URL: https://github.com/libuv/libuv/pull/516 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Saúl Ibarra Corretgé <saghul@gmail.com>",
        "func_before": "static int uv__rwlock_fallback_init(uv_rwlock_t* rwlock) {\n  int err;\n\n  err = uv_mutex_init(&rwlock->fallback_.read_mutex_);\n  if (err)\n    return err;\n\n  err = uv_mutex_init(&rwlock->fallback_.write_mutex_);\n  if (err) {\n    uv_mutex_destroy(&rwlock->fallback_.read_mutex_);\n    return err;\n  }\n\n  rwlock->fallback_.num_readers_ = 0;\n\n  return 0;\n}",
        "func": "static int uv__rwlock_fallback_init(uv_rwlock_t* rwlock) {\n  /* Initialize the semaphore that acts as the write lock. */\n  HANDLE handle = CreateSemaphoreW(NULL, 1, 1, NULL);\n  if (handle == NULL)\n    return uv_translate_sys_error(GetLastError());\n  rwlock->fallback_.write_lock_.sem = handle;\n\n  /* Initialize the critical section protecting the reader count. */\n  InitializeCriticalSection(&rwlock->fallback_.read_lock_.cs);\n\n  /* Initialize the reader count. */\n  rwlock->fallback_.num_readers_ = 0;\n\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,16 +1,14 @@\n static int uv__rwlock_fallback_init(uv_rwlock_t* rwlock) {\n-  int err;\n+  /* Initialize the semaphore that acts as the write lock. */\n+  HANDLE handle = CreateSemaphoreW(NULL, 1, 1, NULL);\n+  if (handle == NULL)\n+    return uv_translate_sys_error(GetLastError());\n+  rwlock->fallback_.write_lock_.sem = handle;\n \n-  err = uv_mutex_init(&rwlock->fallback_.read_mutex_);\n-  if (err)\n-    return err;\n+  /* Initialize the critical section protecting the reader count. */\n+  InitializeCriticalSection(&rwlock->fallback_.read_lock_.cs);\n \n-  err = uv_mutex_init(&rwlock->fallback_.write_mutex_);\n-  if (err) {\n-    uv_mutex_destroy(&rwlock->fallback_.read_mutex_);\n-    return err;\n-  }\n-\n+  /* Initialize the reader count. */\n   rwlock->fallback_.num_readers_ = 0;\n \n   return 0;",
        "diff_line_info": {
            "deleted_lines": [
                "  int err;",
                "  err = uv_mutex_init(&rwlock->fallback_.read_mutex_);",
                "  if (err)",
                "    return err;",
                "  err = uv_mutex_init(&rwlock->fallback_.write_mutex_);",
                "  if (err) {",
                "    uv_mutex_destroy(&rwlock->fallback_.read_mutex_);",
                "    return err;",
                "  }",
                ""
            ],
            "added_lines": [
                "  /* Initialize the semaphore that acts as the write lock. */",
                "  HANDLE handle = CreateSemaphoreW(NULL, 1, 1, NULL);",
                "  if (handle == NULL)",
                "    return uv_translate_sys_error(GetLastError());",
                "  rwlock->fallback_.write_lock_.sem = handle;",
                "  /* Initialize the critical section protecting the reader count. */",
                "  InitializeCriticalSection(&rwlock->fallback_.read_lock_.cs);",
                "  /* Initialize the reader count. */"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv__rwlock_fallback_trywrlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/3eb6764acd2d708f6873c177a77f9bef3b266fa9",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  PR-URL: https://github.com/libuv/libuv/pull/516 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Saúl Ibarra Corretgé <saghul@gmail.com>",
        "func_before": "static int uv__rwlock_fallback_trywrlock(uv_rwlock_t* rwlock) {\n  return uv_mutex_trylock(&rwlock->fallback_.write_mutex_);\n}",
        "func": "static int uv__rwlock_fallback_trywrlock(uv_rwlock_t* rwlock) {\n  DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, 0);\n  if (r == WAIT_OBJECT_0)\n    return 0;\n  else if (r == WAIT_TIMEOUT)\n    return UV_EAGAIN;\n  else if (r == WAIT_FAILED)\n    return uv_translate_sys_error(GetLastError());\n  else\n    return UV_EIO;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,3 +1,11 @@\n static int uv__rwlock_fallback_trywrlock(uv_rwlock_t* rwlock) {\n-  return uv_mutex_trylock(&rwlock->fallback_.write_mutex_);\n+  DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, 0);\n+  if (r == WAIT_OBJECT_0)\n+    return 0;\n+  else if (r == WAIT_TIMEOUT)\n+    return UV_EAGAIN;\n+  else if (r == WAIT_FAILED)\n+    return uv_translate_sys_error(GetLastError());\n+  else\n+    return UV_EIO;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  return uv_mutex_trylock(&rwlock->fallback_.write_mutex_);"
            ],
            "added_lines": [
                "  DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, 0);",
                "  if (r == WAIT_OBJECT_0)",
                "    return 0;",
                "  else if (r == WAIT_TIMEOUT)",
                "    return UV_EAGAIN;",
                "  else if (r == WAIT_FAILED)",
                "    return uv_translate_sys_error(GetLastError());",
                "  else",
                "    return UV_EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv__rwlock_fallback_wrunlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/3eb6764acd2d708f6873c177a77f9bef3b266fa9",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  PR-URL: https://github.com/libuv/libuv/pull/516 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Saúl Ibarra Corretgé <saghul@gmail.com>",
        "func_before": "static void uv__rwlock_fallback_wrunlock(uv_rwlock_t* rwlock) {\n  uv_mutex_unlock(&rwlock->fallback_.write_mutex_);\n}",
        "func": "static void uv__rwlock_fallback_wrunlock(uv_rwlock_t* rwlock) {\n  if (!ReleaseSemaphore(rwlock->fallback_.write_lock_.sem, 1, NULL))\n    uv_fatal_error(GetLastError(), \"ReleaseSemaphore\");\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,3 +1,4 @@\n static void uv__rwlock_fallback_wrunlock(uv_rwlock_t* rwlock) {\n-  uv_mutex_unlock(&rwlock->fallback_.write_mutex_);\n+  if (!ReleaseSemaphore(rwlock->fallback_.write_lock_.sem, 1, NULL))\n+    uv_fatal_error(GetLastError(), \"ReleaseSemaphore\");\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  uv_mutex_unlock(&rwlock->fallback_.write_mutex_);"
            ],
            "added_lines": [
                "  if (!ReleaseSemaphore(rwlock->fallback_.write_lock_.sem, 1, NULL))",
                "    uv_fatal_error(GetLastError(), \"ReleaseSemaphore\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv__rwlock_fallback_tryrdlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/3eb6764acd2d708f6873c177a77f9bef3b266fa9",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  PR-URL: https://github.com/libuv/libuv/pull/516 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Saúl Ibarra Corretgé <saghul@gmail.com>",
        "func_before": "static int uv__rwlock_fallback_tryrdlock(uv_rwlock_t* rwlock) {\n  int err;\n\n  err = uv_mutex_trylock(&rwlock->fallback_.read_mutex_);\n  if (err)\n    goto out;\n\n  err = 0;\n  if (rwlock->fallback_.num_readers_ == 0)\n    err = uv_mutex_trylock(&rwlock->fallback_.write_mutex_);\n\n  if (err == 0)\n    rwlock->fallback_.num_readers_++;\n\n  uv_mutex_unlock(&rwlock->fallback_.read_mutex_);\n\nout:\n  return err;\n}",
        "func": "static int uv__rwlock_fallback_tryrdlock(uv_rwlock_t* rwlock) {\n  int err;\n\n  if (!TryEnterCriticalSection(&rwlock->fallback_.read_lock_.cs))\n    return UV_EAGAIN;\n\n  err = 0;\n  if (rwlock->fallback_.num_readers_ == 0) {\n    DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, 0);\n    if (r == WAIT_OBJECT_0)\n      rwlock->fallback_.num_readers_++;\n    else if (r == WAIT_TIMEOUT)\n      err = UV_EAGAIN;\n    else if (r == WAIT_FAILED)\n      err = uv_translate_sys_error(GetLastError());\n    else\n      err = UV_EIO;\n  }\n\n  LeaveCriticalSection(&rwlock->fallback_.read_lock_.cs);\n  return err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,19 +1,22 @@\n static int uv__rwlock_fallback_tryrdlock(uv_rwlock_t* rwlock) {\n   int err;\n \n-  err = uv_mutex_trylock(&rwlock->fallback_.read_mutex_);\n-  if (err)\n-    goto out;\n+  if (!TryEnterCriticalSection(&rwlock->fallback_.read_lock_.cs))\n+    return UV_EAGAIN;\n \n   err = 0;\n-  if (rwlock->fallback_.num_readers_ == 0)\n-    err = uv_mutex_trylock(&rwlock->fallback_.write_mutex_);\n+  if (rwlock->fallback_.num_readers_ == 0) {\n+    DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, 0);\n+    if (r == WAIT_OBJECT_0)\n+      rwlock->fallback_.num_readers_++;\n+    else if (r == WAIT_TIMEOUT)\n+      err = UV_EAGAIN;\n+    else if (r == WAIT_FAILED)\n+      err = uv_translate_sys_error(GetLastError());\n+    else\n+      err = UV_EIO;\n+  }\n \n-  if (err == 0)\n-    rwlock->fallback_.num_readers_++;\n-\n-  uv_mutex_unlock(&rwlock->fallback_.read_mutex_);\n-\n-out:\n+  LeaveCriticalSection(&rwlock->fallback_.read_lock_.cs);\n   return err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  err = uv_mutex_trylock(&rwlock->fallback_.read_mutex_);",
                "  if (err)",
                "    goto out;",
                "  if (rwlock->fallback_.num_readers_ == 0)",
                "    err = uv_mutex_trylock(&rwlock->fallback_.write_mutex_);",
                "  if (err == 0)",
                "    rwlock->fallback_.num_readers_++;",
                "",
                "  uv_mutex_unlock(&rwlock->fallback_.read_mutex_);",
                "",
                "out:"
            ],
            "added_lines": [
                "  if (!TryEnterCriticalSection(&rwlock->fallback_.read_lock_.cs))",
                "    return UV_EAGAIN;",
                "  if (rwlock->fallback_.num_readers_ == 0) {",
                "    DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, 0);",
                "    if (r == WAIT_OBJECT_0)",
                "      rwlock->fallback_.num_readers_++;",
                "    else if (r == WAIT_TIMEOUT)",
                "      err = UV_EAGAIN;",
                "    else if (r == WAIT_FAILED)",
                "      err = uv_translate_sys_error(GetLastError());",
                "    else",
                "      err = UV_EIO;",
                "  }",
                "  LeaveCriticalSection(&rwlock->fallback_.read_lock_.cs);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv__rwlock_fallback_rdlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/3eb6764acd2d708f6873c177a77f9bef3b266fa9",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  PR-URL: https://github.com/libuv/libuv/pull/516 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Saúl Ibarra Corretgé <saghul@gmail.com>",
        "func_before": "static void uv__rwlock_fallback_rdlock(uv_rwlock_t* rwlock) {\n  uv_mutex_lock(&rwlock->fallback_.read_mutex_);\n\n  if (++rwlock->fallback_.num_readers_ == 1)\n    uv_mutex_lock(&rwlock->fallback_.write_mutex_);\n\n  uv_mutex_unlock(&rwlock->fallback_.read_mutex_);\n}",
        "func": "static void uv__rwlock_fallback_rdlock(uv_rwlock_t* rwlock) {\n  /* Acquire the lock that protects the reader count. */\n  EnterCriticalSection(&rwlock->fallback_.read_lock_.cs);\n\n  /* Increase the reader count, and lock for write if this is the first\n   * reader.\n   */\n  if (++rwlock->fallback_.num_readers_ == 1) {\n    DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, INFINITE);\n    if (r != WAIT_OBJECT_0)\n      uv_fatal_error(GetLastError(), \"WaitForSingleObject\");\n  }\n\n  /* Release the lock that protects the reader count. */\n  LeaveCriticalSection(&rwlock->fallback_.read_lock_.cs);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,16 @@\n static void uv__rwlock_fallback_rdlock(uv_rwlock_t* rwlock) {\n-  uv_mutex_lock(&rwlock->fallback_.read_mutex_);\n+  /* Acquire the lock that protects the reader count. */\n+  EnterCriticalSection(&rwlock->fallback_.read_lock_.cs);\n \n-  if (++rwlock->fallback_.num_readers_ == 1)\n-    uv_mutex_lock(&rwlock->fallback_.write_mutex_);\n+  /* Increase the reader count, and lock for write if this is the first\n+   * reader.\n+   */\n+  if (++rwlock->fallback_.num_readers_ == 1) {\n+    DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, INFINITE);\n+    if (r != WAIT_OBJECT_0)\n+      uv_fatal_error(GetLastError(), \"WaitForSingleObject\");\n+  }\n \n-  uv_mutex_unlock(&rwlock->fallback_.read_mutex_);\n+  /* Release the lock that protects the reader count. */\n+  LeaveCriticalSection(&rwlock->fallback_.read_lock_.cs);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  uv_mutex_lock(&rwlock->fallback_.read_mutex_);",
                "  if (++rwlock->fallback_.num_readers_ == 1)",
                "    uv_mutex_lock(&rwlock->fallback_.write_mutex_);",
                "  uv_mutex_unlock(&rwlock->fallback_.read_mutex_);"
            ],
            "added_lines": [
                "  /* Acquire the lock that protects the reader count. */",
                "  EnterCriticalSection(&rwlock->fallback_.read_lock_.cs);",
                "  /* Increase the reader count, and lock for write if this is the first",
                "   * reader.",
                "   */",
                "  if (++rwlock->fallback_.num_readers_ == 1) {",
                "    DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, INFINITE);",
                "    if (r != WAIT_OBJECT_0)",
                "      uv_fatal_error(GetLastError(), \"WaitForSingleObject\");",
                "  }",
                "  /* Release the lock that protects the reader count. */",
                "  LeaveCriticalSection(&rwlock->fallback_.read_lock_.cs);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv__rwlock_fallback_destroy",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/3eb6764acd2d708f6873c177a77f9bef3b266fa9",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  PR-URL: https://github.com/libuv/libuv/pull/516 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Saúl Ibarra Corretgé <saghul@gmail.com>",
        "func_before": "static void uv__rwlock_fallback_destroy(uv_rwlock_t* rwlock) {\n  uv_mutex_destroy(&rwlock->fallback_.read_mutex_);\n  uv_mutex_destroy(&rwlock->fallback_.write_mutex_);\n}",
        "func": "static void uv__rwlock_fallback_destroy(uv_rwlock_t* rwlock) {\n  DeleteCriticalSection(&rwlock->fallback_.read_lock_.cs);\n  CloseHandle(rwlock->fallback_.write_lock_.sem);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n static void uv__rwlock_fallback_destroy(uv_rwlock_t* rwlock) {\n-  uv_mutex_destroy(&rwlock->fallback_.read_mutex_);\n-  uv_mutex_destroy(&rwlock->fallback_.write_mutex_);\n+  DeleteCriticalSection(&rwlock->fallback_.read_lock_.cs);\n+  CloseHandle(rwlock->fallback_.write_lock_.sem);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  uv_mutex_destroy(&rwlock->fallback_.read_mutex_);",
                "  uv_mutex_destroy(&rwlock->fallback_.write_mutex_);"
            ],
            "added_lines": [
                "  DeleteCriticalSection(&rwlock->fallback_.read_lock_.cs);",
                "  CloseHandle(rwlock->fallback_.write_lock_.sem);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv__rwlock_fallback_wrlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/3eb6764acd2d708f6873c177a77f9bef3b266fa9",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  PR-URL: https://github.com/libuv/libuv/pull/516 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Saúl Ibarra Corretgé <saghul@gmail.com>",
        "func_before": "static void uv__rwlock_fallback_wrlock(uv_rwlock_t* rwlock) {\n  uv_mutex_lock(&rwlock->fallback_.write_mutex_);\n}",
        "func": "static void uv__rwlock_fallback_wrlock(uv_rwlock_t* rwlock) {\n  DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, INFINITE);\n  if (r != WAIT_OBJECT_0)\n    uv_fatal_error(GetLastError(), \"WaitForSingleObject\");\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,3 +1,5 @@\n static void uv__rwlock_fallback_wrlock(uv_rwlock_t* rwlock) {\n-  uv_mutex_lock(&rwlock->fallback_.write_mutex_);\n+  DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, INFINITE);\n+  if (r != WAIT_OBJECT_0)\n+    uv_fatal_error(GetLastError(), \"WaitForSingleObject\");\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  uv_mutex_lock(&rwlock->fallback_.write_mutex_);"
            ],
            "added_lines": [
                "  DWORD r = WaitForSingleObject(rwlock->fallback_.write_lock_.sem, INFINITE);",
                "  if (r != WAIT_OBJECT_0)",
                "    uv_fatal_error(GetLastError(), \"WaitForSingleObject\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-9748",
        "func_name": "libuv/uv__rwlock_fallback_rdunlock",
        "description": "The uv_rwlock_t fallback implementation for Windows XP and Server 2003 in libuv before 1.7.4 does not properly prevent threads from releasing the locks of other threads, which allows attackers to cause a denial of service (deadlock) or possibly have unspecified other impact by leveraging a race condition.",
        "git_url": "https://github.com/libuv/libuv/commit/3eb6764acd2d708f6873c177a77f9bef3b266fa9",
        "commit_title": "win: fix unsavory rwlock fallback implementation",
        "commit_text": " Before this patch an uv_mutex_t (backed by a critical section) could be released by a tread different from the thread that acquired it, which is not allowed. This is fixed by using a semaphore instead.  Note that the affected code paths were used on Windows XP and Windows Server 2003 only.  PR-URL: https://github.com/libuv/libuv/pull/516 Reviewed-By: Ben Noordhuis <info@bnoordhuis.nl> Reviewed-By: Saúl Ibarra Corretgé <saghul@gmail.com>",
        "func_before": "static void uv__rwlock_fallback_rdunlock(uv_rwlock_t* rwlock) {\n  uv_mutex_lock(&rwlock->fallback_.read_mutex_);\n\n  if (--rwlock->fallback_.num_readers_ == 0)\n    uv_mutex_unlock(&rwlock->fallback_.write_mutex_);\n\n  uv_mutex_unlock(&rwlock->fallback_.read_mutex_);\n}",
        "func": "static void uv__rwlock_fallback_rdunlock(uv_rwlock_t* rwlock) {\n  EnterCriticalSection(&rwlock->fallback_.read_lock_.cs);\n\n  if (--rwlock->fallback_.num_readers_ == 0) {\n    if (!ReleaseSemaphore(rwlock->fallback_.write_lock_.sem, 1, NULL))\n      uv_fatal_error(GetLastError(), \"ReleaseSemaphore\");\n  }\n\n  LeaveCriticalSection(&rwlock->fallback_.read_lock_.cs);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,10 @@\n static void uv__rwlock_fallback_rdunlock(uv_rwlock_t* rwlock) {\n-  uv_mutex_lock(&rwlock->fallback_.read_mutex_);\n+  EnterCriticalSection(&rwlock->fallback_.read_lock_.cs);\n \n-  if (--rwlock->fallback_.num_readers_ == 0)\n-    uv_mutex_unlock(&rwlock->fallback_.write_mutex_);\n+  if (--rwlock->fallback_.num_readers_ == 0) {\n+    if (!ReleaseSemaphore(rwlock->fallback_.write_lock_.sem, 1, NULL))\n+      uv_fatal_error(GetLastError(), \"ReleaseSemaphore\");\n+  }\n \n-  uv_mutex_unlock(&rwlock->fallback_.read_mutex_);\n+  LeaveCriticalSection(&rwlock->fallback_.read_lock_.cs);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  uv_mutex_lock(&rwlock->fallback_.read_mutex_);",
                "  if (--rwlock->fallback_.num_readers_ == 0)",
                "    uv_mutex_unlock(&rwlock->fallback_.write_mutex_);",
                "  uv_mutex_unlock(&rwlock->fallback_.read_mutex_);"
            ],
            "added_lines": [
                "  EnterCriticalSection(&rwlock->fallback_.read_lock_.cs);",
                "  if (--rwlock->fallback_.num_readers_ == 0) {",
                "    if (!ReleaseSemaphore(rwlock->fallback_.write_lock_.sem, 1, NULL))",
                "      uv_fatal_error(GetLastError(), \"ReleaseSemaphore\");",
                "  }",
                "  LeaveCriticalSection(&rwlock->fallback_.read_lock_.cs);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45884",
        "func_name": "torvalds/linux/dvb_register_device",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvbdev.c has a use-after-free, related to dvb_register_device dynamically allocating fops.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=627bb528b086b4136315c25d6a447a98ea9448d3",
        "commit_title": "dvb_register_device() dynamically allocates fops with kmemdup()",
        "commit_text": "to set the fops->owner. And these fops are registered in 'file->f_ops' using replace_fops() in the dvb_device_open() process, and kfree()d in dvb_free_device().  However, it is not common to use dynamically allocated fops instead of 'static const' fops as an argument of replace_fops(), and UAF may occur. These UAFs can occur on any dvb type using dvb_register_device(), such as dvb_dvr, dvb_demux, dvb_frontend, dvb_net, etc.  So, instead of kfree() the fops dynamically allocated in dvb_register_device() in dvb_free_device() called during the .disconnect() process, kfree() it collectively in exit_dvbdev() called when the dvbdev.c module is removed.  Link: https://lore.kernel.org/linux-media/20221117045925.14297-4-imv4bel@gmail.com ",
        "func_before": "int dvb_register_device(struct dvb_adapter *adap, struct dvb_device **pdvbdev,\n\t\t\tconst struct dvb_device *template, void *priv,\n\t\t\tenum dvb_device_type type, int demux_sink_pads)\n{\n\tstruct dvb_device *dvbdev;\n\tstruct file_operations *dvbdevfops;\n\tstruct device *clsdev;\n\tint minor;\n\tint id, ret;\n\n\tmutex_lock(&dvbdev_register_lock);\n\n\tif ((id = dvbdev_get_free_id (adap, type)) < 0){\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\t*pdvbdev = NULL;\n\t\tpr_err(\"%s: couldn't find free device id\\n\", __func__);\n\t\treturn -ENFILE;\n\t}\n\n\t*pdvbdev = dvbdev = kzalloc(sizeof(*dvbdev), GFP_KERNEL);\n\n\tif (!dvbdev){\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\tdvbdevfops = kmemdup(template->fops, sizeof(*dvbdevfops), GFP_KERNEL);\n\n\tif (!dvbdevfops){\n\t\tkfree (dvbdev);\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\tmemcpy(dvbdev, template, sizeof(struct dvb_device));\n\tkref_init(&dvbdev->ref);\n\tdvbdev->type = type;\n\tdvbdev->id = id;\n\tdvbdev->adapter = adap;\n\tdvbdev->priv = priv;\n\tdvbdev->fops = dvbdevfops;\n\tinit_waitqueue_head (&dvbdev->wait_queue);\n\n\tdvbdevfops->owner = adap->module;\n\n\tlist_add_tail (&dvbdev->list_head, &adap->device_list);\n\n\tdown_write(&minor_rwsem);\n#ifdef CONFIG_DVB_DYNAMIC_MINORS\n\tfor (minor = 0; minor < MAX_DVB_MINORS; minor++)\n\t\tif (dvb_minors[minor] == NULL)\n\t\t\tbreak;\n\n\tif (minor == MAX_DVB_MINORS) {\n\t\tlist_del (&dvbdev->list_head);\n\t\tkfree(dvbdevfops);\n\t\tkfree(dvbdev);\n\t\tup_write(&minor_rwsem);\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\treturn -EINVAL;\n\t}\n#else\n\tminor = nums2minor(adap->num, type, id);\n#endif\n\n\tdvbdev->minor = minor;\n\tdvb_minors[minor] = dvb_device_get(dvbdev);\n\tup_write(&minor_rwsem);\n\n\tret = dvb_register_media_device(dvbdev, type, minor, demux_sink_pads);\n\tif (ret) {\n\t\tpr_err(\"%s: dvb_register_media_device failed to create the mediagraph\\n\",\n\t\t      __func__);\n\n\t\tdvb_media_device_free(dvbdev);\n\t\tlist_del (&dvbdev->list_head);\n\t\tkfree(dvbdevfops);\n\t\tkfree(dvbdev);\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\treturn ret;\n\t}\n\n\tmutex_unlock(&dvbdev_register_lock);\n\n\tclsdev = device_create(dvb_class, adap->device,\n\t\t\t       MKDEV(DVB_MAJOR, minor),\n\t\t\t       dvbdev, \"dvb%d.%s%d\", adap->num, dnames[type], id);\n\tif (IS_ERR(clsdev)) {\n\t\tpr_err(\"%s: failed to create device dvb%d.%s%d (%ld)\\n\",\n\t\t       __func__, adap->num, dnames[type], id, PTR_ERR(clsdev));\n\t\tdvb_media_device_free(dvbdev);\n\t\tlist_del (&dvbdev->list_head);\n\t\tkfree(dvbdevfops);\n\t\tkfree(dvbdev);\n\t\treturn PTR_ERR(clsdev);\n\t}\n\tdprintk(\"DVB: register adapter%d/%s%d @ minor: %i (0x%02x)\\n\",\n\t\tadap->num, dnames[type], id, minor, minor);\n\n\treturn 0;\n}",
        "func": "int dvb_register_device(struct dvb_adapter *adap, struct dvb_device **pdvbdev,\n\t\t\tconst struct dvb_device *template, void *priv,\n\t\t\tenum dvb_device_type type, int demux_sink_pads)\n{\n\tstruct dvb_device *dvbdev;\n\tstruct file_operations *dvbdevfops = NULL;\n\tstruct dvbdevfops_node *node = NULL, *new_node = NULL;\n\tstruct device *clsdev;\n\tint minor;\n\tint id, ret;\n\n\tmutex_lock(&dvbdev_register_lock);\n\n\tif ((id = dvbdev_get_free_id (adap, type)) < 0) {\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\t*pdvbdev = NULL;\n\t\tpr_err(\"%s: couldn't find free device id\\n\", __func__);\n\t\treturn -ENFILE;\n\t}\n\n\t*pdvbdev = dvbdev = kzalloc(sizeof(*dvbdev), GFP_KERNEL);\n\tif (!dvbdev){\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * When a device of the same type is probe()d more than once,\n\t * the first allocated fops are used. This prevents memory leaks\n\t * that can occur when the same device is probe()d repeatedly.\n\t */\n\tlist_for_each_entry(node, &dvbdevfops_list, list_head) {\n\t\tif (node->fops->owner == adap->module &&\n\t\t\t\tnode->type == type &&\n\t\t\t\tnode->template == template) {\n\t\t\tdvbdevfops = node->fops;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (dvbdevfops == NULL) {\n\t\tdvbdevfops = kmemdup(template->fops, sizeof(*dvbdevfops), GFP_KERNEL);\n\t\tif (!dvbdevfops) {\n\t\t\tkfree(dvbdev);\n\t\t\tmutex_unlock(&dvbdev_register_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tnew_node = kzalloc(sizeof(struct dvbdevfops_node), GFP_KERNEL);\n\t\tif (!new_node) {\n\t\t\tkfree(dvbdevfops);\n\t\t\tkfree(dvbdev);\n\t\t\tmutex_unlock(&dvbdev_register_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tnew_node->fops = dvbdevfops;\n\t\tnew_node->type = type;\n\t\tnew_node->template = template;\n\t\tlist_add_tail (&new_node->list_head, &dvbdevfops_list);\n\t}\n\n\tmemcpy(dvbdev, template, sizeof(struct dvb_device));\n\tkref_init(&dvbdev->ref);\n\tdvbdev->type = type;\n\tdvbdev->id = id;\n\tdvbdev->adapter = adap;\n\tdvbdev->priv = priv;\n\tdvbdev->fops = dvbdevfops;\n\tinit_waitqueue_head (&dvbdev->wait_queue);\n\tdvbdevfops->owner = adap->module;\n\tlist_add_tail (&dvbdev->list_head, &adap->device_list);\n\tdown_write(&minor_rwsem);\n#ifdef CONFIG_DVB_DYNAMIC_MINORS\n\tfor (minor = 0; minor < MAX_DVB_MINORS; minor++)\n\t\tif (dvb_minors[minor] == NULL)\n\t\t\tbreak;\n\tif (minor == MAX_DVB_MINORS) {\n\t\tif (new_node) {\n\t\t\tlist_del (&new_node->list_head);\n\t\t\tkfree(dvbdevfops);\n\t\t\tkfree(new_node);\n\t\t}\n\t\tlist_del (&dvbdev->list_head);\n\t\tkfree(dvbdev);\n\t\tup_write(&minor_rwsem);\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\treturn -EINVAL;\n\t}\n#else\n\tminor = nums2minor(adap->num, type, id);\n#endif\n\tdvbdev->minor = minor;\n\tdvb_minors[minor] = dvb_device_get(dvbdev);\n\tup_write(&minor_rwsem);\n\tret = dvb_register_media_device(dvbdev, type, minor, demux_sink_pads);\n\tif (ret) {\n\t\tpr_err(\"%s: dvb_register_media_device failed to create the mediagraph\\n\",\n\t\t      __func__);\n\t\tif (new_node) {\n\t\t\tlist_del (&new_node->list_head);\n\t\t\tkfree(dvbdevfops);\n\t\t\tkfree(new_node);\n\t\t}\n\t\tdvb_media_device_free(dvbdev);\n\t\tlist_del (&dvbdev->list_head);\n\t\tkfree(dvbdev);\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\treturn ret;\n\t}\n\n\tclsdev = device_create(dvb_class, adap->device,\n\t\t\t       MKDEV(DVB_MAJOR, minor),\n\t\t\t       dvbdev, \"dvb%d.%s%d\", adap->num, dnames[type], id);\n\tif (IS_ERR(clsdev)) {\n\t\tpr_err(\"%s: failed to create device dvb%d.%s%d (%ld)\\n\",\n\t\t       __func__, adap->num, dnames[type], id, PTR_ERR(clsdev));\n\t\tif (new_node) {\n\t\t\tlist_del (&new_node->list_head);\n\t\t\tkfree(dvbdevfops);\n\t\t\tkfree(new_node);\n\t\t}\n\t\tdvb_media_device_free(dvbdev);\n\t\tlist_del (&dvbdev->list_head);\n\t\tkfree(dvbdev);\n\t\tmutex_unlock(&dvbdev_register_lock);\n\t\treturn PTR_ERR(clsdev);\n\t}\n\n\tdprintk(\"DVB: register adapter%d/%s%d @ minor: %i (0x%02x)\\n\",\n\t\tadap->num, dnames[type], id, minor, minor);\n\n\tmutex_unlock(&dvbdev_register_lock);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,14 +3,15 @@\n \t\t\tenum dvb_device_type type, int demux_sink_pads)\n {\n \tstruct dvb_device *dvbdev;\n-\tstruct file_operations *dvbdevfops;\n+\tstruct file_operations *dvbdevfops = NULL;\n+\tstruct dvbdevfops_node *node = NULL, *new_node = NULL;\n \tstruct device *clsdev;\n \tint minor;\n \tint id, ret;\n \n \tmutex_lock(&dvbdev_register_lock);\n \n-\tif ((id = dvbdev_get_free_id (adap, type)) < 0){\n+\tif ((id = dvbdev_get_free_id (adap, type)) < 0) {\n \t\tmutex_unlock(&dvbdev_register_lock);\n \t\t*pdvbdev = NULL;\n \t\tpr_err(\"%s: couldn't find free device id\\n\", __func__);\n@@ -18,18 +19,45 @@\n \t}\n \n \t*pdvbdev = dvbdev = kzalloc(sizeof(*dvbdev), GFP_KERNEL);\n-\n \tif (!dvbdev){\n \t\tmutex_unlock(&dvbdev_register_lock);\n \t\treturn -ENOMEM;\n \t}\n \n-\tdvbdevfops = kmemdup(template->fops, sizeof(*dvbdevfops), GFP_KERNEL);\n+\t/*\n+\t * When a device of the same type is probe()d more than once,\n+\t * the first allocated fops are used. This prevents memory leaks\n+\t * that can occur when the same device is probe()d repeatedly.\n+\t */\n+\tlist_for_each_entry(node, &dvbdevfops_list, list_head) {\n+\t\tif (node->fops->owner == adap->module &&\n+\t\t\t\tnode->type == type &&\n+\t\t\t\tnode->template == template) {\n+\t\t\tdvbdevfops = node->fops;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n \n-\tif (!dvbdevfops){\n-\t\tkfree (dvbdev);\n-\t\tmutex_unlock(&dvbdev_register_lock);\n-\t\treturn -ENOMEM;\n+\tif (dvbdevfops == NULL) {\n+\t\tdvbdevfops = kmemdup(template->fops, sizeof(*dvbdevfops), GFP_KERNEL);\n+\t\tif (!dvbdevfops) {\n+\t\t\tkfree(dvbdev);\n+\t\t\tmutex_unlock(&dvbdev_register_lock);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tnew_node = kzalloc(sizeof(struct dvbdevfops_node), GFP_KERNEL);\n+\t\tif (!new_node) {\n+\t\t\tkfree(dvbdevfops);\n+\t\t\tkfree(dvbdev);\n+\t\t\tmutex_unlock(&dvbdev_register_lock);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tnew_node->fops = dvbdevfops;\n+\t\tnew_node->type = type;\n+\t\tnew_node->template = template;\n+\t\tlist_add_tail (&new_node->list_head, &dvbdevfops_list);\n \t}\n \n \tmemcpy(dvbdev, template, sizeof(struct dvb_device));\n@@ -40,20 +68,20 @@\n \tdvbdev->priv = priv;\n \tdvbdev->fops = dvbdevfops;\n \tinit_waitqueue_head (&dvbdev->wait_queue);\n-\n \tdvbdevfops->owner = adap->module;\n-\n \tlist_add_tail (&dvbdev->list_head, &adap->device_list);\n-\n \tdown_write(&minor_rwsem);\n #ifdef CONFIG_DVB_DYNAMIC_MINORS\n \tfor (minor = 0; minor < MAX_DVB_MINORS; minor++)\n \t\tif (dvb_minors[minor] == NULL)\n \t\t\tbreak;\n-\n \tif (minor == MAX_DVB_MINORS) {\n+\t\tif (new_node) {\n+\t\t\tlist_del (&new_node->list_head);\n+\t\t\tkfree(dvbdevfops);\n+\t\t\tkfree(new_node);\n+\t\t}\n \t\tlist_del (&dvbdev->list_head);\n-\t\tkfree(dvbdevfops);\n \t\tkfree(dvbdev);\n \t\tup_write(&minor_rwsem);\n \t\tmutex_unlock(&dvbdev_register_lock);\n@@ -62,25 +90,24 @@\n #else\n \tminor = nums2minor(adap->num, type, id);\n #endif\n-\n \tdvbdev->minor = minor;\n \tdvb_minors[minor] = dvb_device_get(dvbdev);\n \tup_write(&minor_rwsem);\n-\n \tret = dvb_register_media_device(dvbdev, type, minor, demux_sink_pads);\n \tif (ret) {\n \t\tpr_err(\"%s: dvb_register_media_device failed to create the mediagraph\\n\",\n \t\t      __func__);\n-\n+\t\tif (new_node) {\n+\t\t\tlist_del (&new_node->list_head);\n+\t\t\tkfree(dvbdevfops);\n+\t\t\tkfree(new_node);\n+\t\t}\n \t\tdvb_media_device_free(dvbdev);\n \t\tlist_del (&dvbdev->list_head);\n-\t\tkfree(dvbdevfops);\n \t\tkfree(dvbdev);\n \t\tmutex_unlock(&dvbdev_register_lock);\n \t\treturn ret;\n \t}\n-\n-\tmutex_unlock(&dvbdev_register_lock);\n \n \tclsdev = device_create(dvb_class, adap->device,\n \t\t\t       MKDEV(DVB_MAJOR, minor),\n@@ -88,14 +115,21 @@\n \tif (IS_ERR(clsdev)) {\n \t\tpr_err(\"%s: failed to create device dvb%d.%s%d (%ld)\\n\",\n \t\t       __func__, adap->num, dnames[type], id, PTR_ERR(clsdev));\n+\t\tif (new_node) {\n+\t\t\tlist_del (&new_node->list_head);\n+\t\t\tkfree(dvbdevfops);\n+\t\t\tkfree(new_node);\n+\t\t}\n \t\tdvb_media_device_free(dvbdev);\n \t\tlist_del (&dvbdev->list_head);\n-\t\tkfree(dvbdevfops);\n \t\tkfree(dvbdev);\n+\t\tmutex_unlock(&dvbdev_register_lock);\n \t\treturn PTR_ERR(clsdev);\n \t}\n+\n \tdprintk(\"DVB: register adapter%d/%s%d @ minor: %i (0x%02x)\\n\",\n \t\tadap->num, dnames[type], id, minor, minor);\n \n+\tmutex_unlock(&dvbdev_register_lock);\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct file_operations *dvbdevfops;",
                "\tif ((id = dvbdev_get_free_id (adap, type)) < 0){",
                "",
                "\tdvbdevfops = kmemdup(template->fops, sizeof(*dvbdevfops), GFP_KERNEL);",
                "\tif (!dvbdevfops){",
                "\t\tkfree (dvbdev);",
                "\t\tmutex_unlock(&dvbdev_register_lock);",
                "\t\treturn -ENOMEM;",
                "",
                "",
                "",
                "",
                "\t\tkfree(dvbdevfops);",
                "",
                "",
                "",
                "\t\tkfree(dvbdevfops);",
                "",
                "\tmutex_unlock(&dvbdev_register_lock);",
                "\t\tkfree(dvbdevfops);"
            ],
            "added_lines": [
                "\tstruct file_operations *dvbdevfops = NULL;",
                "\tstruct dvbdevfops_node *node = NULL, *new_node = NULL;",
                "\tif ((id = dvbdev_get_free_id (adap, type)) < 0) {",
                "\t/*",
                "\t * When a device of the same type is probe()d more than once,",
                "\t * the first allocated fops are used. This prevents memory leaks",
                "\t * that can occur when the same device is probe()d repeatedly.",
                "\t */",
                "\tlist_for_each_entry(node, &dvbdevfops_list, list_head) {",
                "\t\tif (node->fops->owner == adap->module &&",
                "\t\t\t\tnode->type == type &&",
                "\t\t\t\tnode->template == template) {",
                "\t\t\tdvbdevfops = node->fops;",
                "\t\t\tbreak;",
                "\t\t}",
                "\t}",
                "\tif (dvbdevfops == NULL) {",
                "\t\tdvbdevfops = kmemdup(template->fops, sizeof(*dvbdevfops), GFP_KERNEL);",
                "\t\tif (!dvbdevfops) {",
                "\t\t\tkfree(dvbdev);",
                "\t\t\tmutex_unlock(&dvbdev_register_lock);",
                "\t\t\treturn -ENOMEM;",
                "\t\t}",
                "",
                "\t\tnew_node = kzalloc(sizeof(struct dvbdevfops_node), GFP_KERNEL);",
                "\t\tif (!new_node) {",
                "\t\t\tkfree(dvbdevfops);",
                "\t\t\tkfree(dvbdev);",
                "\t\t\tmutex_unlock(&dvbdev_register_lock);",
                "\t\t\treturn -ENOMEM;",
                "\t\t}",
                "",
                "\t\tnew_node->fops = dvbdevfops;",
                "\t\tnew_node->type = type;",
                "\t\tnew_node->template = template;",
                "\t\tlist_add_tail (&new_node->list_head, &dvbdevfops_list);",
                "\t\tif (new_node) {",
                "\t\t\tlist_del (&new_node->list_head);",
                "\t\t\tkfree(dvbdevfops);",
                "\t\t\tkfree(new_node);",
                "\t\t}",
                "\t\tif (new_node) {",
                "\t\t\tlist_del (&new_node->list_head);",
                "\t\t\tkfree(dvbdevfops);",
                "\t\t\tkfree(new_node);",
                "\t\t}",
                "\t\tif (new_node) {",
                "\t\t\tlist_del (&new_node->list_head);",
                "\t\t\tkfree(dvbdevfops);",
                "\t\t\tkfree(new_node);",
                "\t\t}",
                "\t\tmutex_unlock(&dvbdev_register_lock);",
                "",
                "\tmutex_unlock(&dvbdev_register_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45884",
        "func_name": "torvalds/linux/dvb_free_device",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvbdev.c has a use-after-free, related to dvb_register_device dynamically allocating fops.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=627bb528b086b4136315c25d6a447a98ea9448d3",
        "commit_title": "dvb_register_device() dynamically allocates fops with kmemdup()",
        "commit_text": "to set the fops->owner. And these fops are registered in 'file->f_ops' using replace_fops() in the dvb_device_open() process, and kfree()d in dvb_free_device().  However, it is not common to use dynamically allocated fops instead of 'static const' fops as an argument of replace_fops(), and UAF may occur. These UAFs can occur on any dvb type using dvb_register_device(), such as dvb_dvr, dvb_demux, dvb_frontend, dvb_net, etc.  So, instead of kfree() the fops dynamically allocated in dvb_register_device() in dvb_free_device() called during the .disconnect() process, kfree() it collectively in exit_dvbdev() called when the dvbdev.c module is removed.  Link: https://lore.kernel.org/linux-media/20221117045925.14297-4-imv4bel@gmail.com ",
        "func_before": "static void dvb_free_device(struct kref *ref)\n{\n\tstruct dvb_device *dvbdev = container_of(ref, struct dvb_device, ref);\n\n\tkfree (dvbdev->fops);\n\tkfree (dvbdev);\n}",
        "func": "static void dvb_free_device(struct kref *ref)\n{\n\tstruct dvb_device *dvbdev = container_of(ref, struct dvb_device, ref);\n\n\tkfree (dvbdev);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,5 @@\n {\n \tstruct dvb_device *dvbdev = container_of(ref, struct dvb_device, ref);\n \n-\tkfree (dvbdev->fops);\n \tkfree (dvbdev);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tkfree (dvbdev->fops);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2022-45884",
        "func_name": "torvalds/linux/exit_dvbdev",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvbdev.c has a use-after-free, related to dvb_register_device dynamically allocating fops.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=627bb528b086b4136315c25d6a447a98ea9448d3",
        "commit_title": "dvb_register_device() dynamically allocates fops with kmemdup()",
        "commit_text": "to set the fops->owner. And these fops are registered in 'file->f_ops' using replace_fops() in the dvb_device_open() process, and kfree()d in dvb_free_device().  However, it is not common to use dynamically allocated fops instead of 'static const' fops as an argument of replace_fops(), and UAF may occur. These UAFs can occur on any dvb type using dvb_register_device(), such as dvb_dvr, dvb_demux, dvb_frontend, dvb_net, etc.  So, instead of kfree() the fops dynamically allocated in dvb_register_device() in dvb_free_device() called during the .disconnect() process, kfree() it collectively in exit_dvbdev() called when the dvbdev.c module is removed.  Link: https://lore.kernel.org/linux-media/20221117045925.14297-4-imv4bel@gmail.com ",
        "func_before": "static void __exit exit_dvbdev(void)\n{\n\tclass_destroy(dvb_class);\n\tcdev_del(&dvb_device_cdev);\n\tunregister_chrdev_region(MKDEV(DVB_MAJOR, 0), MAX_DVB_MINORS);\n}",
        "func": "static void __exit exit_dvbdev(void)\n{\n\tstruct dvbdevfops_node *node, *next;\n\n\tclass_destroy(dvb_class);\n\tcdev_del(&dvb_device_cdev);\n\tunregister_chrdev_region(MKDEV(DVB_MAJOR, 0), MAX_DVB_MINORS);\n\n\tlist_for_each_entry_safe(node, next, &dvbdevfops_list, list_head) {\n\t\tlist_del (&node->list_head);\n\t\tkfree(node->fops);\n\t\tkfree(node);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,14 @@\n static void __exit exit_dvbdev(void)\n {\n+\tstruct dvbdevfops_node *node, *next;\n+\n \tclass_destroy(dvb_class);\n \tcdev_del(&dvb_device_cdev);\n \tunregister_chrdev_region(MKDEV(DVB_MAJOR, 0), MAX_DVB_MINORS);\n+\n+\tlist_for_each_entry_safe(node, next, &dvbdevfops_list, list_head) {\n+\t\tlist_del (&node->list_head);\n+\t\tkfree(node->fops);\n+\t\tkfree(node);\n+\t}\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tstruct dvbdevfops_node *node, *next;",
                "",
                "",
                "\tlist_for_each_entry_safe(node, next, &dvbdevfops_list, list_head) {",
                "\t\tlist_del (&node->list_head);",
                "\t\tkfree(node->fops);",
                "\t\tkfree(node);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45885",
        "func_name": "torvalds/linux/dvb_frontend_stop",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvb_frontend.c has a race condition that can cause a use-after-free when a device is disconnected.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=6769a0b7ee0c3b31e1b22c3fadff2bfb642de23f",
        "commit_title": "If the device node of dvb_frontend is open() and the device is",
        "commit_text": "disconnected, many kinds of UAFs may occur when calling close() on the device node.  The root cause of this is that wake_up() for dvbdev->wait_queue is implemented in the dvb_frontend_release() function, but wait_event() is not implemented in the dvb_frontend_stop() function.  So, implement wait_event() function in dvb_frontend_stop() and add 'remove_mutex' which prevents race condition for 'fe->exit'.  [mchehab: fix a couple of checkpatch warnings and some mistakes at the error handling logic]  Link: https://lore.kernel.org/linux-media/20221117045925.14297-2-imv4bel@gmail.com ",
        "func_before": "static void dvb_frontend_stop(struct dvb_frontend *fe)\n{\n\tstruct dvb_frontend_private *fepriv = fe->frontend_priv;\n\n\tdev_dbg(fe->dvb->device, \"%s:\\n\", __func__);\n\n\tif (fe->exit != DVB_FE_DEVICE_REMOVED)\n\t\tfe->exit = DVB_FE_NORMAL_EXIT;\n\tmb();\n\n\tif (!fepriv->thread)\n\t\treturn;\n\n\tkthread_stop(fepriv->thread);\n\n\tsema_init(&fepriv->sem, 1);\n\tfepriv->state = FESTATE_IDLE;\n\n\t/* paranoia check in case a signal arrived */\n\tif (fepriv->thread)\n\t\tdev_warn(fe->dvb->device,\n\t\t\t \"dvb_frontend_stop: warning: thread %p won't exit\\n\",\n\t\t\t fepriv->thread);\n}",
        "func": "static void dvb_frontend_stop(struct dvb_frontend *fe)\n{\n\tstruct dvb_frontend_private *fepriv = fe->frontend_priv;\n\n\tdev_dbg(fe->dvb->device, \"%s:\\n\", __func__);\n\n\tmutex_lock(&fe->remove_mutex);\n\n\tif (fe->exit != DVB_FE_DEVICE_REMOVED)\n\t\tfe->exit = DVB_FE_NORMAL_EXIT;\n\tmb();\n\n\tif (!fepriv->thread) {\n\t\tmutex_unlock(&fe->remove_mutex);\n\t\treturn;\n\t}\n\n\tkthread_stop(fepriv->thread);\n\n\tmutex_unlock(&fe->remove_mutex);\n\n\tif (fepriv->dvbdev->users < -1) {\n\t\twait_event(fepriv->dvbdev->wait_queue,\n\t\t\t   fepriv->dvbdev->users == -1);\n\t}\n\n\tsema_init(&fepriv->sem, 1);\n\tfepriv->state = FESTATE_IDLE;\n\n\t/* paranoia check in case a signal arrived */\n\tif (fepriv->thread)\n\t\tdev_warn(fe->dvb->device,\n\t\t\t \"dvb_frontend_stop: warning: thread %p won't exit\\n\",\n\t\t\t fepriv->thread);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,14 +4,25 @@\n \n \tdev_dbg(fe->dvb->device, \"%s:\\n\", __func__);\n \n+\tmutex_lock(&fe->remove_mutex);\n+\n \tif (fe->exit != DVB_FE_DEVICE_REMOVED)\n \t\tfe->exit = DVB_FE_NORMAL_EXIT;\n \tmb();\n \n-\tif (!fepriv->thread)\n+\tif (!fepriv->thread) {\n+\t\tmutex_unlock(&fe->remove_mutex);\n \t\treturn;\n+\t}\n \n \tkthread_stop(fepriv->thread);\n+\n+\tmutex_unlock(&fe->remove_mutex);\n+\n+\tif (fepriv->dvbdev->users < -1) {\n+\t\twait_event(fepriv->dvbdev->wait_queue,\n+\t\t\t   fepriv->dvbdev->users == -1);\n+\t}\n \n \tsema_init(&fepriv->sem, 1);\n \tfepriv->state = FESTATE_IDLE;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!fepriv->thread)"
            ],
            "added_lines": [
                "\tmutex_lock(&fe->remove_mutex);",
                "",
                "\tif (!fepriv->thread) {",
                "\t\tmutex_unlock(&fe->remove_mutex);",
                "\t}",
                "",
                "\tmutex_unlock(&fe->remove_mutex);",
                "",
                "\tif (fepriv->dvbdev->users < -1) {",
                "\t\twait_event(fepriv->dvbdev->wait_queue,",
                "\t\t\t   fepriv->dvbdev->users == -1);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45885",
        "func_name": "torvalds/linux/dvb_register_frontend",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvb_frontend.c has a race condition that can cause a use-after-free when a device is disconnected.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=6769a0b7ee0c3b31e1b22c3fadff2bfb642de23f",
        "commit_title": "If the device node of dvb_frontend is open() and the device is",
        "commit_text": "disconnected, many kinds of UAFs may occur when calling close() on the device node.  The root cause of this is that wake_up() for dvbdev->wait_queue is implemented in the dvb_frontend_release() function, but wait_event() is not implemented in the dvb_frontend_stop() function.  So, implement wait_event() function in dvb_frontend_stop() and add 'remove_mutex' which prevents race condition for 'fe->exit'.  [mchehab: fix a couple of checkpatch warnings and some mistakes at the error handling logic]  Link: https://lore.kernel.org/linux-media/20221117045925.14297-2-imv4bel@gmail.com ",
        "func_before": "int dvb_register_frontend(struct dvb_adapter *dvb,\n\t\t\t  struct dvb_frontend *fe)\n{\n\tstruct dvb_frontend_private *fepriv;\n\tconst struct dvb_device dvbdev_template = {\n\t\t.users = ~0,\n\t\t.writers = 1,\n\t\t.readers = (~0) - 1,\n\t\t.fops = &dvb_frontend_fops,\n#if defined(CONFIG_MEDIA_CONTROLLER_DVB)\n\t\t.name = fe->ops.info.name,\n#endif\n\t};\n\tint ret;\n\n\tdev_dbg(dvb->device, \"%s:\\n\", __func__);\n\n\tif (mutex_lock_interruptible(&frontend_mutex))\n\t\treturn -ERESTARTSYS;\n\n\tfe->frontend_priv = kzalloc(sizeof(struct dvb_frontend_private), GFP_KERNEL);\n\tif (!fe->frontend_priv) {\n\t\tmutex_unlock(&frontend_mutex);\n\t\treturn -ENOMEM;\n\t}\n\tfepriv = fe->frontend_priv;\n\n\tkref_init(&fe->refcount);\n\n\t/*\n\t * After initialization, there need to be two references: one\n\t * for dvb_unregister_frontend(), and another one for\n\t * dvb_frontend_detach().\n\t */\n\tdvb_frontend_get(fe);\n\n\tsema_init(&fepriv->sem, 1);\n\tinit_waitqueue_head(&fepriv->wait_queue);\n\tinit_waitqueue_head(&fepriv->events.wait_queue);\n\tmutex_init(&fepriv->events.mtx);\n\tfe->dvb = dvb;\n\tfepriv->inversion = INVERSION_OFF;\n\n\tdev_info(fe->dvb->device,\n\t\t \"DVB: registering adapter %i frontend %i (%s)...\\n\",\n\t\t fe->dvb->num, fe->id, fe->ops.info.name);\n\n\tret = dvb_register_device(fe->dvb, &fepriv->dvbdev, &dvbdev_template,\n\t\t\t    fe, DVB_DEVICE_FRONTEND, 0);\n\tif (ret) {\n\t\tdvb_frontend_put(fe);\n\t\tmutex_unlock(&frontend_mutex);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * Initialize the cache to the proper values according with the\n\t * first supported delivery system (ops->delsys[0])\n\t */\n\n\tfe->dtv_property_cache.delivery_system = fe->ops.delsys[0];\n\tdvb_frontend_clear_cache(fe);\n\n\tmutex_unlock(&frontend_mutex);\n\treturn 0;\n}",
        "func": "int dvb_register_frontend(struct dvb_adapter *dvb,\n\t\t\t  struct dvb_frontend *fe)\n{\n\tstruct dvb_frontend_private *fepriv;\n\tconst struct dvb_device dvbdev_template = {\n\t\t.users = ~0,\n\t\t.writers = 1,\n\t\t.readers = (~0) - 1,\n\t\t.fops = &dvb_frontend_fops,\n#if defined(CONFIG_MEDIA_CONTROLLER_DVB)\n\t\t.name = fe->ops.info.name,\n#endif\n\t};\n\tint ret;\n\n\tdev_dbg(dvb->device, \"%s:\\n\", __func__);\n\n\tif (mutex_lock_interruptible(&frontend_mutex))\n\t\treturn -ERESTARTSYS;\n\n\tfe->frontend_priv = kzalloc(sizeof(struct dvb_frontend_private), GFP_KERNEL);\n\tif (!fe->frontend_priv) {\n\t\tmutex_unlock(&frontend_mutex);\n\t\treturn -ENOMEM;\n\t}\n\tfepriv = fe->frontend_priv;\n\n\tkref_init(&fe->refcount);\n\tmutex_init(&fe->remove_mutex);\n\n\t/*\n\t * After initialization, there need to be two references: one\n\t * for dvb_unregister_frontend(), and another one for\n\t * dvb_frontend_detach().\n\t */\n\tdvb_frontend_get(fe);\n\n\tsema_init(&fepriv->sem, 1);\n\tinit_waitqueue_head(&fepriv->wait_queue);\n\tinit_waitqueue_head(&fepriv->events.wait_queue);\n\tmutex_init(&fepriv->events.mtx);\n\tfe->dvb = dvb;\n\tfepriv->inversion = INVERSION_OFF;\n\n\tdev_info(fe->dvb->device,\n\t\t \"DVB: registering adapter %i frontend %i (%s)...\\n\",\n\t\t fe->dvb->num, fe->id, fe->ops.info.name);\n\n\tret = dvb_register_device(fe->dvb, &fepriv->dvbdev, &dvbdev_template,\n\t\t\t    fe, DVB_DEVICE_FRONTEND, 0);\n\tif (ret) {\n\t\tdvb_frontend_put(fe);\n\t\tmutex_unlock(&frontend_mutex);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * Initialize the cache to the proper values according with the\n\t * first supported delivery system (ops->delsys[0])\n\t */\n\n\tfe->dtv_property_cache.delivery_system = fe->ops.delsys[0];\n\tdvb_frontend_clear_cache(fe);\n\n\tmutex_unlock(&frontend_mutex);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,6 +26,7 @@\n \tfepriv = fe->frontend_priv;\n \n \tkref_init(&fe->refcount);\n+\tmutex_init(&fe->remove_mutex);\n \n \t/*\n \t * After initialization, there need to be two references: one",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_init(&fe->remove_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45885",
        "func_name": "torvalds/linux/dvb_frontend_open",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvb_frontend.c has a race condition that can cause a use-after-free when a device is disconnected.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=6769a0b7ee0c3b31e1b22c3fadff2bfb642de23f",
        "commit_title": "If the device node of dvb_frontend is open() and the device is",
        "commit_text": "disconnected, many kinds of UAFs may occur when calling close() on the device node.  The root cause of this is that wake_up() for dvbdev->wait_queue is implemented in the dvb_frontend_release() function, but wait_event() is not implemented in the dvb_frontend_stop() function.  So, implement wait_event() function in dvb_frontend_stop() and add 'remove_mutex' which prevents race condition for 'fe->exit'.  [mchehab: fix a couple of checkpatch warnings and some mistakes at the error handling logic]  Link: https://lore.kernel.org/linux-media/20221117045925.14297-2-imv4bel@gmail.com ",
        "func_before": "static int dvb_frontend_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_frontend *fe = dvbdev->priv;\n\tstruct dvb_frontend_private *fepriv = fe->frontend_priv;\n\tstruct dvb_adapter *adapter = fe->dvb;\n\tint ret;\n\n\tdev_dbg(fe->dvb->device, \"%s:\\n\", __func__);\n\tif (fe->exit == DVB_FE_DEVICE_REMOVED)\n\t\treturn -ENODEV;\n\n\tif (adapter->mfe_shared == 2) {\n\t\tmutex_lock(&adapter->mfe_lock);\n\t\tif ((file->f_flags & O_ACCMODE) != O_RDONLY) {\n\t\t\tif (adapter->mfe_dvbdev &&\n\t\t\t    !adapter->mfe_dvbdev->writers) {\n\t\t\t\tmutex_unlock(&adapter->mfe_lock);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t\tadapter->mfe_dvbdev = dvbdev;\n\t\t}\n\t} else if (adapter->mfe_shared) {\n\t\tmutex_lock(&adapter->mfe_lock);\n\n\t\tif (!adapter->mfe_dvbdev)\n\t\t\tadapter->mfe_dvbdev = dvbdev;\n\n\t\telse if (adapter->mfe_dvbdev != dvbdev) {\n\t\t\tstruct dvb_device\n\t\t\t\t*mfedev = adapter->mfe_dvbdev;\n\t\t\tstruct dvb_frontend\n\t\t\t\t*mfe = mfedev->priv;\n\t\t\tstruct dvb_frontend_private\n\t\t\t\t*mfepriv = mfe->frontend_priv;\n\t\t\tint mferetry = (dvb_mfe_wait_time << 1);\n\n\t\t\tmutex_unlock(&adapter->mfe_lock);\n\t\t\twhile (mferetry-- && (mfedev->users != -1 ||\n\t\t\t\t\t      mfepriv->thread)) {\n\t\t\t\tif (msleep_interruptible(500)) {\n\t\t\t\t\tif (signal_pending(current))\n\t\t\t\t\t\treturn -EINTR;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tmutex_lock(&adapter->mfe_lock);\n\t\t\tif (adapter->mfe_dvbdev != dvbdev) {\n\t\t\t\tmfedev = adapter->mfe_dvbdev;\n\t\t\t\tmfe = mfedev->priv;\n\t\t\t\tmfepriv = mfe->frontend_priv;\n\t\t\t\tif (mfedev->users != -1 ||\n\t\t\t\t    mfepriv->thread) {\n\t\t\t\t\tmutex_unlock(&adapter->mfe_lock);\n\t\t\t\t\treturn -EBUSY;\n\t\t\t\t}\n\t\t\t\tadapter->mfe_dvbdev = dvbdev;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (dvbdev->users == -1 && fe->ops.ts_bus_ctrl) {\n\t\tif ((ret = fe->ops.ts_bus_ctrl(fe, 1)) < 0)\n\t\t\tgoto err0;\n\n\t\t/* If we took control of the bus, we need to force\n\t\t   reinitialization.  This is because many ts_bus_ctrl()\n\t\t   functions strobe the RESET pin on the demod, and if the\n\t\t   frontend thread already exists then the dvb_init() routine\n\t\t   won't get called (which is what usually does initial\n\t\t   register configuration). */\n\t\tfepriv->reinitialise = 1;\n\t}\n\n\tif ((ret = dvb_generic_open(inode, file)) < 0)\n\t\tgoto err1;\n\n\tif ((file->f_flags & O_ACCMODE) != O_RDONLY) {\n\t\t/* normal tune mode when opened R/W */\n\t\tfepriv->tune_mode_flags &= ~FE_TUNE_MODE_ONESHOT;\n\t\tfepriv->tone = -1;\n\t\tfepriv->voltage = -1;\n\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\t\tmutex_lock(&fe->dvb->mdev_lock);\n\t\tif (fe->dvb->mdev) {\n\t\t\tmutex_lock(&fe->dvb->mdev->graph_mutex);\n\t\t\tif (fe->dvb->mdev->enable_source)\n\t\t\t\tret = fe->dvb->mdev->enable_source(\n\t\t\t\t\t\t\t   dvbdev->entity,\n\t\t\t\t\t\t\t   &fepriv->pipe);\n\t\t\tmutex_unlock(&fe->dvb->mdev->graph_mutex);\n\t\t\tif (ret) {\n\t\t\t\tmutex_unlock(&fe->dvb->mdev_lock);\n\t\t\t\tdev_err(fe->dvb->device,\n\t\t\t\t\t\"Tuner is busy. Error %d\\n\", ret);\n\t\t\t\tgoto err2;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&fe->dvb->mdev_lock);\n#endif\n\t\tret = dvb_frontend_start(fe);\n\t\tif (ret)\n\t\t\tgoto err3;\n\n\t\t/*  empty event queue */\n\t\tfepriv->events.eventr = fepriv->events.eventw = 0;\n\t}\n\n\tdvb_frontend_get(fe);\n\n\tif (adapter->mfe_shared)\n\t\tmutex_unlock(&adapter->mfe_lock);\n\treturn ret;\n\nerr3:\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\tmutex_lock(&fe->dvb->mdev_lock);\n\tif (fe->dvb->mdev) {\n\t\tmutex_lock(&fe->dvb->mdev->graph_mutex);\n\t\tif (fe->dvb->mdev->disable_source)\n\t\t\tfe->dvb->mdev->disable_source(dvbdev->entity);\n\t\tmutex_unlock(&fe->dvb->mdev->graph_mutex);\n\t}\n\tmutex_unlock(&fe->dvb->mdev_lock);\nerr2:\n#endif\n\tdvb_generic_release(inode, file);\nerr1:\n\tif (dvbdev->users == -1 && fe->ops.ts_bus_ctrl)\n\t\tfe->ops.ts_bus_ctrl(fe, 0);\nerr0:\n\tif (adapter->mfe_shared)\n\t\tmutex_unlock(&adapter->mfe_lock);\n\treturn ret;\n}",
        "func": "static int dvb_frontend_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_frontend *fe = dvbdev->priv;\n\tstruct dvb_frontend_private *fepriv = fe->frontend_priv;\n\tstruct dvb_adapter *adapter = fe->dvb;\n\tint ret;\n\n\tmutex_lock(&fe->remove_mutex);\n\n\tdev_dbg(fe->dvb->device, \"%s:\\n\", __func__);\n\tif (fe->exit == DVB_FE_DEVICE_REMOVED) {\n\t\tret = -ENODEV;\n\t\tgoto err_remove_mutex;\n\t}\n\n\tif (adapter->mfe_shared == 2) {\n\t\tmutex_lock(&adapter->mfe_lock);\n\t\tif ((file->f_flags & O_ACCMODE) != O_RDONLY) {\n\t\t\tif (adapter->mfe_dvbdev &&\n\t\t\t    !adapter->mfe_dvbdev->writers) {\n\t\t\t\tmutex_unlock(&adapter->mfe_lock);\n\t\t\t\tret = -EBUSY;\n\t\t\t\tgoto err_remove_mutex;\n\t\t\t}\n\t\t\tadapter->mfe_dvbdev = dvbdev;\n\t\t}\n\t} else if (adapter->mfe_shared) {\n\t\tmutex_lock(&adapter->mfe_lock);\n\n\t\tif (!adapter->mfe_dvbdev)\n\t\t\tadapter->mfe_dvbdev = dvbdev;\n\n\t\telse if (adapter->mfe_dvbdev != dvbdev) {\n\t\t\tstruct dvb_device\n\t\t\t\t*mfedev = adapter->mfe_dvbdev;\n\t\t\tstruct dvb_frontend\n\t\t\t\t*mfe = mfedev->priv;\n\t\t\tstruct dvb_frontend_private\n\t\t\t\t*mfepriv = mfe->frontend_priv;\n\t\t\tint mferetry = (dvb_mfe_wait_time << 1);\n\n\t\t\tmutex_unlock(&adapter->mfe_lock);\n\t\t\twhile (mferetry-- && (mfedev->users != -1 ||\n\t\t\t\t\t      mfepriv->thread)) {\n\t\t\t\tif (msleep_interruptible(500)) {\n\t\t\t\t\tif (signal_pending(current)) {\n\t\t\t\t\t\tret = -EINTR;\n\t\t\t\t\t\tgoto err_remove_mutex;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tmutex_lock(&adapter->mfe_lock);\n\t\t\tif (adapter->mfe_dvbdev != dvbdev) {\n\t\t\t\tmfedev = adapter->mfe_dvbdev;\n\t\t\t\tmfe = mfedev->priv;\n\t\t\t\tmfepriv = mfe->frontend_priv;\n\t\t\t\tif (mfedev->users != -1 ||\n\t\t\t\t    mfepriv->thread) {\n\t\t\t\t\tmutex_unlock(&adapter->mfe_lock);\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\t\tgoto err_remove_mutex;\n\t\t\t\t}\n\t\t\t\tadapter->mfe_dvbdev = dvbdev;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (dvbdev->users == -1 && fe->ops.ts_bus_ctrl) {\n\t\tif ((ret = fe->ops.ts_bus_ctrl(fe, 1)) < 0)\n\t\t\tgoto err0;\n\n\t\t/* If we took control of the bus, we need to force\n\t\t   reinitialization.  This is because many ts_bus_ctrl()\n\t\t   functions strobe the RESET pin on the demod, and if the\n\t\t   frontend thread already exists then the dvb_init() routine\n\t\t   won't get called (which is what usually does initial\n\t\t   register configuration). */\n\t\tfepriv->reinitialise = 1;\n\t}\n\n\tif ((ret = dvb_generic_open(inode, file)) < 0)\n\t\tgoto err1;\n\n\tif ((file->f_flags & O_ACCMODE) != O_RDONLY) {\n\t\t/* normal tune mode when opened R/W */\n\t\tfepriv->tune_mode_flags &= ~FE_TUNE_MODE_ONESHOT;\n\t\tfepriv->tone = -1;\n\t\tfepriv->voltage = -1;\n\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\t\tmutex_lock(&fe->dvb->mdev_lock);\n\t\tif (fe->dvb->mdev) {\n\t\t\tmutex_lock(&fe->dvb->mdev->graph_mutex);\n\t\t\tif (fe->dvb->mdev->enable_source)\n\t\t\t\tret = fe->dvb->mdev->enable_source(\n\t\t\t\t\t\t\t   dvbdev->entity,\n\t\t\t\t\t\t\t   &fepriv->pipe);\n\t\t\tmutex_unlock(&fe->dvb->mdev->graph_mutex);\n\t\t\tif (ret) {\n\t\t\t\tmutex_unlock(&fe->dvb->mdev_lock);\n\t\t\t\tdev_err(fe->dvb->device,\n\t\t\t\t\t\"Tuner is busy. Error %d\\n\", ret);\n\t\t\t\tgoto err2;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&fe->dvb->mdev_lock);\n#endif\n\t\tret = dvb_frontend_start(fe);\n\t\tif (ret)\n\t\t\tgoto err3;\n\n\t\t/*  empty event queue */\n\t\tfepriv->events.eventr = fepriv->events.eventw = 0;\n\t}\n\n\tdvb_frontend_get(fe);\n\n\tif (adapter->mfe_shared)\n\t\tmutex_unlock(&adapter->mfe_lock);\n\n\tmutex_unlock(&fe->remove_mutex);\n\treturn ret;\n\nerr3:\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\tmutex_lock(&fe->dvb->mdev_lock);\n\tif (fe->dvb->mdev) {\n\t\tmutex_lock(&fe->dvb->mdev->graph_mutex);\n\t\tif (fe->dvb->mdev->disable_source)\n\t\t\tfe->dvb->mdev->disable_source(dvbdev->entity);\n\t\tmutex_unlock(&fe->dvb->mdev->graph_mutex);\n\t}\n\tmutex_unlock(&fe->dvb->mdev_lock);\nerr2:\n#endif\n\tdvb_generic_release(inode, file);\nerr1:\n\tif (dvbdev->users == -1 && fe->ops.ts_bus_ctrl)\n\t\tfe->ops.ts_bus_ctrl(fe, 0);\nerr0:\n\tif (adapter->mfe_shared)\n\t\tmutex_unlock(&adapter->mfe_lock);\n\nerr_remove_mutex:\n\tmutex_unlock(&fe->remove_mutex);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,9 +6,13 @@\n \tstruct dvb_adapter *adapter = fe->dvb;\n \tint ret;\n \n+\tmutex_lock(&fe->remove_mutex);\n+\n \tdev_dbg(fe->dvb->device, \"%s:\\n\", __func__);\n-\tif (fe->exit == DVB_FE_DEVICE_REMOVED)\n-\t\treturn -ENODEV;\n+\tif (fe->exit == DVB_FE_DEVICE_REMOVED) {\n+\t\tret = -ENODEV;\n+\t\tgoto err_remove_mutex;\n+\t}\n \n \tif (adapter->mfe_shared == 2) {\n \t\tmutex_lock(&adapter->mfe_lock);\n@@ -16,7 +20,8 @@\n \t\t\tif (adapter->mfe_dvbdev &&\n \t\t\t    !adapter->mfe_dvbdev->writers) {\n \t\t\t\tmutex_unlock(&adapter->mfe_lock);\n-\t\t\t\treturn -EBUSY;\n+\t\t\t\tret = -EBUSY;\n+\t\t\t\tgoto err_remove_mutex;\n \t\t\t}\n \t\t\tadapter->mfe_dvbdev = dvbdev;\n \t\t}\n@@ -39,8 +44,10 @@\n \t\t\twhile (mferetry-- && (mfedev->users != -1 ||\n \t\t\t\t\t      mfepriv->thread)) {\n \t\t\t\tif (msleep_interruptible(500)) {\n-\t\t\t\t\tif (signal_pending(current))\n-\t\t\t\t\t\treturn -EINTR;\n+\t\t\t\t\tif (signal_pending(current)) {\n+\t\t\t\t\t\tret = -EINTR;\n+\t\t\t\t\t\tgoto err_remove_mutex;\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \n@@ -52,7 +59,8 @@\n \t\t\t\tif (mfedev->users != -1 ||\n \t\t\t\t    mfepriv->thread) {\n \t\t\t\t\tmutex_unlock(&adapter->mfe_lock);\n-\t\t\t\t\treturn -EBUSY;\n+\t\t\t\t\tret = -EBUSY;\n+\t\t\t\t\tgoto err_remove_mutex;\n \t\t\t\t}\n \t\t\t\tadapter->mfe_dvbdev = dvbdev;\n \t\t\t}\n@@ -111,6 +119,8 @@\n \n \tif (adapter->mfe_shared)\n \t\tmutex_unlock(&adapter->mfe_lock);\n+\n+\tmutex_unlock(&fe->remove_mutex);\n \treturn ret;\n \n err3:\n@@ -132,5 +142,8 @@\n err0:\n \tif (adapter->mfe_shared)\n \t\tmutex_unlock(&adapter->mfe_lock);\n+\n+err_remove_mutex:\n+\tmutex_unlock(&fe->remove_mutex);\n \treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (fe->exit == DVB_FE_DEVICE_REMOVED)",
                "\t\treturn -ENODEV;",
                "\t\t\t\treturn -EBUSY;",
                "\t\t\t\t\tif (signal_pending(current))",
                "\t\t\t\t\t\treturn -EINTR;",
                "\t\t\t\t\treturn -EBUSY;"
            ],
            "added_lines": [
                "\tmutex_lock(&fe->remove_mutex);",
                "",
                "\tif (fe->exit == DVB_FE_DEVICE_REMOVED) {",
                "\t\tret = -ENODEV;",
                "\t\tgoto err_remove_mutex;",
                "\t}",
                "\t\t\t\tret = -EBUSY;",
                "\t\t\t\tgoto err_remove_mutex;",
                "\t\t\t\t\tif (signal_pending(current)) {",
                "\t\t\t\t\t\tret = -EINTR;",
                "\t\t\t\t\t\tgoto err_remove_mutex;",
                "\t\t\t\t\t}",
                "\t\t\t\t\tret = -EBUSY;",
                "\t\t\t\t\tgoto err_remove_mutex;",
                "",
                "\tmutex_unlock(&fe->remove_mutex);",
                "",
                "err_remove_mutex:",
                "\tmutex_unlock(&fe->remove_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45885",
        "func_name": "torvalds/linux/dvb_frontend_release",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvb_frontend.c has a race condition that can cause a use-after-free when a device is disconnected.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=6769a0b7ee0c3b31e1b22c3fadff2bfb642de23f",
        "commit_title": "If the device node of dvb_frontend is open() and the device is",
        "commit_text": "disconnected, many kinds of UAFs may occur when calling close() on the device node.  The root cause of this is that wake_up() for dvbdev->wait_queue is implemented in the dvb_frontend_release() function, but wait_event() is not implemented in the dvb_frontend_stop() function.  So, implement wait_event() function in dvb_frontend_stop() and add 'remove_mutex' which prevents race condition for 'fe->exit'.  [mchehab: fix a couple of checkpatch warnings and some mistakes at the error handling logic]  Link: https://lore.kernel.org/linux-media/20221117045925.14297-2-imv4bel@gmail.com ",
        "func_before": "static int dvb_frontend_release(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_frontend *fe = dvbdev->priv;\n\tstruct dvb_frontend_private *fepriv = fe->frontend_priv;\n\tint ret;\n\n\tdev_dbg(fe->dvb->device, \"%s:\\n\", __func__);\n\n\tif ((file->f_flags & O_ACCMODE) != O_RDONLY) {\n\t\tfepriv->release_jiffies = jiffies;\n\t\tmb();\n\t}\n\n\tret = dvb_generic_release(inode, file);\n\n\tif (dvbdev->users == -1) {\n\t\twake_up(&fepriv->wait_queue);\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\t\tmutex_lock(&fe->dvb->mdev_lock);\n\t\tif (fe->dvb->mdev) {\n\t\t\tmutex_lock(&fe->dvb->mdev->graph_mutex);\n\t\t\tif (fe->dvb->mdev->disable_source)\n\t\t\t\tfe->dvb->mdev->disable_source(dvbdev->entity);\n\t\t\tmutex_unlock(&fe->dvb->mdev->graph_mutex);\n\t\t}\n\t\tmutex_unlock(&fe->dvb->mdev_lock);\n#endif\n\t\tif (fe->exit != DVB_FE_NO_EXIT)\n\t\t\twake_up(&dvbdev->wait_queue);\n\t\tif (fe->ops.ts_bus_ctrl)\n\t\t\tfe->ops.ts_bus_ctrl(fe, 0);\n\t}\n\n\tdvb_frontend_put(fe);\n\n\treturn ret;\n}",
        "func": "static int dvb_frontend_release(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_frontend *fe = dvbdev->priv;\n\tstruct dvb_frontend_private *fepriv = fe->frontend_priv;\n\tint ret;\n\n\tmutex_lock(&fe->remove_mutex);\n\n\tdev_dbg(fe->dvb->device, \"%s:\\n\", __func__);\n\n\tif ((file->f_flags & O_ACCMODE) != O_RDONLY) {\n\t\tfepriv->release_jiffies = jiffies;\n\t\tmb();\n\t}\n\n\tret = dvb_generic_release(inode, file);\n\n\tif (dvbdev->users == -1) {\n\t\twake_up(&fepriv->wait_queue);\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\t\tmutex_lock(&fe->dvb->mdev_lock);\n\t\tif (fe->dvb->mdev) {\n\t\t\tmutex_lock(&fe->dvb->mdev->graph_mutex);\n\t\t\tif (fe->dvb->mdev->disable_source)\n\t\t\t\tfe->dvb->mdev->disable_source(dvbdev->entity);\n\t\t\tmutex_unlock(&fe->dvb->mdev->graph_mutex);\n\t\t}\n\t\tmutex_unlock(&fe->dvb->mdev_lock);\n#endif\n\t\tif (fe->ops.ts_bus_ctrl)\n\t\t\tfe->ops.ts_bus_ctrl(fe, 0);\n\n\t\tif (fe->exit != DVB_FE_NO_EXIT) {\n\t\t\tmutex_unlock(&fe->remove_mutex);\n\t\t\twake_up(&dvbdev->wait_queue);\n\t\t} else {\n\t\t\tmutex_unlock(&fe->remove_mutex);\n\t\t}\n\n\t} else {\n\t\tmutex_unlock(&fe->remove_mutex);\n\t}\n\n\tdvb_frontend_put(fe);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,8 @@\n \tstruct dvb_frontend *fe = dvbdev->priv;\n \tstruct dvb_frontend_private *fepriv = fe->frontend_priv;\n \tint ret;\n+\n+\tmutex_lock(&fe->remove_mutex);\n \n \tdev_dbg(fe->dvb->device, \"%s:\\n\", __func__);\n \n@@ -26,10 +28,18 @@\n \t\t}\n \t\tmutex_unlock(&fe->dvb->mdev_lock);\n #endif\n-\t\tif (fe->exit != DVB_FE_NO_EXIT)\n-\t\t\twake_up(&dvbdev->wait_queue);\n \t\tif (fe->ops.ts_bus_ctrl)\n \t\t\tfe->ops.ts_bus_ctrl(fe, 0);\n+\n+\t\tif (fe->exit != DVB_FE_NO_EXIT) {\n+\t\t\tmutex_unlock(&fe->remove_mutex);\n+\t\t\twake_up(&dvbdev->wait_queue);\n+\t\t} else {\n+\t\t\tmutex_unlock(&fe->remove_mutex);\n+\t\t}\n+\n+\t} else {\n+\t\tmutex_unlock(&fe->remove_mutex);\n \t}\n \n \tdvb_frontend_put(fe);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (fe->exit != DVB_FE_NO_EXIT)",
                "\t\t\twake_up(&dvbdev->wait_queue);"
            ],
            "added_lines": [
                "",
                "\tmutex_lock(&fe->remove_mutex);",
                "",
                "\t\tif (fe->exit != DVB_FE_NO_EXIT) {",
                "\t\t\tmutex_unlock(&fe->remove_mutex);",
                "\t\t\twake_up(&dvbdev->wait_queue);",
                "\t\t} else {",
                "\t\t\tmutex_unlock(&fe->remove_mutex);",
                "\t\t}",
                "",
                "\t} else {",
                "\t\tmutex_unlock(&fe->remove_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45886",
        "func_name": "torvalds/linux/dvb_net_close",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvb_net.c has a .disconnect versus dvb_device_open race condition that leads to a use-after-free.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=4172385b0c9ac366dcab78eda48c26814b87ed1a",
        "commit_title": "A race condition may occur between the .disconnect function, which",
        "commit_text": "is called when the device is disconnected, and the dvb_device_open() function, which is called when the device node is open()ed. This results in several types of UAFs.  The root cause of this is that you use the dvb_device_open() function, which does not implement a conditional statement that checks 'dvbnet->exit'.  So, add 'remove_mutex` to protect 'dvbnet->exit' and use locked_dvb_net_open() function to check 'dvbnet->exit'.  [mchehab: fix a checkpatch warning]  Link: https://lore.kernel.org/linux-media/20221117045925.14297-3-imv4bel@gmail.com ",
        "func_before": "static int dvb_net_close(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_net *dvbnet = dvbdev->priv;\n\n\tdvb_generic_release(inode, file);\n\n\tif(dvbdev->users == 1 && dvbnet->exit == 1)\n\t\twake_up(&dvbdev->wait_queue);\n\treturn 0;\n}",
        "func": "static int dvb_net_close(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_net *dvbnet = dvbdev->priv;\n\n\tmutex_lock(&dvbnet->remove_mutex);\n\n\tdvb_generic_release(inode, file);\n\n\tif (dvbdev->users == 1 && dvbnet->exit == 1) {\n\t\tmutex_unlock(&dvbnet->remove_mutex);\n\t\twake_up(&dvbdev->wait_queue);\n\t} else {\n\t\tmutex_unlock(&dvbnet->remove_mutex);\n\t}\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,9 +3,16 @@\n \tstruct dvb_device *dvbdev = file->private_data;\n \tstruct dvb_net *dvbnet = dvbdev->priv;\n \n+\tmutex_lock(&dvbnet->remove_mutex);\n+\n \tdvb_generic_release(inode, file);\n \n-\tif(dvbdev->users == 1 && dvbnet->exit == 1)\n+\tif (dvbdev->users == 1 && dvbnet->exit == 1) {\n+\t\tmutex_unlock(&dvbnet->remove_mutex);\n \t\twake_up(&dvbdev->wait_queue);\n+\t} else {\n+\t\tmutex_unlock(&dvbnet->remove_mutex);\n+\t}\n+\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif(dvbdev->users == 1 && dvbnet->exit == 1)"
            ],
            "added_lines": [
                "\tmutex_lock(&dvbnet->remove_mutex);",
                "",
                "\tif (dvbdev->users == 1 && dvbnet->exit == 1) {",
                "\t\tmutex_unlock(&dvbnet->remove_mutex);",
                "\t} else {",
                "\t\tmutex_unlock(&dvbnet->remove_mutex);",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45886",
        "func_name": "torvalds/linux/dvb_net_release",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvb_net.c has a .disconnect versus dvb_device_open race condition that leads to a use-after-free.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=4172385b0c9ac366dcab78eda48c26814b87ed1a",
        "commit_title": "A race condition may occur between the .disconnect function, which",
        "commit_text": "is called when the device is disconnected, and the dvb_device_open() function, which is called when the device node is open()ed. This results in several types of UAFs.  The root cause of this is that you use the dvb_device_open() function, which does not implement a conditional statement that checks 'dvbnet->exit'.  So, add 'remove_mutex` to protect 'dvbnet->exit' and use locked_dvb_net_open() function to check 'dvbnet->exit'.  [mchehab: fix a checkpatch warning]  Link: https://lore.kernel.org/linux-media/20221117045925.14297-3-imv4bel@gmail.com ",
        "func_before": "void dvb_net_release (struct dvb_net *dvbnet)\n{\n\tint i;\n\n\tdvbnet->exit = 1;\n\tif (dvbnet->dvbdev->users < 1)\n\t\twait_event(dvbnet->dvbdev->wait_queue,\n\t\t\t\tdvbnet->dvbdev->users==1);\n\n\tdvb_unregister_device(dvbnet->dvbdev);\n\n\tfor (i=0; i<DVB_NET_DEVICES_MAX; i++) {\n\t\tif (!dvbnet->state[i])\n\t\t\tcontinue;\n\t\tdvb_net_remove_if(dvbnet, i);\n\t}\n}",
        "func": "void dvb_net_release (struct dvb_net *dvbnet)\n{\n\tint i;\n\n\tmutex_lock(&dvbnet->remove_mutex);\n\tdvbnet->exit = 1;\n\tmutex_unlock(&dvbnet->remove_mutex);\n\n\tif (dvbnet->dvbdev->users < 1)\n\t\twait_event(dvbnet->dvbdev->wait_queue,\n\t\t\t\tdvbnet->dvbdev->users == 1);\n\n\tdvb_unregister_device(dvbnet->dvbdev);\n\n\tfor (i=0; i<DVB_NET_DEVICES_MAX; i++) {\n\t\tif (!dvbnet->state[i])\n\t\t\tcontinue;\n\t\tdvb_net_remove_if(dvbnet, i);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,10 +2,13 @@\n {\n \tint i;\n \n+\tmutex_lock(&dvbnet->remove_mutex);\n \tdvbnet->exit = 1;\n+\tmutex_unlock(&dvbnet->remove_mutex);\n+\n \tif (dvbnet->dvbdev->users < 1)\n \t\twait_event(dvbnet->dvbdev->wait_queue,\n-\t\t\t\tdvbnet->dvbdev->users==1);\n+\t\t\t\tdvbnet->dvbdev->users == 1);\n \n \tdvb_unregister_device(dvbnet->dvbdev);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\tdvbnet->dvbdev->users==1);"
            ],
            "added_lines": [
                "\tmutex_lock(&dvbnet->remove_mutex);",
                "\tmutex_unlock(&dvbnet->remove_mutex);",
                "",
                "\t\t\t\tdvbnet->dvbdev->users == 1);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45886",
        "func_name": "torvalds/linux/dvb_net_init",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvb_net.c has a .disconnect versus dvb_device_open race condition that leads to a use-after-free.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=4172385b0c9ac366dcab78eda48c26814b87ed1a",
        "commit_title": "A race condition may occur between the .disconnect function, which",
        "commit_text": "is called when the device is disconnected, and the dvb_device_open() function, which is called when the device node is open()ed. This results in several types of UAFs.  The root cause of this is that you use the dvb_device_open() function, which does not implement a conditional statement that checks 'dvbnet->exit'.  So, add 'remove_mutex` to protect 'dvbnet->exit' and use locked_dvb_net_open() function to check 'dvbnet->exit'.  [mchehab: fix a checkpatch warning]  Link: https://lore.kernel.org/linux-media/20221117045925.14297-3-imv4bel@gmail.com ",
        "func_before": "int dvb_net_init (struct dvb_adapter *adap, struct dvb_net *dvbnet,\n\t\t  struct dmx_demux *dmx)\n{\n\tint i;\n\n\tmutex_init(&dvbnet->ioctl_mutex);\n\tdvbnet->demux = dmx;\n\n\tfor (i=0; i<DVB_NET_DEVICES_MAX; i++)\n\t\tdvbnet->state[i] = 0;\n\n\treturn dvb_register_device(adap, &dvbnet->dvbdev, &dvbdev_net,\n\t\t\t     dvbnet, DVB_DEVICE_NET, 0);\n}",
        "func": "int dvb_net_init (struct dvb_adapter *adap, struct dvb_net *dvbnet,\n\t\t  struct dmx_demux *dmx)\n{\n\tint i;\n\n\tmutex_init(&dvbnet->ioctl_mutex);\n\tmutex_init(&dvbnet->remove_mutex);\n\tdvbnet->demux = dmx;\n\n\tfor (i=0; i<DVB_NET_DEVICES_MAX; i++)\n\t\tdvbnet->state[i] = 0;\n\n\treturn dvb_register_device(adap, &dvbnet->dvbdev, &dvbdev_net,\n\t\t\t     dvbnet, DVB_DEVICE_NET, 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,7 @@\n \tint i;\n \n \tmutex_init(&dvbnet->ioctl_mutex);\n+\tmutex_init(&dvbnet->remove_mutex);\n \tdvbnet->demux = dmx;\n \n \tfor (i=0; i<DVB_NET_DEVICES_MAX; i++)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_init(&dvbnet->remove_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45887",
        "func_name": "torvalds/linux/ttusb_dec_exit_dvb",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/usb/ttusb-dec/ttusb_dec.c has a memory leak because of the lack of a dvb_frontend_detach call.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=517a281338322ff8293f988771c98aaa7205e457",
        "commit_title": "Since dvb_frontend_detach() is not called in ttusb_dec_exit_dvb(),",
        "commit_text": "which is called when the device is disconnected, dvb_frontend_free() is not finally called.  This causes a memory leak just by repeatedly plugging and unplugging the device.  Fix this issue by adding dvb_frontend_detach() to ttusb_dec_exit_dvb().  Link: https://lore.kernel.org/linux-media/20221117045925.14297-5-imv4bel@gmail.com ",
        "func_before": "static void ttusb_dec_exit_dvb(struct ttusb_dec *dec)\n{\n\tdprintk(\"%s\\n\", __func__);\n\n\tdvb_net_release(&dec->dvb_net);\n\tdec->demux.dmx.close(&dec->demux.dmx);\n\tdec->demux.dmx.remove_frontend(&dec->demux.dmx, &dec->frontend);\n\tdvb_dmxdev_release(&dec->dmxdev);\n\tdvb_dmx_release(&dec->demux);\n\tif (dec->fe) {\n\t\tdvb_unregister_frontend(dec->fe);\n\t\tif (dec->fe->ops.release)\n\t\t\tdec->fe->ops.release(dec->fe);\n\t}\n\tdvb_unregister_adapter(&dec->adapter);\n}",
        "func": "static void ttusb_dec_exit_dvb(struct ttusb_dec *dec)\n{\n\tdprintk(\"%s\\n\", __func__);\n\n\tdvb_net_release(&dec->dvb_net);\n\tdec->demux.dmx.close(&dec->demux.dmx);\n\tdec->demux.dmx.remove_frontend(&dec->demux.dmx, &dec->frontend);\n\tdvb_dmxdev_release(&dec->dmxdev);\n\tdvb_dmx_release(&dec->demux);\n\tif (dec->fe) {\n\t\tdvb_unregister_frontend(dec->fe);\n\t\tdvb_frontend_detach(dec->fe);\n\t}\n\tdvb_unregister_adapter(&dec->adapter);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,8 +9,7 @@\n \tdvb_dmx_release(&dec->demux);\n \tif (dec->fe) {\n \t\tdvb_unregister_frontend(dec->fe);\n-\t\tif (dec->fe->ops.release)\n-\t\t\tdec->fe->ops.release(dec->fe);\n+\t\tdvb_frontend_detach(dec->fe);\n \t}\n \tdvb_unregister_adapter(&dec->adapter);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (dec->fe->ops.release)",
                "\t\t\tdec->fe->ops.release(dec->fe);"
            ],
            "added_lines": [
                "\t\tdvb_frontend_detach(dec->fe);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45888",
        "func_name": "torvalds/linux/xillyusb_disconnect",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/char/xillybus/xillyusb.c has a race condition and use-after-free during physical removal of a USB device.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=282a4b71816b6076029017a7bab3a9dcee12a920",
        "commit_title": "The driver for XillyUSB devices maintains a kref reference count on each",
        "commit_text": "xillyusb_dev structure, which represents a physical device. This reference count reaches zero when the device has been disconnected and there are no open file descriptors that are related to the device. When this occurs, kref_put() calls cleanup_dev(), which clears up the device's data, including the structure itself.  However, when xillyusb_open() is called, this reference count becomes tricky: This function needs to obtain the xillyusb_dev structure that relates to the inode's major and minor (as there can be several such). xillybus_find_inode() (which is defined in xillybus_class.c) is called for this purpose. xillybus_find_inode() holds a mutex that is global in xillybus_class.c to protect the list of devices, and releases this mutex before returning. As a result, nothing protects the xillyusb_dev's reference counter from being decremented to zero before xillyusb_open() increments it on its own behalf. Hence the structure can be freed due to a rare race condition.  To solve this, a mutex is added. It is locked by xillyusb_open() before the call to xillybus_find_inode() and is released only after the kref counter has been incremented on behalf of the newly opened inode. This protects the kref reference counters of all xillyusb_dev structs from being decremented by xillyusb_disconnect() during this time segment, as the call to kref_put() in this function is done with the same lock held.  There is no need to hold the lock on other calls to kref_put(), because if xillybus_find_inode() finds a struct, xillyusb_disconnect() has not made the call to remove it, and hence not made its call to kref_put(), which takes place afterwards. Hence preventing xillyusb_disconnect's call to kref_put() is enough to ensure that the reference doesn't reach zero before it's incremented by xillyusb_open().  It would have been more natural to increment the reference count in xillybus_find_inode() of course, however this function is also called by Xillybus' driver for PCIe / OF, which registers a completely different structure. Therefore, xillybus_find_inode() treats these structures as void pointers, and accordingly can't make any changes.  Suggested-by: Alan Stern <stern@rowland.harvard.edu> Link: https://lore.kernel.org/r/20221030094209.65916-1-eli.billauer@gmail.com ",
        "func_before": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tkref_put(&xdev->kref, cleanup_dev);\n}",
        "func": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tmutex_lock(&kref_mutex);\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&kref_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -61,5 +61,7 @@\n \n \txdev->dev = NULL;\n \n+\tmutex_lock(&kref_mutex);\n \tkref_put(&xdev->kref, cleanup_dev);\n+\tmutex_unlock(&kref_mutex);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tmutex_lock(&kref_mutex);",
                "\tmutex_unlock(&kref_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45888",
        "func_name": "torvalds/linux/xillyusb_open",
        "description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/char/xillybus/xillyusb.c has a race condition and use-after-free during physical removal of a USB device.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=282a4b71816b6076029017a7bab3a9dcee12a920",
        "commit_title": "The driver for XillyUSB devices maintains a kref reference count on each",
        "commit_text": "xillyusb_dev structure, which represents a physical device. This reference count reaches zero when the device has been disconnected and there are no open file descriptors that are related to the device. When this occurs, kref_put() calls cleanup_dev(), which clears up the device's data, including the structure itself.  However, when xillyusb_open() is called, this reference count becomes tricky: This function needs to obtain the xillyusb_dev structure that relates to the inode's major and minor (as there can be several such). xillybus_find_inode() (which is defined in xillybus_class.c) is called for this purpose. xillybus_find_inode() holds a mutex that is global in xillybus_class.c to protect the list of devices, and releases this mutex before returning. As a result, nothing protects the xillyusb_dev's reference counter from being decremented to zero before xillyusb_open() increments it on its own behalf. Hence the structure can be freed due to a rare race condition.  To solve this, a mutex is added. It is locked by xillyusb_open() before the call to xillybus_find_inode() and is released only after the kref counter has been incremented on behalf of the newly opened inode. This protects the kref reference counters of all xillyusb_dev structs from being decremented by xillyusb_disconnect() during this time segment, as the call to kref_put() in this function is done with the same lock held.  There is no need to hold the lock on other calls to kref_put(), because if xillybus_find_inode() finds a struct, xillyusb_disconnect() has not made the call to remove it, and hence not made its call to kref_put(), which takes place afterwards. Hence preventing xillyusb_disconnect's call to kref_put() is enough to ensure that the reference doesn't reach zero before it's incremented by xillyusb_open().  It would have been more natural to increment the reference count in xillybus_find_inode() of course, however this function is also called by Xillybus' driver for PCIe / OF, which registers a completely different structure. Therefore, xillybus_find_inode() treats these structures as void pointers, and accordingly can't make any changes.  Suggested-by: Alan Stern <stern@rowland.harvard.edu> Link: https://lore.kernel.org/r/20221030094209.65916-1-eli.billauer@gmail.com ",
        "func_before": "static int xillyusb_open(struct inode *inode, struct file *filp)\n{\n\tstruct xillyusb_dev *xdev;\n\tstruct xillyusb_channel *chan;\n\tstruct xillyfifo *in_fifo = NULL;\n\tstruct xillyusb_endpoint *out_ep = NULL;\n\tint rc;\n\tint index;\n\n\trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n\tif (rc)\n\t\treturn rc;\n\n\tchan = &xdev->channels[index];\n\tfilp->private_data = chan;\n\n\tmutex_lock(&chan->lock);\n\n\trc = -ENODEV;\n\n\tif (xdev->error)\n\t\tgoto unmutex_fail;\n\n\tif (((filp->f_mode & FMODE_READ) && !chan->readable) ||\n\t    ((filp->f_mode & FMODE_WRITE) && !chan->writable))\n\t\tgoto unmutex_fail;\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_READ) &&\n\t    chan->in_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for read on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_WRITE) &&\n\t    chan->out_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for write on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\trc = -EBUSY;\n\n\tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n\t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n\t\tgoto unmutex_fail;\n\n\tkref_get(&xdev->kref);\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 1;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 1;\n\n\tmutex_unlock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tout_ep = endpoint_alloc(xdev,\n\t\t\t\t\t(chan->chan_idx + 2) | USB_DIR_OUT,\n\t\t\t\t\tbulk_out_work, BUF_SIZE_ORDER, BUFNUM);\n\n\t\tif (!out_ep) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto unopen;\n\t\t}\n\n\t\trc = fifo_init(&out_ep->fifo, chan->out_log2_fifo_size);\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\n\t\tout_ep->fill_mask = -(1 << chan->out_log2_element_size);\n\t\tchan->out_bytes = 0;\n\t\tchan->flushed = 0;\n\n\t\t/*\n\t\t * Sending a flush request to a previously closed stream\n\t\t * effectively opens it, and also waits until the command is\n\t\t * confirmed by the FPGA. The latter is necessary because the\n\t\t * data is sent through a separate BULK OUT endpoint, and the\n\t\t * xHCI controller is free to reorder transmissions.\n\t\t *\n\t\t * This can't go wrong unless there's a serious hardware error\n\t\t * (or the computer is stuck for 500 ms?)\n\t\t */\n\t\trc = flush_downstream(chan, XILLY_RESPONSE_TIMEOUT, false);\n\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\trc = -EIO;\n\t\t\treport_io_error(xdev, rc);\n\t\t}\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t}\n\n\tif (filp->f_mode & FMODE_READ) {\n\t\tin_fifo = kzalloc(sizeof(*in_fifo), GFP_KERNEL);\n\n\t\tif (!in_fifo) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto late_unopen;\n\t\t}\n\n\t\trc = fifo_init(in_fifo, chan->in_log2_fifo_size);\n\n\t\tif (rc) {\n\t\t\tkfree(in_fifo);\n\t\t\tgoto late_unopen;\n\t\t}\n\t}\n\n\tmutex_lock(&chan->lock);\n\tif (in_fifo) {\n\t\tchan->in_fifo = in_fifo;\n\t\tchan->read_data_ok = 1;\n\t}\n\tif (out_ep)\n\t\tchan->out_ep = out_ep;\n\tmutex_unlock(&chan->lock);\n\n\tif (in_fifo) {\n\t\tu32 in_checkpoint = 0;\n\n\t\tif (!chan->in_synchronous)\n\t\t\tin_checkpoint = in_fifo->size >>\n\t\t\t\tchan->in_log2_element_size;\n\n\t\tchan->in_consumed_bytes = 0;\n\t\tchan->poll_used = 0;\n\t\tchan->in_current_checkpoint = in_checkpoint;\n\t\trc = xillyusb_send_opcode(xdev, (chan->chan_idx << 1) | 1,\n\t\t\t\t\t  OPCODE_SET_CHECKPOINT,\n\t\t\t\t\t  in_checkpoint);\n\n\t\tif (rc) /* Failure guarantees that opcode wasn't sent */\n\t\t\tgoto unfifo;\n\n\t\t/*\n\t\t * In non-blocking mode, request the FPGA to send any data it\n\t\t * has right away. Otherwise, the first read() will always\n\t\t * return -EAGAIN, which is OK strictly speaking, but ugly.\n\t\t * Checking and unrolling if this fails isn't worth the\n\t\t * effort -- the error is propagated to the first read()\n\t\t * anyhow.\n\t\t */\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\trequest_read_anything(chan, OPCODE_SET_PUSH);\n\t}\n\n\treturn 0;\n\nunfifo:\n\tchan->read_data_ok = 0;\n\tsafely_assign_in_fifo(chan, NULL);\n\tfifo_mem_release(in_fifo);\n\tkfree(in_fifo);\n\n\tif (out_ep) {\n\t\tmutex_lock(&chan->lock);\n\t\tchan->out_ep = NULL;\n\t\tmutex_unlock(&chan->lock);\n\t}\n\nlate_unopen:\n\tif (out_ep)\n\t\tendpoint_dealloc(out_ep);\n\nunopen:\n\tmutex_lock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 0;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 0;\n\n\tmutex_unlock(&chan->lock);\n\n\tkref_put(&xdev->kref, cleanup_dev);\n\n\treturn rc;\n\nunmutex_fail:\n\tmutex_unlock(&chan->lock);\n\treturn rc;\n}",
        "func": "static int xillyusb_open(struct inode *inode, struct file *filp)\n{\n\tstruct xillyusb_dev *xdev;\n\tstruct xillyusb_channel *chan;\n\tstruct xillyfifo *in_fifo = NULL;\n\tstruct xillyusb_endpoint *out_ep = NULL;\n\tint rc;\n\tint index;\n\n\tmutex_lock(&kref_mutex);\n\n\trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n\tif (rc) {\n\t\tmutex_unlock(&kref_mutex);\n\t\treturn rc;\n\t}\n\n\tkref_get(&xdev->kref);\n\tmutex_unlock(&kref_mutex);\n\n\tchan = &xdev->channels[index];\n\tfilp->private_data = chan;\n\n\tmutex_lock(&chan->lock);\n\n\trc = -ENODEV;\n\n\tif (xdev->error)\n\t\tgoto unmutex_fail;\n\n\tif (((filp->f_mode & FMODE_READ) && !chan->readable) ||\n\t    ((filp->f_mode & FMODE_WRITE) && !chan->writable))\n\t\tgoto unmutex_fail;\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_READ) &&\n\t    chan->in_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for read on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_WRITE) &&\n\t    chan->out_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for write on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\trc = -EBUSY;\n\n\tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n\t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n\t\tgoto unmutex_fail;\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 1;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 1;\n\n\tmutex_unlock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tout_ep = endpoint_alloc(xdev,\n\t\t\t\t\t(chan->chan_idx + 2) | USB_DIR_OUT,\n\t\t\t\t\tbulk_out_work, BUF_SIZE_ORDER, BUFNUM);\n\n\t\tif (!out_ep) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto unopen;\n\t\t}\n\n\t\trc = fifo_init(&out_ep->fifo, chan->out_log2_fifo_size);\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\n\t\tout_ep->fill_mask = -(1 << chan->out_log2_element_size);\n\t\tchan->out_bytes = 0;\n\t\tchan->flushed = 0;\n\n\t\t/*\n\t\t * Sending a flush request to a previously closed stream\n\t\t * effectively opens it, and also waits until the command is\n\t\t * confirmed by the FPGA. The latter is necessary because the\n\t\t * data is sent through a separate BULK OUT endpoint, and the\n\t\t * xHCI controller is free to reorder transmissions.\n\t\t *\n\t\t * This can't go wrong unless there's a serious hardware error\n\t\t * (or the computer is stuck for 500 ms?)\n\t\t */\n\t\trc = flush_downstream(chan, XILLY_RESPONSE_TIMEOUT, false);\n\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\trc = -EIO;\n\t\t\treport_io_error(xdev, rc);\n\t\t}\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t}\n\n\tif (filp->f_mode & FMODE_READ) {\n\t\tin_fifo = kzalloc(sizeof(*in_fifo), GFP_KERNEL);\n\n\t\tif (!in_fifo) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto late_unopen;\n\t\t}\n\n\t\trc = fifo_init(in_fifo, chan->in_log2_fifo_size);\n\n\t\tif (rc) {\n\t\t\tkfree(in_fifo);\n\t\t\tgoto late_unopen;\n\t\t}\n\t}\n\n\tmutex_lock(&chan->lock);\n\tif (in_fifo) {\n\t\tchan->in_fifo = in_fifo;\n\t\tchan->read_data_ok = 1;\n\t}\n\tif (out_ep)\n\t\tchan->out_ep = out_ep;\n\tmutex_unlock(&chan->lock);\n\n\tif (in_fifo) {\n\t\tu32 in_checkpoint = 0;\n\n\t\tif (!chan->in_synchronous)\n\t\t\tin_checkpoint = in_fifo->size >>\n\t\t\t\tchan->in_log2_element_size;\n\n\t\tchan->in_consumed_bytes = 0;\n\t\tchan->poll_used = 0;\n\t\tchan->in_current_checkpoint = in_checkpoint;\n\t\trc = xillyusb_send_opcode(xdev, (chan->chan_idx << 1) | 1,\n\t\t\t\t\t  OPCODE_SET_CHECKPOINT,\n\t\t\t\t\t  in_checkpoint);\n\n\t\tif (rc) /* Failure guarantees that opcode wasn't sent */\n\t\t\tgoto unfifo;\n\n\t\t/*\n\t\t * In non-blocking mode, request the FPGA to send any data it\n\t\t * has right away. Otherwise, the first read() will always\n\t\t * return -EAGAIN, which is OK strictly speaking, but ugly.\n\t\t * Checking and unrolling if this fails isn't worth the\n\t\t * effort -- the error is propagated to the first read()\n\t\t * anyhow.\n\t\t */\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\trequest_read_anything(chan, OPCODE_SET_PUSH);\n\t}\n\n\treturn 0;\n\nunfifo:\n\tchan->read_data_ok = 0;\n\tsafely_assign_in_fifo(chan, NULL);\n\tfifo_mem_release(in_fifo);\n\tkfree(in_fifo);\n\n\tif (out_ep) {\n\t\tmutex_lock(&chan->lock);\n\t\tchan->out_ep = NULL;\n\t\tmutex_unlock(&chan->lock);\n\t}\n\nlate_unopen:\n\tif (out_ep)\n\t\tendpoint_dealloc(out_ep);\n\nunopen:\n\tmutex_lock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 0;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 0;\n\n\tmutex_unlock(&chan->lock);\n\n\tkref_put(&xdev->kref, cleanup_dev);\n\n\treturn rc;\n\nunmutex_fail:\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&chan->lock);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,9 +7,16 @@\n \tint rc;\n \tint index;\n \n+\tmutex_lock(&kref_mutex);\n+\n \trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n-\tif (rc)\n+\tif (rc) {\n+\t\tmutex_unlock(&kref_mutex);\n \t\treturn rc;\n+\t}\n+\n+\tkref_get(&xdev->kref);\n+\tmutex_unlock(&kref_mutex);\n \n \tchan = &xdev->channels[index];\n \tfilp->private_data = chan;\n@@ -44,8 +51,6 @@\n \tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n \t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n \t\tgoto unmutex_fail;\n-\n-\tkref_get(&xdev->kref);\n \n \tif (filp->f_mode & FMODE_READ)\n \t\tchan->open_for_read = 1;\n@@ -183,6 +188,7 @@\n \treturn rc;\n \n unmutex_fail:\n+\tkref_put(&xdev->kref, cleanup_dev);\n \tmutex_unlock(&chan->lock);\n \treturn rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (rc)",
                "",
                "\tkref_get(&xdev->kref);"
            ],
            "added_lines": [
                "\tmutex_lock(&kref_mutex);",
                "",
                "\tif (rc) {",
                "\t\tmutex_unlock(&kref_mutex);",
                "\t}",
                "",
                "\tkref_get(&xdev->kref);",
                "\tmutex_unlock(&kref_mutex);",
                "\tkref_put(&xdev->kref, cleanup_dev);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45869",
        "func_name": "torvalds/linux/direct_page_fault",
        "description": "A race condition in the x86 KVM subsystem in the Linux kernel through 6.1-rc6 allows guest OS users to cause a denial of service (host OS crash or host OS memory corruption) when nested virtualisation and the TDP MMU are enabled.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=47b0c2e4c220f2251fd8dcfbb44479819c715e15",
        "commit_title": "make_mmu_pages_available() must be called with mmu_lock held for write.",
        "commit_text": "However, if the TDP MMU is used, it will be called with mmu_lock held for read. This function does nothing unless shadow pages are used, so there is no race unless nested TDP is used. Since nested TDP uses shadow pages, old shadow pages may be zapped by this function even when the TDP MMU is enabled. Since shadow pages are never allocated by kvm_tdp_mmu_map(), a race condition can be avoided by not calling make_mmu_pages_available() if the TDP MMU is currently in use.  I encountered this when repeatedly starting and stopping nested VM. It can be artificially caused by allocating a large number of nested TDP SPTEs.  For example, the following BUG and general protection fault are caused in the host kernel.  pte_list_remove: 00000000cd54fc10 many->many ------------[ cut here ]------------ kernel BUG at arch/x86/kvm/mmu/mmu.c:963! invalid opcode: 0000 [#1] PREEMPT SMP NOPTI RIP: 0010:pte_list_remove.cold+0x16/0x48 [kvm] Call Trace:  <TASK>  drop_spte+0xe0/0x180 [kvm]  mmu_page_zap_pte+0x4f/0x140 [kvm]  __kvm_mmu_prepare_zap_page+0x62/0x3e0 [kvm]  kvm_mmu_zap_oldest_mmu_pages+0x7d/0xf0 [kvm]  direct_page_fault+0x3cb/0x9b0 [kvm]  kvm_tdp_page_fault+0x2c/0xa0 [kvm]  kvm_mmu_page_fault+0x207/0x930 [kvm]  npf_interception+0x47/0xb0 [kvm_amd]  svm_invoke_exit_handler+0x13c/0x1a0 [kvm_amd]  svm_handle_exit+0xfc/0x2c0 [kvm_amd]  kvm_arch_vcpu_ioctl_run+0xa79/0x1780 [kvm]  kvm_vcpu_ioctl+0x29b/0x6f0 [kvm]  __x64_sys_ioctl+0x95/0xd0  do_syscall_64+0x5c/0x90  general protection fault, probably for non-canonical address 0xdead000000000122: 0000 [#1] PREEMPT SMP NOPTI RIP: 0010:kvm_mmu_commit_zap_page.part.0+0x4b/0xe0 [kvm] Call Trace:  <TASK>  kvm_mmu_zap_oldest_mmu_pages+0xae/0xf0 [kvm]  direct_page_fault+0x3cb/0x9b0 [kvm]  kvm_tdp_page_fault+0x2c/0xa0 [kvm]  kvm_mmu_page_fault+0x207/0x930 [kvm]  npf_interception+0x47/0xb0 [kvm_amd]  CVE: CVE-2022-45869 Cc: stable@vger.kernel.org ",
        "func_before": "static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tbool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);\n\n\tunsigned long mmu_seq;\n\tint r;\n\n\tfault->gfn = fault->addr >> PAGE_SHIFT;\n\tfault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);\n\n\tif (page_fault_handle_page_track(vcpu, fault))\n\t\treturn RET_PF_EMULATE;\n\n\tr = fast_page_fault(vcpu, fault);\n\tif (r != RET_PF_INVALID)\n\t\treturn r;\n\n\tr = mmu_topup_memory_caches(vcpu, false);\n\tif (r)\n\t\treturn r;\n\n\tmmu_seq = vcpu->kvm->mmu_invalidate_seq;\n\tsmp_rmb();\n\n\tr = kvm_faultin_pfn(vcpu, fault);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = handle_abnormal_pfn(vcpu, fault, ACC_ALL);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = RET_PF_RETRY;\n\n\tif (is_tdp_mmu_fault)\n\t\tread_lock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_lock(&vcpu->kvm->mmu_lock);\n\n\tif (is_page_fault_stale(vcpu, fault, mmu_seq))\n\t\tgoto out_unlock;\n\n\tr = make_mmu_pages_available(vcpu);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tif (is_tdp_mmu_fault)\n\t\tr = kvm_tdp_mmu_map(vcpu, fault);\n\telse\n\t\tr = __direct_map(vcpu, fault);\n\nout_unlock:\n\tif (is_tdp_mmu_fault)\n\t\tread_unlock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(fault->pfn);\n\treturn r;\n}",
        "func": "static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tbool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);\n\n\tunsigned long mmu_seq;\n\tint r;\n\n\tfault->gfn = fault->addr >> PAGE_SHIFT;\n\tfault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);\n\n\tif (page_fault_handle_page_track(vcpu, fault))\n\t\treturn RET_PF_EMULATE;\n\n\tr = fast_page_fault(vcpu, fault);\n\tif (r != RET_PF_INVALID)\n\t\treturn r;\n\n\tr = mmu_topup_memory_caches(vcpu, false);\n\tif (r)\n\t\treturn r;\n\n\tmmu_seq = vcpu->kvm->mmu_invalidate_seq;\n\tsmp_rmb();\n\n\tr = kvm_faultin_pfn(vcpu, fault);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = handle_abnormal_pfn(vcpu, fault, ACC_ALL);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = RET_PF_RETRY;\n\n\tif (is_tdp_mmu_fault)\n\t\tread_lock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_lock(&vcpu->kvm->mmu_lock);\n\n\tif (is_page_fault_stale(vcpu, fault, mmu_seq))\n\t\tgoto out_unlock;\n\n\tif (is_tdp_mmu_fault) {\n\t\tr = kvm_tdp_mmu_map(vcpu, fault);\n\t} else {\n\t\tr = make_mmu_pages_available(vcpu);\n\t\tif (r)\n\t\t\tgoto out_unlock;\n\t\tr = __direct_map(vcpu, fault);\n\t}\n\nout_unlock:\n\tif (is_tdp_mmu_fault)\n\t\tread_unlock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(fault->pfn);\n\treturn r;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,14 +40,14 @@\n \tif (is_page_fault_stale(vcpu, fault, mmu_seq))\n \t\tgoto out_unlock;\n \n-\tr = make_mmu_pages_available(vcpu);\n-\tif (r)\n-\t\tgoto out_unlock;\n-\n-\tif (is_tdp_mmu_fault)\n+\tif (is_tdp_mmu_fault) {\n \t\tr = kvm_tdp_mmu_map(vcpu, fault);\n-\telse\n+\t} else {\n+\t\tr = make_mmu_pages_available(vcpu);\n+\t\tif (r)\n+\t\t\tgoto out_unlock;\n \t\tr = __direct_map(vcpu, fault);\n+\t}\n \n out_unlock:\n \tif (is_tdp_mmu_fault)",
        "diff_line_info": {
            "deleted_lines": [
                "\tr = make_mmu_pages_available(vcpu);",
                "\tif (r)",
                "\t\tgoto out_unlock;",
                "",
                "\tif (is_tdp_mmu_fault)",
                "\telse"
            ],
            "added_lines": [
                "\tif (is_tdp_mmu_fault) {",
                "\t} else {",
                "\t\tr = make_mmu_pages_available(vcpu);",
                "\t\tif (r)",
                "\t\t\tgoto out_unlock;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45869",
        "func_name": "torvalds/linux/__kvm_mmu_prepare_zap_page",
        "description": "A race condition in the x86 KVM subsystem in the Linux kernel through 6.1-rc6 allows guest OS users to cause a denial of service (host OS crash or host OS memory corruption) when nested virtualisation and the TDP MMU are enabled.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=47b0c2e4c220f2251fd8dcfbb44479819c715e15",
        "commit_title": "make_mmu_pages_available() must be called with mmu_lock held for write.",
        "commit_text": "However, if the TDP MMU is used, it will be called with mmu_lock held for read. This function does nothing unless shadow pages are used, so there is no race unless nested TDP is used. Since nested TDP uses shadow pages, old shadow pages may be zapped by this function even when the TDP MMU is enabled. Since shadow pages are never allocated by kvm_tdp_mmu_map(), a race condition can be avoided by not calling make_mmu_pages_available() if the TDP MMU is currently in use.  I encountered this when repeatedly starting and stopping nested VM. It can be artificially caused by allocating a large number of nested TDP SPTEs.  For example, the following BUG and general protection fault are caused in the host kernel.  pte_list_remove: 00000000cd54fc10 many->many ------------[ cut here ]------------ kernel BUG at arch/x86/kvm/mmu/mmu.c:963! invalid opcode: 0000 [#1] PREEMPT SMP NOPTI RIP: 0010:pte_list_remove.cold+0x16/0x48 [kvm] Call Trace:  <TASK>  drop_spte+0xe0/0x180 [kvm]  mmu_page_zap_pte+0x4f/0x140 [kvm]  __kvm_mmu_prepare_zap_page+0x62/0x3e0 [kvm]  kvm_mmu_zap_oldest_mmu_pages+0x7d/0xf0 [kvm]  direct_page_fault+0x3cb/0x9b0 [kvm]  kvm_tdp_page_fault+0x2c/0xa0 [kvm]  kvm_mmu_page_fault+0x207/0x930 [kvm]  npf_interception+0x47/0xb0 [kvm_amd]  svm_invoke_exit_handler+0x13c/0x1a0 [kvm_amd]  svm_handle_exit+0xfc/0x2c0 [kvm_amd]  kvm_arch_vcpu_ioctl_run+0xa79/0x1780 [kvm]  kvm_vcpu_ioctl+0x29b/0x6f0 [kvm]  __x64_sys_ioctl+0x95/0xd0  do_syscall_64+0x5c/0x90  general protection fault, probably for non-canonical address 0xdead000000000122: 0000 [#1] PREEMPT SMP NOPTI RIP: 0010:kvm_mmu_commit_zap_page.part.0+0x4b/0xe0 [kvm] Call Trace:  <TASK>  kvm_mmu_zap_oldest_mmu_pages+0xae/0xf0 [kvm]  direct_page_fault+0x3cb/0x9b0 [kvm]  kvm_tdp_page_fault+0x2c/0xa0 [kvm]  kvm_mmu_page_fault+0x207/0x930 [kvm]  npf_interception+0x47/0xb0 [kvm_amd]  CVE: CVE-2022-45869 Cc: stable@vger.kernel.org ",
        "func_before": "static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,\n\t\t\t\t       struct kvm_mmu_page *sp,\n\t\t\t\t       struct list_head *invalid_list,\n\t\t\t\t       int *nr_zapped)\n{\n\tbool list_unstable, zapped_root = false;\n\n\ttrace_kvm_mmu_prepare_zap_page(sp);\n\t++kvm->stat.mmu_shadow_zapped;\n\t*nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);\n\t*nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);\n\tkvm_mmu_unlink_parents(sp);\n\n\t/* Zapping children means active_mmu_pages has become unstable. */\n\tlist_unstable = *nr_zapped;\n\n\tif (!sp->role.invalid && sp_has_gptes(sp))\n\t\tunaccount_shadowed(kvm, sp);\n\n\tif (sp->unsync)\n\t\tkvm_unlink_unsync_page(kvm, sp);\n\tif (!sp->root_count) {\n\t\t/* Count self */\n\t\t(*nr_zapped)++;\n\n\t\t/*\n\t\t * Already invalid pages (previously active roots) are not on\n\t\t * the active page list.  See list_del() in the \"else\" case of\n\t\t * !sp->root_count.\n\t\t */\n\t\tif (sp->role.invalid)\n\t\t\tlist_add(&sp->link, invalid_list);\n\t\telse\n\t\t\tlist_move(&sp->link, invalid_list);\n\t\tkvm_unaccount_mmu_page(kvm, sp);\n\t} else {\n\t\t/*\n\t\t * Remove the active root from the active page list, the root\n\t\t * will be explicitly freed when the root_count hits zero.\n\t\t */\n\t\tlist_del(&sp->link);\n\n\t\t/*\n\t\t * Obsolete pages cannot be used on any vCPUs, see the comment\n\t\t * in kvm_mmu_zap_all_fast().  Note, is_obsolete_sp() also\n\t\t * treats invalid shadow pages as being obsolete.\n\t\t */\n\t\tzapped_root = !is_obsolete_sp(kvm, sp);\n\t}\n\n\tif (sp->lpage_disallowed)\n\t\tunaccount_huge_nx_page(kvm, sp);\n\n\tsp->role.invalid = 1;\n\n\t/*\n\t * Make the request to free obsolete roots after marking the root\n\t * invalid, otherwise other vCPUs may not see it as invalid.\n\t */\n\tif (zapped_root)\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);\n\treturn list_unstable;\n}",
        "func": "static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,\n\t\t\t\t       struct kvm_mmu_page *sp,\n\t\t\t\t       struct list_head *invalid_list,\n\t\t\t\t       int *nr_zapped)\n{\n\tbool list_unstable, zapped_root = false;\n\n\tlockdep_assert_held_write(&kvm->mmu_lock);\n\ttrace_kvm_mmu_prepare_zap_page(sp);\n\t++kvm->stat.mmu_shadow_zapped;\n\t*nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);\n\t*nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);\n\tkvm_mmu_unlink_parents(sp);\n\n\t/* Zapping children means active_mmu_pages has become unstable. */\n\tlist_unstable = *nr_zapped;\n\n\tif (!sp->role.invalid && sp_has_gptes(sp))\n\t\tunaccount_shadowed(kvm, sp);\n\n\tif (sp->unsync)\n\t\tkvm_unlink_unsync_page(kvm, sp);\n\tif (!sp->root_count) {\n\t\t/* Count self */\n\t\t(*nr_zapped)++;\n\n\t\t/*\n\t\t * Already invalid pages (previously active roots) are not on\n\t\t * the active page list.  See list_del() in the \"else\" case of\n\t\t * !sp->root_count.\n\t\t */\n\t\tif (sp->role.invalid)\n\t\t\tlist_add(&sp->link, invalid_list);\n\t\telse\n\t\t\tlist_move(&sp->link, invalid_list);\n\t\tkvm_unaccount_mmu_page(kvm, sp);\n\t} else {\n\t\t/*\n\t\t * Remove the active root from the active page list, the root\n\t\t * will be explicitly freed when the root_count hits zero.\n\t\t */\n\t\tlist_del(&sp->link);\n\n\t\t/*\n\t\t * Obsolete pages cannot be used on any vCPUs, see the comment\n\t\t * in kvm_mmu_zap_all_fast().  Note, is_obsolete_sp() also\n\t\t * treats invalid shadow pages as being obsolete.\n\t\t */\n\t\tzapped_root = !is_obsolete_sp(kvm, sp);\n\t}\n\n\tif (sp->lpage_disallowed)\n\t\tunaccount_huge_nx_page(kvm, sp);\n\n\tsp->role.invalid = 1;\n\n\t/*\n\t * Make the request to free obsolete roots after marking the root\n\t * invalid, otherwise other vCPUs may not see it as invalid.\n\t */\n\tif (zapped_root)\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);\n\treturn list_unstable;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n {\n \tbool list_unstable, zapped_root = false;\n \n+\tlockdep_assert_held_write(&kvm->mmu_lock);\n \ttrace_kvm_mmu_prepare_zap_page(sp);\n \t++kvm->stat.mmu_shadow_zapped;\n \t*nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tlockdep_assert_held_write(&kvm->mmu_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35823",
        "func_name": "torvalds/linux/saa7134_ts_fini",
        "description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=30cf57da176cca80f11df0d9b7f71581fe601389",
        "commit_title": "In saa7134_initdev, it will call saa7134_hwinit1. There are three",
        "commit_text": "function invoking here: saa7134_video_init1, saa7134_ts_init1 and saa7134_vbi_init1.  All of them will init a timer with same function. Take saa7134_video_init1 as an example. It'll bound &dev->video_q.timeout with saa7134_buffer_timeout.  In buffer_activate, the timer funtcion is started.  If we remove the module or device which will call saa7134_finidev to make cleanup, there may be a unfinished work. The possible sequence is as follows, which will cause a typical UAF bug.  Fix it by canceling the timer works accordingly before cleanup in saa7134_finidev.  CPU0                  CPU1                      |saa7134_buffer_timeout saa7134_finidev     |   kfree(dev);       |                     |                     | saa7134_buffer_next                     | //use dev  ",
        "func_before": "int saa7134_ts_fini(struct saa7134_dev *dev)\n{\n\tsaa7134_pgtable_free(dev->pci, &dev->ts_q.pt);\n\treturn 0;\n}",
        "func": "int saa7134_ts_fini(struct saa7134_dev *dev)\n{\n\tdel_timer_sync(&dev->ts_q.timeout);\n\tsaa7134_pgtable_free(dev->pci, &dev->ts_q.pt);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n int saa7134_ts_fini(struct saa7134_dev *dev)\n {\n+\tdel_timer_sync(&dev->ts_q.timeout);\n \tsaa7134_pgtable_free(dev->pci, &dev->ts_q.pt);\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tdel_timer_sync(&dev->ts_q.timeout);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35823",
        "func_name": "torvalds/linux/saa7134_vbi_fini",
        "description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=30cf57da176cca80f11df0d9b7f71581fe601389",
        "commit_title": "In saa7134_initdev, it will call saa7134_hwinit1. There are three",
        "commit_text": "function invoking here: saa7134_video_init1, saa7134_ts_init1 and saa7134_vbi_init1.  All of them will init a timer with same function. Take saa7134_video_init1 as an example. It'll bound &dev->video_q.timeout with saa7134_buffer_timeout.  In buffer_activate, the timer funtcion is started.  If we remove the module or device which will call saa7134_finidev to make cleanup, there may be a unfinished work. The possible sequence is as follows, which will cause a typical UAF bug.  Fix it by canceling the timer works accordingly before cleanup in saa7134_finidev.  CPU0                  CPU1                      |saa7134_buffer_timeout saa7134_finidev     |   kfree(dev);       |                     |                     | saa7134_buffer_next                     | //use dev  ",
        "func_before": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\treturn 0;\n}",
        "func": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\tdel_timer_sync(&dev->vbi_q.timeout);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n int saa7134_vbi_fini(struct saa7134_dev *dev)\n {\n \t/* nothing */\n+\tdel_timer_sync(&dev->vbi_q.timeout);\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tdel_timer_sync(&dev->vbi_q.timeout);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35823",
        "func_name": "torvalds/linux/saa7134_video_fini",
        "description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=30cf57da176cca80f11df0d9b7f71581fe601389",
        "commit_title": "In saa7134_initdev, it will call saa7134_hwinit1. There are three",
        "commit_text": "function invoking here: saa7134_video_init1, saa7134_ts_init1 and saa7134_vbi_init1.  All of them will init a timer with same function. Take saa7134_video_init1 as an example. It'll bound &dev->video_q.timeout with saa7134_buffer_timeout.  In buffer_activate, the timer funtcion is started.  If we remove the module or device which will call saa7134_finidev to make cleanup, there may be a unfinished work. The possible sequence is as follows, which will cause a typical UAF bug.  Fix it by canceling the timer works accordingly before cleanup in saa7134_finidev.  CPU0                  CPU1                      |saa7134_buffer_timeout saa7134_finidev     |   kfree(dev);       |                     |                     | saa7134_buffer_next                     | //use dev  ",
        "func_before": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "func": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\tdel_timer_sync(&dev->video_q.timeout);\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n void saa7134_video_fini(struct saa7134_dev *dev)\n {\n+\tdel_timer_sync(&dev->video_q.timeout);\n \t/* free stuff */\n \tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n \tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tdel_timer_sync(&dev->video_q.timeout);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35824",
        "func_name": "torvalds/linux/dm1105_remove",
        "description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in dm1105_remove in drivers/media/pci/dm1105/dm1105.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5abda7a16698d4d1f47af1168d8fa2c640116b4a",
        "commit_title": "In dm1105_probe, it called dm1105_ir_init and bound",
        "commit_text": "&dm1105->ir.work with dm1105_emit_key. When it handles IRQ request with dm1105_irq, it may call schedule_work to start the work.  When we call dm1105_remove to remove the driver, there may be a sequence as follows:  Fix it by finishing the work before cleanup in dm1105_remove  CPU0                  CPU1                      |dm1105_emit_key dm1105_remove      |   dm1105_ir_exit       |     rc_unregister_device |     rc_free_device  |     rc_dev_release  |     kfree(dev);     |                     |                     | rc_keydown                     |   //use  ",
        "func_before": "static void dm1105_remove(struct pci_dev *pdev)\n{\n\tstruct dm1105_dev *dev = pci_get_drvdata(pdev);\n\tstruct dvb_adapter *dvb_adapter = &dev->dvb_adapter;\n\tstruct dvb_demux *dvbdemux = &dev->demux;\n\tstruct dmx_demux *dmx = &dvbdemux->dmx;\n\n\tdm1105_ir_exit(dev);\n\tdmx->close(dmx);\n\tdvb_net_release(&dev->dvbnet);\n\tif (dev->fe)\n\t\tdvb_unregister_frontend(dev->fe);\n\n\tdmx->disconnect_frontend(dmx);\n\tdmx->remove_frontend(dmx, &dev->mem_frontend);\n\tdmx->remove_frontend(dmx, &dev->hw_frontend);\n\tdvb_dmxdev_release(&dev->dmxdev);\n\tdvb_dmx_release(dvbdemux);\n\tdvb_unregister_adapter(dvb_adapter);\n\ti2c_del_adapter(&dev->i2c_adap);\n\n\tdm1105_hw_exit(dev);\n\tfree_irq(pdev->irq, dev);\n\tpci_iounmap(pdev, dev->io_mem);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tdm1105_devcount--;\n\tkfree(dev);\n}",
        "func": "static void dm1105_remove(struct pci_dev *pdev)\n{\n\tstruct dm1105_dev *dev = pci_get_drvdata(pdev);\n\tstruct dvb_adapter *dvb_adapter = &dev->dvb_adapter;\n\tstruct dvb_demux *dvbdemux = &dev->demux;\n\tstruct dmx_demux *dmx = &dvbdemux->dmx;\n\n\tcancel_work_sync(&dev->ir.work);\n\tdm1105_ir_exit(dev);\n\tdmx->close(dmx);\n\tdvb_net_release(&dev->dvbnet);\n\tif (dev->fe)\n\t\tdvb_unregister_frontend(dev->fe);\n\n\tdmx->disconnect_frontend(dmx);\n\tdmx->remove_frontend(dmx, &dev->mem_frontend);\n\tdmx->remove_frontend(dmx, &dev->hw_frontend);\n\tdvb_dmxdev_release(&dev->dmxdev);\n\tdvb_dmx_release(dvbdemux);\n\tdvb_unregister_adapter(dvb_adapter);\n\ti2c_del_adapter(&dev->i2c_adap);\n\n\tdm1105_hw_exit(dev);\n\tfree_irq(pdev->irq, dev);\n\tpci_iounmap(pdev, dev->io_mem);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tdm1105_devcount--;\n\tkfree(dev);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n \tstruct dvb_demux *dvbdemux = &dev->demux;\n \tstruct dmx_demux *dmx = &dvbdemux->dmx;\n \n+\tcancel_work_sync(&dev->ir.work);\n \tdm1105_ir_exit(dev);\n \tdmx->close(dmx);\n \tdvb_net_release(&dev->dvbnet);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tcancel_work_sync(&dev->ir.work);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35826",
        "func_name": "torvalds/linux/cedrus_remove",
        "description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in cedrus_remove in drivers/staging/media/sunxi/cedrus/cedrus.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=50d0a7aea4809cef87979d4669911276aa23b71f",
        "commit_title": "In cedrus_probe, dev->watchdog_work is bound with cedrus_watchdog function.",
        "commit_text": "In cedrus_device_run, it will started by schedule_delayed_work. If there is an unfinished work in cedrus_remove, there may be a race condition and trigger UAF bug.  CPU0                  CPU1                      |cedrus_watchdog cedrus_remove       |   v4l2_m2m_release  |   kfree(m2m_dev)    |                     |                     | v4l2_m2m_get_curr_priv                     |   m2m_dev //use  Fix it by canceling the worker in cedrus_remove.  ",
        "func_before": "static int cedrus_remove(struct platform_device *pdev)\n{\n\tstruct cedrus_dev *dev = platform_get_drvdata(pdev);\n\n\tif (media_devnode_is_registered(dev->mdev.devnode)) {\n\t\tmedia_device_unregister(&dev->mdev);\n\t\tv4l2_m2m_unregister_media_controller(dev->m2m_dev);\n\t\tmedia_device_cleanup(&dev->mdev);\n\t}\n\n\tv4l2_m2m_release(dev->m2m_dev);\n\tvideo_unregister_device(&dev->vfd);\n\tv4l2_device_unregister(&dev->v4l2_dev);\n\n\tcedrus_hw_remove(dev);\n\n\treturn 0;\n}",
        "func": "static int cedrus_remove(struct platform_device *pdev)\n{\n\tstruct cedrus_dev *dev = platform_get_drvdata(pdev);\n\n\tcancel_delayed_work_sync(&dev->watchdog_work);\n\tif (media_devnode_is_registered(dev->mdev.devnode)) {\n\t\tmedia_device_unregister(&dev->mdev);\n\t\tv4l2_m2m_unregister_media_controller(dev->m2m_dev);\n\t\tmedia_device_cleanup(&dev->mdev);\n\t}\n\n\tv4l2_m2m_release(dev->m2m_dev);\n\tvideo_unregister_device(&dev->vfd);\n\tv4l2_device_unregister(&dev->v4l2_dev);\n\n\tcedrus_hw_remove(dev);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n {\n \tstruct cedrus_dev *dev = platform_get_drvdata(pdev);\n \n+\tcancel_delayed_work_sync(&dev->watchdog_work);\n \tif (media_devnode_is_registered(dev->mdev.devnode)) {\n \t\tmedia_device_unregister(&dev->mdev);\n \t\tv4l2_m2m_unregister_media_controller(dev->m2m_dev);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tcancel_delayed_work_sync(&dev->watchdog_work);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35828",
        "func_name": "torvalds/linux/renesas_usb3_remove",
        "description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in renesas_usb3_remove in drivers/usb/gadget/udc/renesas_usb3.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=2b947f8769be8b8181dc795fd292d3e7120f5204",
        "commit_title": "In renesas_usb3_probe, role_work is bound with renesas_usb3_role_work.",
        "commit_text": "renesas_usb3_start will be called to start the work.  If we remove the driver which will call usbhs_remove, there may be an unfinished work. The possible sequence is as follows:  CPU0                  \t\t\tCPU1                      \t\t\t renesas_usb3_role_work renesas_usb3_remove usb_role_switch_unregister device_unregister kfree(sw) //free usb3->role_sw                     \t\t\t usb_role_switch_set_role                     \t\t\t //use usb3->role_sw  The usb3->role_sw could be freed under such circumstance and then used in usb_role_switch_set_role.  This bug was found by static analysis. And note that removing a driver is a root-only operation, and should never happen in normal case. But the root user may directly remove the device which will also trigger the remove function.  Fix it by canceling the work before cleanup in the renesas_usb3_remove.  Link: https://lore.kernel.org/r/20230320062931.505170-1-zyytlz.wz@163.com ",
        "func_before": "static int renesas_usb3_remove(struct platform_device *pdev)\n{\n\tstruct renesas_usb3 *usb3 = platform_get_drvdata(pdev);\n\n\tdebugfs_remove_recursive(usb3->dentry);\n\tdevice_remove_file(&pdev->dev, &dev_attr_role);\n\n\tusb_role_switch_unregister(usb3->role_sw);\n\n\tusb_del_gadget_udc(&usb3->gadget);\n\treset_control_assert(usb3->usbp_rstc);\n\trenesas_usb3_dma_free_prd(usb3, &pdev->dev);\n\n\t__renesas_usb3_ep_free_request(usb3->ep0_req);\n\tpm_runtime_disable(&pdev->dev);\n\n\treturn 0;\n}",
        "func": "static int renesas_usb3_remove(struct platform_device *pdev)\n{\n\tstruct renesas_usb3 *usb3 = platform_get_drvdata(pdev);\n\n\tdebugfs_remove_recursive(usb3->dentry);\n\tdevice_remove_file(&pdev->dev, &dev_attr_role);\n\n\tcancel_work_sync(&usb3->role_work);\n\tusb_role_switch_unregister(usb3->role_sw);\n\n\tusb_del_gadget_udc(&usb3->gadget);\n\treset_control_assert(usb3->usbp_rstc);\n\trenesas_usb3_dma_free_prd(usb3, &pdev->dev);\n\n\t__renesas_usb3_ep_free_request(usb3->ep0_req);\n\tpm_runtime_disable(&pdev->dev);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n \tdebugfs_remove_recursive(usb3->dentry);\n \tdevice_remove_file(&pdev->dev, &dev_attr_role);\n \n+\tcancel_work_sync(&usb3->role_work);\n \tusb_role_switch_unregister(usb3->role_sw);\n \n \tusb_del_gadget_udc(&usb3->gadget);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tcancel_work_sync(&usb3->role_work);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35829",
        "func_name": "torvalds/linux/rkvdec_remove",
        "description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in rkvdec_remove in drivers/staging/media/rkvdec/rkvdec.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=3228cec23b8b29215e18090c6ba635840190993d",
        "commit_title": "In rkvdec_probe, rkvdec->watchdog_work is bound with",
        "commit_text": "rkvdec_watchdog_func. Then rkvdec_vp9_run may be called to start the work.  If we remove the module which will call rkvdec_remove  to make cleanup, there may be a unfinished work.  The possible sequence is as follows, which will  cause a typical UAF bug.  Fix it by canceling the work before cleanup in rkvdec_remove.  CPU0                  CPU1                      |rkvdec_watchdog_func rkvdec_remove       |  rkvdec_v4l2_cleanup|   v4l2_m2m_release  |     kfree(m2m_dev); |                     |                     | v4l2_m2m_get_curr_priv                     |   m2m_dev->curr_ctx //use  ",
        "func_before": "static int rkvdec_remove(struct platform_device *pdev)\n{\n\tstruct rkvdec_dev *rkvdec = platform_get_drvdata(pdev);\n\n\trkvdec_v4l2_cleanup(rkvdec);\n\tpm_runtime_disable(&pdev->dev);\n\tpm_runtime_dont_use_autosuspend(&pdev->dev);\n\treturn 0;\n}",
        "func": "static int rkvdec_remove(struct platform_device *pdev)\n{\n\tstruct rkvdec_dev *rkvdec = platform_get_drvdata(pdev);\n\n\tcancel_delayed_work_sync(&rkvdec->watchdog_work);\n\n\trkvdec_v4l2_cleanup(rkvdec);\n\tpm_runtime_disable(&pdev->dev);\n\tpm_runtime_dont_use_autosuspend(&pdev->dev);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,8 @@\n static int rkvdec_remove(struct platform_device *pdev)\n {\n \tstruct rkvdec_dev *rkvdec = platform_get_drvdata(pdev);\n+\n+\tcancel_delayed_work_sync(&rkvdec->watchdog_work);\n \n \trkvdec_v4l2_cleanup(rkvdec);\n \tpm_runtime_disable(&pdev->dev);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tcancel_delayed_work_sync(&rkvdec->watchdog_work);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_fifo_is_masked",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "static bool_t evtchn_fifo_is_masked(const struct domain *d, evtchn_port_t port)\n{\n    const event_word_t *word = evtchn_fifo_word_from_port(d, port);\n\n    return !word || guest_test_bit(d, EVTCHN_FIFO_MASKED, word);\n}",
        "func": "static bool_t evtchn_fifo_is_masked(const struct domain *d,\n                                    const struct evtchn *evtchn)\n{\n    const event_word_t *word = evtchn_fifo_word_from_port(d, evtchn->port);\n\n    return !word || guest_test_bit(d, EVTCHN_FIFO_MASKED, word);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n-static bool_t evtchn_fifo_is_masked(const struct domain *d, evtchn_port_t port)\n+static bool_t evtchn_fifo_is_masked(const struct domain *d,\n+                                    const struct evtchn *evtchn)\n {\n-    const event_word_t *word = evtchn_fifo_word_from_port(d, port);\n+    const event_word_t *word = evtchn_fifo_word_from_port(d, evtchn->port);\n \n     return !word || guest_test_bit(d, EVTCHN_FIFO_MASKED, word);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static bool_t evtchn_fifo_is_masked(const struct domain *d, evtchn_port_t port)",
                "    const event_word_t *word = evtchn_fifo_word_from_port(d, port);"
            ],
            "added_lines": [
                "static bool_t evtchn_fifo_is_masked(const struct domain *d,",
                "                                    const struct evtchn *evtchn)",
                "    const event_word_t *word = evtchn_fifo_word_from_port(d, evtchn->port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_fifo_is_busy",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "static bool_t evtchn_fifo_is_busy(const struct domain *d, evtchn_port_t port)\n{\n    const event_word_t *word = evtchn_fifo_word_from_port(d, port);\n\n    return word && guest_test_bit(d, EVTCHN_FIFO_LINKED, word);\n}",
        "func": "static bool_t evtchn_fifo_is_busy(const struct domain *d,\n                                  const struct evtchn *evtchn)\n{\n    const event_word_t *word = evtchn_fifo_word_from_port(d, evtchn->port);\n\n    return word && guest_test_bit(d, EVTCHN_FIFO_LINKED, word);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n-static bool_t evtchn_fifo_is_busy(const struct domain *d, evtchn_port_t port)\n+static bool_t evtchn_fifo_is_busy(const struct domain *d,\n+                                  const struct evtchn *evtchn)\n {\n-    const event_word_t *word = evtchn_fifo_word_from_port(d, port);\n+    const event_word_t *word = evtchn_fifo_word_from_port(d, evtchn->port);\n \n     return word && guest_test_bit(d, EVTCHN_FIFO_LINKED, word);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static bool_t evtchn_fifo_is_busy(const struct domain *d, evtchn_port_t port)",
                "    const event_word_t *word = evtchn_fifo_word_from_port(d, port);"
            ],
            "added_lines": [
                "static bool_t evtchn_fifo_is_busy(const struct domain *d,",
                "                                  const struct evtchn *evtchn)",
                "    const event_word_t *word = evtchn_fifo_word_from_port(d, evtchn->port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_fifo_is_pending",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "static bool evtchn_fifo_is_pending(const struct domain *d, evtchn_port_t port)\n{\n    const event_word_t *word = evtchn_fifo_word_from_port(d, port);\n\n    return word && guest_test_bit(d, EVTCHN_FIFO_PENDING, word);\n}",
        "func": "static bool evtchn_fifo_is_pending(const struct domain *d,\n                                   const struct evtchn *evtchn)\n{\n    const event_word_t *word = evtchn_fifo_word_from_port(d, evtchn->port);\n\n    return word && guest_test_bit(d, EVTCHN_FIFO_PENDING, word);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n-static bool evtchn_fifo_is_pending(const struct domain *d, evtchn_port_t port)\n+static bool evtchn_fifo_is_pending(const struct domain *d,\n+                                   const struct evtchn *evtchn)\n {\n-    const event_word_t *word = evtchn_fifo_word_from_port(d, port);\n+    const event_word_t *word = evtchn_fifo_word_from_port(d, evtchn->port);\n \n     return word && guest_test_bit(d, EVTCHN_FIFO_PENDING, word);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static bool evtchn_fifo_is_pending(const struct domain *d, evtchn_port_t port)",
                "    const event_word_t *word = evtchn_fifo_word_from_port(d, port);"
            ],
            "added_lines": [
                "static bool evtchn_fifo_is_pending(const struct domain *d,",
                "                                   const struct evtchn *evtchn)",
                "    const event_word_t *word = evtchn_fifo_word_from_port(d, evtchn->port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/pv_shim_inject_evtchn",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "void pv_shim_inject_evtchn(unsigned int port)\n{\n    if ( port_is_valid(guest, port) )\n    {\n        struct evtchn *chn = evtchn_from_port(guest, port);\n\n        evtchn_port_set_pending(guest, chn->notify_vcpu_id, chn);\n    }\n}",
        "func": "void pv_shim_inject_evtchn(unsigned int port)\n{\n    if ( port_is_valid(guest, port) )\n    {\n        struct evtchn *chn = evtchn_from_port(guest, port);\n        unsigned long flags;\n\n        spin_lock_irqsave(&chn->lock, flags);\n        evtchn_port_set_pending(guest, chn->notify_vcpu_id, chn);\n        spin_unlock_irqrestore(&chn->lock, flags);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,10 @@\n     if ( port_is_valid(guest, port) )\n     {\n         struct evtchn *chn = evtchn_from_port(guest, port);\n+        unsigned long flags;\n \n+        spin_lock_irqsave(&chn->lock, flags);\n         evtchn_port_set_pending(guest, chn->notify_vcpu_id, chn);\n+        spin_unlock_irqrestore(&chn->lock, flags);\n     }\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        unsigned long flags;",
                "        spin_lock_irqsave(&chn->lock, flags);",
                "        spin_unlock_irqrestore(&chn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/dump_irqs",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "static void dump_irqs(unsigned char key)\n{\n    int i, irq, pirq;\n    struct irq_desc *desc;\n    irq_guest_action_t *action;\n    struct domain *d;\n    const struct pirq *info;\n    unsigned long flags;\n    char *ssid;\n\n    printk(\"IRQ information:\\n\");\n\n    for ( irq = 0; irq < nr_irqs; irq++ )\n    {\n        if ( !(irq & 0x1f) )\n            process_pending_softirqs();\n\n        desc = irq_to_desc(irq);\n\n        if ( !irq_desc_initialized(desc) || desc->handler == &no_irq_type )\n            continue;\n\n        ssid = in_irq() ? NULL : xsm_show_irq_sid(irq);\n\n        spin_lock_irqsave(&desc->lock, flags);\n\n        printk(\"   IRQ:%4d vec:%02x %-15s status=%03x aff:{%*pbl}/{%*pbl} \",\n               irq, desc->arch.vector, desc->handler->typename, desc->status,\n               CPUMASK_PR(desc->affinity), CPUMASK_PR(desc->arch.cpu_mask));\n\n        if ( ssid )\n            printk(\"Z=%-25s \", ssid);\n\n        if ( desc->status & IRQ_GUEST )\n        {\n            action = (irq_guest_action_t *)desc->action;\n\n            printk(\"in-flight=%d%c\",\n                   action->in_flight, action->nr_guests ? ' ' : '\\n');\n\n            for ( i = 0; i < action->nr_guests; )\n            {\n                d = action->guest[i++];\n                pirq = domain_irq_to_pirq(d, irq);\n                info = pirq_info(d, pirq);\n                printk(\"d%d:%3d(%c%c%c)%c\",\n                       d->domain_id, pirq,\n                       evtchn_port_is_pending(d, info->evtchn) ? 'P' : '-',\n                       evtchn_port_is_masked(d, info->evtchn) ? 'M' : '-',\n                       info->masked ? 'M' : '-',\n                       i < action->nr_guests ? ',' : '\\n');\n            }\n        }\n        else if ( desc->action )\n            printk(\"%ps()\\n\", desc->action->handler);\n        else\n            printk(\"mapped, unbound\\n\");\n\n        spin_unlock_irqrestore(&desc->lock, flags);\n\n        xfree(ssid);\n    }\n\n    process_pending_softirqs();\n    printk(\"Direct vector information:\\n\");\n    for ( i = FIRST_DYNAMIC_VECTOR; i < X86_NR_VECTORS; ++i )\n        if ( direct_apic_vector[i] )\n            printk(\"   %#02x -> %ps()\\n\", i, direct_apic_vector[i]);\n\n    dump_ioapic_irq_info();\n}",
        "func": "static void dump_irqs(unsigned char key)\n{\n    int i, irq, pirq;\n    struct irq_desc *desc;\n    irq_guest_action_t *action;\n    struct domain *d;\n    const struct pirq *info;\n    unsigned long flags;\n    char *ssid;\n\n    printk(\"IRQ information:\\n\");\n\n    for ( irq = 0; irq < nr_irqs; irq++ )\n    {\n        if ( !(irq & 0x1f) )\n            process_pending_softirqs();\n\n        desc = irq_to_desc(irq);\n\n        if ( !irq_desc_initialized(desc) || desc->handler == &no_irq_type )\n            continue;\n\n        ssid = in_irq() ? NULL : xsm_show_irq_sid(irq);\n\n        spin_lock_irqsave(&desc->lock, flags);\n\n        printk(\"   IRQ:%4d vec:%02x %-15s status=%03x aff:{%*pbl}/{%*pbl} \",\n               irq, desc->arch.vector, desc->handler->typename, desc->status,\n               CPUMASK_PR(desc->affinity), CPUMASK_PR(desc->arch.cpu_mask));\n\n        if ( ssid )\n            printk(\"Z=%-25s \", ssid);\n\n        if ( desc->status & IRQ_GUEST )\n        {\n            action = (irq_guest_action_t *)desc->action;\n\n            printk(\"in-flight=%d%c\",\n                   action->in_flight, action->nr_guests ? ' ' : '\\n');\n\n            for ( i = 0; i < action->nr_guests; )\n            {\n                struct evtchn *evtchn;\n                unsigned int pending = 2, masked = 2;\n\n                d = action->guest[i++];\n                pirq = domain_irq_to_pirq(d, irq);\n                info = pirq_info(d, pirq);\n                evtchn = evtchn_from_port(d, info->evtchn);\n                local_irq_disable();\n                if ( spin_trylock(&evtchn->lock) )\n                {\n                    pending = evtchn_is_pending(d, evtchn);\n                    masked = evtchn_is_masked(d, evtchn);\n                    spin_unlock(&evtchn->lock);\n                }\n                local_irq_enable();\n                printk(\"d%d:%3d(%c%c%c)%c\",\n                       d->domain_id, pirq, \"-P?\"[pending],\n                       \"-M?\"[masked], info->masked ? 'M' : '-',\n                       i < action->nr_guests ? ',' : '\\n');\n            }\n        }\n        else if ( desc->action )\n            printk(\"%ps()\\n\", desc->action->handler);\n        else\n            printk(\"mapped, unbound\\n\");\n\n        spin_unlock_irqrestore(&desc->lock, flags);\n\n        xfree(ssid);\n    }\n\n    process_pending_softirqs();\n    printk(\"Direct vector information:\\n\");\n    for ( i = FIRST_DYNAMIC_VECTOR; i < X86_NR_VECTORS; ++i )\n        if ( direct_apic_vector[i] )\n            printk(\"   %#02x -> %ps()\\n\", i, direct_apic_vector[i]);\n\n    dump_ioapic_irq_info();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,14 +40,24 @@\n \n             for ( i = 0; i < action->nr_guests; )\n             {\n+                struct evtchn *evtchn;\n+                unsigned int pending = 2, masked = 2;\n+\n                 d = action->guest[i++];\n                 pirq = domain_irq_to_pirq(d, irq);\n                 info = pirq_info(d, pirq);\n+                evtchn = evtchn_from_port(d, info->evtchn);\n+                local_irq_disable();\n+                if ( spin_trylock(&evtchn->lock) )\n+                {\n+                    pending = evtchn_is_pending(d, evtchn);\n+                    masked = evtchn_is_masked(d, evtchn);\n+                    spin_unlock(&evtchn->lock);\n+                }\n+                local_irq_enable();\n                 printk(\"d%d:%3d(%c%c%c)%c\",\n-                       d->domain_id, pirq,\n-                       evtchn_port_is_pending(d, info->evtchn) ? 'P' : '-',\n-                       evtchn_port_is_masked(d, info->evtchn) ? 'M' : '-',\n-                       info->masked ? 'M' : '-',\n+                       d->domain_id, pirq, \"-P?\"[pending],\n+                       \"-M?\"[masked], info->masked ? 'M' : '-',\n                        i < action->nr_guests ? ',' : '\\n');\n             }\n         }",
        "diff_line_info": {
            "deleted_lines": [
                "                       d->domain_id, pirq,",
                "                       evtchn_port_is_pending(d, info->evtchn) ? 'P' : '-',",
                "                       evtchn_port_is_masked(d, info->evtchn) ? 'M' : '-',",
                "                       info->masked ? 'M' : '-',"
            ],
            "added_lines": [
                "                struct evtchn *evtchn;",
                "                unsigned int pending = 2, masked = 2;",
                "",
                "                evtchn = evtchn_from_port(d, info->evtchn);",
                "                local_irq_disable();",
                "                if ( spin_trylock(&evtchn->lock) )",
                "                {",
                "                    pending = evtchn_is_pending(d, evtchn);",
                "                    masked = evtchn_is_masked(d, evtchn);",
                "                    spin_unlock(&evtchn->lock);",
                "                }",
                "                local_irq_enable();",
                "                       d->domain_id, pirq, \"-P?\"[pending],",
                "                       \"-M?\"[masked], info->masked ? 'M' : '-',"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/send_guest_pirq",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "void send_guest_pirq(struct domain *d, const struct pirq *pirq)\n{\n    int port;\n    struct evtchn *chn;\n\n    /*\n     * PV guests: It should not be possible to race with __evtchn_close(). The\n     *     caller of this function must synchronise with pirq_guest_unbind().\n     * HVM guests: Port is legitimately zero when the guest disables the\n     *     emulated interrupt/evtchn.\n     */\n    if ( pirq == NULL || (port = pirq->evtchn) == 0 )\n    {\n        BUG_ON(!is_hvm_domain(d));\n        return;\n    }\n\n    chn = evtchn_from_port(d, port);\n    evtchn_port_set_pending(d, chn->notify_vcpu_id, chn);\n}",
        "func": "void send_guest_pirq(struct domain *d, const struct pirq *pirq)\n{\n    int port;\n    struct evtchn *chn;\n    unsigned long flags;\n\n    /*\n     * PV guests: It should not be possible to race with __evtchn_close(). The\n     *     caller of this function must synchronise with pirq_guest_unbind().\n     * HVM guests: Port is legitimately zero when the guest disables the\n     *     emulated interrupt/evtchn.\n     */\n    if ( pirq == NULL || (port = pirq->evtchn) == 0 )\n    {\n        BUG_ON(!is_hvm_domain(d));\n        return;\n    }\n\n    chn = evtchn_from_port(d, port);\n    spin_lock_irqsave(&chn->lock, flags);\n    evtchn_port_set_pending(d, chn->notify_vcpu_id, chn);\n    spin_unlock_irqrestore(&chn->lock, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n {\n     int port;\n     struct evtchn *chn;\n+    unsigned long flags;\n \n     /*\n      * PV guests: It should not be possible to race with __evtchn_close(). The\n@@ -16,5 +17,7 @@\n     }\n \n     chn = evtchn_from_port(d, port);\n+    spin_lock_irqsave(&chn->lock, flags);\n     evtchn_port_set_pending(d, chn->notify_vcpu_id, chn);\n+    spin_unlock_irqrestore(&chn->lock, flags);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    unsigned long flags;",
                "    spin_lock_irqsave(&chn->lock, flags);",
                "    spin_unlock_irqrestore(&chn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/domain_dump_evtchn_info",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "static void domain_dump_evtchn_info(struct domain *d)\n{\n    unsigned int port;\n    int irq;\n\n    printk(\"Event channel information for domain %d:\\n\"\n           \"Polling vCPUs: {%*pbl}\\n\"\n           \"    port [p/m/s]\\n\", d->domain_id, d->max_vcpus, d->poll_mask);\n\n    spin_lock(&d->event_lock);\n\n    for ( port = 1; port_is_valid(d, port); ++port )\n    {\n        const struct evtchn *chn;\n        char *ssid;\n\n        chn = evtchn_from_port(d, port);\n        if ( chn->state == ECS_FREE )\n            continue;\n\n        printk(\"    %4u [%d/%d/\",\n               port,\n               evtchn_port_is_pending(d, port),\n               evtchn_port_is_masked(d, port));\n        evtchn_port_print_state(d, chn);\n        printk(\"]: s=%d n=%d x=%d\",\n               chn->state, chn->notify_vcpu_id, chn->xen_consumer);\n\n        switch ( chn->state )\n        {\n        case ECS_UNBOUND:\n            printk(\" d=%d\", chn->u.unbound.remote_domid);\n            break;\n        case ECS_INTERDOMAIN:\n            printk(\" d=%d p=%d\",\n                   chn->u.interdomain.remote_dom->domain_id,\n                   chn->u.interdomain.remote_port);\n            break;\n        case ECS_PIRQ:\n            irq = domain_pirq_to_irq(d, chn->u.pirq.irq);\n            printk(\" p=%d i=%d\", chn->u.pirq.irq, irq);\n            break;\n        case ECS_VIRQ:\n            printk(\" v=%d\", chn->u.virq);\n            break;\n        }\n\n        ssid = xsm_show_security_evtchn(d, chn);\n        if (ssid) {\n            printk(\" Z=%s\\n\", ssid);\n            xfree(ssid);\n        } else {\n            printk(\"\\n\");\n        }\n    }\n\n    spin_unlock(&d->event_lock);\n}",
        "func": "static void domain_dump_evtchn_info(struct domain *d)\n{\n    unsigned int port;\n    int irq;\n\n    printk(\"Event channel information for domain %d:\\n\"\n           \"Polling vCPUs: {%*pbl}\\n\"\n           \"    port [p/m/s]\\n\", d->domain_id, d->max_vcpus, d->poll_mask);\n\n    spin_lock(&d->event_lock);\n\n    for ( port = 1; port_is_valid(d, port); ++port )\n    {\n        const struct evtchn *chn;\n        char *ssid;\n\n        chn = evtchn_from_port(d, port);\n        if ( chn->state == ECS_FREE )\n            continue;\n\n        printk(\"    %4u [%d/%d/\",\n               port,\n               evtchn_is_pending(d, chn),\n               evtchn_is_masked(d, chn));\n        evtchn_port_print_state(d, chn);\n        printk(\"]: s=%d n=%d x=%d\",\n               chn->state, chn->notify_vcpu_id, chn->xen_consumer);\n\n        switch ( chn->state )\n        {\n        case ECS_UNBOUND:\n            printk(\" d=%d\", chn->u.unbound.remote_domid);\n            break;\n        case ECS_INTERDOMAIN:\n            printk(\" d=%d p=%d\",\n                   chn->u.interdomain.remote_dom->domain_id,\n                   chn->u.interdomain.remote_port);\n            break;\n        case ECS_PIRQ:\n            irq = domain_pirq_to_irq(d, chn->u.pirq.irq);\n            printk(\" p=%d i=%d\", chn->u.pirq.irq, irq);\n            break;\n        case ECS_VIRQ:\n            printk(\" v=%d\", chn->u.virq);\n            break;\n        }\n\n        ssid = xsm_show_security_evtchn(d, chn);\n        if (ssid) {\n            printk(\" Z=%s\\n\", ssid);\n            xfree(ssid);\n        } else {\n            printk(\"\\n\");\n        }\n    }\n\n    spin_unlock(&d->event_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,8 +20,8 @@\n \n         printk(\"    %4u [%d/%d/\",\n                port,\n-               evtchn_port_is_pending(d, port),\n-               evtchn_port_is_masked(d, port));\n+               evtchn_is_pending(d, chn),\n+               evtchn_is_masked(d, chn));\n         evtchn_port_print_state(d, chn);\n         printk(\"]: s=%d n=%d x=%d\",\n                chn->state, chn->notify_vcpu_id, chn->xen_consumer);",
        "diff_line_info": {
            "deleted_lines": [
                "               evtchn_port_is_pending(d, port),",
                "               evtchn_port_is_masked(d, port));"
            ],
            "added_lines": [
                "               evtchn_is_pending(d, chn),",
                "               evtchn_is_masked(d, chn));"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_allocate_port",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "int evtchn_allocate_port(struct domain *d, evtchn_port_t port)\n{\n    if ( port > d->max_evtchn_port || port >= max_evtchns(d) )\n        return -ENOSPC;\n\n    if ( port_is_valid(d, port) )\n    {\n        if ( evtchn_from_port(d, port)->state != ECS_FREE ||\n             evtchn_port_is_busy(d, port) )\n            return -EBUSY;\n    }\n    else\n    {\n        struct evtchn *chn;\n        struct evtchn **grp;\n\n        if ( !group_from_port(d, port) )\n        {\n            grp = xzalloc_array(struct evtchn *, BUCKETS_PER_GROUP);\n            if ( !grp )\n                return -ENOMEM;\n            group_from_port(d, port) = grp;\n        }\n\n        chn = alloc_evtchn_bucket(d, port);\n        if ( !chn )\n            return -ENOMEM;\n        bucket_from_port(d, port) = chn;\n\n        /*\n         * d->valid_evtchns is used to check whether the bucket can be\n         * accessed without the per-domain lock. Therefore,\n         * d->valid_evtchns should be seen *after* the new bucket has\n         * been setup.\n         */\n        smp_wmb();\n        write_atomic(&d->valid_evtchns, d->valid_evtchns + EVTCHNS_PER_BUCKET);\n    }\n\n    write_atomic(&d->active_evtchns, d->active_evtchns + 1);\n\n    return 0;\n}",
        "func": "int evtchn_allocate_port(struct domain *d, evtchn_port_t port)\n{\n    if ( port > d->max_evtchn_port || port >= max_evtchns(d) )\n        return -ENOSPC;\n\n    if ( port_is_valid(d, port) )\n    {\n        const struct evtchn *chn = evtchn_from_port(d, port);\n\n        if ( chn->state != ECS_FREE || evtchn_is_busy(d, chn) )\n            return -EBUSY;\n    }\n    else\n    {\n        struct evtchn *chn;\n        struct evtchn **grp;\n\n        if ( !group_from_port(d, port) )\n        {\n            grp = xzalloc_array(struct evtchn *, BUCKETS_PER_GROUP);\n            if ( !grp )\n                return -ENOMEM;\n            group_from_port(d, port) = grp;\n        }\n\n        chn = alloc_evtchn_bucket(d, port);\n        if ( !chn )\n            return -ENOMEM;\n        bucket_from_port(d, port) = chn;\n\n        /*\n         * d->valid_evtchns is used to check whether the bucket can be\n         * accessed without the per-domain lock. Therefore,\n         * d->valid_evtchns should be seen *after* the new bucket has\n         * been setup.\n         */\n        smp_wmb();\n        write_atomic(&d->valid_evtchns, d->valid_evtchns + EVTCHNS_PER_BUCKET);\n    }\n\n    write_atomic(&d->active_evtchns, d->active_evtchns + 1);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,8 +5,9 @@\n \n     if ( port_is_valid(d, port) )\n     {\n-        if ( evtchn_from_port(d, port)->state != ECS_FREE ||\n-             evtchn_port_is_busy(d, port) )\n+        const struct evtchn *chn = evtchn_from_port(d, port);\n+\n+        if ( chn->state != ECS_FREE || evtchn_is_busy(d, chn) )\n             return -EBUSY;\n     }\n     else",
        "diff_line_info": {
            "deleted_lines": [
                "        if ( evtchn_from_port(d, port)->state != ECS_FREE ||",
                "             evtchn_port_is_busy(d, port) )"
            ],
            "added_lines": [
                "        const struct evtchn *chn = evtchn_from_port(d, port);",
                "",
                "        if ( chn->state != ECS_FREE || evtchn_is_busy(d, chn) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/send_guest_global_virq",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "void send_guest_global_virq(struct domain *d, uint32_t virq)\n{\n    unsigned long flags;\n    int port;\n    struct vcpu *v;\n    struct evtchn *chn;\n\n    ASSERT(virq_is_global(virq));\n\n    if ( unlikely(d == NULL) || unlikely(d->vcpu == NULL) )\n        return;\n\n    v = d->vcpu[0];\n    if ( unlikely(v == NULL) )\n        return;\n\n    spin_lock_irqsave(&v->virq_lock, flags);\n\n    port = v->virq_to_evtchn[virq];\n    if ( unlikely(port == 0) )\n        goto out;\n\n    chn = evtchn_from_port(d, port);\n    evtchn_port_set_pending(d, chn->notify_vcpu_id, chn);\n\n out:\n    spin_unlock_irqrestore(&v->virq_lock, flags);\n}",
        "func": "void send_guest_global_virq(struct domain *d, uint32_t virq)\n{\n    unsigned long flags;\n    int port;\n    struct vcpu *v;\n    struct evtchn *chn;\n\n    ASSERT(virq_is_global(virq));\n\n    if ( unlikely(d == NULL) || unlikely(d->vcpu == NULL) )\n        return;\n\n    v = d->vcpu[0];\n    if ( unlikely(v == NULL) )\n        return;\n\n    spin_lock_irqsave(&v->virq_lock, flags);\n\n    port = v->virq_to_evtchn[virq];\n    if ( unlikely(port == 0) )\n        goto out;\n\n    chn = evtchn_from_port(d, port);\n    spin_lock(&chn->lock);\n    evtchn_port_set_pending(d, chn->notify_vcpu_id, chn);\n    spin_unlock(&chn->lock);\n\n out:\n    spin_unlock_irqrestore(&v->virq_lock, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,7 +21,9 @@\n         goto out;\n \n     chn = evtchn_from_port(d, port);\n+    spin_lock(&chn->lock);\n     evtchn_port_set_pending(d, chn->notify_vcpu_id, chn);\n+    spin_unlock(&chn->lock);\n \n  out:\n     spin_unlock_irqrestore(&v->virq_lock, flags);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    spin_lock(&chn->lock);",
                "    spin_unlock(&chn->lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/send_guest_vcpu_virq",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "void send_guest_vcpu_virq(struct vcpu *v, uint32_t virq)\n{\n    unsigned long flags;\n    int port;\n    struct domain *d;\n\n    ASSERT(!virq_is_global(virq));\n\n    spin_lock_irqsave(&v->virq_lock, flags);\n\n    port = v->virq_to_evtchn[virq];\n    if ( unlikely(port == 0) )\n        goto out;\n\n    d = v->domain;\n    evtchn_port_set_pending(d, v->vcpu_id, evtchn_from_port(d, port));\n\n out:\n    spin_unlock_irqrestore(&v->virq_lock, flags);\n}",
        "func": "void send_guest_vcpu_virq(struct vcpu *v, uint32_t virq)\n{\n    unsigned long flags;\n    int port;\n    struct domain *d;\n    struct evtchn *chn;\n\n    ASSERT(!virq_is_global(virq));\n\n    spin_lock_irqsave(&v->virq_lock, flags);\n\n    port = v->virq_to_evtchn[virq];\n    if ( unlikely(port == 0) )\n        goto out;\n\n    d = v->domain;\n    chn = evtchn_from_port(d, port);\n    spin_lock(&chn->lock);\n    evtchn_port_set_pending(d, v->vcpu_id, chn);\n    spin_unlock(&chn->lock);\n\n out:\n    spin_unlock_irqrestore(&v->virq_lock, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,7 @@\n     unsigned long flags;\n     int port;\n     struct domain *d;\n+    struct evtchn *chn;\n \n     ASSERT(!virq_is_global(virq));\n \n@@ -13,7 +14,10 @@\n         goto out;\n \n     d = v->domain;\n-    evtchn_port_set_pending(d, v->vcpu_id, evtchn_from_port(d, port));\n+    chn = evtchn_from_port(d, port);\n+    spin_lock(&chn->lock);\n+    evtchn_port_set_pending(d, v->vcpu_id, chn);\n+    spin_unlock(&chn->lock);\n \n  out:\n     spin_unlock_irqrestore(&v->virq_lock, flags);",
        "diff_line_info": {
            "deleted_lines": [
                "    evtchn_port_set_pending(d, v->vcpu_id, evtchn_from_port(d, port));"
            ],
            "added_lines": [
                "    struct evtchn *chn;",
                "    chn = evtchn_from_port(d, port);",
                "    spin_lock(&chn->lock);",
                "    evtchn_port_set_pending(d, v->vcpu_id, chn);",
                "    spin_unlock(&chn->lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_unmask",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "int evtchn_unmask(unsigned int port)\n{\n    struct domain *d = current->domain;\n    struct evtchn *evtchn;\n\n    if ( unlikely(!port_is_valid(d, port)) )\n        return -EINVAL;\n\n    evtchn = evtchn_from_port(d, port);\n    evtchn_port_unmask(d, evtchn);\n\n    return 0;\n}",
        "func": "int evtchn_unmask(unsigned int port)\n{\n    struct domain *d = current->domain;\n    struct evtchn *evtchn;\n    unsigned long flags;\n\n    if ( unlikely(!port_is_valid(d, port)) )\n        return -EINVAL;\n\n    evtchn = evtchn_from_port(d, port);\n    spin_lock_irqsave(&evtchn->lock, flags);\n    evtchn_port_unmask(d, evtchn);\n    spin_unlock_irqrestore(&evtchn->lock, flags);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,12 +2,15 @@\n {\n     struct domain *d = current->domain;\n     struct evtchn *evtchn;\n+    unsigned long flags;\n \n     if ( unlikely(!port_is_valid(d, port)) )\n         return -EINVAL;\n \n     evtchn = evtchn_from_port(d, port);\n+    spin_lock_irqsave(&evtchn->lock, flags);\n     evtchn_port_unmask(d, evtchn);\n+    spin_unlock_irqrestore(&evtchn->lock, flags);\n \n     return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    unsigned long flags;",
                "    spin_lock_irqsave(&evtchn->lock, flags);",
                "    spin_unlock_irqrestore(&evtchn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_2l_is_masked",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "static bool evtchn_2l_is_masked(const struct domain *d, evtchn_port_t port)\n{\n    unsigned int max_ports = BITS_PER_EVTCHN_WORD(d) * BITS_PER_EVTCHN_WORD(d);\n\n    ASSERT(port < max_ports);\n    return (port >= max_ports ||\n            guest_test_bit(d, port, &shared_info(d, evtchn_mask)));\n}",
        "func": "static bool evtchn_2l_is_masked(const struct domain *d,\n                                const struct evtchn *evtchn)\n{\n    evtchn_port_t port = evtchn->port;\n    unsigned int max_ports = BITS_PER_EVTCHN_WORD(d) * BITS_PER_EVTCHN_WORD(d);\n\n    ASSERT(port < max_ports);\n    return (port >= max_ports ||\n            guest_test_bit(d, port, &shared_info(d, evtchn_mask)));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,7 @@\n-static bool evtchn_2l_is_masked(const struct domain *d, evtchn_port_t port)\n+static bool evtchn_2l_is_masked(const struct domain *d,\n+                                const struct evtchn *evtchn)\n {\n+    evtchn_port_t port = evtchn->port;\n     unsigned int max_ports = BITS_PER_EVTCHN_WORD(d) * BITS_PER_EVTCHN_WORD(d);\n \n     ASSERT(port < max_ports);",
        "diff_line_info": {
            "deleted_lines": [
                "static bool evtchn_2l_is_masked(const struct domain *d, evtchn_port_t port)"
            ],
            "added_lines": [
                "static bool evtchn_2l_is_masked(const struct domain *d,",
                "                                const struct evtchn *evtchn)",
                "    evtchn_port_t port = evtchn->port;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_2l_is_pending",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/e045199c7c9c5433d7f1461a741ed539a75cbfad",
        "commit_title": "evtchn: address races with evtchn_reset()",
        "commit_text": " Neither d->evtchn_port_ops nor max_evtchns(d) may be used in an entirely lock-less manner, as both may change by a racing evtchn_reset(). In the common case, at least one of the domain's event lock or the per-channel lock needs to be held. In the specific case of the inter-domain sending by evtchn_send() and notify_via_xen_event_channel() holding the other side's per-channel lock is sufficient, as the channel can't change state without both per-channel locks held. Without such a channel changing state, evtchn_reset() can't complete successfully.  Lock-free accesses continue to be permitted for the shim (calling some otherwise internal event channel functions), as this happens while the domain is in effectively single-threaded mode. Special care also needs taking for the shim's marking of in-use ports as ECS_RESERVED (allowing use of such ports in the shim case is okay because switching into and hence also out of FIFO mode is impossihble there).  As a side effect, certain operations on Xen bound event channels which were mistakenly permitted so far (e.g. unmask or poll) will be refused now.  This is part of XSA-343. ",
        "func_before": "static bool evtchn_2l_is_pending(const struct domain *d, evtchn_port_t port)\n{\n    unsigned int max_ports = BITS_PER_EVTCHN_WORD(d) * BITS_PER_EVTCHN_WORD(d);\n\n    ASSERT(port < max_ports);\n    return (port < max_ports &&\n            guest_test_bit(d, port, &shared_info(d, evtchn_pending)));\n}",
        "func": "static bool evtchn_2l_is_pending(const struct domain *d,\n                                 const struct evtchn *evtchn)\n{\n    evtchn_port_t port = evtchn->port;\n    unsigned int max_ports = BITS_PER_EVTCHN_WORD(d) * BITS_PER_EVTCHN_WORD(d);\n\n    ASSERT(port < max_ports);\n    return (port < max_ports &&\n            guest_test_bit(d, port, &shared_info(d, evtchn_pending)));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,7 @@\n-static bool evtchn_2l_is_pending(const struct domain *d, evtchn_port_t port)\n+static bool evtchn_2l_is_pending(const struct domain *d,\n+                                 const struct evtchn *evtchn)\n {\n+    evtchn_port_t port = evtchn->port;\n     unsigned int max_ports = BITS_PER_EVTCHN_WORD(d) * BITS_PER_EVTCHN_WORD(d);\n \n     ASSERT(port < max_ports);",
        "diff_line_info": {
            "deleted_lines": [
                "static bool evtchn_2l_is_pending(const struct domain *d, evtchn_port_t port)"
            ],
            "added_lines": [
                "static bool evtchn_2l_is_pending(const struct domain *d,",
                "                                 const struct evtchn *evtchn)",
                "    evtchn_port_t port = evtchn->port;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_bind_pirq",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/8d385b247bca40ece40c9279391054bc98934325",
        "commit_title": "evtchn: evtchn_reset() shouldn't succeed with still-open ports",
        "commit_text": " While the function closes all ports, it does so without holding any lock, and hence racing requests may be issued causing new ports to get opened. This would have been problematic in particular if such a newly opened port had a port number above the new implementation limit (i.e. when switching from FIFO to 2-level) after the reset, as prior to \"evtchn: relax port_is_valid()\" this could have led to e.g. evtchn_close()'s \"BUG_ON(!port_is_valid(d2, port2))\" to trigger.  Introduce a counter of active ports and check that it's (still) no larger then the number of Xen internally used ones after obtaining the necessary lock in evtchn_reset().  As to the access model of the new {active,xen}_evtchns fields - while all writes get done using write_atomic(), reads ought to use read_atomic() only when outside of a suitably locked region.  Note that as of now evtchn_bind_virq() and evtchn_bind_ipi() don't have a need to call check_free_port().  This is part of XSA-343. ",
        "func_before": "static long evtchn_bind_pirq(evtchn_bind_pirq_t *bind)\n{\n    struct evtchn *chn;\n    struct domain *d = current->domain;\n    struct vcpu   *v = d->vcpu[0];\n    struct pirq   *info;\n    int            port, pirq = bind->pirq;\n    long           rc;\n\n    if ( (pirq < 0) || (pirq >= d->nr_pirqs) )\n        return -EINVAL;\n\n    if ( !is_hvm_domain(d) && !pirq_access_permitted(d, pirq) )\n        return -EPERM;\n\n    spin_lock(&d->event_lock);\n\n    if ( pirq_to_evtchn(d, pirq) != 0 )\n        ERROR_EXIT(-EEXIST);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT(port);\n\n    chn = evtchn_from_port(d, port);\n\n    info = pirq_get_info(d, pirq);\n    if ( !info )\n        ERROR_EXIT(-ENOMEM);\n    info->evtchn = port;\n    rc = (!is_hvm_domain(d)\n          ? pirq_guest_bind(v, info,\n                            !!(bind->flags & BIND_PIRQ__WILL_SHARE))\n          : 0);\n    if ( rc != 0 )\n    {\n        info->evtchn = 0;\n        pirq_cleanup_check(info, d);\n        goto out;\n    }\n\n    spin_lock(&chn->lock);\n\n    chn->state  = ECS_PIRQ;\n    chn->u.pirq.irq = pirq;\n    link_pirq_port(port, chn, v);\n    evtchn_port_init(d, chn);\n\n    spin_unlock(&chn->lock);\n\n    bind->port = port;\n\n    arch_evtchn_bind_pirq(d, pirq);\n\n out:\n    spin_unlock(&d->event_lock);\n\n    return rc;\n}",
        "func": "static long evtchn_bind_pirq(evtchn_bind_pirq_t *bind)\n{\n    struct evtchn *chn;\n    struct domain *d = current->domain;\n    struct vcpu   *v = d->vcpu[0];\n    struct pirq   *info;\n    int            port = 0, pirq = bind->pirq;\n    long           rc;\n\n    if ( (pirq < 0) || (pirq >= d->nr_pirqs) )\n        return -EINVAL;\n\n    if ( !is_hvm_domain(d) && !pirq_access_permitted(d, pirq) )\n        return -EPERM;\n\n    spin_lock(&d->event_lock);\n\n    if ( pirq_to_evtchn(d, pirq) != 0 )\n        ERROR_EXIT(-EEXIST);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT(port);\n\n    chn = evtchn_from_port(d, port);\n\n    info = pirq_get_info(d, pirq);\n    if ( !info )\n        ERROR_EXIT(-ENOMEM);\n    info->evtchn = port;\n    rc = (!is_hvm_domain(d)\n          ? pirq_guest_bind(v, info,\n                            !!(bind->flags & BIND_PIRQ__WILL_SHARE))\n          : 0);\n    if ( rc != 0 )\n    {\n        info->evtchn = 0;\n        pirq_cleanup_check(info, d);\n        goto out;\n    }\n\n    spin_lock(&chn->lock);\n\n    chn->state  = ECS_PIRQ;\n    chn->u.pirq.irq = pirq;\n    link_pirq_port(port, chn, v);\n    evtchn_port_init(d, chn);\n\n    spin_unlock(&chn->lock);\n\n    bind->port = port;\n\n    arch_evtchn_bind_pirq(d, pirq);\n\n out:\n    check_free_port(d, port);\n    spin_unlock(&d->event_lock);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n     struct domain *d = current->domain;\n     struct vcpu   *v = d->vcpu[0];\n     struct pirq   *info;\n-    int            port, pirq = bind->pirq;\n+    int            port = 0, pirq = bind->pirq;\n     long           rc;\n \n     if ( (pirq < 0) || (pirq >= d->nr_pirqs) )\n@@ -52,6 +52,7 @@\n     arch_evtchn_bind_pirq(d, pirq);\n \n  out:\n+    check_free_port(d, port);\n     spin_unlock(&d->event_lock);\n \n     return rc;",
        "diff_line_info": {
            "deleted_lines": [
                "    int            port, pirq = bind->pirq;"
            ],
            "added_lines": [
                "    int            port = 0, pirq = bind->pirq;",
                "    check_free_port(d, port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_reset",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/8d385b247bca40ece40c9279391054bc98934325",
        "commit_title": "evtchn: evtchn_reset() shouldn't succeed with still-open ports",
        "commit_text": " While the function closes all ports, it does so without holding any lock, and hence racing requests may be issued causing new ports to get opened. This would have been problematic in particular if such a newly opened port had a port number above the new implementation limit (i.e. when switching from FIFO to 2-level) after the reset, as prior to \"evtchn: relax port_is_valid()\" this could have led to e.g. evtchn_close()'s \"BUG_ON(!port_is_valid(d2, port2))\" to trigger.  Introduce a counter of active ports and check that it's (still) no larger then the number of Xen internally used ones after obtaining the necessary lock in evtchn_reset().  As to the access model of the new {active,xen}_evtchns fields - while all writes get done using write_atomic(), reads ought to use read_atomic() only when outside of a suitably locked region.  Note that as of now evtchn_bind_virq() and evtchn_bind_ipi() don't have a need to call check_free_port().  This is part of XSA-343. ",
        "func_before": "int evtchn_reset(struct domain *d)\n{\n    unsigned int i;\n\n    if ( d != current->domain && !d->controller_pause_count )\n        return -EINVAL;\n\n    for ( i = 0; port_is_valid(d, i); i++ )\n        evtchn_close(d, i, 1);\n\n    spin_lock(&d->event_lock);\n\n    if ( d->evtchn_fifo )\n    {\n        /* Switching back to 2-level ABI. */\n        evtchn_fifo_destroy(d);\n        evtchn_2l_init(d);\n    }\n\n    spin_unlock(&d->event_lock);\n\n    return 0;\n}",
        "func": "int evtchn_reset(struct domain *d)\n{\n    unsigned int i;\n    int rc = 0;\n\n    if ( d != current->domain && !d->controller_pause_count )\n        return -EINVAL;\n\n    for ( i = 0; port_is_valid(d, i); i++ )\n        evtchn_close(d, i, 1);\n\n    spin_lock(&d->event_lock);\n\n    if ( d->active_evtchns > d->xen_evtchns )\n        rc = -EAGAIN;\n    else if ( d->evtchn_fifo )\n    {\n        /* Switching back to 2-level ABI. */\n        evtchn_fifo_destroy(d);\n        evtchn_2l_init(d);\n    }\n\n    spin_unlock(&d->event_lock);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n int evtchn_reset(struct domain *d)\n {\n     unsigned int i;\n+    int rc = 0;\n \n     if ( d != current->domain && !d->controller_pause_count )\n         return -EINVAL;\n@@ -10,7 +11,9 @@\n \n     spin_lock(&d->event_lock);\n \n-    if ( d->evtchn_fifo )\n+    if ( d->active_evtchns > d->xen_evtchns )\n+        rc = -EAGAIN;\n+    else if ( d->evtchn_fifo )\n     {\n         /* Switching back to 2-level ABI. */\n         evtchn_fifo_destroy(d);\n@@ -19,5 +22,5 @@\n \n     spin_unlock(&d->event_lock);\n \n-    return 0;\n+    return rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( d->evtchn_fifo )",
                "    return 0;"
            ],
            "added_lines": [
                "    int rc = 0;",
                "    if ( d->active_evtchns > d->xen_evtchns )",
                "        rc = -EAGAIN;",
                "    else if ( d->evtchn_fifo )",
                "    return rc;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_alloc_unbound",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/8d385b247bca40ece40c9279391054bc98934325",
        "commit_title": "evtchn: evtchn_reset() shouldn't succeed with still-open ports",
        "commit_text": " While the function closes all ports, it does so without holding any lock, and hence racing requests may be issued causing new ports to get opened. This would have been problematic in particular if such a newly opened port had a port number above the new implementation limit (i.e. when switching from FIFO to 2-level) after the reset, as prior to \"evtchn: relax port_is_valid()\" this could have led to e.g. evtchn_close()'s \"BUG_ON(!port_is_valid(d2, port2))\" to trigger.  Introduce a counter of active ports and check that it's (still) no larger then the number of Xen internally used ones after obtaining the necessary lock in evtchn_reset().  As to the access model of the new {active,xen}_evtchns fields - while all writes get done using write_atomic(), reads ought to use read_atomic() only when outside of a suitably locked region.  Note that as of now evtchn_bind_virq() and evtchn_bind_ipi() don't have a need to call check_free_port().  This is part of XSA-343. ",
        "func_before": "static long evtchn_alloc_unbound(evtchn_alloc_unbound_t *alloc)\n{\n    struct evtchn *chn;\n    struct domain *d;\n    int            port;\n    domid_t        dom = alloc->dom;\n    long           rc;\n\n    d = rcu_lock_domain_by_any_id(dom);\n    if ( d == NULL )\n        return -ESRCH;\n\n    spin_lock(&d->event_lock);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT_DOM(port, d);\n    chn = evtchn_from_port(d, port);\n\n    rc = xsm_evtchn_unbound(XSM_TARGET, d, chn, alloc->remote_dom);\n    if ( rc )\n        goto out;\n\n    spin_lock(&chn->lock);\n\n    chn->state = ECS_UNBOUND;\n    if ( (chn->u.unbound.remote_domid = alloc->remote_dom) == DOMID_SELF )\n        chn->u.unbound.remote_domid = current->domain->domain_id;\n    evtchn_port_init(d, chn);\n\n    spin_unlock(&chn->lock);\n\n    alloc->port = port;\n\n out:\n    spin_unlock(&d->event_lock);\n    rcu_unlock_domain(d);\n\n    return rc;\n}",
        "func": "static long evtchn_alloc_unbound(evtchn_alloc_unbound_t *alloc)\n{\n    struct evtchn *chn;\n    struct domain *d;\n    int            port;\n    domid_t        dom = alloc->dom;\n    long           rc;\n\n    d = rcu_lock_domain_by_any_id(dom);\n    if ( d == NULL )\n        return -ESRCH;\n\n    spin_lock(&d->event_lock);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT_DOM(port, d);\n    chn = evtchn_from_port(d, port);\n\n    rc = xsm_evtchn_unbound(XSM_TARGET, d, chn, alloc->remote_dom);\n    if ( rc )\n        goto out;\n\n    spin_lock(&chn->lock);\n\n    chn->state = ECS_UNBOUND;\n    if ( (chn->u.unbound.remote_domid = alloc->remote_dom) == DOMID_SELF )\n        chn->u.unbound.remote_domid = current->domain->domain_id;\n    evtchn_port_init(d, chn);\n\n    spin_unlock(&chn->lock);\n\n    alloc->port = port;\n\n out:\n    check_free_port(d, port);\n    spin_unlock(&d->event_lock);\n    rcu_unlock_domain(d);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -32,6 +32,7 @@\n     alloc->port = port;\n \n  out:\n+    check_free_port(d, port);\n     spin_unlock(&d->event_lock);\n     rcu_unlock_domain(d);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    check_free_port(d, port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_allocate_port",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/8d385b247bca40ece40c9279391054bc98934325",
        "commit_title": "evtchn: evtchn_reset() shouldn't succeed with still-open ports",
        "commit_text": " While the function closes all ports, it does so without holding any lock, and hence racing requests may be issued causing new ports to get opened. This would have been problematic in particular if such a newly opened port had a port number above the new implementation limit (i.e. when switching from FIFO to 2-level) after the reset, as prior to \"evtchn: relax port_is_valid()\" this could have led to e.g. evtchn_close()'s \"BUG_ON(!port_is_valid(d2, port2))\" to trigger.  Introduce a counter of active ports and check that it's (still) no larger then the number of Xen internally used ones after obtaining the necessary lock in evtchn_reset().  As to the access model of the new {active,xen}_evtchns fields - while all writes get done using write_atomic(), reads ought to use read_atomic() only when outside of a suitably locked region.  Note that as of now evtchn_bind_virq() and evtchn_bind_ipi() don't have a need to call check_free_port().  This is part of XSA-343. ",
        "func_before": "int evtchn_allocate_port(struct domain *d, evtchn_port_t port)\n{\n    if ( port > d->max_evtchn_port || port >= max_evtchns(d) )\n        return -ENOSPC;\n\n    if ( port_is_valid(d, port) )\n    {\n        if ( evtchn_from_port(d, port)->state != ECS_FREE ||\n             evtchn_port_is_busy(d, port) )\n            return -EBUSY;\n    }\n    else\n    {\n        struct evtchn *chn;\n        struct evtchn **grp;\n\n        if ( !group_from_port(d, port) )\n        {\n            grp = xzalloc_array(struct evtchn *, BUCKETS_PER_GROUP);\n            if ( !grp )\n                return -ENOMEM;\n            group_from_port(d, port) = grp;\n        }\n\n        chn = alloc_evtchn_bucket(d, port);\n        if ( !chn )\n            return -ENOMEM;\n        bucket_from_port(d, port) = chn;\n\n        /*\n         * d->valid_evtchns is used to check whether the bucket can be\n         * accessed without the per-domain lock. Therefore,\n         * d->valid_evtchns should be seen *after* the new bucket has\n         * been setup.\n         */\n        smp_wmb();\n        write_atomic(&d->valid_evtchns, d->valid_evtchns + EVTCHNS_PER_BUCKET);\n    }\n\n    return 0;\n}",
        "func": "int evtchn_allocate_port(struct domain *d, evtchn_port_t port)\n{\n    if ( port > d->max_evtchn_port || port >= max_evtchns(d) )\n        return -ENOSPC;\n\n    if ( port_is_valid(d, port) )\n    {\n        if ( evtchn_from_port(d, port)->state != ECS_FREE ||\n             evtchn_port_is_busy(d, port) )\n            return -EBUSY;\n    }\n    else\n    {\n        struct evtchn *chn;\n        struct evtchn **grp;\n\n        if ( !group_from_port(d, port) )\n        {\n            grp = xzalloc_array(struct evtchn *, BUCKETS_PER_GROUP);\n            if ( !grp )\n                return -ENOMEM;\n            group_from_port(d, port) = grp;\n        }\n\n        chn = alloc_evtchn_bucket(d, port);\n        if ( !chn )\n            return -ENOMEM;\n        bucket_from_port(d, port) = chn;\n\n        /*\n         * d->valid_evtchns is used to check whether the bucket can be\n         * accessed without the per-domain lock. Therefore,\n         * d->valid_evtchns should be seen *after* the new bucket has\n         * been setup.\n         */\n        smp_wmb();\n        write_atomic(&d->valid_evtchns, d->valid_evtchns + EVTCHNS_PER_BUCKET);\n    }\n\n    write_atomic(&d->active_evtchns, d->active_evtchns + 1);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -37,5 +37,7 @@\n         write_atomic(&d->valid_evtchns, d->valid_evtchns + EVTCHNS_PER_BUCKET);\n     }\n \n+    write_atomic(&d->active_evtchns, d->active_evtchns + 1);\n+\n     return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    write_atomic(&d->active_evtchns, d->active_evtchns + 1);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_init",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/8d385b247bca40ece40c9279391054bc98934325",
        "commit_title": "evtchn: evtchn_reset() shouldn't succeed with still-open ports",
        "commit_text": " While the function closes all ports, it does so without holding any lock, and hence racing requests may be issued causing new ports to get opened. This would have been problematic in particular if such a newly opened port had a port number above the new implementation limit (i.e. when switching from FIFO to 2-level) after the reset, as prior to \"evtchn: relax port_is_valid()\" this could have led to e.g. evtchn_close()'s \"BUG_ON(!port_is_valid(d2, port2))\" to trigger.  Introduce a counter of active ports and check that it's (still) no larger then the number of Xen internally used ones after obtaining the necessary lock in evtchn_reset().  As to the access model of the new {active,xen}_evtchns fields - while all writes get done using write_atomic(), reads ought to use read_atomic() only when outside of a suitably locked region.  Note that as of now evtchn_bind_virq() and evtchn_bind_ipi() don't have a need to call check_free_port().  This is part of XSA-343. ",
        "func_before": "int evtchn_init(struct domain *d, unsigned int max_port)\n{\n    evtchn_2l_init(d);\n    d->max_evtchn_port = min_t(unsigned int, max_port, INT_MAX);\n\n    d->evtchn = alloc_evtchn_bucket(d, 0);\n    if ( !d->evtchn )\n        return -ENOMEM;\n    d->valid_evtchns = EVTCHNS_PER_BUCKET;\n\n    spin_lock_init_prof(d, event_lock);\n    if ( get_free_port(d) != 0 )\n    {\n        free_evtchn_bucket(d, d->evtchn);\n        return -EINVAL;\n    }\n    evtchn_from_port(d, 0)->state = ECS_RESERVED;\n\n#if MAX_VIRT_CPUS > BITS_PER_LONG\n    d->poll_mask = xzalloc_array(unsigned long, BITS_TO_LONGS(d->max_vcpus));\n    if ( !d->poll_mask )\n    {\n        free_evtchn_bucket(d, d->evtchn);\n        return -ENOMEM;\n    }\n#endif\n\n    return 0;\n}",
        "func": "int evtchn_init(struct domain *d, unsigned int max_port)\n{\n    evtchn_2l_init(d);\n    d->max_evtchn_port = min_t(unsigned int, max_port, INT_MAX);\n\n    d->evtchn = alloc_evtchn_bucket(d, 0);\n    if ( !d->evtchn )\n        return -ENOMEM;\n    d->valid_evtchns = EVTCHNS_PER_BUCKET;\n\n    spin_lock_init_prof(d, event_lock);\n    if ( get_free_port(d) != 0 )\n    {\n        free_evtchn_bucket(d, d->evtchn);\n        return -EINVAL;\n    }\n    evtchn_from_port(d, 0)->state = ECS_RESERVED;\n    write_atomic(&d->active_evtchns, 0);\n\n#if MAX_VIRT_CPUS > BITS_PER_LONG\n    d->poll_mask = xzalloc_array(unsigned long, BITS_TO_LONGS(d->max_vcpus));\n    if ( !d->poll_mask )\n    {\n        free_evtchn_bucket(d, d->evtchn);\n        return -ENOMEM;\n    }\n#endif\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,6 +15,7 @@\n         return -EINVAL;\n     }\n     evtchn_from_port(d, 0)->state = ECS_RESERVED;\n+    write_atomic(&d->active_evtchns, 0);\n \n #if MAX_VIRT_CPUS > BITS_PER_LONG\n     d->poll_mask = xzalloc_array(unsigned long, BITS_TO_LONGS(d->max_vcpus));",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    write_atomic(&d->active_evtchns, 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_destroy",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/8d385b247bca40ece40c9279391054bc98934325",
        "commit_title": "evtchn: evtchn_reset() shouldn't succeed with still-open ports",
        "commit_text": " While the function closes all ports, it does so without holding any lock, and hence racing requests may be issued causing new ports to get opened. This would have been problematic in particular if such a newly opened port had a port number above the new implementation limit (i.e. when switching from FIFO to 2-level) after the reset, as prior to \"evtchn: relax port_is_valid()\" this could have led to e.g. evtchn_close()'s \"BUG_ON(!port_is_valid(d2, port2))\" to trigger.  Introduce a counter of active ports and check that it's (still) no larger then the number of Xen internally used ones after obtaining the necessary lock in evtchn_reset().  As to the access model of the new {active,xen}_evtchns fields - while all writes get done using write_atomic(), reads ought to use read_atomic() only when outside of a suitably locked region.  Note that as of now evtchn_bind_virq() and evtchn_bind_ipi() don't have a need to call check_free_port().  This is part of XSA-343. ",
        "func_before": "void evtchn_destroy(struct domain *d)\n{\n    unsigned int i;\n\n    /* After this barrier no new event-channel allocations can occur. */\n    BUG_ON(!d->is_dying);\n    spin_barrier(&d->event_lock);\n\n    /* Close all existing event channels. */\n    for ( i = 0; port_is_valid(d, i); i++ )\n        evtchn_close(d, i, 0);\n\n    clear_global_virq_handlers(d);\n\n    evtchn_fifo_destroy(d);\n}",
        "func": "void evtchn_destroy(struct domain *d)\n{\n    unsigned int i;\n\n    /* After this barrier no new event-channel allocations can occur. */\n    BUG_ON(!d->is_dying);\n    spin_barrier(&d->event_lock);\n\n    /* Close all existing event channels. */\n    for ( i = 0; port_is_valid(d, i); i++ )\n        evtchn_close(d, i, 0);\n\n    ASSERT(!d->active_evtchns);\n\n    clear_global_virq_handlers(d);\n\n    evtchn_fifo_destroy(d);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,8 @@\n     for ( i = 0; port_is_valid(d, i); i++ )\n         evtchn_close(d, i, 0);\n \n+    ASSERT(!d->active_evtchns);\n+\n     clear_global_virq_handlers(d);\n \n     evtchn_fifo_destroy(d);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    ASSERT(!d->active_evtchns);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/alloc_unbound_xen_event_channel",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/8d385b247bca40ece40c9279391054bc98934325",
        "commit_title": "evtchn: evtchn_reset() shouldn't succeed with still-open ports",
        "commit_text": " While the function closes all ports, it does so without holding any lock, and hence racing requests may be issued causing new ports to get opened. This would have been problematic in particular if such a newly opened port had a port number above the new implementation limit (i.e. when switching from FIFO to 2-level) after the reset, as prior to \"evtchn: relax port_is_valid()\" this could have led to e.g. evtchn_close()'s \"BUG_ON(!port_is_valid(d2, port2))\" to trigger.  Introduce a counter of active ports and check that it's (still) no larger then the number of Xen internally used ones after obtaining the necessary lock in evtchn_reset().  As to the access model of the new {active,xen}_evtchns fields - while all writes get done using write_atomic(), reads ought to use read_atomic() only when outside of a suitably locked region.  Note that as of now evtchn_bind_virq() and evtchn_bind_ipi() don't have a need to call check_free_port().  This is part of XSA-343. ",
        "func_before": "int alloc_unbound_xen_event_channel(\n    struct domain *ld, unsigned int lvcpu, domid_t remote_domid,\n    xen_event_channel_notification_t notification_fn)\n{\n    struct evtchn *chn;\n    int            port, rc;\n\n    spin_lock(&ld->event_lock);\n\n    rc = get_free_port(ld);\n    if ( rc < 0 )\n        goto out;\n    port = rc;\n    chn = evtchn_from_port(ld, port);\n\n    rc = xsm_evtchn_unbound(XSM_TARGET, ld, chn, remote_domid);\n    if ( rc )\n        goto out;\n\n    spin_lock(&chn->lock);\n\n    chn->state = ECS_UNBOUND;\n    chn->xen_consumer = get_xen_consumer(notification_fn);\n    chn->notify_vcpu_id = lvcpu;\n    chn->u.unbound.remote_domid = remote_domid;\n\n    spin_unlock(&chn->lock);\n\n out:\n    spin_unlock(&ld->event_lock);\n\n    return rc < 0 ? rc : port;\n}",
        "func": "int alloc_unbound_xen_event_channel(\n    struct domain *ld, unsigned int lvcpu, domid_t remote_domid,\n    xen_event_channel_notification_t notification_fn)\n{\n    struct evtchn *chn;\n    int            port, rc;\n\n    spin_lock(&ld->event_lock);\n\n    port = rc = get_free_port(ld);\n    if ( rc < 0 )\n        goto out;\n    chn = evtchn_from_port(ld, port);\n\n    rc = xsm_evtchn_unbound(XSM_TARGET, ld, chn, remote_domid);\n    if ( rc )\n        goto out;\n\n    spin_lock(&chn->lock);\n\n    chn->state = ECS_UNBOUND;\n    chn->xen_consumer = get_xen_consumer(notification_fn);\n    chn->notify_vcpu_id = lvcpu;\n    chn->u.unbound.remote_domid = remote_domid;\n\n    spin_unlock(&chn->lock);\n\n    write_atomic(&ld->xen_evtchns, ld->xen_evtchns + 1);\n\n out:\n    check_free_port(ld, port);\n    spin_unlock(&ld->event_lock);\n\n    return rc < 0 ? rc : port;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,10 +7,9 @@\n \n     spin_lock(&ld->event_lock);\n \n-    rc = get_free_port(ld);\n+    port = rc = get_free_port(ld);\n     if ( rc < 0 )\n         goto out;\n-    port = rc;\n     chn = evtchn_from_port(ld, port);\n \n     rc = xsm_evtchn_unbound(XSM_TARGET, ld, chn, remote_domid);\n@@ -26,7 +25,10 @@\n \n     spin_unlock(&chn->lock);\n \n+    write_atomic(&ld->xen_evtchns, ld->xen_evtchns + 1);\n+\n  out:\n+    check_free_port(ld, port);\n     spin_unlock(&ld->event_lock);\n \n     return rc < 0 ? rc : port;",
        "diff_line_info": {
            "deleted_lines": [
                "    rc = get_free_port(ld);",
                "    port = rc;"
            ],
            "added_lines": [
                "    port = rc = get_free_port(ld);",
                "    write_atomic(&ld->xen_evtchns, ld->xen_evtchns + 1);",
                "",
                "    check_free_port(ld, port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_free",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/8d385b247bca40ece40c9279391054bc98934325",
        "commit_title": "evtchn: evtchn_reset() shouldn't succeed with still-open ports",
        "commit_text": " While the function closes all ports, it does so without holding any lock, and hence racing requests may be issued causing new ports to get opened. This would have been problematic in particular if such a newly opened port had a port number above the new implementation limit (i.e. when switching from FIFO to 2-level) after the reset, as prior to \"evtchn: relax port_is_valid()\" this could have led to e.g. evtchn_close()'s \"BUG_ON(!port_is_valid(d2, port2))\" to trigger.  Introduce a counter of active ports and check that it's (still) no larger then the number of Xen internally used ones after obtaining the necessary lock in evtchn_reset().  As to the access model of the new {active,xen}_evtchns fields - while all writes get done using write_atomic(), reads ought to use read_atomic() only when outside of a suitably locked region.  Note that as of now evtchn_bind_virq() and evtchn_bind_ipi() don't have a need to call check_free_port().  This is part of XSA-343. ",
        "func_before": "void evtchn_free(struct domain *d, struct evtchn *chn)\n{\n    /* Clear pending event to avoid unexpected behavior on re-bind. */\n    evtchn_port_clear_pending(d, chn);\n\n    /* Reset binding to vcpu0 when the channel is freed. */\n    chn->state          = ECS_FREE;\n    chn->notify_vcpu_id = 0;\n    chn->xen_consumer   = 0;\n\n    xsm_evtchn_close_post(chn);\n}",
        "func": "void evtchn_free(struct domain *d, struct evtchn *chn)\n{\n    /* Clear pending event to avoid unexpected behavior on re-bind. */\n    evtchn_port_clear_pending(d, chn);\n\n    if ( consumer_is_xen(chn) )\n        write_atomic(&d->xen_evtchns, d->xen_evtchns - 1);\n    write_atomic(&d->active_evtchns, d->active_evtchns - 1);\n\n    /* Reset binding to vcpu0 when the channel is freed. */\n    chn->state          = ECS_FREE;\n    chn->notify_vcpu_id = 0;\n    chn->xen_consumer   = 0;\n\n    xsm_evtchn_close_post(chn);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,10 @@\n {\n     /* Clear pending event to avoid unexpected behavior on re-bind. */\n     evtchn_port_clear_pending(d, chn);\n+\n+    if ( consumer_is_xen(chn) )\n+        write_atomic(&d->xen_evtchns, d->xen_evtchns - 1);\n+    write_atomic(&d->active_evtchns, d->active_evtchns - 1);\n \n     /* Reset binding to vcpu0 when the channel is freed. */\n     chn->state          = ECS_FREE;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    if ( consumer_is_xen(chn) )",
                "        write_atomic(&d->xen_evtchns, d->xen_evtchns - 1);",
                "    write_atomic(&d->active_evtchns, d->active_evtchns - 1);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_bind_interdomain",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/8d385b247bca40ece40c9279391054bc98934325",
        "commit_title": "evtchn: evtchn_reset() shouldn't succeed with still-open ports",
        "commit_text": " While the function closes all ports, it does so without holding any lock, and hence racing requests may be issued causing new ports to get opened. This would have been problematic in particular if such a newly opened port had a port number above the new implementation limit (i.e. when switching from FIFO to 2-level) after the reset, as prior to \"evtchn: relax port_is_valid()\" this could have led to e.g. evtchn_close()'s \"BUG_ON(!port_is_valid(d2, port2))\" to trigger.  Introduce a counter of active ports and check that it's (still) no larger then the number of Xen internally used ones after obtaining the necessary lock in evtchn_reset().  As to the access model of the new {active,xen}_evtchns fields - while all writes get done using write_atomic(), reads ought to use read_atomic() only when outside of a suitably locked region.  Note that as of now evtchn_bind_virq() and evtchn_bind_ipi() don't have a need to call check_free_port().  This is part of XSA-343. ",
        "func_before": "static long evtchn_bind_interdomain(evtchn_bind_interdomain_t *bind)\n{\n    struct evtchn *lchn, *rchn;\n    struct domain *ld = current->domain, *rd;\n    int            lport, rport = bind->remote_port;\n    domid_t        rdom = bind->remote_dom;\n    long           rc;\n\n    if ( rdom == DOMID_SELF )\n        rdom = current->domain->domain_id;\n\n    if ( (rd = rcu_lock_domain_by_id(rdom)) == NULL )\n        return -ESRCH;\n\n    /* Avoid deadlock by first acquiring lock of domain with smaller id. */\n    if ( ld < rd )\n    {\n        spin_lock(&ld->event_lock);\n        spin_lock(&rd->event_lock);\n    }\n    else\n    {\n        if ( ld != rd )\n            spin_lock(&rd->event_lock);\n        spin_lock(&ld->event_lock);\n    }\n\n    if ( (lport = get_free_port(ld)) < 0 )\n        ERROR_EXIT(lport);\n    lchn = evtchn_from_port(ld, lport);\n\n    if ( !port_is_valid(rd, rport) )\n        ERROR_EXIT_DOM(-EINVAL, rd);\n    rchn = evtchn_from_port(rd, rport);\n    if ( (rchn->state != ECS_UNBOUND) ||\n         (rchn->u.unbound.remote_domid != ld->domain_id) )\n        ERROR_EXIT_DOM(-EINVAL, rd);\n\n    rc = xsm_evtchn_interdomain(XSM_HOOK, ld, lchn, rd, rchn);\n    if ( rc )\n        goto out;\n\n    double_evtchn_lock(lchn, rchn);\n\n    lchn->u.interdomain.remote_dom  = rd;\n    lchn->u.interdomain.remote_port = rport;\n    lchn->state                     = ECS_INTERDOMAIN;\n    evtchn_port_init(ld, lchn);\n    \n    rchn->u.interdomain.remote_dom  = ld;\n    rchn->u.interdomain.remote_port = lport;\n    rchn->state                     = ECS_INTERDOMAIN;\n\n    /*\n     * We may have lost notifications on the remote unbound port. Fix that up\n     * here by conservatively always setting a notification on the local port.\n     */\n    evtchn_port_set_pending(ld, lchn->notify_vcpu_id, lchn);\n\n    double_evtchn_unlock(lchn, rchn);\n\n    bind->local_port = lport;\n\n out:\n    spin_unlock(&ld->event_lock);\n    if ( ld != rd )\n        spin_unlock(&rd->event_lock);\n    \n    rcu_unlock_domain(rd);\n\n    return rc;\n}",
        "func": "static long evtchn_bind_interdomain(evtchn_bind_interdomain_t *bind)\n{\n    struct evtchn *lchn, *rchn;\n    struct domain *ld = current->domain, *rd;\n    int            lport, rport = bind->remote_port;\n    domid_t        rdom = bind->remote_dom;\n    long           rc;\n\n    if ( rdom == DOMID_SELF )\n        rdom = current->domain->domain_id;\n\n    if ( (rd = rcu_lock_domain_by_id(rdom)) == NULL )\n        return -ESRCH;\n\n    /* Avoid deadlock by first acquiring lock of domain with smaller id. */\n    if ( ld < rd )\n    {\n        spin_lock(&ld->event_lock);\n        spin_lock(&rd->event_lock);\n    }\n    else\n    {\n        if ( ld != rd )\n            spin_lock(&rd->event_lock);\n        spin_lock(&ld->event_lock);\n    }\n\n    if ( (lport = get_free_port(ld)) < 0 )\n        ERROR_EXIT(lport);\n    lchn = evtchn_from_port(ld, lport);\n\n    if ( !port_is_valid(rd, rport) )\n        ERROR_EXIT_DOM(-EINVAL, rd);\n    rchn = evtchn_from_port(rd, rport);\n    if ( (rchn->state != ECS_UNBOUND) ||\n         (rchn->u.unbound.remote_domid != ld->domain_id) )\n        ERROR_EXIT_DOM(-EINVAL, rd);\n\n    rc = xsm_evtchn_interdomain(XSM_HOOK, ld, lchn, rd, rchn);\n    if ( rc )\n        goto out;\n\n    double_evtchn_lock(lchn, rchn);\n\n    lchn->u.interdomain.remote_dom  = rd;\n    lchn->u.interdomain.remote_port = rport;\n    lchn->state                     = ECS_INTERDOMAIN;\n    evtchn_port_init(ld, lchn);\n    \n    rchn->u.interdomain.remote_dom  = ld;\n    rchn->u.interdomain.remote_port = lport;\n    rchn->state                     = ECS_INTERDOMAIN;\n\n    /*\n     * We may have lost notifications on the remote unbound port. Fix that up\n     * here by conservatively always setting a notification on the local port.\n     */\n    evtchn_port_set_pending(ld, lchn->notify_vcpu_id, lchn);\n\n    double_evtchn_unlock(lchn, rchn);\n\n    bind->local_port = lport;\n\n out:\n    check_free_port(ld, lport);\n    spin_unlock(&ld->event_lock);\n    if ( ld != rd )\n        spin_unlock(&rd->event_lock);\n    \n    rcu_unlock_domain(rd);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -62,6 +62,7 @@\n     bind->local_port = lport;\n \n  out:\n+    check_free_port(ld, lport);\n     spin_unlock(&ld->event_lock);\n     if ( ld != rd )\n         spin_unlock(&rd->event_lock);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    check_free_port(ld, lport);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_bind_virq",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "int evtchn_bind_virq(evtchn_bind_virq_t *bind, evtchn_port_t port)\n{\n    struct evtchn *chn;\n    struct vcpu   *v;\n    struct domain *d = current->domain;\n    int            virq = bind->virq, vcpu = bind->vcpu;\n    int            rc = 0;\n\n    if ( (virq < 0) || (virq >= ARRAY_SIZE(v->virq_to_evtchn)) )\n        return -EINVAL;\n\n   /*\n    * Make sure the guest controlled value virq is bounded even during\n    * speculative execution.\n    */\n    virq = array_index_nospec(virq, ARRAY_SIZE(v->virq_to_evtchn));\n\n    if ( virq_is_global(virq) && (vcpu != 0) )\n        return -EINVAL;\n\n    if ( (v = domain_vcpu(d, vcpu)) == NULL )\n        return -ENOENT;\n\n    spin_lock(&d->event_lock);\n\n    if ( v->virq_to_evtchn[virq] != 0 )\n        ERROR_EXIT(-EEXIST);\n\n    if ( port != 0 )\n    {\n        if ( (rc = evtchn_allocate_port(d, port)) != 0 )\n            ERROR_EXIT(rc);\n    }\n    else\n    {\n        int alloc_port = get_free_port(d);\n\n        if ( alloc_port < 0 )\n            ERROR_EXIT(alloc_port);\n        port = alloc_port;\n    }\n\n    chn = evtchn_from_port(d, port);\n\n    spin_lock(&chn->lock);\n\n    chn->state          = ECS_VIRQ;\n    chn->notify_vcpu_id = vcpu;\n    chn->u.virq         = virq;\n    evtchn_port_init(d, chn);\n\n    spin_unlock(&chn->lock);\n\n    v->virq_to_evtchn[virq] = bind->port = port;\n\n out:\n    spin_unlock(&d->event_lock);\n\n    return rc;\n}",
        "func": "int evtchn_bind_virq(evtchn_bind_virq_t *bind, evtchn_port_t port)\n{\n    struct evtchn *chn;\n    struct vcpu   *v;\n    struct domain *d = current->domain;\n    int            virq = bind->virq, vcpu = bind->vcpu;\n    int            rc = 0;\n    unsigned long  flags;\n\n    if ( (virq < 0) || (virq >= ARRAY_SIZE(v->virq_to_evtchn)) )\n        return -EINVAL;\n\n   /*\n    * Make sure the guest controlled value virq is bounded even during\n    * speculative execution.\n    */\n    virq = array_index_nospec(virq, ARRAY_SIZE(v->virq_to_evtchn));\n\n    if ( virq_is_global(virq) && (vcpu != 0) )\n        return -EINVAL;\n\n    if ( (v = domain_vcpu(d, vcpu)) == NULL )\n        return -ENOENT;\n\n    spin_lock(&d->event_lock);\n\n    if ( v->virq_to_evtchn[virq] != 0 )\n        ERROR_EXIT(-EEXIST);\n\n    if ( port != 0 )\n    {\n        if ( (rc = evtchn_allocate_port(d, port)) != 0 )\n            ERROR_EXIT(rc);\n    }\n    else\n    {\n        int alloc_port = get_free_port(d);\n\n        if ( alloc_port < 0 )\n            ERROR_EXIT(alloc_port);\n        port = alloc_port;\n    }\n\n    chn = evtchn_from_port(d, port);\n\n    spin_lock_irqsave(&chn->lock, flags);\n\n    chn->state          = ECS_VIRQ;\n    chn->notify_vcpu_id = vcpu;\n    chn->u.virq         = virq;\n    evtchn_port_init(d, chn);\n\n    spin_unlock_irqrestore(&chn->lock, flags);\n\n    v->virq_to_evtchn[virq] = bind->port = port;\n\n out:\n    spin_unlock(&d->event_lock);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n     struct domain *d = current->domain;\n     int            virq = bind->virq, vcpu = bind->vcpu;\n     int            rc = 0;\n+    unsigned long  flags;\n \n     if ( (virq < 0) || (virq >= ARRAY_SIZE(v->virq_to_evtchn)) )\n         return -EINVAL;\n@@ -42,14 +43,14 @@\n \n     chn = evtchn_from_port(d, port);\n \n-    spin_lock(&chn->lock);\n+    spin_lock_irqsave(&chn->lock, flags);\n \n     chn->state          = ECS_VIRQ;\n     chn->notify_vcpu_id = vcpu;\n     chn->u.virq         = virq;\n     evtchn_port_init(d, chn);\n \n-    spin_unlock(&chn->lock);\n+    spin_unlock_irqrestore(&chn->lock, flags);\n \n     v->virq_to_evtchn[virq] = bind->port = port;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&chn->lock);",
                "    spin_unlock(&chn->lock);"
            ],
            "added_lines": [
                "    unsigned long  flags;",
                "    spin_lock_irqsave(&chn->lock, flags);",
                "    spin_unlock_irqrestore(&chn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_bind_pirq",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "static long evtchn_bind_pirq(evtchn_bind_pirq_t *bind)\n{\n    struct evtchn *chn;\n    struct domain *d = current->domain;\n    struct vcpu   *v = d->vcpu[0];\n    struct pirq   *info;\n    int            port = 0, pirq = bind->pirq;\n    long           rc;\n\n    if ( (pirq < 0) || (pirq >= d->nr_pirqs) )\n        return -EINVAL;\n\n    if ( !is_hvm_domain(d) && !pirq_access_permitted(d, pirq) )\n        return -EPERM;\n\n    spin_lock(&d->event_lock);\n\n    if ( pirq_to_evtchn(d, pirq) != 0 )\n        ERROR_EXIT(-EEXIST);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT(port);\n\n    chn = evtchn_from_port(d, port);\n\n    info = pirq_get_info(d, pirq);\n    if ( !info )\n        ERROR_EXIT(-ENOMEM);\n    info->evtchn = port;\n    rc = (!is_hvm_domain(d)\n          ? pirq_guest_bind(v, info,\n                            !!(bind->flags & BIND_PIRQ__WILL_SHARE))\n          : 0);\n    if ( rc != 0 )\n    {\n        info->evtchn = 0;\n        pirq_cleanup_check(info, d);\n        goto out;\n    }\n\n    spin_lock(&chn->lock);\n\n    chn->state  = ECS_PIRQ;\n    chn->u.pirq.irq = pirq;\n    link_pirq_port(port, chn, v);\n    evtchn_port_init(d, chn);\n\n    spin_unlock(&chn->lock);\n\n    bind->port = port;\n\n    arch_evtchn_bind_pirq(d, pirq);\n\n out:\n    check_free_port(d, port);\n    spin_unlock(&d->event_lock);\n\n    return rc;\n}",
        "func": "static long evtchn_bind_pirq(evtchn_bind_pirq_t *bind)\n{\n    struct evtchn *chn;\n    struct domain *d = current->domain;\n    struct vcpu   *v = d->vcpu[0];\n    struct pirq   *info;\n    int            port = 0, pirq = bind->pirq;\n    long           rc;\n    unsigned long  flags;\n\n    if ( (pirq < 0) || (pirq >= d->nr_pirqs) )\n        return -EINVAL;\n\n    if ( !is_hvm_domain(d) && !pirq_access_permitted(d, pirq) )\n        return -EPERM;\n\n    spin_lock(&d->event_lock);\n\n    if ( pirq_to_evtchn(d, pirq) != 0 )\n        ERROR_EXIT(-EEXIST);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT(port);\n\n    chn = evtchn_from_port(d, port);\n\n    info = pirq_get_info(d, pirq);\n    if ( !info )\n        ERROR_EXIT(-ENOMEM);\n    info->evtchn = port;\n    rc = (!is_hvm_domain(d)\n          ? pirq_guest_bind(v, info,\n                            !!(bind->flags & BIND_PIRQ__WILL_SHARE))\n          : 0);\n    if ( rc != 0 )\n    {\n        info->evtchn = 0;\n        pirq_cleanup_check(info, d);\n        goto out;\n    }\n\n    spin_lock_irqsave(&chn->lock, flags);\n\n    chn->state  = ECS_PIRQ;\n    chn->u.pirq.irq = pirq;\n    link_pirq_port(port, chn, v);\n    evtchn_port_init(d, chn);\n\n    spin_unlock_irqrestore(&chn->lock, flags);\n\n    bind->port = port;\n\n    arch_evtchn_bind_pirq(d, pirq);\n\n out:\n    check_free_port(d, port);\n    spin_unlock(&d->event_lock);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,7 @@\n     struct pirq   *info;\n     int            port = 0, pirq = bind->pirq;\n     long           rc;\n+    unsigned long  flags;\n \n     if ( (pirq < 0) || (pirq >= d->nr_pirqs) )\n         return -EINVAL;\n@@ -38,14 +39,14 @@\n         goto out;\n     }\n \n-    spin_lock(&chn->lock);\n+    spin_lock_irqsave(&chn->lock, flags);\n \n     chn->state  = ECS_PIRQ;\n     chn->u.pirq.irq = pirq;\n     link_pirq_port(port, chn, v);\n     evtchn_port_init(d, chn);\n \n-    spin_unlock(&chn->lock);\n+    spin_unlock_irqrestore(&chn->lock, flags);\n \n     bind->port = port;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&chn->lock);",
                "    spin_unlock(&chn->lock);"
            ],
            "added_lines": [
                "    unsigned long  flags;",
                "    spin_lock_irqsave(&chn->lock, flags);",
                "    spin_unlock_irqrestore(&chn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_alloc_unbound",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "static long evtchn_alloc_unbound(evtchn_alloc_unbound_t *alloc)\n{\n    struct evtchn *chn;\n    struct domain *d;\n    int            port;\n    domid_t        dom = alloc->dom;\n    long           rc;\n\n    d = rcu_lock_domain_by_any_id(dom);\n    if ( d == NULL )\n        return -ESRCH;\n\n    spin_lock(&d->event_lock);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT_DOM(port, d);\n    chn = evtchn_from_port(d, port);\n\n    rc = xsm_evtchn_unbound(XSM_TARGET, d, chn, alloc->remote_dom);\n    if ( rc )\n        goto out;\n\n    spin_lock(&chn->lock);\n\n    chn->state = ECS_UNBOUND;\n    if ( (chn->u.unbound.remote_domid = alloc->remote_dom) == DOMID_SELF )\n        chn->u.unbound.remote_domid = current->domain->domain_id;\n    evtchn_port_init(d, chn);\n\n    spin_unlock(&chn->lock);\n\n    alloc->port = port;\n\n out:\n    check_free_port(d, port);\n    spin_unlock(&d->event_lock);\n    rcu_unlock_domain(d);\n\n    return rc;\n}",
        "func": "static long evtchn_alloc_unbound(evtchn_alloc_unbound_t *alloc)\n{\n    struct evtchn *chn;\n    struct domain *d;\n    int            port;\n    domid_t        dom = alloc->dom;\n    long           rc;\n    unsigned long  flags;\n\n    d = rcu_lock_domain_by_any_id(dom);\n    if ( d == NULL )\n        return -ESRCH;\n\n    spin_lock(&d->event_lock);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT_DOM(port, d);\n    chn = evtchn_from_port(d, port);\n\n    rc = xsm_evtchn_unbound(XSM_TARGET, d, chn, alloc->remote_dom);\n    if ( rc )\n        goto out;\n\n    spin_lock_irqsave(&chn->lock, flags);\n\n    chn->state = ECS_UNBOUND;\n    if ( (chn->u.unbound.remote_domid = alloc->remote_dom) == DOMID_SELF )\n        chn->u.unbound.remote_domid = current->domain->domain_id;\n    evtchn_port_init(d, chn);\n\n    spin_unlock_irqrestore(&chn->lock, flags);\n\n    alloc->port = port;\n\n out:\n    check_free_port(d, port);\n    spin_unlock(&d->event_lock);\n    rcu_unlock_domain(d);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n     int            port;\n     domid_t        dom = alloc->dom;\n     long           rc;\n+    unsigned long  flags;\n \n     d = rcu_lock_domain_by_any_id(dom);\n     if ( d == NULL )\n@@ -20,14 +21,14 @@\n     if ( rc )\n         goto out;\n \n-    spin_lock(&chn->lock);\n+    spin_lock_irqsave(&chn->lock, flags);\n \n     chn->state = ECS_UNBOUND;\n     if ( (chn->u.unbound.remote_domid = alloc->remote_dom) == DOMID_SELF )\n         chn->u.unbound.remote_domid = current->domain->domain_id;\n     evtchn_port_init(d, chn);\n \n-    spin_unlock(&chn->lock);\n+    spin_unlock_irqrestore(&chn->lock, flags);\n \n     alloc->port = port;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&chn->lock);",
                "    spin_unlock(&chn->lock);"
            ],
            "added_lines": [
                "    unsigned long  flags;",
                "    spin_lock_irqsave(&chn->lock, flags);",
                "    spin_unlock_irqrestore(&chn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/double_evtchn_lock",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "static void double_evtchn_lock(struct evtchn *lchn, struct evtchn *rchn)\n{\n    if ( lchn < rchn )\n    {\n        spin_lock(&lchn->lock);\n        spin_lock(&rchn->lock);\n    }\n    else\n    {\n        if ( lchn != rchn )\n            spin_lock(&rchn->lock);\n        spin_lock(&lchn->lock);\n    }\n}",
        "func": "static unsigned long double_evtchn_lock(struct evtchn *lchn,\n                                        struct evtchn *rchn)\n{\n    unsigned long flags;\n\n    if ( lchn <= rchn )\n    {\n        spin_lock_irqsave(&lchn->lock, flags);\n        if ( lchn != rchn )\n            spin_lock(&rchn->lock);\n    }\n    else\n    {\n        spin_lock_irqsave(&rchn->lock, flags);\n        spin_lock(&lchn->lock);\n    }\n\n    return flags;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,14 +1,19 @@\n-static void double_evtchn_lock(struct evtchn *lchn, struct evtchn *rchn)\n+static unsigned long double_evtchn_lock(struct evtchn *lchn,\n+                                        struct evtchn *rchn)\n {\n-    if ( lchn < rchn )\n+    unsigned long flags;\n+\n+    if ( lchn <= rchn )\n     {\n-        spin_lock(&lchn->lock);\n-        spin_lock(&rchn->lock);\n+        spin_lock_irqsave(&lchn->lock, flags);\n+        if ( lchn != rchn )\n+            spin_lock(&rchn->lock);\n     }\n     else\n     {\n-        if ( lchn != rchn )\n-            spin_lock(&rchn->lock);\n+        spin_lock_irqsave(&rchn->lock, flags);\n         spin_lock(&lchn->lock);\n     }\n+\n+    return flags;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static void double_evtchn_lock(struct evtchn *lchn, struct evtchn *rchn)",
                "    if ( lchn < rchn )",
                "        spin_lock(&lchn->lock);",
                "        spin_lock(&rchn->lock);",
                "        if ( lchn != rchn )",
                "            spin_lock(&rchn->lock);"
            ],
            "added_lines": [
                "static unsigned long double_evtchn_lock(struct evtchn *lchn,",
                "                                        struct evtchn *rchn)",
                "    unsigned long flags;",
                "",
                "    if ( lchn <= rchn )",
                "        spin_lock_irqsave(&lchn->lock, flags);",
                "        if ( lchn != rchn )",
                "            spin_lock(&rchn->lock);",
                "        spin_lock_irqsave(&rchn->lock, flags);",
                "",
                "    return flags;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/double_evtchn_unlock",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "static void double_evtchn_unlock(struct evtchn *lchn, struct evtchn *rchn)\n{\n    spin_unlock(&lchn->lock);\n    if ( lchn != rchn )\n        spin_unlock(&rchn->lock);\n}",
        "func": "static void double_evtchn_unlock(struct evtchn *lchn, struct evtchn *rchn,\n                                 unsigned long flags)\n{\n    if ( lchn != rchn )\n        spin_unlock(&lchn->lock);\n    spin_unlock_irqrestore(&rchn->lock, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n-static void double_evtchn_unlock(struct evtchn *lchn, struct evtchn *rchn)\n+static void double_evtchn_unlock(struct evtchn *lchn, struct evtchn *rchn,\n+                                 unsigned long flags)\n {\n-    spin_unlock(&lchn->lock);\n     if ( lchn != rchn )\n-        spin_unlock(&rchn->lock);\n+        spin_unlock(&lchn->lock);\n+    spin_unlock_irqrestore(&rchn->lock, flags);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static void double_evtchn_unlock(struct evtchn *lchn, struct evtchn *rchn)",
                "    spin_unlock(&lchn->lock);",
                "        spin_unlock(&rchn->lock);"
            ],
            "added_lines": [
                "static void double_evtchn_unlock(struct evtchn *lchn, struct evtchn *rchn,",
                "                                 unsigned long flags)",
                "        spin_unlock(&lchn->lock);",
                "    spin_unlock_irqrestore(&rchn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/alloc_unbound_xen_event_channel",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "int alloc_unbound_xen_event_channel(\n    struct domain *ld, unsigned int lvcpu, domid_t remote_domid,\n    xen_event_channel_notification_t notification_fn)\n{\n    struct evtchn *chn;\n    int            port, rc;\n\n    spin_lock(&ld->event_lock);\n\n    port = rc = get_free_port(ld);\n    if ( rc < 0 )\n        goto out;\n    chn = evtchn_from_port(ld, port);\n\n    rc = xsm_evtchn_unbound(XSM_TARGET, ld, chn, remote_domid);\n    if ( rc )\n        goto out;\n\n    spin_lock(&chn->lock);\n\n    chn->state = ECS_UNBOUND;\n    chn->xen_consumer = get_xen_consumer(notification_fn);\n    chn->notify_vcpu_id = lvcpu;\n    chn->u.unbound.remote_domid = remote_domid;\n\n    spin_unlock(&chn->lock);\n\n    write_atomic(&ld->xen_evtchns, ld->xen_evtchns + 1);\n\n out:\n    check_free_port(ld, port);\n    spin_unlock(&ld->event_lock);\n\n    return rc < 0 ? rc : port;\n}",
        "func": "int alloc_unbound_xen_event_channel(\n    struct domain *ld, unsigned int lvcpu, domid_t remote_domid,\n    xen_event_channel_notification_t notification_fn)\n{\n    struct evtchn *chn;\n    int            port, rc;\n    unsigned long  flags;\n\n    spin_lock(&ld->event_lock);\n\n    port = rc = get_free_port(ld);\n    if ( rc < 0 )\n        goto out;\n    chn = evtchn_from_port(ld, port);\n\n    rc = xsm_evtchn_unbound(XSM_TARGET, ld, chn, remote_domid);\n    if ( rc )\n        goto out;\n\n    spin_lock_irqsave(&chn->lock, flags);\n\n    chn->state = ECS_UNBOUND;\n    chn->xen_consumer = get_xen_consumer(notification_fn);\n    chn->notify_vcpu_id = lvcpu;\n    chn->u.unbound.remote_domid = remote_domid;\n\n    spin_unlock_irqrestore(&chn->lock, flags);\n\n    write_atomic(&ld->xen_evtchns, ld->xen_evtchns + 1);\n\n out:\n    check_free_port(ld, port);\n    spin_unlock(&ld->event_lock);\n\n    return rc < 0 ? rc : port;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,7 @@\n {\n     struct evtchn *chn;\n     int            port, rc;\n+    unsigned long  flags;\n \n     spin_lock(&ld->event_lock);\n \n@@ -16,14 +17,14 @@\n     if ( rc )\n         goto out;\n \n-    spin_lock(&chn->lock);\n+    spin_lock_irqsave(&chn->lock, flags);\n \n     chn->state = ECS_UNBOUND;\n     chn->xen_consumer = get_xen_consumer(notification_fn);\n     chn->notify_vcpu_id = lvcpu;\n     chn->u.unbound.remote_domid = remote_domid;\n \n-    spin_unlock(&chn->lock);\n+    spin_unlock_irqrestore(&chn->lock, flags);\n \n     write_atomic(&ld->xen_evtchns, ld->xen_evtchns + 1);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&chn->lock);",
                "    spin_unlock(&chn->lock);"
            ],
            "added_lines": [
                "    unsigned long  flags;",
                "    spin_lock_irqsave(&chn->lock, flags);",
                "    spin_unlock_irqrestore(&chn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_bind_ipi",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "static long evtchn_bind_ipi(evtchn_bind_ipi_t *bind)\n{\n    struct evtchn *chn;\n    struct domain *d = current->domain;\n    int            port, vcpu = bind->vcpu;\n    long           rc = 0;\n\n    if ( domain_vcpu(d, vcpu) == NULL )\n        return -ENOENT;\n\n    spin_lock(&d->event_lock);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT(port);\n\n    chn = evtchn_from_port(d, port);\n\n    spin_lock(&chn->lock);\n\n    chn->state          = ECS_IPI;\n    chn->notify_vcpu_id = vcpu;\n    evtchn_port_init(d, chn);\n\n    spin_unlock(&chn->lock);\n\n    bind->port = port;\n\n out:\n    spin_unlock(&d->event_lock);\n\n    return rc;\n}",
        "func": "static long evtchn_bind_ipi(evtchn_bind_ipi_t *bind)\n{\n    struct evtchn *chn;\n    struct domain *d = current->domain;\n    int            port, vcpu = bind->vcpu;\n    long           rc = 0;\n    unsigned long  flags;\n\n    if ( domain_vcpu(d, vcpu) == NULL )\n        return -ENOENT;\n\n    spin_lock(&d->event_lock);\n\n    if ( (port = get_free_port(d)) < 0 )\n        ERROR_EXIT(port);\n\n    chn = evtchn_from_port(d, port);\n\n    spin_lock_irqsave(&chn->lock, flags);\n\n    chn->state          = ECS_IPI;\n    chn->notify_vcpu_id = vcpu;\n    evtchn_port_init(d, chn);\n\n    spin_unlock_irqrestore(&chn->lock, flags);\n\n    bind->port = port;\n\n out:\n    spin_unlock(&d->event_lock);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,7 @@\n     struct domain *d = current->domain;\n     int            port, vcpu = bind->vcpu;\n     long           rc = 0;\n+    unsigned long  flags;\n \n     if ( domain_vcpu(d, vcpu) == NULL )\n         return -ENOENT;\n@@ -15,13 +16,13 @@\n \n     chn = evtchn_from_port(d, port);\n \n-    spin_lock(&chn->lock);\n+    spin_lock_irqsave(&chn->lock, flags);\n \n     chn->state          = ECS_IPI;\n     chn->notify_vcpu_id = vcpu;\n     evtchn_port_init(d, chn);\n \n-    spin_unlock(&chn->lock);\n+    spin_unlock_irqrestore(&chn->lock, flags);\n \n     bind->port = port;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&chn->lock);",
                "    spin_unlock(&chn->lock);"
            ],
            "added_lines": [
                "    unsigned long  flags;",
                "    spin_lock_irqsave(&chn->lock, flags);",
                "    spin_unlock_irqrestore(&chn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_send",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "int evtchn_send(struct domain *ld, unsigned int lport)\n{\n    struct evtchn *lchn, *rchn;\n    struct domain *rd;\n    int            rport, ret = 0;\n\n    if ( !port_is_valid(ld, lport) )\n        return -EINVAL;\n\n    lchn = evtchn_from_port(ld, lport);\n\n    spin_lock(&lchn->lock);\n\n    /* Guest cannot send via a Xen-attached event channel. */\n    if ( unlikely(consumer_is_xen(lchn)) )\n    {\n        ret = -EINVAL;\n        goto out;\n    }\n\n    ret = xsm_evtchn_send(XSM_HOOK, ld, lchn);\n    if ( ret )\n        goto out;\n\n    switch ( lchn->state )\n    {\n    case ECS_INTERDOMAIN:\n        rd    = lchn->u.interdomain.remote_dom;\n        rport = lchn->u.interdomain.remote_port;\n        rchn  = evtchn_from_port(rd, rport);\n        if ( consumer_is_xen(rchn) )\n            xen_notification_fn(rchn)(rd->vcpu[rchn->notify_vcpu_id], rport);\n        else\n            evtchn_port_set_pending(rd, rchn->notify_vcpu_id, rchn);\n        break;\n    case ECS_IPI:\n        evtchn_port_set_pending(ld, lchn->notify_vcpu_id, lchn);\n        break;\n    case ECS_UNBOUND:\n        /* silently drop the notification */\n        break;\n    default:\n        ret = -EINVAL;\n    }\n\nout:\n    spin_unlock(&lchn->lock);\n\n    return ret;\n}",
        "func": "int evtchn_send(struct domain *ld, unsigned int lport)\n{\n    struct evtchn *lchn, *rchn;\n    struct domain *rd;\n    int            rport, ret = 0;\n    unsigned long  flags;\n\n    if ( !port_is_valid(ld, lport) )\n        return -EINVAL;\n\n    lchn = evtchn_from_port(ld, lport);\n\n    spin_lock_irqsave(&lchn->lock, flags);\n\n    /* Guest cannot send via a Xen-attached event channel. */\n    if ( unlikely(consumer_is_xen(lchn)) )\n    {\n        ret = -EINVAL;\n        goto out;\n    }\n\n    ret = xsm_evtchn_send(XSM_HOOK, ld, lchn);\n    if ( ret )\n        goto out;\n\n    switch ( lchn->state )\n    {\n    case ECS_INTERDOMAIN:\n        rd    = lchn->u.interdomain.remote_dom;\n        rport = lchn->u.interdomain.remote_port;\n        rchn  = evtchn_from_port(rd, rport);\n        if ( consumer_is_xen(rchn) )\n            xen_notification_fn(rchn)(rd->vcpu[rchn->notify_vcpu_id], rport);\n        else\n            evtchn_port_set_pending(rd, rchn->notify_vcpu_id, rchn);\n        break;\n    case ECS_IPI:\n        evtchn_port_set_pending(ld, lchn->notify_vcpu_id, lchn);\n        break;\n    case ECS_UNBOUND:\n        /* silently drop the notification */\n        break;\n    default:\n        ret = -EINVAL;\n    }\n\nout:\n    spin_unlock_irqrestore(&lchn->lock, flags);\n\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,13 +3,14 @@\n     struct evtchn *lchn, *rchn;\n     struct domain *rd;\n     int            rport, ret = 0;\n+    unsigned long  flags;\n \n     if ( !port_is_valid(ld, lport) )\n         return -EINVAL;\n \n     lchn = evtchn_from_port(ld, lport);\n \n-    spin_lock(&lchn->lock);\n+    spin_lock_irqsave(&lchn->lock, flags);\n \n     /* Guest cannot send via a Xen-attached event channel. */\n     if ( unlikely(consumer_is_xen(lchn)) )\n@@ -44,7 +45,7 @@\n     }\n \n out:\n-    spin_unlock(&lchn->lock);\n+    spin_unlock_irqrestore(&lchn->lock, flags);\n \n     return ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&lchn->lock);",
                "    spin_unlock(&lchn->lock);"
            ],
            "added_lines": [
                "    unsigned long  flags;",
                "    spin_lock_irqsave(&lchn->lock, flags);",
                "    spin_unlock_irqrestore(&lchn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_close",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "int evtchn_close(struct domain *d1, int port1, bool guest)\n{\n    struct domain *d2 = NULL;\n    struct vcpu   *v;\n    struct evtchn *chn1, *chn2;\n    int            port2;\n    long           rc = 0;\n\n again:\n    spin_lock(&d1->event_lock);\n\n    if ( !port_is_valid(d1, port1) )\n    {\n        rc = -EINVAL;\n        goto out;\n    }\n\n    chn1 = evtchn_from_port(d1, port1);\n\n    /* Guest cannot close a Xen-attached event channel. */\n    if ( unlikely(consumer_is_xen(chn1)) && guest )\n    {\n        rc = -EINVAL;\n        goto out;\n    }\n\n    switch ( chn1->state )\n    {\n    case ECS_FREE:\n    case ECS_RESERVED:\n        rc = -EINVAL;\n        goto out;\n\n    case ECS_UNBOUND:\n        break;\n\n    case ECS_PIRQ: {\n        struct pirq *pirq = pirq_info(d1, chn1->u.pirq.irq);\n\n        if ( !pirq )\n            break;\n        if ( !is_hvm_domain(d1) )\n            pirq_guest_unbind(d1, pirq);\n        pirq->evtchn = 0;\n        pirq_cleanup_check(pirq, d1);\n        unlink_pirq_port(chn1, d1->vcpu[chn1->notify_vcpu_id]);\n#ifdef CONFIG_X86\n        if ( is_hvm_domain(d1) && domain_pirq_to_irq(d1, pirq->pirq) > 0 )\n            unmap_domain_pirq_emuirq(d1, pirq->pirq);\n#endif\n        break;\n    }\n\n    case ECS_VIRQ:\n        for_each_vcpu ( d1, v )\n        {\n            if ( v->virq_to_evtchn[chn1->u.virq] != port1 )\n                continue;\n            v->virq_to_evtchn[chn1->u.virq] = 0;\n            spin_barrier(&v->virq_lock);\n        }\n        break;\n\n    case ECS_IPI:\n        break;\n\n    case ECS_INTERDOMAIN:\n        if ( d2 == NULL )\n        {\n            d2 = chn1->u.interdomain.remote_dom;\n\n            /* If we unlock d1 then we could lose d2. Must get a reference. */\n            if ( unlikely(!get_domain(d2)) )\n                BUG();\n\n            if ( d1 < d2 )\n            {\n                spin_lock(&d2->event_lock);\n            }\n            else if ( d1 != d2 )\n            {\n                spin_unlock(&d1->event_lock);\n                spin_lock(&d2->event_lock);\n                goto again;\n            }\n        }\n        else if ( d2 != chn1->u.interdomain.remote_dom )\n        {\n            /*\n             * We can only get here if the port was closed and re-bound after\n             * unlocking d1 but before locking d2 above. We could retry but\n             * it is easier to return the same error as if we had seen the\n             * port in ECS_CLOSED. It must have passed through that state for\n             * us to end up here, so it's a valid error to return.\n             */\n            rc = -EINVAL;\n            goto out;\n        }\n\n        port2 = chn1->u.interdomain.remote_port;\n        BUG_ON(!port_is_valid(d2, port2));\n\n        chn2 = evtchn_from_port(d2, port2);\n        BUG_ON(chn2->state != ECS_INTERDOMAIN);\n        BUG_ON(chn2->u.interdomain.remote_dom != d1);\n\n        double_evtchn_lock(chn1, chn2);\n\n        evtchn_free(d1, chn1);\n\n        chn2->state = ECS_UNBOUND;\n        chn2->u.unbound.remote_domid = d1->domain_id;\n\n        double_evtchn_unlock(chn1, chn2);\n\n        goto out;\n\n    default:\n        BUG();\n    }\n\n    spin_lock(&chn1->lock);\n    evtchn_free(d1, chn1);\n    spin_unlock(&chn1->lock);\n\n out:\n    if ( d2 != NULL )\n    {\n        if ( d1 != d2 )\n            spin_unlock(&d2->event_lock);\n        put_domain(d2);\n    }\n\n    spin_unlock(&d1->event_lock);\n\n    return rc;\n}",
        "func": "int evtchn_close(struct domain *d1, int port1, bool guest)\n{\n    struct domain *d2 = NULL;\n    struct vcpu   *v;\n    struct evtchn *chn1, *chn2;\n    int            port2;\n    long           rc = 0;\n    unsigned long  flags;\n\n again:\n    spin_lock(&d1->event_lock);\n\n    if ( !port_is_valid(d1, port1) )\n    {\n        rc = -EINVAL;\n        goto out;\n    }\n\n    chn1 = evtchn_from_port(d1, port1);\n\n    /* Guest cannot close a Xen-attached event channel. */\n    if ( unlikely(consumer_is_xen(chn1)) && guest )\n    {\n        rc = -EINVAL;\n        goto out;\n    }\n\n    switch ( chn1->state )\n    {\n    case ECS_FREE:\n    case ECS_RESERVED:\n        rc = -EINVAL;\n        goto out;\n\n    case ECS_UNBOUND:\n        break;\n\n    case ECS_PIRQ: {\n        struct pirq *pirq = pirq_info(d1, chn1->u.pirq.irq);\n\n        if ( !pirq )\n            break;\n        if ( !is_hvm_domain(d1) )\n            pirq_guest_unbind(d1, pirq);\n        pirq->evtchn = 0;\n        pirq_cleanup_check(pirq, d1);\n        unlink_pirq_port(chn1, d1->vcpu[chn1->notify_vcpu_id]);\n#ifdef CONFIG_X86\n        if ( is_hvm_domain(d1) && domain_pirq_to_irq(d1, pirq->pirq) > 0 )\n            unmap_domain_pirq_emuirq(d1, pirq->pirq);\n#endif\n        break;\n    }\n\n    case ECS_VIRQ:\n        for_each_vcpu ( d1, v )\n        {\n            if ( v->virq_to_evtchn[chn1->u.virq] != port1 )\n                continue;\n            v->virq_to_evtchn[chn1->u.virq] = 0;\n            spin_barrier(&v->virq_lock);\n        }\n        break;\n\n    case ECS_IPI:\n        break;\n\n    case ECS_INTERDOMAIN:\n        if ( d2 == NULL )\n        {\n            d2 = chn1->u.interdomain.remote_dom;\n\n            /* If we unlock d1 then we could lose d2. Must get a reference. */\n            if ( unlikely(!get_domain(d2)) )\n                BUG();\n\n            if ( d1 < d2 )\n            {\n                spin_lock(&d2->event_lock);\n            }\n            else if ( d1 != d2 )\n            {\n                spin_unlock(&d1->event_lock);\n                spin_lock(&d2->event_lock);\n                goto again;\n            }\n        }\n        else if ( d2 != chn1->u.interdomain.remote_dom )\n        {\n            /*\n             * We can only get here if the port was closed and re-bound after\n             * unlocking d1 but before locking d2 above. We could retry but\n             * it is easier to return the same error as if we had seen the\n             * port in ECS_CLOSED. It must have passed through that state for\n             * us to end up here, so it's a valid error to return.\n             */\n            rc = -EINVAL;\n            goto out;\n        }\n\n        port2 = chn1->u.interdomain.remote_port;\n        BUG_ON(!port_is_valid(d2, port2));\n\n        chn2 = evtchn_from_port(d2, port2);\n        BUG_ON(chn2->state != ECS_INTERDOMAIN);\n        BUG_ON(chn2->u.interdomain.remote_dom != d1);\n\n        flags = double_evtchn_lock(chn1, chn2);\n\n        evtchn_free(d1, chn1);\n\n        chn2->state = ECS_UNBOUND;\n        chn2->u.unbound.remote_domid = d1->domain_id;\n\n        double_evtchn_unlock(chn1, chn2, flags);\n\n        goto out;\n\n    default:\n        BUG();\n    }\n\n    spin_lock_irqsave(&chn1->lock, flags);\n    evtchn_free(d1, chn1);\n    spin_unlock_irqrestore(&chn1->lock, flags);\n\n out:\n    if ( d2 != NULL )\n    {\n        if ( d1 != d2 )\n            spin_unlock(&d2->event_lock);\n        put_domain(d2);\n    }\n\n    spin_unlock(&d1->event_lock);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n     struct evtchn *chn1, *chn2;\n     int            port2;\n     long           rc = 0;\n+    unsigned long  flags;\n \n  again:\n     spin_lock(&d1->event_lock);\n@@ -104,14 +105,14 @@\n         BUG_ON(chn2->state != ECS_INTERDOMAIN);\n         BUG_ON(chn2->u.interdomain.remote_dom != d1);\n \n-        double_evtchn_lock(chn1, chn2);\n+        flags = double_evtchn_lock(chn1, chn2);\n \n         evtchn_free(d1, chn1);\n \n         chn2->state = ECS_UNBOUND;\n         chn2->u.unbound.remote_domid = d1->domain_id;\n \n-        double_evtchn_unlock(chn1, chn2);\n+        double_evtchn_unlock(chn1, chn2, flags);\n \n         goto out;\n \n@@ -119,9 +120,9 @@\n         BUG();\n     }\n \n-    spin_lock(&chn1->lock);\n+    spin_lock_irqsave(&chn1->lock, flags);\n     evtchn_free(d1, chn1);\n-    spin_unlock(&chn1->lock);\n+    spin_unlock_irqrestore(&chn1->lock, flags);\n \n  out:\n     if ( d2 != NULL )",
        "diff_line_info": {
            "deleted_lines": [
                "        double_evtchn_lock(chn1, chn2);",
                "        double_evtchn_unlock(chn1, chn2);",
                "    spin_lock(&chn1->lock);",
                "    spin_unlock(&chn1->lock);"
            ],
            "added_lines": [
                "    unsigned long  flags;",
                "        flags = double_evtchn_lock(chn1, chn2);",
                "        double_evtchn_unlock(chn1, chn2, flags);",
                "    spin_lock_irqsave(&chn1->lock, flags);",
                "    spin_unlock_irqrestore(&chn1->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/notify_via_xen_event_channel",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "void notify_via_xen_event_channel(struct domain *ld, int lport)\n{\n    struct evtchn *lchn, *rchn;\n    struct domain *rd;\n\n    ASSERT(port_is_valid(ld, lport));\n    lchn = evtchn_from_port(ld, lport);\n\n    spin_lock(&lchn->lock);\n\n    if ( likely(lchn->state == ECS_INTERDOMAIN) )\n    {\n        ASSERT(consumer_is_xen(lchn));\n        rd    = lchn->u.interdomain.remote_dom;\n        rchn  = evtchn_from_port(rd, lchn->u.interdomain.remote_port);\n        evtchn_port_set_pending(rd, rchn->notify_vcpu_id, rchn);\n    }\n\n    spin_unlock(&lchn->lock);\n}",
        "func": "void notify_via_xen_event_channel(struct domain *ld, int lport)\n{\n    struct evtchn *lchn, *rchn;\n    struct domain *rd;\n    unsigned long flags;\n\n    ASSERT(port_is_valid(ld, lport));\n    lchn = evtchn_from_port(ld, lport);\n\n    spin_lock_irqsave(&lchn->lock, flags);\n\n    if ( likely(lchn->state == ECS_INTERDOMAIN) )\n    {\n        ASSERT(consumer_is_xen(lchn));\n        rd    = lchn->u.interdomain.remote_dom;\n        rchn  = evtchn_from_port(rd, lchn->u.interdomain.remote_port);\n        evtchn_port_set_pending(rd, rchn->notify_vcpu_id, rchn);\n    }\n\n    spin_unlock_irqrestore(&lchn->lock, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,11 +2,12 @@\n {\n     struct evtchn *lchn, *rchn;\n     struct domain *rd;\n+    unsigned long flags;\n \n     ASSERT(port_is_valid(ld, lport));\n     lchn = evtchn_from_port(ld, lport);\n \n-    spin_lock(&lchn->lock);\n+    spin_lock_irqsave(&lchn->lock, flags);\n \n     if ( likely(lchn->state == ECS_INTERDOMAIN) )\n     {\n@@ -16,5 +17,5 @@\n         evtchn_port_set_pending(rd, rchn->notify_vcpu_id, rchn);\n     }\n \n-    spin_unlock(&lchn->lock);\n+    spin_unlock_irqrestore(&lchn->lock, flags);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&lchn->lock);",
                "    spin_unlock(&lchn->lock);"
            ],
            "added_lines": [
                "    unsigned long flags;",
                "    spin_lock_irqsave(&lchn->lock, flags);",
                "    spin_unlock_irqrestore(&lchn->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25599",
        "func_name": "xen-project/xen/evtchn_bind_interdomain",
        "description": "An issue was discovered in Xen through 4.14.x. There are evtchn_reset() race conditions. Uses of EVTCHNOP_reset (potentially by a guest on itself) or XEN_DOMCTL_soft_reset (by itself covered by XSA-77) can lead to the violation of various internal assumptions. This may lead to out of bounds memory accesses or triggering of bug checks. In particular, x86 PV guests may be able to elevate their privilege to that of the host. Host and guest crashes are also possible, leading to a Denial of Service (DoS). Information leaks cannot be ruled out. All Xen versions from 4.5 onwards are vulnerable. Xen versions 4.4 and earlier are not vulnerable.",
        "git_url": "https://github.com/xen-project/xen/commit/c0ddc8634845aba50774add6e4b73fdaffc82656",
        "commit_title": "evtchn: convert per-channel lock to be IRQ-safe",
        "commit_text": " ... in order for send_guest_{global,vcpu}_virq() to be able to make use of it.  This is part of XSA-343. ",
        "func_before": "static long evtchn_bind_interdomain(evtchn_bind_interdomain_t *bind)\n{\n    struct evtchn *lchn, *rchn;\n    struct domain *ld = current->domain, *rd;\n    int            lport, rport = bind->remote_port;\n    domid_t        rdom = bind->remote_dom;\n    long           rc;\n\n    if ( rdom == DOMID_SELF )\n        rdom = current->domain->domain_id;\n\n    if ( (rd = rcu_lock_domain_by_id(rdom)) == NULL )\n        return -ESRCH;\n\n    /* Avoid deadlock by first acquiring lock of domain with smaller id. */\n    if ( ld < rd )\n    {\n        spin_lock(&ld->event_lock);\n        spin_lock(&rd->event_lock);\n    }\n    else\n    {\n        if ( ld != rd )\n            spin_lock(&rd->event_lock);\n        spin_lock(&ld->event_lock);\n    }\n\n    if ( (lport = get_free_port(ld)) < 0 )\n        ERROR_EXIT(lport);\n    lchn = evtchn_from_port(ld, lport);\n\n    if ( !port_is_valid(rd, rport) )\n        ERROR_EXIT_DOM(-EINVAL, rd);\n    rchn = evtchn_from_port(rd, rport);\n    if ( (rchn->state != ECS_UNBOUND) ||\n         (rchn->u.unbound.remote_domid != ld->domain_id) )\n        ERROR_EXIT_DOM(-EINVAL, rd);\n\n    rc = xsm_evtchn_interdomain(XSM_HOOK, ld, lchn, rd, rchn);\n    if ( rc )\n        goto out;\n\n    double_evtchn_lock(lchn, rchn);\n\n    lchn->u.interdomain.remote_dom  = rd;\n    lchn->u.interdomain.remote_port = rport;\n    lchn->state                     = ECS_INTERDOMAIN;\n    evtchn_port_init(ld, lchn);\n    \n    rchn->u.interdomain.remote_dom  = ld;\n    rchn->u.interdomain.remote_port = lport;\n    rchn->state                     = ECS_INTERDOMAIN;\n\n    /*\n     * We may have lost notifications on the remote unbound port. Fix that up\n     * here by conservatively always setting a notification on the local port.\n     */\n    evtchn_port_set_pending(ld, lchn->notify_vcpu_id, lchn);\n\n    double_evtchn_unlock(lchn, rchn);\n\n    bind->local_port = lport;\n\n out:\n    check_free_port(ld, lport);\n    spin_unlock(&ld->event_lock);\n    if ( ld != rd )\n        spin_unlock(&rd->event_lock);\n    \n    rcu_unlock_domain(rd);\n\n    return rc;\n}",
        "func": "static long evtchn_bind_interdomain(evtchn_bind_interdomain_t *bind)\n{\n    struct evtchn *lchn, *rchn;\n    struct domain *ld = current->domain, *rd;\n    int            lport, rport = bind->remote_port;\n    domid_t        rdom = bind->remote_dom;\n    long           rc;\n    unsigned long  flags;\n\n    if ( rdom == DOMID_SELF )\n        rdom = current->domain->domain_id;\n\n    if ( (rd = rcu_lock_domain_by_id(rdom)) == NULL )\n        return -ESRCH;\n\n    /* Avoid deadlock by first acquiring lock of domain with smaller id. */\n    if ( ld < rd )\n    {\n        spin_lock(&ld->event_lock);\n        spin_lock(&rd->event_lock);\n    }\n    else\n    {\n        if ( ld != rd )\n            spin_lock(&rd->event_lock);\n        spin_lock(&ld->event_lock);\n    }\n\n    if ( (lport = get_free_port(ld)) < 0 )\n        ERROR_EXIT(lport);\n    lchn = evtchn_from_port(ld, lport);\n\n    if ( !port_is_valid(rd, rport) )\n        ERROR_EXIT_DOM(-EINVAL, rd);\n    rchn = evtchn_from_port(rd, rport);\n    if ( (rchn->state != ECS_UNBOUND) ||\n         (rchn->u.unbound.remote_domid != ld->domain_id) )\n        ERROR_EXIT_DOM(-EINVAL, rd);\n\n    rc = xsm_evtchn_interdomain(XSM_HOOK, ld, lchn, rd, rchn);\n    if ( rc )\n        goto out;\n\n    flags = double_evtchn_lock(lchn, rchn);\n\n    lchn->u.interdomain.remote_dom  = rd;\n    lchn->u.interdomain.remote_port = rport;\n    lchn->state                     = ECS_INTERDOMAIN;\n    evtchn_port_init(ld, lchn);\n    \n    rchn->u.interdomain.remote_dom  = ld;\n    rchn->u.interdomain.remote_port = lport;\n    rchn->state                     = ECS_INTERDOMAIN;\n\n    /*\n     * We may have lost notifications on the remote unbound port. Fix that up\n     * here by conservatively always setting a notification on the local port.\n     */\n    evtchn_port_set_pending(ld, lchn->notify_vcpu_id, lchn);\n\n    double_evtchn_unlock(lchn, rchn, flags);\n\n    bind->local_port = lport;\n\n out:\n    check_free_port(ld, lport);\n    spin_unlock(&ld->event_lock);\n    if ( ld != rd )\n        spin_unlock(&rd->event_lock);\n    \n    rcu_unlock_domain(rd);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n     int            lport, rport = bind->remote_port;\n     domid_t        rdom = bind->remote_dom;\n     long           rc;\n+    unsigned long  flags;\n \n     if ( rdom == DOMID_SELF )\n         rdom = current->domain->domain_id;\n@@ -40,7 +41,7 @@\n     if ( rc )\n         goto out;\n \n-    double_evtchn_lock(lchn, rchn);\n+    flags = double_evtchn_lock(lchn, rchn);\n \n     lchn->u.interdomain.remote_dom  = rd;\n     lchn->u.interdomain.remote_port = rport;\n@@ -57,7 +58,7 @@\n      */\n     evtchn_port_set_pending(ld, lchn->notify_vcpu_id, lchn);\n \n-    double_evtchn_unlock(lchn, rchn);\n+    double_evtchn_unlock(lchn, rchn, flags);\n \n     bind->local_port = lport;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    double_evtchn_lock(lchn, rchn);",
                "    double_evtchn_unlock(lchn, rchn);"
            ],
            "added_lines": [
                "    unsigned long  flags;",
                "    flags = double_evtchn_lock(lchn, rchn);",
                "    double_evtchn_unlock(lchn, rchn, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/pt_restore_timer",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "void pt_restore_timer(struct vcpu *v)\n{\n    struct list_head *head = &v->arch.hvm.tm_list;\n    struct periodic_time *pt;\n\n    spin_lock(&v->arch.hvm.tm_lock);\n\n    list_for_each_entry ( pt, head, list )\n    {\n        if ( pt->pending_intr_nr == 0 )\n        {\n            pt_process_missed_ticks(pt);\n            set_timer(&pt->timer, pt->scheduled);\n        }\n    }\n\n    pt_thaw_time(v);\n\n    spin_unlock(&v->arch.hvm.tm_lock);\n}",
        "func": "void pt_restore_timer(struct vcpu *v)\n{\n    struct list_head *head = &v->arch.hvm.tm_list;\n    struct periodic_time *pt;\n\n    pt_vcpu_lock(v);\n\n    list_for_each_entry ( pt, head, list )\n    {\n        if ( pt->pending_intr_nr == 0 )\n        {\n            pt_process_missed_ticks(pt);\n            set_timer(&pt->timer, pt->scheduled);\n        }\n    }\n\n    pt_thaw_time(v);\n\n    pt_vcpu_unlock(v);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n     struct list_head *head = &v->arch.hvm.tm_list;\n     struct periodic_time *pt;\n \n-    spin_lock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_lock(v);\n \n     list_for_each_entry ( pt, head, list )\n     {\n@@ -16,5 +16,5 @@\n \n     pt_thaw_time(v);\n \n-    spin_unlock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_unlock(v);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&v->arch.hvm.tm_lock);",
                "    spin_unlock(&v->arch.hvm.tm_lock);"
            ],
            "added_lines": [
                "    pt_vcpu_lock(v);",
                "    pt_vcpu_unlock(v);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/pt_adjust_vcpu",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "static void pt_adjust_vcpu(struct periodic_time *pt, struct vcpu *v)\n{\n    int on_list;\n\n    ASSERT(pt->source == PTSRC_isa || pt->source == PTSRC_ioapic);\n\n    if ( pt->vcpu == NULL )\n        return;\n\n    pt_lock(pt);\n    on_list = pt->on_list;\n    if ( pt->on_list )\n        list_del(&pt->list);\n    pt->on_list = 0;\n    pt_unlock(pt);\n\n    spin_lock(&v->arch.hvm.tm_lock);\n    pt->vcpu = v;\n    if ( on_list )\n    {\n        pt->on_list = 1;\n        list_add(&pt->list, &v->arch.hvm.tm_list);\n\n        migrate_timer(&pt->timer, v->processor);\n    }\n    spin_unlock(&v->arch.hvm.tm_lock);\n}",
        "func": "static void pt_adjust_vcpu(struct periodic_time *pt, struct vcpu *v)\n{\n    ASSERT(pt->source == PTSRC_isa || pt->source == PTSRC_ioapic);\n\n    if ( pt->vcpu == NULL )\n        return;\n\n    write_lock(&pt->vcpu->domain->arch.hvm.pl_time->pt_migrate);\n    pt->vcpu = v;\n    if ( pt->on_list )\n    {\n        list_del(&pt->list);\n        list_add(&pt->list, &v->arch.hvm.tm_list);\n        migrate_timer(&pt->timer, v->processor);\n    }\n    write_unlock(&pt->vcpu->domain->arch.hvm.pl_time->pt_migrate);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,27 +1,17 @@\n static void pt_adjust_vcpu(struct periodic_time *pt, struct vcpu *v)\n {\n-    int on_list;\n-\n     ASSERT(pt->source == PTSRC_isa || pt->source == PTSRC_ioapic);\n \n     if ( pt->vcpu == NULL )\n         return;\n \n-    pt_lock(pt);\n-    on_list = pt->on_list;\n+    write_lock(&pt->vcpu->domain->arch.hvm.pl_time->pt_migrate);\n+    pt->vcpu = v;\n     if ( pt->on_list )\n+    {\n         list_del(&pt->list);\n-    pt->on_list = 0;\n-    pt_unlock(pt);\n-\n-    spin_lock(&v->arch.hvm.tm_lock);\n-    pt->vcpu = v;\n-    if ( on_list )\n-    {\n-        pt->on_list = 1;\n         list_add(&pt->list, &v->arch.hvm.tm_list);\n-\n         migrate_timer(&pt->timer, v->processor);\n     }\n-    spin_unlock(&v->arch.hvm.tm_lock);\n+    write_unlock(&pt->vcpu->domain->arch.hvm.pl_time->pt_migrate);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    int on_list;",
                "",
                "    pt_lock(pt);",
                "    on_list = pt->on_list;",
                "    pt->on_list = 0;",
                "    pt_unlock(pt);",
                "",
                "    spin_lock(&v->arch.hvm.tm_lock);",
                "    pt->vcpu = v;",
                "    if ( on_list )",
                "    {",
                "        pt->on_list = 1;",
                "",
                "    spin_unlock(&v->arch.hvm.tm_lock);"
            ],
            "added_lines": [
                "    write_lock(&pt->vcpu->domain->arch.hvm.pl_time->pt_migrate);",
                "    pt->vcpu = v;",
                "    {",
                "    write_unlock(&pt->vcpu->domain->arch.hvm.pl_time->pt_migrate);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/pt_intr_post",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "void pt_intr_post(struct vcpu *v, struct hvm_intack intack)\n{\n    struct periodic_time *pt;\n    time_cb *cb;\n    void *cb_priv;\n\n    if ( intack.source == hvm_intsrc_vector )\n        return;\n\n    spin_lock(&v->arch.hvm.tm_lock);\n\n    pt = is_pt_irq(v, intack);\n    if ( pt == NULL )\n    {\n        spin_unlock(&v->arch.hvm.tm_lock);\n        return;\n    }\n\n    pt_irq_fired(v, pt);\n\n    cb = pt->cb;\n    cb_priv = pt->priv;\n\n    spin_unlock(&v->arch.hvm.tm_lock);\n\n    if ( cb != NULL )\n        cb(v, cb_priv);\n}",
        "func": "void pt_intr_post(struct vcpu *v, struct hvm_intack intack)\n{\n    struct periodic_time *pt;\n    time_cb *cb;\n    void *cb_priv;\n\n    if ( intack.source == hvm_intsrc_vector )\n        return;\n\n    pt_vcpu_lock(v);\n\n    pt = is_pt_irq(v, intack);\n    if ( pt == NULL )\n    {\n        pt_vcpu_unlock(v);\n        return;\n    }\n\n    pt_irq_fired(v, pt);\n\n    cb = pt->cb;\n    cb_priv = pt->priv;\n\n    pt_vcpu_unlock(v);\n\n    if ( cb != NULL )\n        cb(v, cb_priv);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,12 +7,12 @@\n     if ( intack.source == hvm_intsrc_vector )\n         return;\n \n-    spin_lock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_lock(v);\n \n     pt = is_pt_irq(v, intack);\n     if ( pt == NULL )\n     {\n-        spin_unlock(&v->arch.hvm.tm_lock);\n+        pt_vcpu_unlock(v);\n         return;\n     }\n \n@@ -21,7 +21,7 @@\n     cb = pt->cb;\n     cb_priv = pt->priv;\n \n-    spin_unlock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_unlock(v);\n \n     if ( cb != NULL )\n         cb(v, cb_priv);",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&v->arch.hvm.tm_lock);",
                "        spin_unlock(&v->arch.hvm.tm_lock);",
                "    spin_unlock(&v->arch.hvm.tm_lock);"
            ],
            "added_lines": [
                "    pt_vcpu_lock(v);",
                "        pt_vcpu_unlock(v);",
                "    pt_vcpu_unlock(v);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/pt_migrate",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "void pt_migrate(struct vcpu *v)\n{\n    struct list_head *head = &v->arch.hvm.tm_list;\n    struct periodic_time *pt;\n\n    spin_lock(&v->arch.hvm.tm_lock);\n\n    list_for_each_entry ( pt, head, list )\n        migrate_timer(&pt->timer, v->processor);\n\n    spin_unlock(&v->arch.hvm.tm_lock);\n}",
        "func": "void pt_migrate(struct vcpu *v)\n{\n    struct list_head *head = &v->arch.hvm.tm_list;\n    struct periodic_time *pt;\n\n    pt_vcpu_lock(v);\n\n    list_for_each_entry ( pt, head, list )\n        migrate_timer(&pt->timer, v->processor);\n\n    pt_vcpu_unlock(v);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,10 +3,10 @@\n     struct list_head *head = &v->arch.hvm.tm_list;\n     struct periodic_time *pt;\n \n-    spin_lock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_lock(v);\n \n     list_for_each_entry ( pt, head, list )\n         migrate_timer(&pt->timer, v->processor);\n \n-    spin_unlock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_unlock(v);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&v->arch.hvm.tm_lock);",
                "    spin_unlock(&v->arch.hvm.tm_lock);"
            ],
            "added_lines": [
                "    pt_vcpu_lock(v);",
                "    pt_vcpu_unlock(v);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/create_periodic_time",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "void create_periodic_time(\n    struct vcpu *v, struct periodic_time *pt, uint64_t delta,\n    uint64_t period, uint8_t irq, time_cb *cb, void *data, bool level)\n{\n    if ( !pt->source ||\n         (irq >= NR_ISAIRQS && pt->source == PTSRC_isa) ||\n         (level && period) ||\n         (pt->source == PTSRC_ioapic ? irq >= hvm_domain_irq(v->domain)->nr_gsis\n                                     : level) )\n    {\n        ASSERT_UNREACHABLE();\n        return;\n    }\n\n    destroy_periodic_time(pt);\n\n    spin_lock(&v->arch.hvm.tm_lock);\n\n    pt->pending_intr_nr = 0;\n    pt->do_not_freeze = 0;\n    pt->irq_issued = 0;\n\n    /* Periodic timer must be at least 0.1ms. */\n    if ( (period < 100000) && period )\n    {\n        if ( !test_and_set_bool(pt->warned_timeout_too_short) )\n            gdprintk(XENLOG_WARNING, \"HVM_PlatformTime: program too \"\n                     \"small period %\"PRIu64\"\\n\", period);\n        period = 100000;\n    }\n\n    pt->period = period;\n    pt->vcpu = v;\n    pt->last_plt_gtime = hvm_get_guest_time(pt->vcpu);\n    pt->irq = irq;\n    pt->one_shot = !period;\n    pt->level = level;\n    pt->scheduled = NOW() + delta;\n\n    if ( !pt->one_shot )\n    {\n        if ( v->domain->arch.hvm.params[HVM_PARAM_VPT_ALIGN] )\n        {\n            pt->scheduled = align_timer(pt->scheduled, pt->period);\n        }\n        else if ( pt->source == PTSRC_lapic )\n        {\n            /*\n             * Offset LAPIC ticks from other timer ticks. Otherwise guests\n             * which use LAPIC ticks for process accounting can see long\n             * sequences of process ticks incorrectly accounted to interrupt\n             * processing (seen with RHEL3 guest).\n             */\n            pt->scheduled += delta >> 1;\n        }\n    }\n\n    pt->cb = cb;\n    pt->priv = data;\n\n    pt->on_list = 1;\n    list_add(&pt->list, &v->arch.hvm.tm_list);\n\n    init_timer(&pt->timer, pt_timer_fn, pt, v->processor);\n    set_timer(&pt->timer, pt->scheduled);\n\n    spin_unlock(&v->arch.hvm.tm_lock);\n}",
        "func": "void create_periodic_time(\n    struct vcpu *v, struct periodic_time *pt, uint64_t delta,\n    uint64_t period, uint8_t irq, time_cb *cb, void *data, bool level)\n{\n    if ( !pt->source ||\n         (irq >= NR_ISAIRQS && pt->source == PTSRC_isa) ||\n         (level && period) ||\n         (pt->source == PTSRC_ioapic ? irq >= hvm_domain_irq(v->domain)->nr_gsis\n                                     : level) )\n    {\n        ASSERT_UNREACHABLE();\n        return;\n    }\n\n    destroy_periodic_time(pt);\n\n    write_lock(&v->domain->arch.hvm.pl_time->pt_migrate);\n\n    pt->pending_intr_nr = 0;\n    pt->do_not_freeze = 0;\n    pt->irq_issued = 0;\n\n    /* Periodic timer must be at least 0.1ms. */\n    if ( (period < 100000) && period )\n    {\n        if ( !test_and_set_bool(pt->warned_timeout_too_short) )\n            gdprintk(XENLOG_WARNING, \"HVM_PlatformTime: program too \"\n                     \"small period %\"PRIu64\"\\n\", period);\n        period = 100000;\n    }\n\n    pt->period = period;\n    pt->vcpu = v;\n    pt->last_plt_gtime = hvm_get_guest_time(pt->vcpu);\n    pt->irq = irq;\n    pt->one_shot = !period;\n    pt->level = level;\n    pt->scheduled = NOW() + delta;\n\n    if ( !pt->one_shot )\n    {\n        if ( v->domain->arch.hvm.params[HVM_PARAM_VPT_ALIGN] )\n        {\n            pt->scheduled = align_timer(pt->scheduled, pt->period);\n        }\n        else if ( pt->source == PTSRC_lapic )\n        {\n            /*\n             * Offset LAPIC ticks from other timer ticks. Otherwise guests\n             * which use LAPIC ticks for process accounting can see long\n             * sequences of process ticks incorrectly accounted to interrupt\n             * processing (seen with RHEL3 guest).\n             */\n            pt->scheduled += delta >> 1;\n        }\n    }\n\n    pt->cb = cb;\n    pt->priv = data;\n\n    pt->on_list = 1;\n    list_add(&pt->list, &v->arch.hvm.tm_list);\n\n    init_timer(&pt->timer, pt_timer_fn, pt, v->processor);\n    set_timer(&pt->timer, pt->scheduled);\n\n    write_unlock(&v->domain->arch.hvm.pl_time->pt_migrate);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,7 @@\n \n     destroy_periodic_time(pt);\n \n-    spin_lock(&v->arch.hvm.tm_lock);\n+    write_lock(&v->domain->arch.hvm.pl_time->pt_migrate);\n \n     pt->pending_intr_nr = 0;\n     pt->do_not_freeze = 0;\n@@ -64,5 +64,5 @@\n     init_timer(&pt->timer, pt_timer_fn, pt, v->processor);\n     set_timer(&pt->timer, pt->scheduled);\n \n-    spin_unlock(&v->arch.hvm.tm_lock);\n+    write_unlock(&v->domain->arch.hvm.pl_time->pt_migrate);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&v->arch.hvm.tm_lock);",
                "    spin_unlock(&v->arch.hvm.tm_lock);"
            ],
            "added_lines": [
                "    write_lock(&v->domain->arch.hvm.pl_time->pt_migrate);",
                "    write_unlock(&v->domain->arch.hvm.pl_time->pt_migrate);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/pt_lock",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "static void pt_lock(struct periodic_time *pt)\n{\n    struct vcpu *v;\n\n    for ( ; ; )\n    {\n        v = pt->vcpu;\n        spin_lock(&v->arch.hvm.tm_lock);\n        if ( likely(pt->vcpu == v) )\n            break;\n        spin_unlock(&v->arch.hvm.tm_lock);\n    }\n}",
        "func": "static void pt_lock(struct periodic_time *pt)\n{\n    /*\n     * We cannot use pt_vcpu_lock here, because we need to acquire the\n     * per-domain lock first and then (re-)fetch the value of pt->vcpu, or\n     * else we might be using a stale value of pt->vcpu.\n     */\n    read_lock(&pt->vcpu->domain->arch.hvm.pl_time->pt_migrate);\n    spin_lock(&pt->vcpu->arch.hvm.tm_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,13 +1,10 @@\n static void pt_lock(struct periodic_time *pt)\n {\n-    struct vcpu *v;\n-\n-    for ( ; ; )\n-    {\n-        v = pt->vcpu;\n-        spin_lock(&v->arch.hvm.tm_lock);\n-        if ( likely(pt->vcpu == v) )\n-            break;\n-        spin_unlock(&v->arch.hvm.tm_lock);\n-    }\n+    /*\n+     * We cannot use pt_vcpu_lock here, because we need to acquire the\n+     * per-domain lock first and then (re-)fetch the value of pt->vcpu, or\n+     * else we might be using a stale value of pt->vcpu.\n+     */\n+    read_lock(&pt->vcpu->domain->arch.hvm.pl_time->pt_migrate);\n+    spin_lock(&pt->vcpu->arch.hvm.tm_lock);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    struct vcpu *v;",
                "",
                "    for ( ; ; )",
                "    {",
                "        v = pt->vcpu;",
                "        spin_lock(&v->arch.hvm.tm_lock);",
                "        if ( likely(pt->vcpu == v) )",
                "            break;",
                "        spin_unlock(&v->arch.hvm.tm_lock);",
                "    }"
            ],
            "added_lines": [
                "    /*",
                "     * We cannot use pt_vcpu_lock here, because we need to acquire the",
                "     * per-domain lock first and then (re-)fetch the value of pt->vcpu, or",
                "     * else we might be using a stale value of pt->vcpu.",
                "     */",
                "    read_lock(&pt->vcpu->domain->arch.hvm.pl_time->pt_migrate);",
                "    spin_lock(&pt->vcpu->arch.hvm.tm_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/pt_unlock",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "static void pt_unlock(struct periodic_time *pt)\n{\n    spin_unlock(&pt->vcpu->arch.hvm.tm_lock);\n}",
        "func": "static void pt_unlock(struct periodic_time *pt)\n{\n    pt_vcpu_unlock(pt->vcpu);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n static void pt_unlock(struct periodic_time *pt)\n {\n-    spin_unlock(&pt->vcpu->arch.hvm.tm_lock);\n+    pt_vcpu_unlock(pt->vcpu);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_unlock(&pt->vcpu->arch.hvm.tm_lock);"
            ],
            "added_lines": [
                "    pt_vcpu_unlock(pt->vcpu);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/pt_save_timer",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "void pt_save_timer(struct vcpu *v)\n{\n    struct list_head *head = &v->arch.hvm.tm_list;\n    struct periodic_time *pt;\n\n    if ( v->pause_flags & VPF_blocked )\n        return;\n\n    spin_lock(&v->arch.hvm.tm_lock);\n\n    list_for_each_entry ( pt, head, list )\n        if ( !pt->do_not_freeze )\n            stop_timer(&pt->timer);\n\n    pt_freeze_time(v);\n\n    spin_unlock(&v->arch.hvm.tm_lock);\n}",
        "func": "void pt_save_timer(struct vcpu *v)\n{\n    struct list_head *head = &v->arch.hvm.tm_list;\n    struct periodic_time *pt;\n\n    if ( v->pause_flags & VPF_blocked )\n        return;\n\n    pt_vcpu_lock(v);\n\n    list_for_each_entry ( pt, head, list )\n        if ( !pt->do_not_freeze )\n            stop_timer(&pt->timer);\n\n    pt_freeze_time(v);\n\n    pt_vcpu_unlock(v);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n     if ( v->pause_flags & VPF_blocked )\n         return;\n \n-    spin_lock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_lock(v);\n \n     list_for_each_entry ( pt, head, list )\n         if ( !pt->do_not_freeze )\n@@ -14,5 +14,5 @@\n \n     pt_freeze_time(v);\n \n-    spin_unlock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_unlock(v);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&v->arch.hvm.tm_lock);",
                "    spin_unlock(&v->arch.hvm.tm_lock);"
            ],
            "added_lines": [
                "    pt_vcpu_lock(v);",
                "    pt_vcpu_unlock(v);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/pt_update_irq",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "int pt_update_irq(struct vcpu *v)\n{\n    struct list_head *head = &v->arch.hvm.tm_list;\n    struct periodic_time *pt, *temp, *earliest_pt;\n    uint64_t max_lag;\n    int irq, pt_vector = -1;\n    bool level;\n\n    spin_lock(&v->arch.hvm.tm_lock);\n\n    earliest_pt = NULL;\n    max_lag = -1ULL;\n    list_for_each_entry_safe ( pt, temp, head, list )\n    {\n        if ( pt->pending_intr_nr )\n        {\n            /* RTC code takes care of disabling the timer itself. */\n            if ( (pt->irq != RTC_IRQ || !pt->priv) && pt_irq_masked(pt) &&\n                 /* Level interrupts should be asserted even if masked. */\n                 !pt->level )\n            {\n                /* suspend timer emulation */\n                list_del(&pt->list);\n                pt->on_list = 0;\n            }\n            else\n            {\n                if ( (pt->last_plt_gtime + pt->period) < max_lag )\n                {\n                    max_lag = pt->last_plt_gtime + pt->period;\n                    earliest_pt = pt;\n                }\n            }\n        }\n    }\n\n    if ( earliest_pt == NULL )\n    {\n        spin_unlock(&v->arch.hvm.tm_lock);\n        return -1;\n    }\n\n    earliest_pt->irq_issued = 1;\n    irq = earliest_pt->irq;\n    level = earliest_pt->level;\n\n    spin_unlock(&v->arch.hvm.tm_lock);\n\n    switch ( earliest_pt->source )\n    {\n    case PTSRC_lapic:\n        /*\n         * If periodic timer interrupt is handled by lapic, its vector in\n         * IRR is returned and used to set eoi_exit_bitmap for virtual\n         * interrupt delivery case. Otherwise return -1 to do nothing.\n         */\n        vlapic_set_irq(vcpu_vlapic(v), irq, 0);\n        pt_vector = irq;\n        break;\n\n    case PTSRC_isa:\n        hvm_isa_irq_deassert(v->domain, irq);\n        if ( platform_legacy_irq(irq) && vlapic_accept_pic_intr(v) &&\n             v->domain->arch.hvm.vpic[irq >> 3].int_output )\n            hvm_isa_irq_assert(v->domain, irq, NULL);\n        else\n        {\n            pt_vector = hvm_isa_irq_assert(v->domain, irq, vioapic_get_vector);\n            /*\n             * hvm_isa_irq_assert may not set the corresponding bit in vIRR\n             * when mask field of IOAPIC RTE is set. Check it again.\n             */\n            if ( pt_vector < 0 || !vlapic_test_irq(vcpu_vlapic(v), pt_vector) )\n                pt_vector = -1;\n        }\n        break;\n\n    case PTSRC_ioapic:\n        pt_vector = hvm_ioapic_assert(v->domain, irq, level);\n        if ( pt_vector < 0 || !vlapic_test_irq(vcpu_vlapic(v), pt_vector) )\n        {\n            pt_vector = -1;\n            if ( level )\n            {\n                /*\n                 * Level interrupts are always asserted because the pin assert\n                 * count is incremented regardless of whether the pin is masked\n                 * or the vector latched in IRR, so also execute the callback\n                 * associated with the timer.\n                 */\n                time_cb *cb = NULL;\n                void *cb_priv;\n\n                spin_lock(&v->arch.hvm.tm_lock);\n                /* Make sure the timer is still on the list. */\n                list_for_each_entry ( pt, &v->arch.hvm.tm_list, list )\n                    if ( pt == earliest_pt )\n                    {\n                        pt_irq_fired(v, pt);\n                        cb = pt->cb;\n                        cb_priv = pt->priv;\n                        break;\n                    }\n                spin_unlock(&v->arch.hvm.tm_lock);\n\n                if ( cb != NULL )\n                    cb(v, cb_priv);\n            }\n        }\n        break;\n    }\n\n    return pt_vector;\n}",
        "func": "int pt_update_irq(struct vcpu *v)\n{\n    struct list_head *head = &v->arch.hvm.tm_list;\n    struct periodic_time *pt, *temp, *earliest_pt;\n    uint64_t max_lag;\n    int irq, pt_vector = -1;\n    bool level;\n\n    pt_vcpu_lock(v);\n\n    earliest_pt = NULL;\n    max_lag = -1ULL;\n    list_for_each_entry_safe ( pt, temp, head, list )\n    {\n        if ( pt->pending_intr_nr )\n        {\n            /* RTC code takes care of disabling the timer itself. */\n            if ( (pt->irq != RTC_IRQ || !pt->priv) && pt_irq_masked(pt) &&\n                 /* Level interrupts should be asserted even if masked. */\n                 !pt->level )\n            {\n                /* suspend timer emulation */\n                list_del(&pt->list);\n                pt->on_list = 0;\n            }\n            else\n            {\n                if ( (pt->last_plt_gtime + pt->period) < max_lag )\n                {\n                    max_lag = pt->last_plt_gtime + pt->period;\n                    earliest_pt = pt;\n                }\n            }\n        }\n    }\n\n    if ( earliest_pt == NULL )\n    {\n        pt_vcpu_unlock(v);\n        return -1;\n    }\n\n    earliest_pt->irq_issued = 1;\n    irq = earliest_pt->irq;\n    level = earliest_pt->level;\n\n    pt_vcpu_unlock(v);\n\n    switch ( earliest_pt->source )\n    {\n    case PTSRC_lapic:\n        /*\n         * If periodic timer interrupt is handled by lapic, its vector in\n         * IRR is returned and used to set eoi_exit_bitmap for virtual\n         * interrupt delivery case. Otherwise return -1 to do nothing.\n         */\n        vlapic_set_irq(vcpu_vlapic(v), irq, 0);\n        pt_vector = irq;\n        break;\n\n    case PTSRC_isa:\n        hvm_isa_irq_deassert(v->domain, irq);\n        if ( platform_legacy_irq(irq) && vlapic_accept_pic_intr(v) &&\n             v->domain->arch.hvm.vpic[irq >> 3].int_output )\n            hvm_isa_irq_assert(v->domain, irq, NULL);\n        else\n        {\n            pt_vector = hvm_isa_irq_assert(v->domain, irq, vioapic_get_vector);\n            /*\n             * hvm_isa_irq_assert may not set the corresponding bit in vIRR\n             * when mask field of IOAPIC RTE is set. Check it again.\n             */\n            if ( pt_vector < 0 || !vlapic_test_irq(vcpu_vlapic(v), pt_vector) )\n                pt_vector = -1;\n        }\n        break;\n\n    case PTSRC_ioapic:\n        pt_vector = hvm_ioapic_assert(v->domain, irq, level);\n        if ( pt_vector < 0 || !vlapic_test_irq(vcpu_vlapic(v), pt_vector) )\n        {\n            pt_vector = -1;\n            if ( level )\n            {\n                /*\n                 * Level interrupts are always asserted because the pin assert\n                 * count is incremented regardless of whether the pin is masked\n                 * or the vector latched in IRR, so also execute the callback\n                 * associated with the timer.\n                 */\n                time_cb *cb = NULL;\n                void *cb_priv;\n\n                pt_vcpu_lock(v);\n                /* Make sure the timer is still on the list. */\n                list_for_each_entry ( pt, &v->arch.hvm.tm_list, list )\n                    if ( pt == earliest_pt )\n                    {\n                        pt_irq_fired(v, pt);\n                        cb = pt->cb;\n                        cb_priv = pt->priv;\n                        break;\n                    }\n                pt_vcpu_unlock(v);\n\n                if ( cb != NULL )\n                    cb(v, cb_priv);\n            }\n        }\n        break;\n    }\n\n    return pt_vector;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n     int irq, pt_vector = -1;\n     bool level;\n \n-    spin_lock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_lock(v);\n \n     earliest_pt = NULL;\n     max_lag = -1ULL;\n@@ -36,7 +36,7 @@\n \n     if ( earliest_pt == NULL )\n     {\n-        spin_unlock(&v->arch.hvm.tm_lock);\n+        pt_vcpu_unlock(v);\n         return -1;\n     }\n \n@@ -44,7 +44,7 @@\n     irq = earliest_pt->irq;\n     level = earliest_pt->level;\n \n-    spin_unlock(&v->arch.hvm.tm_lock);\n+    pt_vcpu_unlock(v);\n \n     switch ( earliest_pt->source )\n     {\n@@ -91,7 +91,7 @@\n                 time_cb *cb = NULL;\n                 void *cb_priv;\n \n-                spin_lock(&v->arch.hvm.tm_lock);\n+                pt_vcpu_lock(v);\n                 /* Make sure the timer is still on the list. */\n                 list_for_each_entry ( pt, &v->arch.hvm.tm_list, list )\n                     if ( pt == earliest_pt )\n@@ -101,7 +101,7 @@\n                         cb_priv = pt->priv;\n                         break;\n                     }\n-                spin_unlock(&v->arch.hvm.tm_lock);\n+                pt_vcpu_unlock(v);\n \n                 if ( cb != NULL )\n                     cb(v, cb_priv);",
        "diff_line_info": {
            "deleted_lines": [
                "    spin_lock(&v->arch.hvm.tm_lock);",
                "        spin_unlock(&v->arch.hvm.tm_lock);",
                "    spin_unlock(&v->arch.hvm.tm_lock);",
                "                spin_lock(&v->arch.hvm.tm_lock);",
                "                spin_unlock(&v->arch.hvm.tm_lock);"
            ],
            "added_lines": [
                "    pt_vcpu_lock(v);",
                "        pt_vcpu_unlock(v);",
                "    pt_vcpu_unlock(v);",
                "                pt_vcpu_lock(v);",
                "                pt_vcpu_unlock(v);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25604",
        "func_name": "xen-project/xen/hvm_domain_initialise",
        "description": "An issue was discovered in Xen through 4.14.x. There is a race condition when migrating timers between x86 HVM vCPUs. When migrating timers of x86 HVM guests between its vCPUs, the locking model used allows for a second vCPU of the same guest (also operating on the timers) to release a lock that it didn't acquire. The most likely effect of the issue is a hang or crash of the hypervisor, i.e., a Denial of Service (DoS). All versions of Xen are affected. Only x86 systems are vulnerable. Arm systems are not vulnerable. Only x86 HVM guests can leverage the vulnerability. x86 PV and PVH cannot leverage the vulnerability. Only guests with more than one vCPU can exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/8e76aef72820435e766c7f339ed36da33da90c40",
        "commit_title": "x86/vpt: fix race when migrating timers between vCPUs",
        "commit_text": " The current vPT code will migrate the emulated timers between vCPUs (change the pt->vcpu field) while just holding the destination lock, either from create_periodic_time or pt_adjust_global_vcpu_target if the global target is adjusted. Changing the periodic_timer vCPU field in this way creates a race where a third party could grab the lock in the unlocked region of pt_adjust_global_vcpu_target (or before create_periodic_time performs the vcpu change) and then release the lock from a different vCPU, creating a locking imbalance.  Introduce a per-domain rwlock in order to protect periodic_time migration between vCPU lists. Taking the lock in read mode prevents any timer from being migrated to a different vCPU, while taking it in write mode allows performing migration of timers across vCPUs. The per-vcpu locks are still used to protect all the other fields from the periodic_timer struct.  Note that such migration shouldn't happen frequently, and hence there's no performance drop as a result of such locking.  This is XSA-336. ",
        "func_before": "int hvm_domain_initialise(struct domain *d)\n{\n    unsigned int nr_gsis;\n    int rc;\n\n    if ( !hvm_enabled )\n    {\n        gdprintk(XENLOG_WARNING, \"Attempt to create a HVM guest \"\n                 \"on a non-VT/AMDV platform.\\n\");\n        return -EINVAL;\n    }\n\n    spin_lock_init(&d->arch.hvm.irq_lock);\n    spin_lock_init(&d->arch.hvm.uc_lock);\n    spin_lock_init(&d->arch.hvm.write_map.lock);\n    rwlock_init(&d->arch.hvm.mmcfg_lock);\n    INIT_LIST_HEAD(&d->arch.hvm.write_map.list);\n    INIT_LIST_HEAD(&d->arch.hvm.g2m_ioport_list);\n    INIT_LIST_HEAD(&d->arch.hvm.mmcfg_regions);\n    INIT_LIST_HEAD(&d->arch.hvm.msix_tables);\n\n    rc = create_perdomain_mapping(d, PERDOMAIN_VIRT_START, 0, NULL, NULL);\n    if ( rc )\n        goto fail;\n\n    hvm_init_cacheattr_region_list(d);\n\n    rc = paging_enable(d, PG_refcounts|PG_translate|PG_external);\n    if ( rc != 0 )\n        goto fail0;\n\n    nr_gsis = is_hardware_domain(d) ? nr_irqs_gsi : NR_HVM_DOMU_IRQS;\n    d->arch.hvm.pl_time = xzalloc(struct pl_time);\n    d->arch.hvm.params = xzalloc_array(uint64_t, HVM_NR_PARAMS);\n    d->arch.hvm.io_handler = xzalloc_array(struct hvm_io_handler,\n                                           NR_IO_HANDLERS);\n    d->arch.hvm.irq = xzalloc_bytes(hvm_irq_size(nr_gsis));\n\n    rc = -ENOMEM;\n    if ( !d->arch.hvm.pl_time || !d->arch.hvm.irq ||\n         !d->arch.hvm.params  || !d->arch.hvm.io_handler )\n        goto fail1;\n\n    /* Set the number of GSIs */\n    hvm_domain_irq(d)->nr_gsis = nr_gsis;\n\n    BUILD_BUG_ON(NR_HVM_DOMU_IRQS < NR_ISAIRQS);\n    ASSERT(hvm_domain_irq(d)->nr_gsis >= NR_ISAIRQS);\n\n    /* need link to containing domain */\n    d->arch.hvm.pl_time->domain = d;\n\n    /* Set the default IO Bitmap. */\n    if ( is_hardware_domain(d) )\n    {\n        d->arch.hvm.io_bitmap = _xmalloc(HVM_IOBITMAP_SIZE, PAGE_SIZE);\n        if ( d->arch.hvm.io_bitmap == NULL )\n        {\n            rc = -ENOMEM;\n            goto fail1;\n        }\n        memset(d->arch.hvm.io_bitmap, ~0, HVM_IOBITMAP_SIZE);\n    }\n    else\n        d->arch.hvm.io_bitmap = hvm_io_bitmap;\n\n    register_g2m_portio_handler(d);\n    register_vpci_portio_handler(d);\n\n    hvm_ioreq_init(d);\n\n    hvm_init_guest_time(d);\n\n    d->arch.hvm.params[HVM_PARAM_TRIPLE_FAULT_REASON] = SHUTDOWN_reboot;\n\n    vpic_init(d);\n\n    rc = vioapic_init(d);\n    if ( rc != 0 )\n        goto fail1;\n\n    stdvga_init(d);\n\n    rtc_init(d);\n\n    register_portio_handler(d, XEN_HVM_DEBUGCONS_IOPORT, 1, hvm_print_line);\n\n    if ( hvm_tsc_scaling_supported )\n        d->arch.hvm.tsc_scaling_ratio = hvm_default_tsc_scaling_ratio;\n\n    rc = viridian_domain_init(d);\n    if ( rc )\n        goto fail2;\n\n    rc = hvm_funcs.domain_initialise(d);\n    if ( rc != 0 )\n        goto fail2;\n\n    return 0;\n\n fail2:\n    stdvga_deinit(d);\n    vioapic_deinit(d);\n fail1:\n    if ( is_hardware_domain(d) )\n        xfree(d->arch.hvm.io_bitmap);\n    XFREE(d->arch.hvm.io_handler);\n    XFREE(d->arch.hvm.params);\n    XFREE(d->arch.hvm.pl_time);\n    XFREE(d->arch.hvm.irq);\n fail0:\n    hvm_destroy_cacheattr_region_list(d);\n    destroy_perdomain_mapping(d, PERDOMAIN_VIRT_START, 0);\n fail:\n    hvm_domain_relinquish_resources(d);\n    return rc;\n}",
        "func": "int hvm_domain_initialise(struct domain *d)\n{\n    unsigned int nr_gsis;\n    int rc;\n\n    if ( !hvm_enabled )\n    {\n        gdprintk(XENLOG_WARNING, \"Attempt to create a HVM guest \"\n                 \"on a non-VT/AMDV platform.\\n\");\n        return -EINVAL;\n    }\n\n    spin_lock_init(&d->arch.hvm.irq_lock);\n    spin_lock_init(&d->arch.hvm.uc_lock);\n    spin_lock_init(&d->arch.hvm.write_map.lock);\n    rwlock_init(&d->arch.hvm.mmcfg_lock);\n    INIT_LIST_HEAD(&d->arch.hvm.write_map.list);\n    INIT_LIST_HEAD(&d->arch.hvm.g2m_ioport_list);\n    INIT_LIST_HEAD(&d->arch.hvm.mmcfg_regions);\n    INIT_LIST_HEAD(&d->arch.hvm.msix_tables);\n\n    rc = create_perdomain_mapping(d, PERDOMAIN_VIRT_START, 0, NULL, NULL);\n    if ( rc )\n        goto fail;\n\n    hvm_init_cacheattr_region_list(d);\n\n    rc = paging_enable(d, PG_refcounts|PG_translate|PG_external);\n    if ( rc != 0 )\n        goto fail0;\n\n    nr_gsis = is_hardware_domain(d) ? nr_irqs_gsi : NR_HVM_DOMU_IRQS;\n    d->arch.hvm.pl_time = xzalloc(struct pl_time);\n    d->arch.hvm.params = xzalloc_array(uint64_t, HVM_NR_PARAMS);\n    d->arch.hvm.io_handler = xzalloc_array(struct hvm_io_handler,\n                                           NR_IO_HANDLERS);\n    d->arch.hvm.irq = xzalloc_bytes(hvm_irq_size(nr_gsis));\n\n    rc = -ENOMEM;\n    if ( !d->arch.hvm.pl_time || !d->arch.hvm.irq ||\n         !d->arch.hvm.params  || !d->arch.hvm.io_handler )\n        goto fail1;\n\n    /* Set the number of GSIs */\n    hvm_domain_irq(d)->nr_gsis = nr_gsis;\n\n    BUILD_BUG_ON(NR_HVM_DOMU_IRQS < NR_ISAIRQS);\n    ASSERT(hvm_domain_irq(d)->nr_gsis >= NR_ISAIRQS);\n\n    /* need link to containing domain */\n    d->arch.hvm.pl_time->domain = d;\n\n    rwlock_init(&d->arch.hvm.pl_time->pt_migrate);\n\n    /* Set the default IO Bitmap. */\n    if ( is_hardware_domain(d) )\n    {\n        d->arch.hvm.io_bitmap = _xmalloc(HVM_IOBITMAP_SIZE, PAGE_SIZE);\n        if ( d->arch.hvm.io_bitmap == NULL )\n        {\n            rc = -ENOMEM;\n            goto fail1;\n        }\n        memset(d->arch.hvm.io_bitmap, ~0, HVM_IOBITMAP_SIZE);\n    }\n    else\n        d->arch.hvm.io_bitmap = hvm_io_bitmap;\n\n    register_g2m_portio_handler(d);\n    register_vpci_portio_handler(d);\n\n    hvm_ioreq_init(d);\n\n    hvm_init_guest_time(d);\n\n    d->arch.hvm.params[HVM_PARAM_TRIPLE_FAULT_REASON] = SHUTDOWN_reboot;\n\n    vpic_init(d);\n\n    rc = vioapic_init(d);\n    if ( rc != 0 )\n        goto fail1;\n\n    stdvga_init(d);\n\n    rtc_init(d);\n\n    register_portio_handler(d, XEN_HVM_DEBUGCONS_IOPORT, 1, hvm_print_line);\n\n    if ( hvm_tsc_scaling_supported )\n        d->arch.hvm.tsc_scaling_ratio = hvm_default_tsc_scaling_ratio;\n\n    rc = viridian_domain_init(d);\n    if ( rc )\n        goto fail2;\n\n    rc = hvm_funcs.domain_initialise(d);\n    if ( rc != 0 )\n        goto fail2;\n\n    return 0;\n\n fail2:\n    stdvga_deinit(d);\n    vioapic_deinit(d);\n fail1:\n    if ( is_hardware_domain(d) )\n        xfree(d->arch.hvm.io_bitmap);\n    XFREE(d->arch.hvm.io_handler);\n    XFREE(d->arch.hvm.params);\n    XFREE(d->arch.hvm.pl_time);\n    XFREE(d->arch.hvm.irq);\n fail0:\n    hvm_destroy_cacheattr_region_list(d);\n    destroy_perdomain_mapping(d, PERDOMAIN_VIRT_START, 0);\n fail:\n    hvm_domain_relinquish_resources(d);\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -49,6 +49,8 @@\n \n     /* need link to containing domain */\n     d->arch.hvm.pl_time->domain = d;\n+\n+    rwlock_init(&d->arch.hvm.pl_time->pt_migrate);\n \n     /* Set the default IO Bitmap. */\n     if ( is_hardware_domain(d) )",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    rwlock_init(&d->arch.hvm.pl_time->pt_migrate);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27672",
        "func_name": "xen-project/xen/map_pages_to_xen",
        "description": "An issue was discovered in Xen through 4.14.x allowing x86 guest OS users to cause a host OS denial of service, achieve data corruption, or possibly gain privileges by exploiting a race condition that leads to a use-after-free involving 2MiB and 1GiB superpages.",
        "git_url": "https://github.com/xen-project/xen/commit/08e6c6f80b018878476adc2c4e5679d2ce5cb4b1",
        "commit_title": "x86/mm: Refactor map_pages_to_xen to have only a single exit path",
        "commit_text": " We will soon need to perform clean-ups before returning.  No functional change.  This is part of XSA-345. ",
        "func_before": "int map_pages_to_xen(\n    unsigned long virt,\n    mfn_t mfn,\n    unsigned long nr_mfns,\n    unsigned int flags)\n{\n    bool locking = system_state > SYS_STATE_boot;\n    l2_pgentry_t *pl2e, ol2e;\n    l1_pgentry_t *pl1e, ol1e;\n    unsigned int  i;\n\n#define flush_flags(oldf) do {                 \\\n    unsigned int o_ = (oldf);                  \\\n    if ( (o_) & _PAGE_GLOBAL )                 \\\n        flush_flags |= FLUSH_TLB_GLOBAL;       \\\n    if ( (flags & _PAGE_PRESENT) &&            \\\n         (((o_) ^ flags) & PAGE_CACHE_ATTRS) ) \\\n    {                                          \\\n        flush_flags |= FLUSH_CACHE;            \\\n        if ( virt >= DIRECTMAP_VIRT_START &&   \\\n             virt < HYPERVISOR_VIRT_END )      \\\n            flush_flags |= FLUSH_VA_VALID;     \\\n    }                                          \\\n} while (0)\n\n    while ( nr_mfns != 0 )\n    {\n        l3_pgentry_t ol3e, *pl3e = virt_to_xen_l3e(virt);\n\n        if ( !pl3e )\n            return -ENOMEM;\n        ol3e = *pl3e;\n\n        if ( cpu_has_page1gb &&\n             !(((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n               ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1)) &&\n             nr_mfns >= (1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) &&\n             !(flags & (_PAGE_PAT | MAP_SMALL_PAGES)) )\n        {\n            /* 1GB-page mapping. */\n            l3e_write_atomic(pl3e, l3e_from_mfn(mfn, l1f_to_lNf(flags)));\n\n            if ( (l3e_get_flags(ol3e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(2 * PAGETABLE_ORDER);\n\n                if ( l3e_get_flags(ol3e) & _PAGE_PSE )\n                {\n                    flush_flags(lNf_to_l1f(l3e_get_flags(ol3e)));\n                    flush_area(virt, flush_flags);\n                }\n                else\n                {\n                    l2_pgentry_t *l2t = l3e_to_l2e(ol3e);\n\n                    for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                    {\n                        ol2e = l2t[i];\n                        if ( !(l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n                            continue;\n                        if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                            flush_flags(lNf_to_l1f(l2e_get_flags(ol2e)));\n                        else\n                        {\n                            unsigned int j;\n                            const l1_pgentry_t *l1t = l2e_to_l1e(ol2e);\n\n                            for ( j = 0; j < L1_PAGETABLE_ENTRIES; j++ )\n                                flush_flags(l1e_get_flags(l1t[j]));\n                        }\n                    }\n                    flush_area(virt, flush_flags);\n                    for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                    {\n                        ol2e = l2t[i];\n                        if ( (l2e_get_flags(ol2e) & _PAGE_PRESENT) &&\n                             !(l2e_get_flags(ol2e) & _PAGE_PSE) )\n                            free_xen_pagetable(l2e_to_l1e(ol2e));\n                    }\n                    free_xen_pagetable(l2t);\n                }\n            }\n\n            virt    += 1UL << L3_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn  = mfn_add(mfn, 1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT));\n            nr_mfns -= 1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT);\n            continue;\n        }\n\n        if ( (l3e_get_flags(ol3e) & _PAGE_PRESENT) &&\n             (l3e_get_flags(ol3e) & _PAGE_PSE) )\n        {\n            unsigned int flush_flags =\n                FLUSH_TLB | FLUSH_ORDER(2 * PAGETABLE_ORDER);\n            l2_pgentry_t *l2t;\n\n            /* Skip this PTE if there is no change. */\n            if ( ((l3e_get_pfn(ol3e) & ~(L2_PAGETABLE_ENTRIES *\n                                         L1_PAGETABLE_ENTRIES - 1)) +\n                  (l2_table_offset(virt) << PAGETABLE_ORDER) +\n                  l1_table_offset(virt) == mfn_x(mfn)) &&\n                 ((lNf_to_l1f(l3e_get_flags(ol3e)) ^ flags) &\n                  ~(_PAGE_ACCESSED|_PAGE_DIRTY)) == 0 )\n            {\n                /* We can skip to end of L3 superpage if we got a match. */\n                i = (1u << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) -\n                    (mfn_x(mfn) & ((1 << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1));\n                if ( i > nr_mfns )\n                    i = nr_mfns;\n                virt    += i << PAGE_SHIFT;\n                if ( !mfn_eq(mfn, INVALID_MFN) )\n                    mfn = mfn_add(mfn, i);\n                nr_mfns -= i;\n                continue;\n            }\n\n            l2t = alloc_xen_pagetable();\n            if ( l2t == NULL )\n                return -ENOMEM;\n\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                l2e_write(l2t + i,\n                          l2e_from_pfn(l3e_get_pfn(ol3e) +\n                                       (i << PAGETABLE_ORDER),\n                                       l3e_get_flags(ol3e)));\n\n            if ( l3e_get_flags(ol3e) & _PAGE_GLOBAL )\n                flush_flags |= FLUSH_TLB_GLOBAL;\n\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n            if ( (l3e_get_flags(*pl3e) & _PAGE_PRESENT) &&\n                 (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n            {\n                l3e_write_atomic(pl3e, l3e_from_mfn(virt_to_mfn(l2t),\n                                                    __PAGE_HYPERVISOR));\n                l2t = NULL;\n            }\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            flush_area(virt, flush_flags);\n            if ( l2t )\n                free_xen_pagetable(l2t);\n        }\n\n        pl2e = virt_to_xen_l2e(virt);\n        if ( !pl2e )\n            return -ENOMEM;\n\n        if ( ((((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n               ((1u << PAGETABLE_ORDER) - 1)) == 0) &&\n             (nr_mfns >= (1u << PAGETABLE_ORDER)) &&\n             !(flags & (_PAGE_PAT|MAP_SMALL_PAGES)) )\n        {\n            /* Super-page mapping. */\n            ol2e = *pl2e;\n            l2e_write_atomic(pl2e, l2e_from_mfn(mfn, l1f_to_lNf(flags)));\n\n            if ( (l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(PAGETABLE_ORDER);\n\n                if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                {\n                    flush_flags(lNf_to_l1f(l2e_get_flags(ol2e)));\n                    flush_area(virt, flush_flags);\n                }\n                else\n                {\n                    l1_pgentry_t *l1t = l2e_to_l1e(ol2e);\n\n                    for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                        flush_flags(l1e_get_flags(l1t[i]));\n                    flush_area(virt, flush_flags);\n                    free_xen_pagetable(l1t);\n                }\n            }\n\n            virt    += 1UL << L2_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn = mfn_add(mfn, 1UL << PAGETABLE_ORDER);\n            nr_mfns -= 1UL << PAGETABLE_ORDER;\n        }\n        else\n        {\n            /* Normal page mapping. */\n            if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n            {\n                pl1e = virt_to_xen_l1e(virt);\n                if ( pl1e == NULL )\n                    return -ENOMEM;\n            }\n            else if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(PAGETABLE_ORDER);\n                l1_pgentry_t *l1t;\n\n                /* Skip this PTE if there is no change. */\n                if ( (((l2e_get_pfn(*pl2e) & ~(L1_PAGETABLE_ENTRIES - 1)) +\n                       l1_table_offset(virt)) == mfn_x(mfn)) &&\n                     (((lNf_to_l1f(l2e_get_flags(*pl2e)) ^ flags) &\n                       ~(_PAGE_ACCESSED|_PAGE_DIRTY)) == 0) )\n                {\n                    /* We can skip to end of L2 superpage if we got a match. */\n                    i = (1u << (L2_PAGETABLE_SHIFT - PAGE_SHIFT)) -\n                        (mfn_x(mfn) & ((1u << (L2_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1));\n                    if ( i > nr_mfns )\n                        i = nr_mfns;\n                    virt    += i << L1_PAGETABLE_SHIFT;\n                    if ( !mfn_eq(mfn, INVALID_MFN) )\n                        mfn = mfn_add(mfn, i);\n                    nr_mfns -= i;\n                    goto check_l3;\n                }\n\n                l1t = alloc_xen_pagetable();\n                if ( l1t == NULL )\n                    return -ENOMEM;\n\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    l1e_write(&l1t[i],\n                              l1e_from_pfn(l2e_get_pfn(*pl2e) + i,\n                                           lNf_to_l1f(l2e_get_flags(*pl2e))));\n\n                if ( l2e_get_flags(*pl2e) & _PAGE_GLOBAL )\n                    flush_flags |= FLUSH_TLB_GLOBAL;\n\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n                if ( (l2e_get_flags(*pl2e) & _PAGE_PRESENT) &&\n                     (l2e_get_flags(*pl2e) & _PAGE_PSE) )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_mfn(virt_to_mfn(l1t),\n                                                        __PAGE_HYPERVISOR));\n                    l1t = NULL;\n                }\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(virt, flush_flags);\n                if ( l1t )\n                    free_xen_pagetable(l1t);\n            }\n\n            pl1e  = l2e_to_l1e(*pl2e) + l1_table_offset(virt);\n            ol1e  = *pl1e;\n            l1e_write_atomic(pl1e, l1e_from_mfn(mfn, flags));\n            if ( (l1e_get_flags(ol1e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags = FLUSH_TLB | FLUSH_ORDER(0);\n\n                flush_flags(l1e_get_flags(ol1e));\n                flush_area(virt, flush_flags);\n            }\n\n            virt    += 1UL << L1_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn = mfn_add(mfn, 1UL);\n            nr_mfns -= 1UL;\n\n            if ( (flags == PAGE_HYPERVISOR) &&\n                 ((nr_mfns == 0) ||\n                  ((((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n                    ((1u << PAGETABLE_ORDER) - 1)) == 0)) )\n            {\n                unsigned long base_mfn;\n                const l1_pgentry_t *l1t;\n\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n\n                ol2e = *pl2e;\n                /*\n                 * L2E may be already cleared, or set to a superpage, by\n                 * concurrent paging structure modifications on other CPUs.\n                 */\n                if ( !(l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n                {\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    continue;\n                }\n\n                if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                {\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    goto check_l3;\n                }\n\n                l1t = l2e_to_l1e(ol2e);\n                base_mfn = l1e_get_pfn(l1t[0]) & ~(L1_PAGETABLE_ENTRIES - 1);\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    if ( (l1e_get_pfn(l1t[i]) != (base_mfn + i)) ||\n                         (l1e_get_flags(l1t[i]) != flags) )\n                        break;\n                if ( i == L1_PAGETABLE_ENTRIES )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_pfn(base_mfn,\n                                                        l1f_to_lNf(flags)));\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    flush_area(virt - PAGE_SIZE,\n                               FLUSH_TLB_GLOBAL |\n                               FLUSH_ORDER(PAGETABLE_ORDER));\n                    free_xen_pagetable(l2e_to_l1e(ol2e));\n                }\n                else if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n            }\n        }\n\n check_l3:\n        if ( cpu_has_page1gb &&\n             (flags == PAGE_HYPERVISOR) &&\n             ((nr_mfns == 0) ||\n              !(((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n                ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1))) )\n        {\n            unsigned long base_mfn;\n            const l2_pgentry_t *l2t;\n\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n\n            ol3e = *pl3e;\n            /*\n             * L3E may be already cleared, or set to a superpage, by\n             * concurrent paging structure modifications on other CPUs.\n             */\n            if ( !(l3e_get_flags(ol3e) & _PAGE_PRESENT) ||\n                (l3e_get_flags(ol3e) & _PAGE_PSE) )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                continue;\n            }\n\n            l2t = l3e_to_l2e(ol3e);\n            base_mfn = l2e_get_pfn(l2t[0]) & ~(L2_PAGETABLE_ENTRIES *\n                                              L1_PAGETABLE_ENTRIES - 1);\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                if ( (l2e_get_pfn(l2t[i]) !=\n                      (base_mfn + (i << PAGETABLE_ORDER))) ||\n                     (l2e_get_flags(l2t[i]) != l1f_to_lNf(flags)) )\n                    break;\n            if ( i == L2_PAGETABLE_ENTRIES )\n            {\n                l3e_write_atomic(pl3e, l3e_from_pfn(base_mfn,\n                                                    l1f_to_lNf(flags)));\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(virt - PAGE_SIZE,\n                           FLUSH_TLB_GLOBAL |\n                           FLUSH_ORDER(2*PAGETABLE_ORDER));\n                free_xen_pagetable(l3e_to_l2e(ol3e));\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n    }\n\n#undef flush_flags\n\n    return 0;\n}",
        "func": "int map_pages_to_xen(\n    unsigned long virt,\n    mfn_t mfn,\n    unsigned long nr_mfns,\n    unsigned int flags)\n{\n    bool locking = system_state > SYS_STATE_boot;\n    l2_pgentry_t *pl2e, ol2e;\n    l1_pgentry_t *pl1e, ol1e;\n    unsigned int  i;\n    int rc = -ENOMEM;\n\n#define flush_flags(oldf) do {                 \\\n    unsigned int o_ = (oldf);                  \\\n    if ( (o_) & _PAGE_GLOBAL )                 \\\n        flush_flags |= FLUSH_TLB_GLOBAL;       \\\n    if ( (flags & _PAGE_PRESENT) &&            \\\n         (((o_) ^ flags) & PAGE_CACHE_ATTRS) ) \\\n    {                                          \\\n        flush_flags |= FLUSH_CACHE;            \\\n        if ( virt >= DIRECTMAP_VIRT_START &&   \\\n             virt < HYPERVISOR_VIRT_END )      \\\n            flush_flags |= FLUSH_VA_VALID;     \\\n    }                                          \\\n} while (0)\n\n    while ( nr_mfns != 0 )\n    {\n        l3_pgentry_t ol3e, *pl3e = virt_to_xen_l3e(virt);\n\n        if ( !pl3e )\n            goto out;\n\n        ol3e = *pl3e;\n\n        if ( cpu_has_page1gb &&\n             !(((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n               ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1)) &&\n             nr_mfns >= (1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) &&\n             !(flags & (_PAGE_PAT | MAP_SMALL_PAGES)) )\n        {\n            /* 1GB-page mapping. */\n            l3e_write_atomic(pl3e, l3e_from_mfn(mfn, l1f_to_lNf(flags)));\n\n            if ( (l3e_get_flags(ol3e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(2 * PAGETABLE_ORDER);\n\n                if ( l3e_get_flags(ol3e) & _PAGE_PSE )\n                {\n                    flush_flags(lNf_to_l1f(l3e_get_flags(ol3e)));\n                    flush_area(virt, flush_flags);\n                }\n                else\n                {\n                    l2_pgentry_t *l2t = l3e_to_l2e(ol3e);\n\n                    for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                    {\n                        ol2e = l2t[i];\n                        if ( !(l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n                            continue;\n                        if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                            flush_flags(lNf_to_l1f(l2e_get_flags(ol2e)));\n                        else\n                        {\n                            unsigned int j;\n                            const l1_pgentry_t *l1t = l2e_to_l1e(ol2e);\n\n                            for ( j = 0; j < L1_PAGETABLE_ENTRIES; j++ )\n                                flush_flags(l1e_get_flags(l1t[j]));\n                        }\n                    }\n                    flush_area(virt, flush_flags);\n                    for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                    {\n                        ol2e = l2t[i];\n                        if ( (l2e_get_flags(ol2e) & _PAGE_PRESENT) &&\n                             !(l2e_get_flags(ol2e) & _PAGE_PSE) )\n                            free_xen_pagetable(l2e_to_l1e(ol2e));\n                    }\n                    free_xen_pagetable(l2t);\n                }\n            }\n\n            virt    += 1UL << L3_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn  = mfn_add(mfn, 1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT));\n            nr_mfns -= 1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT);\n            continue;\n        }\n\n        if ( (l3e_get_flags(ol3e) & _PAGE_PRESENT) &&\n             (l3e_get_flags(ol3e) & _PAGE_PSE) )\n        {\n            unsigned int flush_flags =\n                FLUSH_TLB | FLUSH_ORDER(2 * PAGETABLE_ORDER);\n            l2_pgentry_t *l2t;\n\n            /* Skip this PTE if there is no change. */\n            if ( ((l3e_get_pfn(ol3e) & ~(L2_PAGETABLE_ENTRIES *\n                                         L1_PAGETABLE_ENTRIES - 1)) +\n                  (l2_table_offset(virt) << PAGETABLE_ORDER) +\n                  l1_table_offset(virt) == mfn_x(mfn)) &&\n                 ((lNf_to_l1f(l3e_get_flags(ol3e)) ^ flags) &\n                  ~(_PAGE_ACCESSED|_PAGE_DIRTY)) == 0 )\n            {\n                /* We can skip to end of L3 superpage if we got a match. */\n                i = (1u << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) -\n                    (mfn_x(mfn) & ((1 << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1));\n                if ( i > nr_mfns )\n                    i = nr_mfns;\n                virt    += i << PAGE_SHIFT;\n                if ( !mfn_eq(mfn, INVALID_MFN) )\n                    mfn = mfn_add(mfn, i);\n                nr_mfns -= i;\n                continue;\n            }\n\n            l2t = alloc_xen_pagetable();\n            if ( l2t == NULL )\n                goto out;\n\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                l2e_write(l2t + i,\n                          l2e_from_pfn(l3e_get_pfn(ol3e) +\n                                       (i << PAGETABLE_ORDER),\n                                       l3e_get_flags(ol3e)));\n\n            if ( l3e_get_flags(ol3e) & _PAGE_GLOBAL )\n                flush_flags |= FLUSH_TLB_GLOBAL;\n\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n            if ( (l3e_get_flags(*pl3e) & _PAGE_PRESENT) &&\n                 (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n            {\n                l3e_write_atomic(pl3e, l3e_from_mfn(virt_to_mfn(l2t),\n                                                    __PAGE_HYPERVISOR));\n                l2t = NULL;\n            }\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            flush_area(virt, flush_flags);\n            if ( l2t )\n                free_xen_pagetable(l2t);\n        }\n\n        pl2e = virt_to_xen_l2e(virt);\n        if ( !pl2e )\n            goto out;\n\n        if ( ((((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n               ((1u << PAGETABLE_ORDER) - 1)) == 0) &&\n             (nr_mfns >= (1u << PAGETABLE_ORDER)) &&\n             !(flags & (_PAGE_PAT|MAP_SMALL_PAGES)) )\n        {\n            /* Super-page mapping. */\n            ol2e = *pl2e;\n            l2e_write_atomic(pl2e, l2e_from_mfn(mfn, l1f_to_lNf(flags)));\n\n            if ( (l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(PAGETABLE_ORDER);\n\n                if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                {\n                    flush_flags(lNf_to_l1f(l2e_get_flags(ol2e)));\n                    flush_area(virt, flush_flags);\n                }\n                else\n                {\n                    l1_pgentry_t *l1t = l2e_to_l1e(ol2e);\n\n                    for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                        flush_flags(l1e_get_flags(l1t[i]));\n                    flush_area(virt, flush_flags);\n                    free_xen_pagetable(l1t);\n                }\n            }\n\n            virt    += 1UL << L2_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn = mfn_add(mfn, 1UL << PAGETABLE_ORDER);\n            nr_mfns -= 1UL << PAGETABLE_ORDER;\n        }\n        else\n        {\n            /* Normal page mapping. */\n            if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n            {\n                pl1e = virt_to_xen_l1e(virt);\n                if ( pl1e == NULL )\n                    goto out;\n            }\n            else if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(PAGETABLE_ORDER);\n                l1_pgentry_t *l1t;\n\n                /* Skip this PTE if there is no change. */\n                if ( (((l2e_get_pfn(*pl2e) & ~(L1_PAGETABLE_ENTRIES - 1)) +\n                       l1_table_offset(virt)) == mfn_x(mfn)) &&\n                     (((lNf_to_l1f(l2e_get_flags(*pl2e)) ^ flags) &\n                       ~(_PAGE_ACCESSED|_PAGE_DIRTY)) == 0) )\n                {\n                    /* We can skip to end of L2 superpage if we got a match. */\n                    i = (1u << (L2_PAGETABLE_SHIFT - PAGE_SHIFT)) -\n                        (mfn_x(mfn) & ((1u << (L2_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1));\n                    if ( i > nr_mfns )\n                        i = nr_mfns;\n                    virt    += i << L1_PAGETABLE_SHIFT;\n                    if ( !mfn_eq(mfn, INVALID_MFN) )\n                        mfn = mfn_add(mfn, i);\n                    nr_mfns -= i;\n                    goto check_l3;\n                }\n\n                l1t = alloc_xen_pagetable();\n                if ( l1t == NULL )\n                    goto out;\n\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    l1e_write(&l1t[i],\n                              l1e_from_pfn(l2e_get_pfn(*pl2e) + i,\n                                           lNf_to_l1f(l2e_get_flags(*pl2e))));\n\n                if ( l2e_get_flags(*pl2e) & _PAGE_GLOBAL )\n                    flush_flags |= FLUSH_TLB_GLOBAL;\n\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n                if ( (l2e_get_flags(*pl2e) & _PAGE_PRESENT) &&\n                     (l2e_get_flags(*pl2e) & _PAGE_PSE) )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_mfn(virt_to_mfn(l1t),\n                                                        __PAGE_HYPERVISOR));\n                    l1t = NULL;\n                }\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(virt, flush_flags);\n                if ( l1t )\n                    free_xen_pagetable(l1t);\n            }\n\n            pl1e  = l2e_to_l1e(*pl2e) + l1_table_offset(virt);\n            ol1e  = *pl1e;\n            l1e_write_atomic(pl1e, l1e_from_mfn(mfn, flags));\n            if ( (l1e_get_flags(ol1e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags = FLUSH_TLB | FLUSH_ORDER(0);\n\n                flush_flags(l1e_get_flags(ol1e));\n                flush_area(virt, flush_flags);\n            }\n\n            virt    += 1UL << L1_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn = mfn_add(mfn, 1UL);\n            nr_mfns -= 1UL;\n\n            if ( (flags == PAGE_HYPERVISOR) &&\n                 ((nr_mfns == 0) ||\n                  ((((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n                    ((1u << PAGETABLE_ORDER) - 1)) == 0)) )\n            {\n                unsigned long base_mfn;\n                const l1_pgentry_t *l1t;\n\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n\n                ol2e = *pl2e;\n                /*\n                 * L2E may be already cleared, or set to a superpage, by\n                 * concurrent paging structure modifications on other CPUs.\n                 */\n                if ( !(l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n                {\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    continue;\n                }\n\n                if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                {\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    goto check_l3;\n                }\n\n                l1t = l2e_to_l1e(ol2e);\n                base_mfn = l1e_get_pfn(l1t[0]) & ~(L1_PAGETABLE_ENTRIES - 1);\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    if ( (l1e_get_pfn(l1t[i]) != (base_mfn + i)) ||\n                         (l1e_get_flags(l1t[i]) != flags) )\n                        break;\n                if ( i == L1_PAGETABLE_ENTRIES )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_pfn(base_mfn,\n                                                        l1f_to_lNf(flags)));\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    flush_area(virt - PAGE_SIZE,\n                               FLUSH_TLB_GLOBAL |\n                               FLUSH_ORDER(PAGETABLE_ORDER));\n                    free_xen_pagetable(l2e_to_l1e(ol2e));\n                }\n                else if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n            }\n        }\n\n check_l3:\n        if ( cpu_has_page1gb &&\n             (flags == PAGE_HYPERVISOR) &&\n             ((nr_mfns == 0) ||\n              !(((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n                ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1))) )\n        {\n            unsigned long base_mfn;\n            const l2_pgentry_t *l2t;\n\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n\n            ol3e = *pl3e;\n            /*\n             * L3E may be already cleared, or set to a superpage, by\n             * concurrent paging structure modifications on other CPUs.\n             */\n            if ( !(l3e_get_flags(ol3e) & _PAGE_PRESENT) ||\n                (l3e_get_flags(ol3e) & _PAGE_PSE) )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                continue;\n            }\n\n            l2t = l3e_to_l2e(ol3e);\n            base_mfn = l2e_get_pfn(l2t[0]) & ~(L2_PAGETABLE_ENTRIES *\n                                              L1_PAGETABLE_ENTRIES - 1);\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                if ( (l2e_get_pfn(l2t[i]) !=\n                      (base_mfn + (i << PAGETABLE_ORDER))) ||\n                     (l2e_get_flags(l2t[i]) != l1f_to_lNf(flags)) )\n                    break;\n            if ( i == L2_PAGETABLE_ENTRIES )\n            {\n                l3e_write_atomic(pl3e, l3e_from_pfn(base_mfn,\n                                                    l1f_to_lNf(flags)));\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(virt - PAGE_SIZE,\n                           FLUSH_TLB_GLOBAL |\n                           FLUSH_ORDER(2*PAGETABLE_ORDER));\n                free_xen_pagetable(l3e_to_l2e(ol3e));\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n    }\n\n#undef flush_flags\n\n    rc = 0;\n\n out:\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,7 @@\n     l2_pgentry_t *pl2e, ol2e;\n     l1_pgentry_t *pl1e, ol1e;\n     unsigned int  i;\n+    int rc = -ENOMEM;\n \n #define flush_flags(oldf) do {                 \\\n     unsigned int o_ = (oldf);                  \\\n@@ -28,7 +29,8 @@\n         l3_pgentry_t ol3e, *pl3e = virt_to_xen_l3e(virt);\n \n         if ( !pl3e )\n-            return -ENOMEM;\n+            goto out;\n+\n         ol3e = *pl3e;\n \n         if ( cpu_has_page1gb &&\n@@ -118,7 +120,7 @@\n \n             l2t = alloc_xen_pagetable();\n             if ( l2t == NULL )\n-                return -ENOMEM;\n+                goto out;\n \n             for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                 l2e_write(l2t + i,\n@@ -147,7 +149,7 @@\n \n         pl2e = virt_to_xen_l2e(virt);\n         if ( !pl2e )\n-            return -ENOMEM;\n+            goto out;\n \n         if ( ((((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n                ((1u << PAGETABLE_ORDER) - 1)) == 0) &&\n@@ -191,7 +193,7 @@\n             {\n                 pl1e = virt_to_xen_l1e(virt);\n                 if ( pl1e == NULL )\n-                    return -ENOMEM;\n+                    goto out;\n             }\n             else if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n             {\n@@ -219,7 +221,7 @@\n \n                 l1t = alloc_xen_pagetable();\n                 if ( l1t == NULL )\n-                    return -ENOMEM;\n+                    goto out;\n \n                 for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                     l1e_write(&l1t[i],\n@@ -365,5 +367,8 @@\n \n #undef flush_flags\n \n-    return 0;\n+    rc = 0;\n+\n+ out:\n+    return rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "            return -ENOMEM;",
                "                return -ENOMEM;",
                "            return -ENOMEM;",
                "                    return -ENOMEM;",
                "                    return -ENOMEM;",
                "    return 0;"
            ],
            "added_lines": [
                "    int rc = -ENOMEM;",
                "            goto out;",
                "",
                "                goto out;",
                "            goto out;",
                "                    goto out;",
                "                    goto out;",
                "    rc = 0;",
                "",
                " out:",
                "    return rc;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27672",
        "func_name": "xen-project/xen/map_pages_to_xen",
        "description": "An issue was discovered in Xen through 4.14.x allowing x86 guest OS users to cause a host OS denial of service, achieve data corruption, or possibly gain privileges by exploiting a race condition that leads to a use-after-free involving 2MiB and 1GiB superpages.",
        "git_url": "https://github.com/xen-project/xen/commit/1ce75e99d75907aaffae05fcf658a833802bce49",
        "commit_title": "x86/mm: Prevent some races in hypervisor mapping updates",
        "commit_text": " map_pages_to_xen will attempt to coalesce mappings into 2MiB and 1GiB superpages if possible, to maximize TLB efficiency.  This means both replacing superpage entries with smaller entries, and replacing smaller entries with superpages.  Unfortunately, while some potential races are handled correctly, others are not.  These include:  1. When one processor modifies a sub-superpage mapping while another processor replaces the entire range with a superpage.  Take the following example:  Suppose L3[N] points to L2.  And suppose we have two processors, A and B.  * A walks the pagetables, get a pointer to L2. * B replaces L3[N] with a 1GiB mapping. * B Frees L2 * A writes L2[M] #  This is race exacerbated by the fact that virt_to_xen_l[21]e doesn't handle higher-level superpages properly: If you call virt_xen_to_l2e on a virtual address within an L3 superpage, you'll either hit a BUG() (most likely), or get a pointer into the middle of a data page; same with virt_xen_to_l1 on a virtual address within either an L3 or L2 superpage.  So take the following example:  * A reads pl3e and discovers it to point to an L2. * B replaces L3[N] with a 1GiB mapping * A calls virt_to_xen_l2e() and hits the BUG_ON() #  2. When two processors simultaneously try to replace a sub-superpage mapping with a superpage mapping.  Take the following example:  Suppose L3[N] points to L2.  And suppose we have two processors, A and B, both trying to replace L3[N] with a superpage.  * A walks the pagetables, get a pointer to pl3e, and takes a copy ol3e pointing to L2. * B walks the pagetables, gets a pointre to pl3e, and takes a copy ol3e pointing to L2. * A writes the new value into L3[N] * B writes the new value into L3[N] * A recursively frees all the L1's under L2, then frees L2 * B recursively double-frees all the L1's under L2, then double-frees L2 #  Fix this by grabbing a lock for the entirety of the mapping update operation.  Rather than grabbing map_pgdir_lock for the entire operation, however, repurpose the PGT_locked bit from L3's page->type_info as a lock. This means that rather than locking the entire address space, we \"only\" lock a single 512GiB chunk of hypervisor address space at a time.  There was a proposal for a lock-and-reverify approach, where we walk the pagetables to the point where we decide what to do; then grab the map_pgdir_lock, re-verify the information we collected without the lock, and finally make the change (starting over again if anything had changed).  Without being able to guarantee that the L2 table wasn't freed, however, that means every read would need to be considered potentially unsafe.  Thinking carefully about that is probably something that wants to be done on public, not under time pressure.  This is part of XSA-345. ",
        "func_before": "int map_pages_to_xen(\n    unsigned long virt,\n    mfn_t mfn,\n    unsigned long nr_mfns,\n    unsigned int flags)\n{\n    bool locking = system_state > SYS_STATE_boot;\n    l2_pgentry_t *pl2e, ol2e;\n    l1_pgentry_t *pl1e, ol1e;\n    unsigned int  i;\n    int rc = -ENOMEM;\n\n#define flush_flags(oldf) do {                 \\\n    unsigned int o_ = (oldf);                  \\\n    if ( (o_) & _PAGE_GLOBAL )                 \\\n        flush_flags |= FLUSH_TLB_GLOBAL;       \\\n    if ( (flags & _PAGE_PRESENT) &&            \\\n         (((o_) ^ flags) & PAGE_CACHE_ATTRS) ) \\\n    {                                          \\\n        flush_flags |= FLUSH_CACHE;            \\\n        if ( virt >= DIRECTMAP_VIRT_START &&   \\\n             virt < HYPERVISOR_VIRT_END )      \\\n            flush_flags |= FLUSH_VA_VALID;     \\\n    }                                          \\\n} while (0)\n\n    while ( nr_mfns != 0 )\n    {\n        l3_pgentry_t ol3e, *pl3e = virt_to_xen_l3e(virt);\n\n        if ( !pl3e )\n            goto out;\n\n        ol3e = *pl3e;\n\n        if ( cpu_has_page1gb &&\n             !(((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n               ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1)) &&\n             nr_mfns >= (1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) &&\n             !(flags & (_PAGE_PAT | MAP_SMALL_PAGES)) )\n        {\n            /* 1GB-page mapping. */\n            l3e_write_atomic(pl3e, l3e_from_mfn(mfn, l1f_to_lNf(flags)));\n\n            if ( (l3e_get_flags(ol3e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(2 * PAGETABLE_ORDER);\n\n                if ( l3e_get_flags(ol3e) & _PAGE_PSE )\n                {\n                    flush_flags(lNf_to_l1f(l3e_get_flags(ol3e)));\n                    flush_area(virt, flush_flags);\n                }\n                else\n                {\n                    l2_pgentry_t *l2t = l3e_to_l2e(ol3e);\n\n                    for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                    {\n                        ol2e = l2t[i];\n                        if ( !(l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n                            continue;\n                        if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                            flush_flags(lNf_to_l1f(l2e_get_flags(ol2e)));\n                        else\n                        {\n                            unsigned int j;\n                            const l1_pgentry_t *l1t = l2e_to_l1e(ol2e);\n\n                            for ( j = 0; j < L1_PAGETABLE_ENTRIES; j++ )\n                                flush_flags(l1e_get_flags(l1t[j]));\n                        }\n                    }\n                    flush_area(virt, flush_flags);\n                    for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                    {\n                        ol2e = l2t[i];\n                        if ( (l2e_get_flags(ol2e) & _PAGE_PRESENT) &&\n                             !(l2e_get_flags(ol2e) & _PAGE_PSE) )\n                            free_xen_pagetable(l2e_to_l1e(ol2e));\n                    }\n                    free_xen_pagetable(l2t);\n                }\n            }\n\n            virt    += 1UL << L3_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn  = mfn_add(mfn, 1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT));\n            nr_mfns -= 1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT);\n            continue;\n        }\n\n        if ( (l3e_get_flags(ol3e) & _PAGE_PRESENT) &&\n             (l3e_get_flags(ol3e) & _PAGE_PSE) )\n        {\n            unsigned int flush_flags =\n                FLUSH_TLB | FLUSH_ORDER(2 * PAGETABLE_ORDER);\n            l2_pgentry_t *l2t;\n\n            /* Skip this PTE if there is no change. */\n            if ( ((l3e_get_pfn(ol3e) & ~(L2_PAGETABLE_ENTRIES *\n                                         L1_PAGETABLE_ENTRIES - 1)) +\n                  (l2_table_offset(virt) << PAGETABLE_ORDER) +\n                  l1_table_offset(virt) == mfn_x(mfn)) &&\n                 ((lNf_to_l1f(l3e_get_flags(ol3e)) ^ flags) &\n                  ~(_PAGE_ACCESSED|_PAGE_DIRTY)) == 0 )\n            {\n                /* We can skip to end of L3 superpage if we got a match. */\n                i = (1u << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) -\n                    (mfn_x(mfn) & ((1 << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1));\n                if ( i > nr_mfns )\n                    i = nr_mfns;\n                virt    += i << PAGE_SHIFT;\n                if ( !mfn_eq(mfn, INVALID_MFN) )\n                    mfn = mfn_add(mfn, i);\n                nr_mfns -= i;\n                continue;\n            }\n\n            l2t = alloc_xen_pagetable();\n            if ( l2t == NULL )\n                goto out;\n\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                l2e_write(l2t + i,\n                          l2e_from_pfn(l3e_get_pfn(ol3e) +\n                                       (i << PAGETABLE_ORDER),\n                                       l3e_get_flags(ol3e)));\n\n            if ( l3e_get_flags(ol3e) & _PAGE_GLOBAL )\n                flush_flags |= FLUSH_TLB_GLOBAL;\n\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n            if ( (l3e_get_flags(*pl3e) & _PAGE_PRESENT) &&\n                 (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n            {\n                l3e_write_atomic(pl3e, l3e_from_mfn(virt_to_mfn(l2t),\n                                                    __PAGE_HYPERVISOR));\n                l2t = NULL;\n            }\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            flush_area(virt, flush_flags);\n            if ( l2t )\n                free_xen_pagetable(l2t);\n        }\n\n        pl2e = virt_to_xen_l2e(virt);\n        if ( !pl2e )\n            goto out;\n\n        if ( ((((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n               ((1u << PAGETABLE_ORDER) - 1)) == 0) &&\n             (nr_mfns >= (1u << PAGETABLE_ORDER)) &&\n             !(flags & (_PAGE_PAT|MAP_SMALL_PAGES)) )\n        {\n            /* Super-page mapping. */\n            ol2e = *pl2e;\n            l2e_write_atomic(pl2e, l2e_from_mfn(mfn, l1f_to_lNf(flags)));\n\n            if ( (l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(PAGETABLE_ORDER);\n\n                if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                {\n                    flush_flags(lNf_to_l1f(l2e_get_flags(ol2e)));\n                    flush_area(virt, flush_flags);\n                }\n                else\n                {\n                    l1_pgentry_t *l1t = l2e_to_l1e(ol2e);\n\n                    for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                        flush_flags(l1e_get_flags(l1t[i]));\n                    flush_area(virt, flush_flags);\n                    free_xen_pagetable(l1t);\n                }\n            }\n\n            virt    += 1UL << L2_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn = mfn_add(mfn, 1UL << PAGETABLE_ORDER);\n            nr_mfns -= 1UL << PAGETABLE_ORDER;\n        }\n        else\n        {\n            /* Normal page mapping. */\n            if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n            {\n                pl1e = virt_to_xen_l1e(virt);\n                if ( pl1e == NULL )\n                    goto out;\n            }\n            else if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(PAGETABLE_ORDER);\n                l1_pgentry_t *l1t;\n\n                /* Skip this PTE if there is no change. */\n                if ( (((l2e_get_pfn(*pl2e) & ~(L1_PAGETABLE_ENTRIES - 1)) +\n                       l1_table_offset(virt)) == mfn_x(mfn)) &&\n                     (((lNf_to_l1f(l2e_get_flags(*pl2e)) ^ flags) &\n                       ~(_PAGE_ACCESSED|_PAGE_DIRTY)) == 0) )\n                {\n                    /* We can skip to end of L2 superpage if we got a match. */\n                    i = (1u << (L2_PAGETABLE_SHIFT - PAGE_SHIFT)) -\n                        (mfn_x(mfn) & ((1u << (L2_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1));\n                    if ( i > nr_mfns )\n                        i = nr_mfns;\n                    virt    += i << L1_PAGETABLE_SHIFT;\n                    if ( !mfn_eq(mfn, INVALID_MFN) )\n                        mfn = mfn_add(mfn, i);\n                    nr_mfns -= i;\n                    goto check_l3;\n                }\n\n                l1t = alloc_xen_pagetable();\n                if ( l1t == NULL )\n                    goto out;\n\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    l1e_write(&l1t[i],\n                              l1e_from_pfn(l2e_get_pfn(*pl2e) + i,\n                                           lNf_to_l1f(l2e_get_flags(*pl2e))));\n\n                if ( l2e_get_flags(*pl2e) & _PAGE_GLOBAL )\n                    flush_flags |= FLUSH_TLB_GLOBAL;\n\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n                if ( (l2e_get_flags(*pl2e) & _PAGE_PRESENT) &&\n                     (l2e_get_flags(*pl2e) & _PAGE_PSE) )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_mfn(virt_to_mfn(l1t),\n                                                        __PAGE_HYPERVISOR));\n                    l1t = NULL;\n                }\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(virt, flush_flags);\n                if ( l1t )\n                    free_xen_pagetable(l1t);\n            }\n\n            pl1e  = l2e_to_l1e(*pl2e) + l1_table_offset(virt);\n            ol1e  = *pl1e;\n            l1e_write_atomic(pl1e, l1e_from_mfn(mfn, flags));\n            if ( (l1e_get_flags(ol1e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags = FLUSH_TLB | FLUSH_ORDER(0);\n\n                flush_flags(l1e_get_flags(ol1e));\n                flush_area(virt, flush_flags);\n            }\n\n            virt    += 1UL << L1_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn = mfn_add(mfn, 1UL);\n            nr_mfns -= 1UL;\n\n            if ( (flags == PAGE_HYPERVISOR) &&\n                 ((nr_mfns == 0) ||\n                  ((((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n                    ((1u << PAGETABLE_ORDER) - 1)) == 0)) )\n            {\n                unsigned long base_mfn;\n                const l1_pgentry_t *l1t;\n\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n\n                ol2e = *pl2e;\n                /*\n                 * L2E may be already cleared, or set to a superpage, by\n                 * concurrent paging structure modifications on other CPUs.\n                 */\n                if ( !(l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n                {\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    continue;\n                }\n\n                if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                {\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    goto check_l3;\n                }\n\n                l1t = l2e_to_l1e(ol2e);\n                base_mfn = l1e_get_pfn(l1t[0]) & ~(L1_PAGETABLE_ENTRIES - 1);\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    if ( (l1e_get_pfn(l1t[i]) != (base_mfn + i)) ||\n                         (l1e_get_flags(l1t[i]) != flags) )\n                        break;\n                if ( i == L1_PAGETABLE_ENTRIES )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_pfn(base_mfn,\n                                                        l1f_to_lNf(flags)));\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    flush_area(virt - PAGE_SIZE,\n                               FLUSH_TLB_GLOBAL |\n                               FLUSH_ORDER(PAGETABLE_ORDER));\n                    free_xen_pagetable(l2e_to_l1e(ol2e));\n                }\n                else if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n            }\n        }\n\n check_l3:\n        if ( cpu_has_page1gb &&\n             (flags == PAGE_HYPERVISOR) &&\n             ((nr_mfns == 0) ||\n              !(((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n                ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1))) )\n        {\n            unsigned long base_mfn;\n            const l2_pgentry_t *l2t;\n\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n\n            ol3e = *pl3e;\n            /*\n             * L3E may be already cleared, or set to a superpage, by\n             * concurrent paging structure modifications on other CPUs.\n             */\n            if ( !(l3e_get_flags(ol3e) & _PAGE_PRESENT) ||\n                (l3e_get_flags(ol3e) & _PAGE_PSE) )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                continue;\n            }\n\n            l2t = l3e_to_l2e(ol3e);\n            base_mfn = l2e_get_pfn(l2t[0]) & ~(L2_PAGETABLE_ENTRIES *\n                                              L1_PAGETABLE_ENTRIES - 1);\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                if ( (l2e_get_pfn(l2t[i]) !=\n                      (base_mfn + (i << PAGETABLE_ORDER))) ||\n                     (l2e_get_flags(l2t[i]) != l1f_to_lNf(flags)) )\n                    break;\n            if ( i == L2_PAGETABLE_ENTRIES )\n            {\n                l3e_write_atomic(pl3e, l3e_from_pfn(base_mfn,\n                                                    l1f_to_lNf(flags)));\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(virt - PAGE_SIZE,\n                           FLUSH_TLB_GLOBAL |\n                           FLUSH_ORDER(2*PAGETABLE_ORDER));\n                free_xen_pagetable(l3e_to_l2e(ol3e));\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n    }\n\n#undef flush_flags\n\n    rc = 0;\n\n out:\n    return rc;\n}",
        "func": "int map_pages_to_xen(\n    unsigned long virt,\n    mfn_t mfn,\n    unsigned long nr_mfns,\n    unsigned int flags)\n{\n    bool locking = system_state > SYS_STATE_boot;\n    l2_pgentry_t *pl2e, ol2e;\n    l1_pgentry_t *pl1e, ol1e;\n    unsigned int  i;\n    int rc = -ENOMEM;\n    struct page_info *current_l3page;\n\n#define flush_flags(oldf) do {                 \\\n    unsigned int o_ = (oldf);                  \\\n    if ( (o_) & _PAGE_GLOBAL )                 \\\n        flush_flags |= FLUSH_TLB_GLOBAL;       \\\n    if ( (flags & _PAGE_PRESENT) &&            \\\n         (((o_) ^ flags) & PAGE_CACHE_ATTRS) ) \\\n    {                                          \\\n        flush_flags |= FLUSH_CACHE;            \\\n        if ( virt >= DIRECTMAP_VIRT_START &&   \\\n             virt < HYPERVISOR_VIRT_END )      \\\n            flush_flags |= FLUSH_VA_VALID;     \\\n    }                                          \\\n} while (0)\n\n    L3T_INIT(current_l3page);\n\n    while ( nr_mfns != 0 )\n    {\n        l3_pgentry_t *pl3e, ol3e;\n\n        L3T_UNLOCK(current_l3page);\n\n        pl3e = virt_to_xen_l3e(virt);\n        if ( !pl3e )\n            goto out;\n\n        current_l3page = virt_to_page(pl3e);\n        L3T_LOCK(current_l3page);\n        ol3e = *pl3e;\n\n        if ( cpu_has_page1gb &&\n             !(((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n               ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1)) &&\n             nr_mfns >= (1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) &&\n             !(flags & (_PAGE_PAT | MAP_SMALL_PAGES)) )\n        {\n            /* 1GB-page mapping. */\n            l3e_write_atomic(pl3e, l3e_from_mfn(mfn, l1f_to_lNf(flags)));\n\n            if ( (l3e_get_flags(ol3e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(2 * PAGETABLE_ORDER);\n\n                if ( l3e_get_flags(ol3e) & _PAGE_PSE )\n                {\n                    flush_flags(lNf_to_l1f(l3e_get_flags(ol3e)));\n                    flush_area(virt, flush_flags);\n                }\n                else\n                {\n                    l2_pgentry_t *l2t = l3e_to_l2e(ol3e);\n\n                    for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                    {\n                        ol2e = l2t[i];\n                        if ( !(l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n                            continue;\n                        if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                            flush_flags(lNf_to_l1f(l2e_get_flags(ol2e)));\n                        else\n                        {\n                            unsigned int j;\n                            const l1_pgentry_t *l1t = l2e_to_l1e(ol2e);\n\n                            for ( j = 0; j < L1_PAGETABLE_ENTRIES; j++ )\n                                flush_flags(l1e_get_flags(l1t[j]));\n                        }\n                    }\n                    flush_area(virt, flush_flags);\n                    for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                    {\n                        ol2e = l2t[i];\n                        if ( (l2e_get_flags(ol2e) & _PAGE_PRESENT) &&\n                             !(l2e_get_flags(ol2e) & _PAGE_PSE) )\n                            free_xen_pagetable(l2e_to_l1e(ol2e));\n                    }\n                    free_xen_pagetable(l2t);\n                }\n            }\n\n            virt    += 1UL << L3_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn  = mfn_add(mfn, 1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT));\n            nr_mfns -= 1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT);\n            continue;\n        }\n\n        if ( (l3e_get_flags(ol3e) & _PAGE_PRESENT) &&\n             (l3e_get_flags(ol3e) & _PAGE_PSE) )\n        {\n            unsigned int flush_flags =\n                FLUSH_TLB | FLUSH_ORDER(2 * PAGETABLE_ORDER);\n            l2_pgentry_t *l2t;\n\n            /* Skip this PTE if there is no change. */\n            if ( ((l3e_get_pfn(ol3e) & ~(L2_PAGETABLE_ENTRIES *\n                                         L1_PAGETABLE_ENTRIES - 1)) +\n                  (l2_table_offset(virt) << PAGETABLE_ORDER) +\n                  l1_table_offset(virt) == mfn_x(mfn)) &&\n                 ((lNf_to_l1f(l3e_get_flags(ol3e)) ^ flags) &\n                  ~(_PAGE_ACCESSED|_PAGE_DIRTY)) == 0 )\n            {\n                /* We can skip to end of L3 superpage if we got a match. */\n                i = (1u << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) -\n                    (mfn_x(mfn) & ((1 << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1));\n                if ( i > nr_mfns )\n                    i = nr_mfns;\n                virt    += i << PAGE_SHIFT;\n                if ( !mfn_eq(mfn, INVALID_MFN) )\n                    mfn = mfn_add(mfn, i);\n                nr_mfns -= i;\n                continue;\n            }\n\n            l2t = alloc_xen_pagetable();\n            if ( l2t == NULL )\n                goto out;\n\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                l2e_write(l2t + i,\n                          l2e_from_pfn(l3e_get_pfn(ol3e) +\n                                       (i << PAGETABLE_ORDER),\n                                       l3e_get_flags(ol3e)));\n\n            if ( l3e_get_flags(ol3e) & _PAGE_GLOBAL )\n                flush_flags |= FLUSH_TLB_GLOBAL;\n\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n            if ( (l3e_get_flags(*pl3e) & _PAGE_PRESENT) &&\n                 (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n            {\n                l3e_write_atomic(pl3e, l3e_from_mfn(virt_to_mfn(l2t),\n                                                    __PAGE_HYPERVISOR));\n                l2t = NULL;\n            }\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            flush_area(virt, flush_flags);\n            if ( l2t )\n                free_xen_pagetable(l2t);\n        }\n\n        pl2e = virt_to_xen_l2e(virt);\n        if ( !pl2e )\n            goto out;\n\n        if ( ((((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n               ((1u << PAGETABLE_ORDER) - 1)) == 0) &&\n             (nr_mfns >= (1u << PAGETABLE_ORDER)) &&\n             !(flags & (_PAGE_PAT|MAP_SMALL_PAGES)) )\n        {\n            /* Super-page mapping. */\n            ol2e = *pl2e;\n            l2e_write_atomic(pl2e, l2e_from_mfn(mfn, l1f_to_lNf(flags)));\n\n            if ( (l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(PAGETABLE_ORDER);\n\n                if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                {\n                    flush_flags(lNf_to_l1f(l2e_get_flags(ol2e)));\n                    flush_area(virt, flush_flags);\n                }\n                else\n                {\n                    l1_pgentry_t *l1t = l2e_to_l1e(ol2e);\n\n                    for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                        flush_flags(l1e_get_flags(l1t[i]));\n                    flush_area(virt, flush_flags);\n                    free_xen_pagetable(l1t);\n                }\n            }\n\n            virt    += 1UL << L2_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn = mfn_add(mfn, 1UL << PAGETABLE_ORDER);\n            nr_mfns -= 1UL << PAGETABLE_ORDER;\n        }\n        else\n        {\n            /* Normal page mapping. */\n            if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n            {\n                pl1e = virt_to_xen_l1e(virt);\n                if ( pl1e == NULL )\n                    goto out;\n            }\n            else if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n            {\n                unsigned int flush_flags =\n                    FLUSH_TLB | FLUSH_ORDER(PAGETABLE_ORDER);\n                l1_pgentry_t *l1t;\n\n                /* Skip this PTE if there is no change. */\n                if ( (((l2e_get_pfn(*pl2e) & ~(L1_PAGETABLE_ENTRIES - 1)) +\n                       l1_table_offset(virt)) == mfn_x(mfn)) &&\n                     (((lNf_to_l1f(l2e_get_flags(*pl2e)) ^ flags) &\n                       ~(_PAGE_ACCESSED|_PAGE_DIRTY)) == 0) )\n                {\n                    /* We can skip to end of L2 superpage if we got a match. */\n                    i = (1u << (L2_PAGETABLE_SHIFT - PAGE_SHIFT)) -\n                        (mfn_x(mfn) & ((1u << (L2_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1));\n                    if ( i > nr_mfns )\n                        i = nr_mfns;\n                    virt    += i << L1_PAGETABLE_SHIFT;\n                    if ( !mfn_eq(mfn, INVALID_MFN) )\n                        mfn = mfn_add(mfn, i);\n                    nr_mfns -= i;\n                    goto check_l3;\n                }\n\n                l1t = alloc_xen_pagetable();\n                if ( l1t == NULL )\n                    goto out;\n\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    l1e_write(&l1t[i],\n                              l1e_from_pfn(l2e_get_pfn(*pl2e) + i,\n                                           lNf_to_l1f(l2e_get_flags(*pl2e))));\n\n                if ( l2e_get_flags(*pl2e) & _PAGE_GLOBAL )\n                    flush_flags |= FLUSH_TLB_GLOBAL;\n\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n                if ( (l2e_get_flags(*pl2e) & _PAGE_PRESENT) &&\n                     (l2e_get_flags(*pl2e) & _PAGE_PSE) )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_mfn(virt_to_mfn(l1t),\n                                                        __PAGE_HYPERVISOR));\n                    l1t = NULL;\n                }\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(virt, flush_flags);\n                if ( l1t )\n                    free_xen_pagetable(l1t);\n            }\n\n            pl1e  = l2e_to_l1e(*pl2e) + l1_table_offset(virt);\n            ol1e  = *pl1e;\n            l1e_write_atomic(pl1e, l1e_from_mfn(mfn, flags));\n            if ( (l1e_get_flags(ol1e) & _PAGE_PRESENT) )\n            {\n                unsigned int flush_flags = FLUSH_TLB | FLUSH_ORDER(0);\n\n                flush_flags(l1e_get_flags(ol1e));\n                flush_area(virt, flush_flags);\n            }\n\n            virt    += 1UL << L1_PAGETABLE_SHIFT;\n            if ( !mfn_eq(mfn, INVALID_MFN) )\n                mfn = mfn_add(mfn, 1UL);\n            nr_mfns -= 1UL;\n\n            if ( (flags == PAGE_HYPERVISOR) &&\n                 ((nr_mfns == 0) ||\n                  ((((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n                    ((1u << PAGETABLE_ORDER) - 1)) == 0)) )\n            {\n                unsigned long base_mfn;\n                const l1_pgentry_t *l1t;\n\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n\n                ol2e = *pl2e;\n                /*\n                 * L2E may be already cleared, or set to a superpage, by\n                 * concurrent paging structure modifications on other CPUs.\n                 */\n                if ( !(l2e_get_flags(ol2e) & _PAGE_PRESENT) )\n                {\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    continue;\n                }\n\n                if ( l2e_get_flags(ol2e) & _PAGE_PSE )\n                {\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    goto check_l3;\n                }\n\n                l1t = l2e_to_l1e(ol2e);\n                base_mfn = l1e_get_pfn(l1t[0]) & ~(L1_PAGETABLE_ENTRIES - 1);\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    if ( (l1e_get_pfn(l1t[i]) != (base_mfn + i)) ||\n                         (l1e_get_flags(l1t[i]) != flags) )\n                        break;\n                if ( i == L1_PAGETABLE_ENTRIES )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_pfn(base_mfn,\n                                                        l1f_to_lNf(flags)));\n                    if ( locking )\n                        spin_unlock(&map_pgdir_lock);\n                    flush_area(virt - PAGE_SIZE,\n                               FLUSH_TLB_GLOBAL |\n                               FLUSH_ORDER(PAGETABLE_ORDER));\n                    free_xen_pagetable(l2e_to_l1e(ol2e));\n                }\n                else if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n            }\n        }\n\n check_l3:\n        if ( cpu_has_page1gb &&\n             (flags == PAGE_HYPERVISOR) &&\n             ((nr_mfns == 0) ||\n              !(((virt >> PAGE_SHIFT) | mfn_x(mfn)) &\n                ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1))) )\n        {\n            unsigned long base_mfn;\n            const l2_pgentry_t *l2t;\n\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n\n            ol3e = *pl3e;\n            /*\n             * L3E may be already cleared, or set to a superpage, by\n             * concurrent paging structure modifications on other CPUs.\n             */\n            if ( !(l3e_get_flags(ol3e) & _PAGE_PRESENT) ||\n                (l3e_get_flags(ol3e) & _PAGE_PSE) )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                continue;\n            }\n\n            l2t = l3e_to_l2e(ol3e);\n            base_mfn = l2e_get_pfn(l2t[0]) & ~(L2_PAGETABLE_ENTRIES *\n                                              L1_PAGETABLE_ENTRIES - 1);\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                if ( (l2e_get_pfn(l2t[i]) !=\n                      (base_mfn + (i << PAGETABLE_ORDER))) ||\n                     (l2e_get_flags(l2t[i]) != l1f_to_lNf(flags)) )\n                    break;\n            if ( i == L2_PAGETABLE_ENTRIES )\n            {\n                l3e_write_atomic(pl3e, l3e_from_pfn(base_mfn,\n                                                    l1f_to_lNf(flags)));\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(virt - PAGE_SIZE,\n                           FLUSH_TLB_GLOBAL |\n                           FLUSH_ORDER(2*PAGETABLE_ORDER));\n                free_xen_pagetable(l3e_to_l2e(ol3e));\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n    }\n\n#undef flush_flags\n\n    rc = 0;\n\n out:\n    L3T_UNLOCK(current_l3page);\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,7 @@\n     l1_pgentry_t *pl1e, ol1e;\n     unsigned int  i;\n     int rc = -ENOMEM;\n+    struct page_info *current_l3page;\n \n #define flush_flags(oldf) do {                 \\\n     unsigned int o_ = (oldf);                  \\\n@@ -24,13 +25,20 @@\n     }                                          \\\n } while (0)\n \n+    L3T_INIT(current_l3page);\n+\n     while ( nr_mfns != 0 )\n     {\n-        l3_pgentry_t ol3e, *pl3e = virt_to_xen_l3e(virt);\n-\n+        l3_pgentry_t *pl3e, ol3e;\n+\n+        L3T_UNLOCK(current_l3page);\n+\n+        pl3e = virt_to_xen_l3e(virt);\n         if ( !pl3e )\n             goto out;\n \n+        current_l3page = virt_to_page(pl3e);\n+        L3T_LOCK(current_l3page);\n         ol3e = *pl3e;\n \n         if ( cpu_has_page1gb &&\n@@ -370,5 +378,6 @@\n     rc = 0;\n \n  out:\n+    L3T_UNLOCK(current_l3page);\n     return rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "        l3_pgentry_t ol3e, *pl3e = virt_to_xen_l3e(virt);",
                ""
            ],
            "added_lines": [
                "    struct page_info *current_l3page;",
                "    L3T_INIT(current_l3page);",
                "",
                "        l3_pgentry_t *pl3e, ol3e;",
                "",
                "        L3T_UNLOCK(current_l3page);",
                "",
                "        pl3e = virt_to_xen_l3e(virt);",
                "        current_l3page = virt_to_page(pl3e);",
                "        L3T_LOCK(current_l3page);",
                "    L3T_UNLOCK(current_l3page);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27672",
        "func_name": "xen-project/xen/modify_xen_mappings",
        "description": "An issue was discovered in Xen through 4.14.x allowing x86 guest OS users to cause a host OS denial of service, achieve data corruption, or possibly gain privileges by exploiting a race condition that leads to a use-after-free involving 2MiB and 1GiB superpages.",
        "git_url": "https://github.com/xen-project/xen/commit/1ce75e99d75907aaffae05fcf658a833802bce49",
        "commit_title": "x86/mm: Prevent some races in hypervisor mapping updates",
        "commit_text": " map_pages_to_xen will attempt to coalesce mappings into 2MiB and 1GiB superpages if possible, to maximize TLB efficiency.  This means both replacing superpage entries with smaller entries, and replacing smaller entries with superpages.  Unfortunately, while some potential races are handled correctly, others are not.  These include:  1. When one processor modifies a sub-superpage mapping while another processor replaces the entire range with a superpage.  Take the following example:  Suppose L3[N] points to L2.  And suppose we have two processors, A and B.  * A walks the pagetables, get a pointer to L2. * B replaces L3[N] with a 1GiB mapping. * B Frees L2 * A writes L2[M] #  This is race exacerbated by the fact that virt_to_xen_l[21]e doesn't handle higher-level superpages properly: If you call virt_xen_to_l2e on a virtual address within an L3 superpage, you'll either hit a BUG() (most likely), or get a pointer into the middle of a data page; same with virt_xen_to_l1 on a virtual address within either an L3 or L2 superpage.  So take the following example:  * A reads pl3e and discovers it to point to an L2. * B replaces L3[N] with a 1GiB mapping * A calls virt_to_xen_l2e() and hits the BUG_ON() #  2. When two processors simultaneously try to replace a sub-superpage mapping with a superpage mapping.  Take the following example:  Suppose L3[N] points to L2.  And suppose we have two processors, A and B, both trying to replace L3[N] with a superpage.  * A walks the pagetables, get a pointer to pl3e, and takes a copy ol3e pointing to L2. * B walks the pagetables, gets a pointre to pl3e, and takes a copy ol3e pointing to L2. * A writes the new value into L3[N] * B writes the new value into L3[N] * A recursively frees all the L1's under L2, then frees L2 * B recursively double-frees all the L1's under L2, then double-frees L2 #  Fix this by grabbing a lock for the entirety of the mapping update operation.  Rather than grabbing map_pgdir_lock for the entire operation, however, repurpose the PGT_locked bit from L3's page->type_info as a lock. This means that rather than locking the entire address space, we \"only\" lock a single 512GiB chunk of hypervisor address space at a time.  There was a proposal for a lock-and-reverify approach, where we walk the pagetables to the point where we decide what to do; then grab the map_pgdir_lock, re-verify the information we collected without the lock, and finally make the change (starting over again if anything had changed).  Without being able to guarantee that the L2 table wasn't freed, however, that means every read would need to be considered potentially unsafe.  Thinking carefully about that is probably something that wants to be done on public, not under time pressure.  This is part of XSA-345. ",
        "func_before": "int modify_xen_mappings(unsigned long s, unsigned long e, unsigned int nf)\n{\n    bool locking = system_state > SYS_STATE_boot;\n    l2_pgentry_t *pl2e;\n    l1_pgentry_t *pl1e;\n    unsigned int  i;\n    unsigned long v = s;\n    int rc = -ENOMEM;\n\n    /* Set of valid PTE bits which may be altered. */\n#define FLAGS_MASK (_PAGE_NX|_PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_RW|_PAGE_PRESENT)\n    nf &= FLAGS_MASK;\n\n    ASSERT(IS_ALIGNED(s, PAGE_SIZE));\n    ASSERT(IS_ALIGNED(e, PAGE_SIZE));\n\n    while ( v < e )\n    {\n        l3_pgentry_t *pl3e = virt_to_xen_l3e(v);\n\n        if ( !pl3e || !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) )\n        {\n            /* Confirm the caller isn't trying to create new mappings. */\n            ASSERT(!(nf & _PAGE_PRESENT));\n\n            v += 1UL << L3_PAGETABLE_SHIFT;\n            v &= ~((1UL << L3_PAGETABLE_SHIFT) - 1);\n            continue;\n        }\n\n        if ( l3e_get_flags(*pl3e) & _PAGE_PSE )\n        {\n            l2_pgentry_t *l2t;\n\n            if ( l2_table_offset(v) == 0 &&\n                 l1_table_offset(v) == 0 &&\n                 ((e - v) >= (1UL << L3_PAGETABLE_SHIFT)) )\n            {\n                /* PAGE1GB: whole superpage is modified. */\n                l3_pgentry_t nl3e = !(nf & _PAGE_PRESENT) ? l3e_empty()\n                    : l3e_from_pfn(l3e_get_pfn(*pl3e),\n                                   (l3e_get_flags(*pl3e) & ~FLAGS_MASK) | nf);\n\n                l3e_write_atomic(pl3e, nl3e);\n                v += 1UL << L3_PAGETABLE_SHIFT;\n                continue;\n            }\n\n            /* PAGE1GB: shatter the superpage and fall through. */\n            l2t = alloc_xen_pagetable();\n            if ( !l2t )\n                goto out;\n\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                l2e_write(l2t + i,\n                          l2e_from_pfn(l3e_get_pfn(*pl3e) +\n                                       (i << PAGETABLE_ORDER),\n                                       l3e_get_flags(*pl3e)));\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n            if ( (l3e_get_flags(*pl3e) & _PAGE_PRESENT) &&\n                 (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n            {\n                l3e_write_atomic(pl3e, l3e_from_mfn(virt_to_mfn(l2t),\n                                                    __PAGE_HYPERVISOR));\n                l2t = NULL;\n            }\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            if ( l2t )\n                free_xen_pagetable(l2t);\n        }\n\n        /*\n         * The L3 entry has been verified to be present, and we've dealt with\n         * 1G pages as well, so the L2 table cannot require allocation.\n         */\n        pl2e = l3e_to_l2e(*pl3e) + l2_table_offset(v);\n\n        if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n        {\n            /* Confirm the caller isn't trying to create new mappings. */\n            ASSERT(!(nf & _PAGE_PRESENT));\n\n            v += 1UL << L2_PAGETABLE_SHIFT;\n            v &= ~((1UL << L2_PAGETABLE_SHIFT) - 1);\n            continue;\n        }\n\n        if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n        {\n            if ( (l1_table_offset(v) == 0) &&\n                 ((e-v) >= (1UL << L2_PAGETABLE_SHIFT)) )\n            {\n                /* PSE: whole superpage is modified. */\n                l2_pgentry_t nl2e = !(nf & _PAGE_PRESENT) ? l2e_empty()\n                    : l2e_from_pfn(l2e_get_pfn(*pl2e),\n                                   (l2e_get_flags(*pl2e) & ~FLAGS_MASK) | nf);\n\n                l2e_write_atomic(pl2e, nl2e);\n                v += 1UL << L2_PAGETABLE_SHIFT;\n            }\n            else\n            {\n                l1_pgentry_t *l1t;\n\n                /* PSE: shatter the superpage and try again. */\n                l1t = alloc_xen_pagetable();\n                if ( !l1t )\n                    goto out;\n\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    l1e_write(&l1t[i],\n                              l1e_from_pfn(l2e_get_pfn(*pl2e) + i,\n                                           l2e_get_flags(*pl2e) & ~_PAGE_PSE));\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n                if ( (l2e_get_flags(*pl2e) & _PAGE_PRESENT) &&\n                     (l2e_get_flags(*pl2e) & _PAGE_PSE) )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_mfn(virt_to_mfn(l1t),\n                                                        __PAGE_HYPERVISOR));\n                    l1t = NULL;\n                }\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                if ( l1t )\n                    free_xen_pagetable(l1t);\n            }\n        }\n        else\n        {\n            l1_pgentry_t nl1e, *l1t;\n\n            /*\n             * Ordinary 4kB mapping: The L2 entry has been verified to be\n             * present, and we've dealt with 2M pages as well, so the L1 table\n             * cannot require allocation.\n             */\n            pl1e = l2e_to_l1e(*pl2e) + l1_table_offset(v);\n\n            /* Confirm the caller isn't trying to create new mappings. */\n            if ( !(l1e_get_flags(*pl1e) & _PAGE_PRESENT) )\n                ASSERT(!(nf & _PAGE_PRESENT));\n\n            nl1e = !(nf & _PAGE_PRESENT) ? l1e_empty()\n                : l1e_from_pfn(l1e_get_pfn(*pl1e),\n                               (l1e_get_flags(*pl1e) & ~FLAGS_MASK) | nf);\n\n            l1e_write_atomic(pl1e, nl1e);\n            v += PAGE_SIZE;\n\n            /*\n             * If we are not destroying mappings, or not done with the L2E,\n             * skip the empty&free check.\n             */\n            if ( (nf & _PAGE_PRESENT) || ((v != e) && (l1_table_offset(v) != 0)) )\n                continue;\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n\n            /*\n             * L2E may be already cleared, or set to a superpage, by\n             * concurrent paging structure modifications on other CPUs.\n             */\n            if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                goto check_l3;\n            }\n\n            if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                continue;\n            }\n\n            l1t = l2e_to_l1e(*pl2e);\n            for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                if ( l1e_get_intpte(l1t[i]) != 0 )\n                    break;\n            if ( i == L1_PAGETABLE_ENTRIES )\n            {\n                /* Empty: zap the L2E and free the L1 page. */\n                l2e_write_atomic(pl2e, l2e_empty());\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(NULL, FLUSH_TLB_GLOBAL); /* flush before free */\n                free_xen_pagetable(l1t);\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n\n check_l3:\n        /*\n         * If we are not destroying mappings, or not done with the L3E,\n         * skip the empty&free check.\n         */\n        if ( (nf & _PAGE_PRESENT) ||\n             ((v != e) && (l2_table_offset(v) + l1_table_offset(v) != 0)) )\n            continue;\n        if ( locking )\n            spin_lock(&map_pgdir_lock);\n\n        /*\n         * L3E may be already cleared, or set to a superpage, by\n         * concurrent paging structure modifications on other CPUs.\n         */\n        if ( !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) ||\n              (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n        {\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            continue;\n        }\n\n        {\n            l2_pgentry_t *l2t;\n\n            l2t = l3e_to_l2e(*pl3e);\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                if ( l2e_get_intpte(l2t[i]) != 0 )\n                    break;\n            if ( i == L2_PAGETABLE_ENTRIES )\n            {\n                /* Empty: zap the L3E and free the L2 page. */\n                l3e_write_atomic(pl3e, l3e_empty());\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(NULL, FLUSH_TLB_GLOBAL); /* flush before free */\n                free_xen_pagetable(l2t);\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n    }\n\n    flush_area(NULL, FLUSH_TLB_GLOBAL);\n\n#undef FLAGS_MASK\n    rc = 0;\n\n out:\n    return rc;\n}",
        "func": "int modify_xen_mappings(unsigned long s, unsigned long e, unsigned int nf)\n{\n    bool locking = system_state > SYS_STATE_boot;\n    l2_pgentry_t *pl2e;\n    l1_pgentry_t *pl1e;\n    unsigned int  i;\n    unsigned long v = s;\n    int rc = -ENOMEM;\n    struct page_info *current_l3page;\n\n    /* Set of valid PTE bits which may be altered. */\n#define FLAGS_MASK (_PAGE_NX|_PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_RW|_PAGE_PRESENT)\n    nf &= FLAGS_MASK;\n\n    ASSERT(IS_ALIGNED(s, PAGE_SIZE));\n    ASSERT(IS_ALIGNED(e, PAGE_SIZE));\n\n    L3T_INIT(current_l3page);\n\n    while ( v < e )\n    {\n        l3_pgentry_t *pl3e;\n\n        L3T_UNLOCK(current_l3page);\n\n        pl3e = virt_to_xen_l3e(v);\n        if ( !pl3e )\n            goto out;\n\n        current_l3page = virt_to_page(pl3e);\n        L3T_LOCK(current_l3page);\n\n        if ( !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) )\n        {\n            /* Confirm the caller isn't trying to create new mappings. */\n            ASSERT(!(nf & _PAGE_PRESENT));\n\n            v += 1UL << L3_PAGETABLE_SHIFT;\n            v &= ~((1UL << L3_PAGETABLE_SHIFT) - 1);\n            continue;\n        }\n\n        if ( l3e_get_flags(*pl3e) & _PAGE_PSE )\n        {\n            l2_pgentry_t *l2t;\n\n            if ( l2_table_offset(v) == 0 &&\n                 l1_table_offset(v) == 0 &&\n                 ((e - v) >= (1UL << L3_PAGETABLE_SHIFT)) )\n            {\n                /* PAGE1GB: whole superpage is modified. */\n                l3_pgentry_t nl3e = !(nf & _PAGE_PRESENT) ? l3e_empty()\n                    : l3e_from_pfn(l3e_get_pfn(*pl3e),\n                                   (l3e_get_flags(*pl3e) & ~FLAGS_MASK) | nf);\n\n                l3e_write_atomic(pl3e, nl3e);\n                v += 1UL << L3_PAGETABLE_SHIFT;\n                continue;\n            }\n\n            /* PAGE1GB: shatter the superpage and fall through. */\n            l2t = alloc_xen_pagetable();\n            if ( !l2t )\n                goto out;\n\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                l2e_write(l2t + i,\n                          l2e_from_pfn(l3e_get_pfn(*pl3e) +\n                                       (i << PAGETABLE_ORDER),\n                                       l3e_get_flags(*pl3e)));\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n            if ( (l3e_get_flags(*pl3e) & _PAGE_PRESENT) &&\n                 (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n            {\n                l3e_write_atomic(pl3e, l3e_from_mfn(virt_to_mfn(l2t),\n                                                    __PAGE_HYPERVISOR));\n                l2t = NULL;\n            }\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            if ( l2t )\n                free_xen_pagetable(l2t);\n        }\n\n        /*\n         * The L3 entry has been verified to be present, and we've dealt with\n         * 1G pages as well, so the L2 table cannot require allocation.\n         */\n        pl2e = l3e_to_l2e(*pl3e) + l2_table_offset(v);\n\n        if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n        {\n            /* Confirm the caller isn't trying to create new mappings. */\n            ASSERT(!(nf & _PAGE_PRESENT));\n\n            v += 1UL << L2_PAGETABLE_SHIFT;\n            v &= ~((1UL << L2_PAGETABLE_SHIFT) - 1);\n            continue;\n        }\n\n        if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n        {\n            if ( (l1_table_offset(v) == 0) &&\n                 ((e-v) >= (1UL << L2_PAGETABLE_SHIFT)) )\n            {\n                /* PSE: whole superpage is modified. */\n                l2_pgentry_t nl2e = !(nf & _PAGE_PRESENT) ? l2e_empty()\n                    : l2e_from_pfn(l2e_get_pfn(*pl2e),\n                                   (l2e_get_flags(*pl2e) & ~FLAGS_MASK) | nf);\n\n                l2e_write_atomic(pl2e, nl2e);\n                v += 1UL << L2_PAGETABLE_SHIFT;\n            }\n            else\n            {\n                l1_pgentry_t *l1t;\n\n                /* PSE: shatter the superpage and try again. */\n                l1t = alloc_xen_pagetable();\n                if ( !l1t )\n                    goto out;\n\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    l1e_write(&l1t[i],\n                              l1e_from_pfn(l2e_get_pfn(*pl2e) + i,\n                                           l2e_get_flags(*pl2e) & ~_PAGE_PSE));\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n                if ( (l2e_get_flags(*pl2e) & _PAGE_PRESENT) &&\n                     (l2e_get_flags(*pl2e) & _PAGE_PSE) )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_mfn(virt_to_mfn(l1t),\n                                                        __PAGE_HYPERVISOR));\n                    l1t = NULL;\n                }\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                if ( l1t )\n                    free_xen_pagetable(l1t);\n            }\n        }\n        else\n        {\n            l1_pgentry_t nl1e, *l1t;\n\n            /*\n             * Ordinary 4kB mapping: The L2 entry has been verified to be\n             * present, and we've dealt with 2M pages as well, so the L1 table\n             * cannot require allocation.\n             */\n            pl1e = l2e_to_l1e(*pl2e) + l1_table_offset(v);\n\n            /* Confirm the caller isn't trying to create new mappings. */\n            if ( !(l1e_get_flags(*pl1e) & _PAGE_PRESENT) )\n                ASSERT(!(nf & _PAGE_PRESENT));\n\n            nl1e = !(nf & _PAGE_PRESENT) ? l1e_empty()\n                : l1e_from_pfn(l1e_get_pfn(*pl1e),\n                               (l1e_get_flags(*pl1e) & ~FLAGS_MASK) | nf);\n\n            l1e_write_atomic(pl1e, nl1e);\n            v += PAGE_SIZE;\n\n            /*\n             * If we are not destroying mappings, or not done with the L2E,\n             * skip the empty&free check.\n             */\n            if ( (nf & _PAGE_PRESENT) || ((v != e) && (l1_table_offset(v) != 0)) )\n                continue;\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n\n            /*\n             * L2E may be already cleared, or set to a superpage, by\n             * concurrent paging structure modifications on other CPUs.\n             */\n            if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                goto check_l3;\n            }\n\n            if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                continue;\n            }\n\n            l1t = l2e_to_l1e(*pl2e);\n            for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                if ( l1e_get_intpte(l1t[i]) != 0 )\n                    break;\n            if ( i == L1_PAGETABLE_ENTRIES )\n            {\n                /* Empty: zap the L2E and free the L1 page. */\n                l2e_write_atomic(pl2e, l2e_empty());\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(NULL, FLUSH_TLB_GLOBAL); /* flush before free */\n                free_xen_pagetable(l1t);\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n\n check_l3:\n        /*\n         * If we are not destroying mappings, or not done with the L3E,\n         * skip the empty&free check.\n         */\n        if ( (nf & _PAGE_PRESENT) ||\n             ((v != e) && (l2_table_offset(v) + l1_table_offset(v) != 0)) )\n            continue;\n        if ( locking )\n            spin_lock(&map_pgdir_lock);\n\n        /*\n         * L3E may be already cleared, or set to a superpage, by\n         * concurrent paging structure modifications on other CPUs.\n         */\n        if ( !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) ||\n              (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n        {\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            continue;\n        }\n\n        {\n            l2_pgentry_t *l2t;\n\n            l2t = l3e_to_l2e(*pl3e);\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                if ( l2e_get_intpte(l2t[i]) != 0 )\n                    break;\n            if ( i == L2_PAGETABLE_ENTRIES )\n            {\n                /* Empty: zap the L3E and free the L2 page. */\n                l3e_write_atomic(pl3e, l3e_empty());\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(NULL, FLUSH_TLB_GLOBAL); /* flush before free */\n                free_xen_pagetable(l2t);\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n    }\n\n    flush_area(NULL, FLUSH_TLB_GLOBAL);\n\n#undef FLAGS_MASK\n    rc = 0;\n\n out:\n    L3T_UNLOCK(current_l3page);\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,7 @@\n     unsigned int  i;\n     unsigned long v = s;\n     int rc = -ENOMEM;\n+    struct page_info *current_l3page;\n \n     /* Set of valid PTE bits which may be altered. */\n #define FLAGS_MASK (_PAGE_NX|_PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_RW|_PAGE_PRESENT)\n@@ -14,11 +15,22 @@\n     ASSERT(IS_ALIGNED(s, PAGE_SIZE));\n     ASSERT(IS_ALIGNED(e, PAGE_SIZE));\n \n+    L3T_INIT(current_l3page);\n+\n     while ( v < e )\n     {\n-        l3_pgentry_t *pl3e = virt_to_xen_l3e(v);\n-\n-        if ( !pl3e || !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) )\n+        l3_pgentry_t *pl3e;\n+\n+        L3T_UNLOCK(current_l3page);\n+\n+        pl3e = virt_to_xen_l3e(v);\n+        if ( !pl3e )\n+            goto out;\n+\n+        current_l3page = virt_to_page(pl3e);\n+        L3T_LOCK(current_l3page);\n+\n+        if ( !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) )\n         {\n             /* Confirm the caller isn't trying to create new mappings. */\n             ASSERT(!(nf & _PAGE_PRESENT));\n@@ -244,5 +256,6 @@\n     rc = 0;\n \n  out:\n+    L3T_UNLOCK(current_l3page);\n     return rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "        l3_pgentry_t *pl3e = virt_to_xen_l3e(v);",
                "",
                "        if ( !pl3e || !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) )"
            ],
            "added_lines": [
                "    struct page_info *current_l3page;",
                "    L3T_INIT(current_l3page);",
                "",
                "        l3_pgentry_t *pl3e;",
                "",
                "        L3T_UNLOCK(current_l3page);",
                "",
                "        pl3e = virt_to_xen_l3e(v);",
                "        if ( !pl3e )",
                "            goto out;",
                "",
                "        current_l3page = virt_to_page(pl3e);",
                "        L3T_LOCK(current_l3page);",
                "",
                "        if ( !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) )",
                "    L3T_UNLOCK(current_l3page);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27672",
        "func_name": "xen-project/xen/modify_xen_mappings",
        "description": "An issue was discovered in Xen through 4.14.x allowing x86 guest OS users to cause a host OS denial of service, achieve data corruption, or possibly gain privileges by exploiting a race condition that leads to a use-after-free involving 2MiB and 1GiB superpages.",
        "git_url": "https://github.com/xen-project/xen/commit/b733f8a8b8db83f2d438cab3adb38b387cecfce0",
        "commit_title": "x86/mm: Refactor modify_xen_mappings to have one exit path",
        "commit_text": " We will soon need to perform clean-ups before returning.  No functional change.  This is part of XSA-345. ",
        "func_before": "int modify_xen_mappings(unsigned long s, unsigned long e, unsigned int nf)\n{\n    bool locking = system_state > SYS_STATE_boot;\n    l2_pgentry_t *pl2e;\n    l1_pgentry_t *pl1e;\n    unsigned int  i;\n    unsigned long v = s;\n\n    /* Set of valid PTE bits which may be altered. */\n#define FLAGS_MASK (_PAGE_NX|_PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_RW|_PAGE_PRESENT)\n    nf &= FLAGS_MASK;\n\n    ASSERT(IS_ALIGNED(s, PAGE_SIZE));\n    ASSERT(IS_ALIGNED(e, PAGE_SIZE));\n\n    while ( v < e )\n    {\n        l3_pgentry_t *pl3e = virt_to_xen_l3e(v);\n\n        if ( !pl3e || !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) )\n        {\n            /* Confirm the caller isn't trying to create new mappings. */\n            ASSERT(!(nf & _PAGE_PRESENT));\n\n            v += 1UL << L3_PAGETABLE_SHIFT;\n            v &= ~((1UL << L3_PAGETABLE_SHIFT) - 1);\n            continue;\n        }\n\n        if ( l3e_get_flags(*pl3e) & _PAGE_PSE )\n        {\n            l2_pgentry_t *l2t;\n\n            if ( l2_table_offset(v) == 0 &&\n                 l1_table_offset(v) == 0 &&\n                 ((e - v) >= (1UL << L3_PAGETABLE_SHIFT)) )\n            {\n                /* PAGE1GB: whole superpage is modified. */\n                l3_pgentry_t nl3e = !(nf & _PAGE_PRESENT) ? l3e_empty()\n                    : l3e_from_pfn(l3e_get_pfn(*pl3e),\n                                   (l3e_get_flags(*pl3e) & ~FLAGS_MASK) | nf);\n\n                l3e_write_atomic(pl3e, nl3e);\n                v += 1UL << L3_PAGETABLE_SHIFT;\n                continue;\n            }\n\n            /* PAGE1GB: shatter the superpage and fall through. */\n            l2t = alloc_xen_pagetable();\n            if ( !l2t )\n                return -ENOMEM;\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                l2e_write(l2t + i,\n                          l2e_from_pfn(l3e_get_pfn(*pl3e) +\n                                       (i << PAGETABLE_ORDER),\n                                       l3e_get_flags(*pl3e)));\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n            if ( (l3e_get_flags(*pl3e) & _PAGE_PRESENT) &&\n                 (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n            {\n                l3e_write_atomic(pl3e, l3e_from_mfn(virt_to_mfn(l2t),\n                                                    __PAGE_HYPERVISOR));\n                l2t = NULL;\n            }\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            if ( l2t )\n                free_xen_pagetable(l2t);\n        }\n\n        /*\n         * The L3 entry has been verified to be present, and we've dealt with\n         * 1G pages as well, so the L2 table cannot require allocation.\n         */\n        pl2e = l3e_to_l2e(*pl3e) + l2_table_offset(v);\n\n        if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n        {\n            /* Confirm the caller isn't trying to create new mappings. */\n            ASSERT(!(nf & _PAGE_PRESENT));\n\n            v += 1UL << L2_PAGETABLE_SHIFT;\n            v &= ~((1UL << L2_PAGETABLE_SHIFT) - 1);\n            continue;\n        }\n\n        if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n        {\n            if ( (l1_table_offset(v) == 0) &&\n                 ((e-v) >= (1UL << L2_PAGETABLE_SHIFT)) )\n            {\n                /* PSE: whole superpage is modified. */\n                l2_pgentry_t nl2e = !(nf & _PAGE_PRESENT) ? l2e_empty()\n                    : l2e_from_pfn(l2e_get_pfn(*pl2e),\n                                   (l2e_get_flags(*pl2e) & ~FLAGS_MASK) | nf);\n\n                l2e_write_atomic(pl2e, nl2e);\n                v += 1UL << L2_PAGETABLE_SHIFT;\n            }\n            else\n            {\n                l1_pgentry_t *l1t;\n\n                /* PSE: shatter the superpage and try again. */\n                l1t = alloc_xen_pagetable();\n                if ( !l1t )\n                    return -ENOMEM;\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    l1e_write(&l1t[i],\n                              l1e_from_pfn(l2e_get_pfn(*pl2e) + i,\n                                           l2e_get_flags(*pl2e) & ~_PAGE_PSE));\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n                if ( (l2e_get_flags(*pl2e) & _PAGE_PRESENT) &&\n                     (l2e_get_flags(*pl2e) & _PAGE_PSE) )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_mfn(virt_to_mfn(l1t),\n                                                        __PAGE_HYPERVISOR));\n                    l1t = NULL;\n                }\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                if ( l1t )\n                    free_xen_pagetable(l1t);\n            }\n        }\n        else\n        {\n            l1_pgentry_t nl1e, *l1t;\n\n            /*\n             * Ordinary 4kB mapping: The L2 entry has been verified to be\n             * present, and we've dealt with 2M pages as well, so the L1 table\n             * cannot require allocation.\n             */\n            pl1e = l2e_to_l1e(*pl2e) + l1_table_offset(v);\n\n            /* Confirm the caller isn't trying to create new mappings. */\n            if ( !(l1e_get_flags(*pl1e) & _PAGE_PRESENT) )\n                ASSERT(!(nf & _PAGE_PRESENT));\n\n            nl1e = !(nf & _PAGE_PRESENT) ? l1e_empty()\n                : l1e_from_pfn(l1e_get_pfn(*pl1e),\n                               (l1e_get_flags(*pl1e) & ~FLAGS_MASK) | nf);\n\n            l1e_write_atomic(pl1e, nl1e);\n            v += PAGE_SIZE;\n\n            /*\n             * If we are not destroying mappings, or not done with the L2E,\n             * skip the empty&free check.\n             */\n            if ( (nf & _PAGE_PRESENT) || ((v != e) && (l1_table_offset(v) != 0)) )\n                continue;\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n\n            /*\n             * L2E may be already cleared, or set to a superpage, by\n             * concurrent paging structure modifications on other CPUs.\n             */\n            if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                goto check_l3;\n            }\n\n            if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                continue;\n            }\n\n            l1t = l2e_to_l1e(*pl2e);\n            for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                if ( l1e_get_intpte(l1t[i]) != 0 )\n                    break;\n            if ( i == L1_PAGETABLE_ENTRIES )\n            {\n                /* Empty: zap the L2E and free the L1 page. */\n                l2e_write_atomic(pl2e, l2e_empty());\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(NULL, FLUSH_TLB_GLOBAL); /* flush before free */\n                free_xen_pagetable(l1t);\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n\n check_l3:\n        /*\n         * If we are not destroying mappings, or not done with the L3E,\n         * skip the empty&free check.\n         */\n        if ( (nf & _PAGE_PRESENT) ||\n             ((v != e) && (l2_table_offset(v) + l1_table_offset(v) != 0)) )\n            continue;\n        if ( locking )\n            spin_lock(&map_pgdir_lock);\n\n        /*\n         * L3E may be already cleared, or set to a superpage, by\n         * concurrent paging structure modifications on other CPUs.\n         */\n        if ( !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) ||\n              (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n        {\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            continue;\n        }\n\n        {\n            l2_pgentry_t *l2t;\n\n            l2t = l3e_to_l2e(*pl3e);\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                if ( l2e_get_intpte(l2t[i]) != 0 )\n                    break;\n            if ( i == L2_PAGETABLE_ENTRIES )\n            {\n                /* Empty: zap the L3E and free the L2 page. */\n                l3e_write_atomic(pl3e, l3e_empty());\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(NULL, FLUSH_TLB_GLOBAL); /* flush before free */\n                free_xen_pagetable(l2t);\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n    }\n\n    flush_area(NULL, FLUSH_TLB_GLOBAL);\n\n#undef FLAGS_MASK\n    return 0;\n}",
        "func": "int modify_xen_mappings(unsigned long s, unsigned long e, unsigned int nf)\n{\n    bool locking = system_state > SYS_STATE_boot;\n    l2_pgentry_t *pl2e;\n    l1_pgentry_t *pl1e;\n    unsigned int  i;\n    unsigned long v = s;\n    int rc = -ENOMEM;\n\n    /* Set of valid PTE bits which may be altered. */\n#define FLAGS_MASK (_PAGE_NX|_PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_RW|_PAGE_PRESENT)\n    nf &= FLAGS_MASK;\n\n    ASSERT(IS_ALIGNED(s, PAGE_SIZE));\n    ASSERT(IS_ALIGNED(e, PAGE_SIZE));\n\n    while ( v < e )\n    {\n        l3_pgentry_t *pl3e = virt_to_xen_l3e(v);\n\n        if ( !pl3e || !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) )\n        {\n            /* Confirm the caller isn't trying to create new mappings. */\n            ASSERT(!(nf & _PAGE_PRESENT));\n\n            v += 1UL << L3_PAGETABLE_SHIFT;\n            v &= ~((1UL << L3_PAGETABLE_SHIFT) - 1);\n            continue;\n        }\n\n        if ( l3e_get_flags(*pl3e) & _PAGE_PSE )\n        {\n            l2_pgentry_t *l2t;\n\n            if ( l2_table_offset(v) == 0 &&\n                 l1_table_offset(v) == 0 &&\n                 ((e - v) >= (1UL << L3_PAGETABLE_SHIFT)) )\n            {\n                /* PAGE1GB: whole superpage is modified. */\n                l3_pgentry_t nl3e = !(nf & _PAGE_PRESENT) ? l3e_empty()\n                    : l3e_from_pfn(l3e_get_pfn(*pl3e),\n                                   (l3e_get_flags(*pl3e) & ~FLAGS_MASK) | nf);\n\n                l3e_write_atomic(pl3e, nl3e);\n                v += 1UL << L3_PAGETABLE_SHIFT;\n                continue;\n            }\n\n            /* PAGE1GB: shatter the superpage and fall through. */\n            l2t = alloc_xen_pagetable();\n            if ( !l2t )\n                goto out;\n\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                l2e_write(l2t + i,\n                          l2e_from_pfn(l3e_get_pfn(*pl3e) +\n                                       (i << PAGETABLE_ORDER),\n                                       l3e_get_flags(*pl3e)));\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n            if ( (l3e_get_flags(*pl3e) & _PAGE_PRESENT) &&\n                 (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n            {\n                l3e_write_atomic(pl3e, l3e_from_mfn(virt_to_mfn(l2t),\n                                                    __PAGE_HYPERVISOR));\n                l2t = NULL;\n            }\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            if ( l2t )\n                free_xen_pagetable(l2t);\n        }\n\n        /*\n         * The L3 entry has been verified to be present, and we've dealt with\n         * 1G pages as well, so the L2 table cannot require allocation.\n         */\n        pl2e = l3e_to_l2e(*pl3e) + l2_table_offset(v);\n\n        if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n        {\n            /* Confirm the caller isn't trying to create new mappings. */\n            ASSERT(!(nf & _PAGE_PRESENT));\n\n            v += 1UL << L2_PAGETABLE_SHIFT;\n            v &= ~((1UL << L2_PAGETABLE_SHIFT) - 1);\n            continue;\n        }\n\n        if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n        {\n            if ( (l1_table_offset(v) == 0) &&\n                 ((e-v) >= (1UL << L2_PAGETABLE_SHIFT)) )\n            {\n                /* PSE: whole superpage is modified. */\n                l2_pgentry_t nl2e = !(nf & _PAGE_PRESENT) ? l2e_empty()\n                    : l2e_from_pfn(l2e_get_pfn(*pl2e),\n                                   (l2e_get_flags(*pl2e) & ~FLAGS_MASK) | nf);\n\n                l2e_write_atomic(pl2e, nl2e);\n                v += 1UL << L2_PAGETABLE_SHIFT;\n            }\n            else\n            {\n                l1_pgentry_t *l1t;\n\n                /* PSE: shatter the superpage and try again. */\n                l1t = alloc_xen_pagetable();\n                if ( !l1t )\n                    goto out;\n\n                for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                    l1e_write(&l1t[i],\n                              l1e_from_pfn(l2e_get_pfn(*pl2e) + i,\n                                           l2e_get_flags(*pl2e) & ~_PAGE_PSE));\n                if ( locking )\n                    spin_lock(&map_pgdir_lock);\n                if ( (l2e_get_flags(*pl2e) & _PAGE_PRESENT) &&\n                     (l2e_get_flags(*pl2e) & _PAGE_PSE) )\n                {\n                    l2e_write_atomic(pl2e, l2e_from_mfn(virt_to_mfn(l1t),\n                                                        __PAGE_HYPERVISOR));\n                    l1t = NULL;\n                }\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                if ( l1t )\n                    free_xen_pagetable(l1t);\n            }\n        }\n        else\n        {\n            l1_pgentry_t nl1e, *l1t;\n\n            /*\n             * Ordinary 4kB mapping: The L2 entry has been verified to be\n             * present, and we've dealt with 2M pages as well, so the L1 table\n             * cannot require allocation.\n             */\n            pl1e = l2e_to_l1e(*pl2e) + l1_table_offset(v);\n\n            /* Confirm the caller isn't trying to create new mappings. */\n            if ( !(l1e_get_flags(*pl1e) & _PAGE_PRESENT) )\n                ASSERT(!(nf & _PAGE_PRESENT));\n\n            nl1e = !(nf & _PAGE_PRESENT) ? l1e_empty()\n                : l1e_from_pfn(l1e_get_pfn(*pl1e),\n                               (l1e_get_flags(*pl1e) & ~FLAGS_MASK) | nf);\n\n            l1e_write_atomic(pl1e, nl1e);\n            v += PAGE_SIZE;\n\n            /*\n             * If we are not destroying mappings, or not done with the L2E,\n             * skip the empty&free check.\n             */\n            if ( (nf & _PAGE_PRESENT) || ((v != e) && (l1_table_offset(v) != 0)) )\n                continue;\n            if ( locking )\n                spin_lock(&map_pgdir_lock);\n\n            /*\n             * L2E may be already cleared, or set to a superpage, by\n             * concurrent paging structure modifications on other CPUs.\n             */\n            if ( !(l2e_get_flags(*pl2e) & _PAGE_PRESENT) )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                goto check_l3;\n            }\n\n            if ( l2e_get_flags(*pl2e) & _PAGE_PSE )\n            {\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                continue;\n            }\n\n            l1t = l2e_to_l1e(*pl2e);\n            for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                if ( l1e_get_intpte(l1t[i]) != 0 )\n                    break;\n            if ( i == L1_PAGETABLE_ENTRIES )\n            {\n                /* Empty: zap the L2E and free the L1 page. */\n                l2e_write_atomic(pl2e, l2e_empty());\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(NULL, FLUSH_TLB_GLOBAL); /* flush before free */\n                free_xen_pagetable(l1t);\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n\n check_l3:\n        /*\n         * If we are not destroying mappings, or not done with the L3E,\n         * skip the empty&free check.\n         */\n        if ( (nf & _PAGE_PRESENT) ||\n             ((v != e) && (l2_table_offset(v) + l1_table_offset(v) != 0)) )\n            continue;\n        if ( locking )\n            spin_lock(&map_pgdir_lock);\n\n        /*\n         * L3E may be already cleared, or set to a superpage, by\n         * concurrent paging structure modifications on other CPUs.\n         */\n        if ( !(l3e_get_flags(*pl3e) & _PAGE_PRESENT) ||\n              (l3e_get_flags(*pl3e) & _PAGE_PSE) )\n        {\n            if ( locking )\n                spin_unlock(&map_pgdir_lock);\n            continue;\n        }\n\n        {\n            l2_pgentry_t *l2t;\n\n            l2t = l3e_to_l2e(*pl3e);\n            for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                if ( l2e_get_intpte(l2t[i]) != 0 )\n                    break;\n            if ( i == L2_PAGETABLE_ENTRIES )\n            {\n                /* Empty: zap the L3E and free the L2 page. */\n                l3e_write_atomic(pl3e, l3e_empty());\n                if ( locking )\n                    spin_unlock(&map_pgdir_lock);\n                flush_area(NULL, FLUSH_TLB_GLOBAL); /* flush before free */\n                free_xen_pagetable(l2t);\n            }\n            else if ( locking )\n                spin_unlock(&map_pgdir_lock);\n        }\n    }\n\n    flush_area(NULL, FLUSH_TLB_GLOBAL);\n\n#undef FLAGS_MASK\n    rc = 0;\n\n out:\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n     l1_pgentry_t *pl1e;\n     unsigned int  i;\n     unsigned long v = s;\n+    int rc = -ENOMEM;\n \n     /* Set of valid PTE bits which may be altered. */\n #define FLAGS_MASK (_PAGE_NX|_PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_RW|_PAGE_PRESENT)\n@@ -48,7 +49,8 @@\n             /* PAGE1GB: shatter the superpage and fall through. */\n             l2t = alloc_xen_pagetable();\n             if ( !l2t )\n-                return -ENOMEM;\n+                goto out;\n+\n             for ( i = 0; i < L2_PAGETABLE_ENTRIES; i++ )\n                 l2e_write(l2t + i,\n                           l2e_from_pfn(l3e_get_pfn(*pl3e) +\n@@ -105,7 +107,8 @@\n                 /* PSE: shatter the superpage and try again. */\n                 l1t = alloc_xen_pagetable();\n                 if ( !l1t )\n-                    return -ENOMEM;\n+                    goto out;\n+\n                 for ( i = 0; i < L1_PAGETABLE_ENTRIES; i++ )\n                     l1e_write(&l1t[i],\n                               l1e_from_pfn(l2e_get_pfn(*pl2e) + i,\n@@ -238,5 +241,8 @@\n     flush_area(NULL, FLUSH_TLB_GLOBAL);\n \n #undef FLAGS_MASK\n-    return 0;\n+    rc = 0;\n+\n+ out:\n+    return rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "                return -ENOMEM;",
                "                    return -ENOMEM;",
                "    return 0;"
            ],
            "added_lines": [
                "    int rc = -ENOMEM;",
                "                goto out;",
                "",
                "                    goto out;",
                "",
                "    rc = 0;",
                "",
                " out:",
                "    return rc;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27675",
        "func_name": "torvalds/linux/set_evtchn_to_irq",
        "description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "git_url": "https://github.com/torvalds/linux/commit/073d0552ead5bfc7a3a9c01de590e924f11b5dd2",
        "commit_title": "xen/events: avoid removing an event channel while handling it",
        "commit_text": " Today it can happen that an event channel is being removed from the system while the event handling loop is active. This can lead to a race resulting in crashes or WARN() splats when trying to access the irq_info structure related to the event channel.  Fix this problem by using a rwlock taken as reader in the event handling loop and as writer when deallocating the irq_info structure.  As the observed problem was a NULL dereference in evtchn_from_irq() make this function more robust against races by testing the irq_info pointer to be not NULL before dereferencing it.  And finally make all accesses to evtchn_to_irq[row][col] atomic ones in order to avoid seeing partial updates of an array element in irq handling. Note that irq handling can be entered only for event channels which have been valid before, so any not populated row isn't a problem in this regard, as rows are only ever added and never removed.  This is XSA-331.  Cc: stable@vger.kernel.org",
        "func_before": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tevtchn_to_irq[row][col] = irq;\n\treturn 0;\n}",
        "func": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tWRITE_ONCE(evtchn_to_irq[row][col], irq);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,6 +21,6 @@\n \t\tclear_evtchn_to_irq_row(row);\n \t}\n \n-\tevtchn_to_irq[row][col] = irq;\n+\tWRITE_ONCE(evtchn_to_irq[row][col], irq);\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tevtchn_to_irq[row][col] = irq;"
            ],
            "added_lines": [
                "\tWRITE_ONCE(evtchn_to_irq[row][col], irq);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27675",
        "func_name": "torvalds/linux/get_evtchn_to_irq",
        "description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "git_url": "https://github.com/torvalds/linux/commit/073d0552ead5bfc7a3a9c01de590e924f11b5dd2",
        "commit_title": "xen/events: avoid removing an event channel while handling it",
        "commit_text": " Today it can happen that an event channel is being removed from the system while the event handling loop is active. This can lead to a race resulting in crashes or WARN() splats when trying to access the irq_info structure related to the event channel.  Fix this problem by using a rwlock taken as reader in the event handling loop and as writer when deallocating the irq_info structure.  As the observed problem was a NULL dereference in evtchn_from_irq() make this function more robust against races by testing the irq_info pointer to be not NULL before dereferencing it.  And finally make all accesses to evtchn_to_irq[row][col] atomic ones in order to avoid seeing partial updates of an array element in irq handling. Note that irq handling can be entered only for event channels which have been valid before, so any not populated row isn't a problem in this regard, as rows are only ever added and never removed.  This is XSA-331.  Cc: stable@vger.kernel.org",
        "func_before": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n}",
        "func": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,5 +4,5 @@\n \t\treturn -1;\n \tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n \t\treturn -1;\n-\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n+\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];"
            ],
            "added_lines": [
                "\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27675",
        "func_name": "torvalds/linux/clear_evtchn_to_irq_row",
        "description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "git_url": "https://github.com/torvalds/linux/commit/073d0552ead5bfc7a3a9c01de590e924f11b5dd2",
        "commit_title": "xen/events: avoid removing an event channel while handling it",
        "commit_text": " Today it can happen that an event channel is being removed from the system while the event handling loop is active. This can lead to a race resulting in crashes or WARN() splats when trying to access the irq_info structure related to the event channel.  Fix this problem by using a rwlock taken as reader in the event handling loop and as writer when deallocating the irq_info structure.  As the observed problem was a NULL dereference in evtchn_from_irq() make this function more robust against races by testing the irq_info pointer to be not NULL before dereferencing it.  And finally make all accesses to evtchn_to_irq[row][col] atomic ones in order to avoid seeing partial updates of an array element in irq handling. Note that irq handling can be entered only for event channels which have been valid before, so any not populated row isn't a problem in this regard, as rows are only ever added and never removed.  This is XSA-331.  Cc: stable@vger.kernel.org",
        "func_before": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tevtchn_to_irq[row][col] = -1;\n}",
        "func": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,5 +3,5 @@\n \tunsigned col;\n \n \tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n-\t\tevtchn_to_irq[row][col] = -1;\n+\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tevtchn_to_irq[row][col] = -1;"
            ],
            "added_lines": [
                "\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27675",
        "func_name": "torvalds/linux/evtchn_from_irq",
        "description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "git_url": "https://github.com/torvalds/linux/commit/073d0552ead5bfc7a3a9c01de590e924f11b5dd2",
        "commit_title": "xen/events: avoid removing an event channel while handling it",
        "commit_text": " Today it can happen that an event channel is being removed from the system while the event handling loop is active. This can lead to a race resulting in crashes or WARN() splats when trying to access the irq_info structure related to the event channel.  Fix this problem by using a rwlock taken as reader in the event handling loop and as writer when deallocating the irq_info structure.  As the observed problem was a NULL dereference in evtchn_from_irq() make this function more robust against races by testing the irq_info pointer to be not NULL before dereferencing it.  And finally make all accesses to evtchn_to_irq[row][col] atomic ones in order to avoid seeing partial updates of an array element in irq handling. Note that irq handling can be entered only for event channels which have been valid before, so any not populated row isn't a problem in this regard, as rows are only ever added and never removed.  This is XSA-331.  Cc: stable@vger.kernel.org",
        "func_before": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))\n\t\treturn 0;\n\n\treturn info_for_irq(irq)->evtchn;\n}",
        "func": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tconst struct irq_info *info = NULL;\n\n\tif (likely(irq < nr_irqs))\n\t\tinfo = info_for_irq(irq);\n\tif (!info)\n\t\treturn 0;\n\n\treturn info->evtchn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,11 @@\n evtchn_port_t evtchn_from_irq(unsigned irq)\n {\n-\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))\n+\tconst struct irq_info *info = NULL;\n+\n+\tif (likely(irq < nr_irqs))\n+\t\tinfo = info_for_irq(irq);\n+\tif (!info)\n \t\treturn 0;\n \n-\treturn info_for_irq(irq)->evtchn;\n+\treturn info->evtchn;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))",
                "\treturn info_for_irq(irq)->evtchn;"
            ],
            "added_lines": [
                "\tconst struct irq_info *info = NULL;",
                "",
                "\tif (likely(irq < nr_irqs))",
                "\t\tinfo = info_for_irq(irq);",
                "\tif (!info)",
                "\treturn info->evtchn;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27675",
        "func_name": "torvalds/linux/xen_free_irq",
        "description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "git_url": "https://github.com/torvalds/linux/commit/073d0552ead5bfc7a3a9c01de590e924f11b5dd2",
        "commit_title": "xen/events: avoid removing an event channel while handling it",
        "commit_text": " Today it can happen that an event channel is being removed from the system while the event handling loop is active. This can lead to a race resulting in crashes or WARN() splats when trying to access the irq_info structure related to the event channel.  Fix this problem by using a rwlock taken as reader in the event handling loop and as writer when deallocating the irq_info structure.  As the observed problem was a NULL dereference in evtchn_from_irq() make this function more robust against races by testing the irq_info pointer to be not NULL before dereferencing it.  And finally make all accesses to evtchn_to_irq[row][col] atomic ones in order to avoid seeing partial updates of an array element in irq handling. Note that irq handling can be entered only for event channels which have been valid before, so any not populated row isn't a problem in this regard, as rows are only ever added and never removed.  This is XSA-331.  Cc: stable@vger.kernel.org",
        "func_before": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
        "func": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tunsigned long flags;\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\twrite_lock_irqsave(&evtchn_rwlock, flags);\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\twrite_unlock_irqrestore(&evtchn_rwlock, flags);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,15 +1,20 @@\n static void xen_free_irq(unsigned irq)\n {\n \tstruct irq_info *info = info_for_irq(irq);\n+\tunsigned long flags;\n \n \tif (WARN_ON(!info))\n \t\treturn;\n+\n+\twrite_lock_irqsave(&evtchn_rwlock, flags);\n \n \tlist_del(&info->list);\n \n \tset_info_for_irq(irq, NULL);\n \n \tWARN_ON(info->refcnt > 0);\n+\n+\twrite_unlock_irqrestore(&evtchn_rwlock, flags);\n \n \tkfree(info);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tunsigned long flags;",
                "",
                "\twrite_lock_irqsave(&evtchn_rwlock, flags);",
                "",
                "\twrite_unlock_irqrestore(&evtchn_rwlock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27675",
        "func_name": "torvalds/linux/__xen_evtchn_do_upcall",
        "description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "git_url": "https://github.com/torvalds/linux/commit/073d0552ead5bfc7a3a9c01de590e924f11b5dd2",
        "commit_title": "xen/events: avoid removing an event channel while handling it",
        "commit_text": " Today it can happen that an event channel is being removed from the system while the event handling loop is active. This can lead to a race resulting in crashes or WARN() splats when trying to access the irq_info structure related to the event channel.  Fix this problem by using a rwlock taken as reader in the event handling loop and as writer when deallocating the irq_info structure.  As the observed problem was a NULL dereference in evtchn_from_irq() make this function more robust against races by testing the irq_info pointer to be not NULL before dereferencing it.  And finally make all accesses to evtchn_to_irq[row][col] atomic ones in order to avoid seeing partial updates of an array element in irq handling. Note that irq handling can be entered only for event channels which have been valid before, so any not populated row isn't a problem in this regard, as rows are only ever added and never removed.  This is XSA-331.  Cc: stable@vger.kernel.org",
        "func_before": "static void __xen_evtchn_do_upcall(void)\n{\n\tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n\tint cpu = smp_processor_id();\n\n\tdo {\n\t\tvcpu_info->evtchn_upcall_pending = 0;\n\n\t\txen_evtchn_handle_events(cpu);\n\n\t\tBUG_ON(!irqs_disabled());\n\n\t\tvirt_rmb(); /* Hypervisor can set upcall pending. */\n\n\t} while (vcpu_info->evtchn_upcall_pending);\n}",
        "func": "static void __xen_evtchn_do_upcall(void)\n{\n\tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n\tint cpu = smp_processor_id();\n\n\tread_lock(&evtchn_rwlock);\n\n\tdo {\n\t\tvcpu_info->evtchn_upcall_pending = 0;\n\n\t\txen_evtchn_handle_events(cpu);\n\n\t\tBUG_ON(!irqs_disabled());\n\n\t\tvirt_rmb(); /* Hypervisor can set upcall pending. */\n\n\t} while (vcpu_info->evtchn_upcall_pending);\n\n\tread_unlock(&evtchn_rwlock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,8 @@\n {\n \tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n \tint cpu = smp_processor_id();\n+\n+\tread_lock(&evtchn_rwlock);\n \n \tdo {\n \t\tvcpu_info->evtchn_upcall_pending = 0;\n@@ -13,4 +15,6 @@\n \t\tvirt_rmb(); /* Hypervisor can set upcall pending. */\n \n \t} while (vcpu_info->evtchn_upcall_pending);\n+\n+\tread_unlock(&evtchn_rwlock);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tread_lock(&evtchn_rwlock);",
                "",
                "\tread_unlock(&evtchn_rwlock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29368",
        "func_name": "torvalds/linux/__split_huge_pmd",
        "description": "An issue was discovered in __split_huge_pmd in mm/huge_memory.c in the Linux kernel before 5.7.5. The copy-on-write implementation can grant unintended write access because of a race condition in a THP mapcount check, aka CID-c444eb564fb1.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c444eb564fb16645c172d550359cb3d75fe8a040",
        "commit_title": "Write protect anon page faults require an accurate mapcount to decide",
        "commit_text": "if to break the COW or not. This is implemented in the THP path with reuse_swap_page() -> page_trans_huge_map_swapcount()/page_trans_huge_mapcount().  If the COW triggers while the other processes sharing the page are under a huge pmd split, to do an accurate reading, we must ensure the mapcount isn't computed while it's being transferred from the head page to the tail pages.  reuse_swap_cache() already runs serialized by the page lock, so it's enough to add the page lock around __split_huge_pmd_locked too, in order to add the missing serialization.  Note: the commit in \"Fixes\" is just to facilitate the backporting, because the code before such commit didn't try to do an accurate THP mapcount calculation and it instead used the page_count() to decide if to COW or not. Both the page_count and the pin_count are THP-wide refcounts, so they're inaccurate if used in reuse_swap_page(). Reverting such commit (besides the unrelated fix to the local anon_vma assignment) would have also opened the window for memory corruption side effects to certain workloads as documented in such commit header.  Suggested-by: Jann Horn <jannh@google.com> Cc: stable@vger.kernel.org ",
        "func_before": "void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long address, bool freeze, struct page *page)\n{\n\tspinlock_t *ptl;\n\tstruct mmu_notifier_range range;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n\t\t\t\taddress & HPAGE_PMD_MASK,\n\t\t\t\t(address & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\tptl = pmd_lock(vma->vm_mm, pmd);\n\n\t/*\n\t * If caller asks to setup a migration entries, we need a page to check\n\t * pmd against. Otherwise we can end up replacing wrong page.\n\t */\n\tVM_BUG_ON(freeze && !page);\n\tif (page && page != pmd_page(*pmd))\n\t        goto out;\n\n\tif (pmd_trans_huge(*pmd)) {\n\t\tpage = pmd_page(*pmd);\n\t\tif (PageMlocked(page))\n\t\t\tclear_page_mlock(page);\n\t} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))\n\t\tgoto out;\n\t__split_huge_pmd_locked(vma, pmd, range.start, freeze);\nout:\n\tspin_unlock(ptl);\n\t/*\n\t * No need to double call mmu_notifier->invalidate_range() callback.\n\t * They are 3 cases to consider inside __split_huge_pmd_locked():\n\t *  1) pmdp_huge_clear_flush_notify() call invalidate_range() obvious\n\t *  2) __split_huge_zero_page_pmd() read only zero page and any write\n\t *    fault will trigger a flush_notify before pointing to a new page\n\t *    (it is fine if the secondary mmu keeps pointing to the old zero\n\t *    page in the meantime)\n\t *  3) Split a huge pmd into pte pointing to the same page. No need\n\t *     to invalidate secondary tlb entry they are all still valid.\n\t *     any further changes to individual pte will notify. So no need\n\t *     to call mmu_notifier->invalidate_range()\n\t */\n\tmmu_notifier_invalidate_range_only_end(&range);\n}",
        "func": "void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long address, bool freeze, struct page *page)\n{\n\tspinlock_t *ptl;\n\tstruct mmu_notifier_range range;\n\tbool was_locked = false;\n\tpmd_t _pmd;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n\t\t\t\taddress & HPAGE_PMD_MASK,\n\t\t\t\t(address & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\tptl = pmd_lock(vma->vm_mm, pmd);\n\n\t/*\n\t * If caller asks to setup a migration entries, we need a page to check\n\t * pmd against. Otherwise we can end up replacing wrong page.\n\t */\n\tVM_BUG_ON(freeze && !page);\n\tif (page) {\n\t\tVM_WARN_ON_ONCE(!PageLocked(page));\n\t\twas_locked = true;\n\t\tif (page != pmd_page(*pmd))\n\t\t\tgoto out;\n\t}\n\nrepeat:\n\tif (pmd_trans_huge(*pmd)) {\n\t\tif (!page) {\n\t\t\tpage = pmd_page(*pmd);\n\t\t\tif (unlikely(!trylock_page(page))) {\n\t\t\t\tget_page(page);\n\t\t\t\t_pmd = *pmd;\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tlock_page(page);\n\t\t\t\tspin_lock(ptl);\n\t\t\t\tif (unlikely(!pmd_same(*pmd, _pmd))) {\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tput_page(page);\n\t\t\t\t\tpage = NULL;\n\t\t\t\t\tgoto repeat;\n\t\t\t\t}\n\t\t\t\tput_page(page);\n\t\t\t}\n\t\t}\n\t\tif (PageMlocked(page))\n\t\t\tclear_page_mlock(page);\n\t} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))\n\t\tgoto out;\n\t__split_huge_pmd_locked(vma, pmd, range.start, freeze);\nout:\n\tspin_unlock(ptl);\n\tif (!was_locked && page)\n\t\tunlock_page(page);\n\t/*\n\t * No need to double call mmu_notifier->invalidate_range() callback.\n\t * They are 3 cases to consider inside __split_huge_pmd_locked():\n\t *  1) pmdp_huge_clear_flush_notify() call invalidate_range() obvious\n\t *  2) __split_huge_zero_page_pmd() read only zero page and any write\n\t *    fault will trigger a flush_notify before pointing to a new page\n\t *    (it is fine if the secondary mmu keeps pointing to the old zero\n\t *    page in the meantime)\n\t *  3) Split a huge pmd into pte pointing to the same page. No need\n\t *     to invalidate secondary tlb entry they are all still valid.\n\t *     any further changes to individual pte will notify. So no need\n\t *     to call mmu_notifier->invalidate_range()\n\t */\n\tmmu_notifier_invalidate_range_only_end(&range);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,8 @@\n {\n \tspinlock_t *ptl;\n \tstruct mmu_notifier_range range;\n+\tbool was_locked = false;\n+\tpmd_t _pmd;\n \n \tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n \t\t\t\taddress & HPAGE_PMD_MASK,\n@@ -15,11 +17,32 @@\n \t * pmd against. Otherwise we can end up replacing wrong page.\n \t */\n \tVM_BUG_ON(freeze && !page);\n-\tif (page && page != pmd_page(*pmd))\n-\t        goto out;\n+\tif (page) {\n+\t\tVM_WARN_ON_ONCE(!PageLocked(page));\n+\t\twas_locked = true;\n+\t\tif (page != pmd_page(*pmd))\n+\t\t\tgoto out;\n+\t}\n \n+repeat:\n \tif (pmd_trans_huge(*pmd)) {\n-\t\tpage = pmd_page(*pmd);\n+\t\tif (!page) {\n+\t\t\tpage = pmd_page(*pmd);\n+\t\t\tif (unlikely(!trylock_page(page))) {\n+\t\t\t\tget_page(page);\n+\t\t\t\t_pmd = *pmd;\n+\t\t\t\tspin_unlock(ptl);\n+\t\t\t\tlock_page(page);\n+\t\t\t\tspin_lock(ptl);\n+\t\t\t\tif (unlikely(!pmd_same(*pmd, _pmd))) {\n+\t\t\t\t\tunlock_page(page);\n+\t\t\t\t\tput_page(page);\n+\t\t\t\t\tpage = NULL;\n+\t\t\t\t\tgoto repeat;\n+\t\t\t\t}\n+\t\t\t\tput_page(page);\n+\t\t\t}\n+\t\t}\n \t\tif (PageMlocked(page))\n \t\t\tclear_page_mlock(page);\n \t} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))\n@@ -27,6 +50,8 @@\n \t__split_huge_pmd_locked(vma, pmd, range.start, freeze);\n out:\n \tspin_unlock(ptl);\n+\tif (!was_locked && page)\n+\t\tunlock_page(page);\n \t/*\n \t * No need to double call mmu_notifier->invalidate_range() callback.\n \t * They are 3 cases to consider inside __split_huge_pmd_locked():",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (page && page != pmd_page(*pmd))",
                "\t        goto out;",
                "\t\tpage = pmd_page(*pmd);"
            ],
            "added_lines": [
                "\tbool was_locked = false;",
                "\tpmd_t _pmd;",
                "\tif (page) {",
                "\t\tVM_WARN_ON_ONCE(!PageLocked(page));",
                "\t\twas_locked = true;",
                "\t\tif (page != pmd_page(*pmd))",
                "\t\t\tgoto out;",
                "\t}",
                "repeat:",
                "\t\tif (!page) {",
                "\t\t\tpage = pmd_page(*pmd);",
                "\t\t\tif (unlikely(!trylock_page(page))) {",
                "\t\t\t\tget_page(page);",
                "\t\t\t\t_pmd = *pmd;",
                "\t\t\t\tspin_unlock(ptl);",
                "\t\t\t\tlock_page(page);",
                "\t\t\t\tspin_lock(ptl);",
                "\t\t\t\tif (unlikely(!pmd_same(*pmd, _pmd))) {",
                "\t\t\t\t\tunlock_page(page);",
                "\t\t\t\t\tput_page(page);",
                "\t\t\t\t\tpage = NULL;",
                "\t\t\t\t\tgoto repeat;",
                "\t\t\t\t}",
                "\t\t\t\tput_page(page);",
                "\t\t\t}",
                "\t\t}",
                "\tif (!was_locked && page)",
                "\t\tunlock_page(page);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29369",
        "func_name": "torvalds/linux/detach_vmas_to_be_unmapped",
        "description": "An issue was discovered in mm/mmap.c in the Linux kernel before 5.7.11. There is a race condition between certain expand functions (expand_downwards and expand_upwards) and page-table free operations from an munmap call, aka CID-246c320a8cfe.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=246c320a8cfe0b11d81a4af38fa9985ef0cc9a4c",
        "commit_title": "VMA with VM_GROWSDOWN or VM_GROWSUP flag set can change their size under",
        "commit_text": "mmap_read_lock().  It can lead to race with __do_munmap():  \tThread A\t\t\tThread B __do_munmap()   detach_vmas_to_be_unmapped()   mmap_write_downgrade() \t\t\t\texpand_downwards() \t\t\t\t  vma->vm_start = address; \t\t\t\t  // The VMA now overlaps with \t\t\t\t  // VMAs detached by the Thread A \t\t\t\t// page fault populates expanded part \t\t\t\t// of the VMA   unmap_region()     // Zaps pagetables partly     // populated by Thread B  Similar race exists for expand_upwards().  The fix is to avoid downgrading mmap_lock in __do_munmap() if detached VMAs are next to VM_GROWSDOWN or VM_GROWSUP VMA.  [akpm@linux-foundation.org: s/mmap_sem/mmap_lock/ in comment]  Cc: Oleg Nesterov <oleg@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> Cc: <stable@vger.kernel.org>\t[4.20+] Link: http://lkml.kernel.org/r/20200709105309.42495-1-kirill.shutemov@linux.intel.com ",
        "func_before": "static void\ndetach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,\n\tstruct vm_area_struct *prev, unsigned long end)\n{\n\tstruct vm_area_struct **insertion_point;\n\tstruct vm_area_struct *tail_vma = NULL;\n\n\tinsertion_point = (prev ? &prev->vm_next : &mm->mmap);\n\tvma->vm_prev = NULL;\n\tdo {\n\t\tvma_rb_erase(vma, &mm->mm_rb);\n\t\tmm->map_count--;\n\t\ttail_vma = vma;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\n\t*insertion_point = vma;\n\tif (vma) {\n\t\tvma->vm_prev = prev;\n\t\tvma_gap_update(vma);\n\t} else\n\t\tmm->highest_vm_end = prev ? vm_end_gap(prev) : 0;\n\ttail_vma->vm_next = NULL;\n\n\t/* Kill the cache */\n\tvmacache_invalidate(mm);\n}",
        "func": "static bool\ndetach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,\n\tstruct vm_area_struct *prev, unsigned long end)\n{\n\tstruct vm_area_struct **insertion_point;\n\tstruct vm_area_struct *tail_vma = NULL;\n\n\tinsertion_point = (prev ? &prev->vm_next : &mm->mmap);\n\tvma->vm_prev = NULL;\n\tdo {\n\t\tvma_rb_erase(vma, &mm->mm_rb);\n\t\tmm->map_count--;\n\t\ttail_vma = vma;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\n\t*insertion_point = vma;\n\tif (vma) {\n\t\tvma->vm_prev = prev;\n\t\tvma_gap_update(vma);\n\t} else\n\t\tmm->highest_vm_end = prev ? vm_end_gap(prev) : 0;\n\ttail_vma->vm_next = NULL;\n\n\t/* Kill the cache */\n\tvmacache_invalidate(mm);\n\n\t/*\n\t * Do not downgrade mmap_lock if we are next to VM_GROWSDOWN or\n\t * VM_GROWSUP VMA. Such VMAs can change their size under\n\t * down_read(mmap_lock) and collide with the VMA we are about to unmap.\n\t */\n\tif (vma && (vma->vm_flags & VM_GROWSDOWN))\n\t\treturn false;\n\tif (prev && (prev->vm_flags & VM_GROWSUP))\n\t\treturn false;\n\treturn true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-static void\n+static bool\n detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,\n \tstruct vm_area_struct *prev, unsigned long end)\n {\n@@ -23,4 +23,15 @@\n \n \t/* Kill the cache */\n \tvmacache_invalidate(mm);\n+\n+\t/*\n+\t * Do not downgrade mmap_lock if we are next to VM_GROWSDOWN or\n+\t * VM_GROWSUP VMA. Such VMAs can change their size under\n+\t * down_read(mmap_lock) and collide with the VMA we are about to unmap.\n+\t */\n+\tif (vma && (vma->vm_flags & VM_GROWSDOWN))\n+\t\treturn false;\n+\tif (prev && (prev->vm_flags & VM_GROWSUP))\n+\t\treturn false;\n+\treturn true;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static void"
            ],
            "added_lines": [
                "static bool",
                "",
                "\t/*",
                "\t * Do not downgrade mmap_lock if we are next to VM_GROWSDOWN or",
                "\t * VM_GROWSUP VMA. Such VMAs can change their size under",
                "\t * down_read(mmap_lock) and collide with the VMA we are about to unmap.",
                "\t */",
                "\tif (vma && (vma->vm_flags & VM_GROWSDOWN))",
                "\t\treturn false;",
                "\tif (prev && (prev->vm_flags & VM_GROWSUP))",
                "\t\treturn false;",
                "\treturn true;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29369",
        "func_name": "torvalds/linux/__do_munmap",
        "description": "An issue was discovered in mm/mmap.c in the Linux kernel before 5.7.11. There is a race condition between certain expand functions (expand_downwards and expand_upwards) and page-table free operations from an munmap call, aka CID-246c320a8cfe.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=246c320a8cfe0b11d81a4af38fa9985ef0cc9a4c",
        "commit_title": "VMA with VM_GROWSDOWN or VM_GROWSUP flag set can change their size under",
        "commit_text": "mmap_read_lock().  It can lead to race with __do_munmap():  \tThread A\t\t\tThread B __do_munmap()   detach_vmas_to_be_unmapped()   mmap_write_downgrade() \t\t\t\texpand_downwards() \t\t\t\t  vma->vm_start = address; \t\t\t\t  // The VMA now overlaps with \t\t\t\t  // VMAs detached by the Thread A \t\t\t\t// page fault populates expanded part \t\t\t\t// of the VMA   unmap_region()     // Zaps pagetables partly     // populated by Thread B  Similar race exists for expand_upwards().  The fix is to avoid downgrading mmap_lock in __do_munmap() if detached VMAs are next to VM_GROWSDOWN or VM_GROWSUP VMA.  [akpm@linux-foundation.org: s/mmap_sem/mmap_lock/ in comment]  Cc: Oleg Nesterov <oleg@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> Cc: <stable@vger.kernel.org>\t[4.20+] Link: http://lkml.kernel.org/r/20200709105309.42495-1-kirill.shutemov@linux.intel.com ",
        "func_before": "int __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t\tstruct list_head *uf, bool downgrade)\n{\n\tunsigned long end;\n\tstruct vm_area_struct *vma, *prev, *last;\n\n\tif ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * arch_unmap() might do unmaps itself.  It must be called\n\t * and finish any rbtree manipulation before this code\n\t * runs and also starts to manipulate the rbtree.\n\t */\n\tarch_unmap(mm, start, end);\n\n\t/* Find the first overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma)\n\t\treturn 0;\n\tprev = vma->vm_prev;\n\t/* we have  start < vma->vm_end  */\n\n\t/* if it doesn't overlap, we have nothing.. */\n\tif (vma->vm_start >= end)\n\t\treturn 0;\n\n\t/*\n\t * If we need to split any vma, do it now to save pain later.\n\t *\n\t * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially\n\t * unmapped vm_area_struct will remain in use: so lower split_vma\n\t * places tmp vma above, and higher split_vma places tmp vma below.\n\t */\n\tif (start > vma->vm_start) {\n\t\tint error;\n\n\t\t/*\n\t\t * Make sure that map_count on return from munmap() will\n\t\t * not exceed its limit; but let map_count go just above\n\t\t * its limit temporarily, to help free resources as expected.\n\t\t */\n\t\tif (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)\n\t\t\treturn -ENOMEM;\n\n\t\terror = __split_vma(mm, vma, start, 0);\n\t\tif (error)\n\t\t\treturn error;\n\t\tprev = vma;\n\t}\n\n\t/* Does it split the last one? */\n\tlast = find_vma(mm, end);\n\tif (last && end > last->vm_start) {\n\t\tint error = __split_vma(mm, last, end, 1);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tvma = prev ? prev->vm_next : mm->mmap;\n\n\tif (unlikely(uf)) {\n\t\t/*\n\t\t * If userfaultfd_unmap_prep returns an error the vmas\n\t\t * will remain splitted, but userland will get a\n\t\t * highly unexpected error anyway. This is no\n\t\t * different than the case where the first of the two\n\t\t * __split_vma fails, but we don't undo the first\n\t\t * split, despite we could. This is unlikely enough\n\t\t * failure that it's not worth optimizing it for.\n\t\t */\n\t\tint error = userfaultfd_unmap_prep(vma, start, end, uf);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * unlock any mlock()ed ranges before detaching vmas\n\t */\n\tif (mm->locked_vm) {\n\t\tstruct vm_area_struct *tmp = vma;\n\t\twhile (tmp && tmp->vm_start < end) {\n\t\t\tif (tmp->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm -= vma_pages(tmp);\n\t\t\t\tmunlock_vma_pages_all(tmp);\n\t\t\t}\n\n\t\t\ttmp = tmp->vm_next;\n\t\t}\n\t}\n\n\t/* Detach vmas from rbtree */\n\tdetach_vmas_to_be_unmapped(mm, vma, prev, end);\n\n\tif (downgrade)\n\t\tmmap_write_downgrade(mm);\n\n\tunmap_region(mm, vma, prev, start, end);\n\n\t/* Fix up all other VM information */\n\tremove_vma_list(mm, vma);\n\n\treturn downgrade ? 1 : 0;\n}",
        "func": "int __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t\tstruct list_head *uf, bool downgrade)\n{\n\tunsigned long end;\n\tstruct vm_area_struct *vma, *prev, *last;\n\n\tif ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * arch_unmap() might do unmaps itself.  It must be called\n\t * and finish any rbtree manipulation before this code\n\t * runs and also starts to manipulate the rbtree.\n\t */\n\tarch_unmap(mm, start, end);\n\n\t/* Find the first overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma)\n\t\treturn 0;\n\tprev = vma->vm_prev;\n\t/* we have  start < vma->vm_end  */\n\n\t/* if it doesn't overlap, we have nothing.. */\n\tif (vma->vm_start >= end)\n\t\treturn 0;\n\n\t/*\n\t * If we need to split any vma, do it now to save pain later.\n\t *\n\t * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially\n\t * unmapped vm_area_struct will remain in use: so lower split_vma\n\t * places tmp vma above, and higher split_vma places tmp vma below.\n\t */\n\tif (start > vma->vm_start) {\n\t\tint error;\n\n\t\t/*\n\t\t * Make sure that map_count on return from munmap() will\n\t\t * not exceed its limit; but let map_count go just above\n\t\t * its limit temporarily, to help free resources as expected.\n\t\t */\n\t\tif (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)\n\t\t\treturn -ENOMEM;\n\n\t\terror = __split_vma(mm, vma, start, 0);\n\t\tif (error)\n\t\t\treturn error;\n\t\tprev = vma;\n\t}\n\n\t/* Does it split the last one? */\n\tlast = find_vma(mm, end);\n\tif (last && end > last->vm_start) {\n\t\tint error = __split_vma(mm, last, end, 1);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tvma = prev ? prev->vm_next : mm->mmap;\n\n\tif (unlikely(uf)) {\n\t\t/*\n\t\t * If userfaultfd_unmap_prep returns an error the vmas\n\t\t * will remain splitted, but userland will get a\n\t\t * highly unexpected error anyway. This is no\n\t\t * different than the case where the first of the two\n\t\t * __split_vma fails, but we don't undo the first\n\t\t * split, despite we could. This is unlikely enough\n\t\t * failure that it's not worth optimizing it for.\n\t\t */\n\t\tint error = userfaultfd_unmap_prep(vma, start, end, uf);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * unlock any mlock()ed ranges before detaching vmas\n\t */\n\tif (mm->locked_vm) {\n\t\tstruct vm_area_struct *tmp = vma;\n\t\twhile (tmp && tmp->vm_start < end) {\n\t\t\tif (tmp->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm -= vma_pages(tmp);\n\t\t\t\tmunlock_vma_pages_all(tmp);\n\t\t\t}\n\n\t\t\ttmp = tmp->vm_next;\n\t\t}\n\t}\n\n\t/* Detach vmas from rbtree */\n\tif (!detach_vmas_to_be_unmapped(mm, vma, prev, end))\n\t\tdowngrade = false;\n\n\tif (downgrade)\n\t\tmmap_write_downgrade(mm);\n\n\tunmap_region(mm, vma, prev, start, end);\n\n\t/* Fix up all other VM information */\n\tremove_vma_list(mm, vma);\n\n\treturn downgrade ? 1 : 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -94,7 +94,8 @@\n \t}\n \n \t/* Detach vmas from rbtree */\n-\tdetach_vmas_to_be_unmapped(mm, vma, prev, end);\n+\tif (!detach_vmas_to_be_unmapped(mm, vma, prev, end))\n+\t\tdowngrade = false;\n \n \tif (downgrade)\n \t\tmmap_write_downgrade(mm);",
        "diff_line_info": {
            "deleted_lines": [
                "\tdetach_vmas_to_be_unmapped(mm, vma, prev, end);"
            ],
            "added_lines": [
                "\tif (!detach_vmas_to_be_unmapped(mm, vma, prev, end))",
                "\t\tdowngrade = false;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29370",
        "func_name": "torvalds/linux/kmem_cache_alloc_bulk",
        "description": "An issue was discovered in kmem_cache_alloc_bulk in mm/slub.c in the Linux kernel before 5.5.11. The slowpath lacks the required TID increment, aka CID-fd4d9c7d0c71.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=fd4d9c7d0c71866ec0c2825189ebd2ce35bd95b8",
        "commit_title": "When kmem_cache_alloc_bulk() attempts to allocate N objects from a percpu",
        "commit_text": "freelist of length M, and N > M > 0, it will first remove the M elements from the percpu freelist, then call ___slab_alloc() to allocate the next element and repopulate the percpu freelist. ___slab_alloc() can re-enable IRQs via allocate_slab(), so the TID must be bumped before ___slab_alloc() to properly commit the freelist head change.  Fix it by unconditionally bumping c->tid when entering the slowpath.  Cc: stable@vger.kernel.org ",
        "func_before": "int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,\n\t\t\t  void **p)\n{\n\tstruct kmem_cache_cpu *c;\n\tint i;\n\n\t/* memcg and kmem_cache debug support */\n\ts = slab_pre_alloc_hook(s, flags);\n\tif (unlikely(!s))\n\t\treturn false;\n\t/*\n\t * Drain objects in the per cpu slab, while disabling local\n\t * IRQs, which protects against PREEMPT and interrupts\n\t * handlers invoking normal fastpath.\n\t */\n\tlocal_irq_disable();\n\tc = this_cpu_ptr(s->cpu_slab);\n\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *object = c->freelist;\n\n\t\tif (unlikely(!object)) {\n\t\t\t/*\n\t\t\t * Invoking slow path likely have side-effect\n\t\t\t * of re-populating per CPU c->freelist\n\t\t\t */\n\t\t\tp[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,\n\t\t\t\t\t    _RET_IP_, c);\n\t\t\tif (unlikely(!p[i]))\n\t\t\t\tgoto error;\n\n\t\t\tc = this_cpu_ptr(s->cpu_slab);\n\t\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\n\t\t\tcontinue; /* goto for-loop */\n\t\t}\n\t\tc->freelist = get_freepointer(s, object);\n\t\tp[i] = object;\n\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\t}\n\tc->tid = next_tid(c->tid);\n\tlocal_irq_enable();\n\n\t/* Clear memory outside IRQ disabled fastpath loop */\n\tif (unlikely(slab_want_init_on_alloc(flags, s))) {\n\t\tint j;\n\n\t\tfor (j = 0; j < i; j++)\n\t\t\tmemset(p[j], 0, s->object_size);\n\t}\n\n\t/* memcg and kmem_cache debug support */\n\tslab_post_alloc_hook(s, flags, size, p);\n\treturn i;\nerror:\n\tlocal_irq_enable();\n\tslab_post_alloc_hook(s, flags, i, p);\n\t__kmem_cache_free_bulk(s, i, p);\n\treturn 0;\n}",
        "func": "int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,\n\t\t\t  void **p)\n{\n\tstruct kmem_cache_cpu *c;\n\tint i;\n\n\t/* memcg and kmem_cache debug support */\n\ts = slab_pre_alloc_hook(s, flags);\n\tif (unlikely(!s))\n\t\treturn false;\n\t/*\n\t * Drain objects in the per cpu slab, while disabling local\n\t * IRQs, which protects against PREEMPT and interrupts\n\t * handlers invoking normal fastpath.\n\t */\n\tlocal_irq_disable();\n\tc = this_cpu_ptr(s->cpu_slab);\n\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *object = c->freelist;\n\n\t\tif (unlikely(!object)) {\n\t\t\t/*\n\t\t\t * We may have removed an object from c->freelist using\n\t\t\t * the fastpath in the previous iteration; in that case,\n\t\t\t * c->tid has not been bumped yet.\n\t\t\t * Since ___slab_alloc() may reenable interrupts while\n\t\t\t * allocating memory, we should bump c->tid now.\n\t\t\t */\n\t\t\tc->tid = next_tid(c->tid);\n\n\t\t\t/*\n\t\t\t * Invoking slow path likely have side-effect\n\t\t\t * of re-populating per CPU c->freelist\n\t\t\t */\n\t\t\tp[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,\n\t\t\t\t\t    _RET_IP_, c);\n\t\t\tif (unlikely(!p[i]))\n\t\t\t\tgoto error;\n\n\t\t\tc = this_cpu_ptr(s->cpu_slab);\n\t\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\n\t\t\tcontinue; /* goto for-loop */\n\t\t}\n\t\tc->freelist = get_freepointer(s, object);\n\t\tp[i] = object;\n\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\t}\n\tc->tid = next_tid(c->tid);\n\tlocal_irq_enable();\n\n\t/* Clear memory outside IRQ disabled fastpath loop */\n\tif (unlikely(slab_want_init_on_alloc(flags, s))) {\n\t\tint j;\n\n\t\tfor (j = 0; j < i; j++)\n\t\t\tmemset(p[j], 0, s->object_size);\n\t}\n\n\t/* memcg and kmem_cache debug support */\n\tslab_post_alloc_hook(s, flags, size, p);\n\treturn i;\nerror:\n\tlocal_irq_enable();\n\tslab_post_alloc_hook(s, flags, i, p);\n\t__kmem_cache_free_bulk(s, i, p);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,6 +20,15 @@\n \t\tvoid *object = c->freelist;\n \n \t\tif (unlikely(!object)) {\n+\t\t\t/*\n+\t\t\t * We may have removed an object from c->freelist using\n+\t\t\t * the fastpath in the previous iteration; in that case,\n+\t\t\t * c->tid has not been bumped yet.\n+\t\t\t * Since ___slab_alloc() may reenable interrupts while\n+\t\t\t * allocating memory, we should bump c->tid now.\n+\t\t\t */\n+\t\t\tc->tid = next_tid(c->tid);\n+\n \t\t\t/*\n \t\t\t * Invoking slow path likely have side-effect\n \t\t\t * of re-populating per CPU c->freelist",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\t/*",
                "\t\t\t * We may have removed an object from c->freelist using",
                "\t\t\t * the fastpath in the previous iteration; in that case,",
                "\t\t\t * c->tid has not been bumped yet.",
                "\t\t\t * Since ___slab_alloc() may reenable interrupts while",
                "\t\t\t * allocating memory, we should bump c->tid now.",
                "\t\t\t */",
                "\t\t\tc->tid = next_tid(c->tid);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29372",
        "func_name": "torvalds/linux/do_madvise",
        "description": "An issue was discovered in do_madvise in mm/madvise.c in the Linux kernel before 5.6.8. There is a race condition between coredump operations and the IORING_OP_MADVISE implementation, aka CID-bc0c4d1e176e.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=bc0c4d1e176eeb614dc8734fc3ace34292771f11",
        "commit_title": "IORING_OP_MADVISE can end up basically doing mprotect() on the VM of",
        "commit_text": "another process, which means that it can race with our crazy core dump handling which accesses the VM state without holding the mmap_sem (because it incorrectly thinks that it is the final user).  This is clearly a core dumping problem, but we've never fixed it the right way, and instead have the notion of \"check that the mm is still ok\" using mmget_still_valid() after getting the mmap_sem for writing in any situation where we're not the original VM thread.  See commit 04f5866e41fb (\"coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping\") for more background on this whole mmget_still_valid() thing.  You might want to have a barf bag handy when you do.  We're discussing just fixing this properly in the only remaining core dumping routines.  But even if we do that, let's make do_madvise() do the right thing, and then when we fix core dumping, we can remove all these mmget_still_valid() checks.  Reported-and-tested-by: Jann Horn <jannh@google.com> ",
        "func_before": "int do_madvise(unsigned long start, size_t len_in, int behavior)\n{\n\tunsigned long end, tmp;\n\tstruct vm_area_struct *vma, *prev;\n\tint unmapped_error = 0;\n\tint error = -EINVAL;\n\tint write;\n\tsize_t len;\n\tstruct blk_plug plug;\n\n\tstart = untagged_addr(start);\n\n\tif (!madvise_behavior_valid(behavior))\n\t\treturn error;\n\n\tif (!PAGE_ALIGNED(start))\n\t\treturn error;\n\tlen = PAGE_ALIGN(len_in);\n\n\t/* Check to see whether len was rounded up from small -ve to zero */\n\tif (len_in && !len)\n\t\treturn error;\n\n\tend = start + len;\n\tif (end < start)\n\t\treturn error;\n\n\terror = 0;\n\tif (end == start)\n\t\treturn error;\n\n#ifdef CONFIG_MEMORY_FAILURE\n\tif (behavior == MADV_HWPOISON || behavior == MADV_SOFT_OFFLINE)\n\t\treturn madvise_inject_error(behavior, start, start + len_in);\n#endif\n\n\twrite = madvise_need_mmap_write(behavior);\n\tif (write) {\n\t\tif (down_write_killable(&current->mm->mmap_sem))\n\t\t\treturn -EINTR;\n\t} else {\n\t\tdown_read(&current->mm->mmap_sem);\n\t}\n\n\t/*\n\t * If the interval [start,end) covers some unmapped address\n\t * ranges, just ignore them, but return -ENOMEM at the end.\n\t * - different from the way of handling in mlock etc.\n\t */\n\tvma = find_vma_prev(current->mm, start, &prev);\n\tif (vma && start > vma->vm_start)\n\t\tprev = vma;\n\n\tblk_start_plug(&plug);\n\tfor (;;) {\n\t\t/* Still start < end. */\n\t\terror = -ENOMEM;\n\t\tif (!vma)\n\t\t\tgoto out;\n\n\t\t/* Here start < (end|vma->vm_end). */\n\t\tif (start < vma->vm_start) {\n\t\t\tunmapped_error = -ENOMEM;\n\t\t\tstart = vma->vm_start;\n\t\t\tif (start >= end)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/* Here vma->vm_start <= start < (end|vma->vm_end) */\n\t\ttmp = vma->vm_end;\n\t\tif (end < tmp)\n\t\t\ttmp = end;\n\n\t\t/* Here vma->vm_start <= start < tmp <= (end|vma->vm_end). */\n\t\terror = madvise_vma(vma, &prev, start, tmp, behavior);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tstart = tmp;\n\t\tif (prev && start < prev->vm_end)\n\t\t\tstart = prev->vm_end;\n\t\terror = unmapped_error;\n\t\tif (start >= end)\n\t\t\tgoto out;\n\t\tif (prev)\n\t\t\tvma = prev->vm_next;\n\t\telse\t/* madvise_remove dropped mmap_sem */\n\t\t\tvma = find_vma(current->mm, start);\n\t}\nout:\n\tblk_finish_plug(&plug);\n\tif (write)\n\t\tup_write(&current->mm->mmap_sem);\n\telse\n\t\tup_read(&current->mm->mmap_sem);\n\n\treturn error;\n}",
        "func": "int do_madvise(unsigned long start, size_t len_in, int behavior)\n{\n\tunsigned long end, tmp;\n\tstruct vm_area_struct *vma, *prev;\n\tint unmapped_error = 0;\n\tint error = -EINVAL;\n\tint write;\n\tsize_t len;\n\tstruct blk_plug plug;\n\n\tstart = untagged_addr(start);\n\n\tif (!madvise_behavior_valid(behavior))\n\t\treturn error;\n\n\tif (!PAGE_ALIGNED(start))\n\t\treturn error;\n\tlen = PAGE_ALIGN(len_in);\n\n\t/* Check to see whether len was rounded up from small -ve to zero */\n\tif (len_in && !len)\n\t\treturn error;\n\n\tend = start + len;\n\tif (end < start)\n\t\treturn error;\n\n\terror = 0;\n\tif (end == start)\n\t\treturn error;\n\n#ifdef CONFIG_MEMORY_FAILURE\n\tif (behavior == MADV_HWPOISON || behavior == MADV_SOFT_OFFLINE)\n\t\treturn madvise_inject_error(behavior, start, start + len_in);\n#endif\n\n\twrite = madvise_need_mmap_write(behavior);\n\tif (write) {\n\t\tif (down_write_killable(&current->mm->mmap_sem))\n\t\t\treturn -EINTR;\n\n\t\t/*\n\t\t * We may have stolen the mm from another process\n\t\t * that is undergoing core dumping.\n\t\t *\n\t\t * Right now that's io_ring, in the future it may\n\t\t * be remote process management and not \"current\"\n\t\t * at all.\n\t\t *\n\t\t * We need to fix core dumping to not do this,\n\t\t * but for now we have the mmget_still_valid()\n\t\t * model.\n\t\t */\n\t\tif (!mmget_still_valid(current->mm)) {\n\t\t\tup_write(&current->mm->mmap_sem);\n\t\t\treturn -EINTR;\n\t\t}\n\t} else {\n\t\tdown_read(&current->mm->mmap_sem);\n\t}\n\n\t/*\n\t * If the interval [start,end) covers some unmapped address\n\t * ranges, just ignore them, but return -ENOMEM at the end.\n\t * - different from the way of handling in mlock etc.\n\t */\n\tvma = find_vma_prev(current->mm, start, &prev);\n\tif (vma && start > vma->vm_start)\n\t\tprev = vma;\n\n\tblk_start_plug(&plug);\n\tfor (;;) {\n\t\t/* Still start < end. */\n\t\terror = -ENOMEM;\n\t\tif (!vma)\n\t\t\tgoto out;\n\n\t\t/* Here start < (end|vma->vm_end). */\n\t\tif (start < vma->vm_start) {\n\t\t\tunmapped_error = -ENOMEM;\n\t\t\tstart = vma->vm_start;\n\t\t\tif (start >= end)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/* Here vma->vm_start <= start < (end|vma->vm_end) */\n\t\ttmp = vma->vm_end;\n\t\tif (end < tmp)\n\t\t\ttmp = end;\n\n\t\t/* Here vma->vm_start <= start < tmp <= (end|vma->vm_end). */\n\t\terror = madvise_vma(vma, &prev, start, tmp, behavior);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tstart = tmp;\n\t\tif (prev && start < prev->vm_end)\n\t\t\tstart = prev->vm_end;\n\t\terror = unmapped_error;\n\t\tif (start >= end)\n\t\t\tgoto out;\n\t\tif (prev)\n\t\t\tvma = prev->vm_next;\n\t\telse\t/* madvise_remove dropped mmap_sem */\n\t\t\tvma = find_vma(current->mm, start);\n\t}\nout:\n\tblk_finish_plug(&plug);\n\tif (write)\n\t\tup_write(&current->mm->mmap_sem);\n\telse\n\t\tup_read(&current->mm->mmap_sem);\n\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,6 +38,23 @@\n \tif (write) {\n \t\tif (down_write_killable(&current->mm->mmap_sem))\n \t\t\treturn -EINTR;\n+\n+\t\t/*\n+\t\t * We may have stolen the mm from another process\n+\t\t * that is undergoing core dumping.\n+\t\t *\n+\t\t * Right now that's io_ring, in the future it may\n+\t\t * be remote process management and not \"current\"\n+\t\t * at all.\n+\t\t *\n+\t\t * We need to fix core dumping to not do this,\n+\t\t * but for now we have the mmget_still_valid()\n+\t\t * model.\n+\t\t */\n+\t\tif (!mmget_still_valid(current->mm)) {\n+\t\t\tup_write(&current->mm->mmap_sem);\n+\t\t\treturn -EINTR;\n+\t\t}\n \t} else {\n \t\tdown_read(&current->mm->mmap_sem);\n \t}",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t\t/*",
                "\t\t * We may have stolen the mm from another process",
                "\t\t * that is undergoing core dumping.",
                "\t\t *",
                "\t\t * Right now that's io_ring, in the future it may",
                "\t\t * be remote process management and not \"current\"",
                "\t\t * at all.",
                "\t\t *",
                "\t\t * We need to fix core dumping to not do this,",
                "\t\t * but for now we have the mmget_still_valid()",
                "\t\t * model.",
                "\t\t */",
                "\t\tif (!mmget_still_valid(current->mm)) {",
                "\t\t\tup_write(&current->mm->mmap_sem);",
                "\t\t\treturn -EINTR;",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/can_follow_write_pmd",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)\n{\n\treturn pmd_write(pmd) ||\n\t       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));\n}",
        "func": "static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)\n{\n\treturn pmd_write(pmd) || ((flags & FOLL_COW) && pmd_dirty(pmd));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,4 @@\n static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)\n {\n-\treturn pmd_write(pmd) ||\n-\t       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));\n+\treturn pmd_write(pmd) || ((flags & FOLL_COW) && pmd_dirty(pmd));\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn pmd_write(pmd) ||",
                "\t       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));"
            ],
            "added_lines": [
                "\treturn pmd_write(pmd) || ((flags & FOLL_COW) && pmd_dirty(pmd));"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/i915_gem_userptr_get_pages",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)\n{\n\tconst unsigned long num_pages = obj->base.size >> PAGE_SHIFT;\n\tstruct mm_struct *mm = obj->userptr.mm->mm;\n\tstruct page **pvec;\n\tstruct sg_table *pages;\n\tbool active;\n\tint pinned;\n\n\t/* If userspace should engineer that these pages are replaced in\n\t * the vma between us binding this page into the GTT and completion\n\t * of rendering... Their loss. If they change the mapping of their\n\t * pages they need to create a new bo to point to the new vma.\n\t *\n\t * However, that still leaves open the possibility of the vma\n\t * being copied upon fork. Which falls under the same userspace\n\t * synchronisation issue as a regular bo, except that this time\n\t * the process may not be expecting that a particular piece of\n\t * memory is tied to the GPU.\n\t *\n\t * Fortunately, we can hook into the mmu_notifier in order to\n\t * discard the page references prior to anything nasty happening\n\t * to the vma (discard or cloning) which should prevent the more\n\t * egregious cases from causing harm.\n\t */\n\n\tif (obj->userptr.work) {\n\t\t/* active flag should still be held for the pending work */\n\t\tif (IS_ERR(obj->userptr.work))\n\t\t\treturn PTR_ERR(obj->userptr.work);\n\t\telse\n\t\t\treturn -EAGAIN;\n\t}\n\n\tpvec = NULL;\n\tpinned = 0;\n\n\tif (mm == current->mm) {\n\t\tpvec = kvmalloc_array(num_pages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL |\n\t\t\t\t      __GFP_NORETRY |\n\t\t\t\t      __GFP_NOWARN);\n\t\tif (pvec) /* defer to worker if malloc fails */\n\t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n\t\t\t\t\t\t       num_pages,\n\t\t\t\t\t\t       !i915_gem_object_is_readonly(obj),\n\t\t\t\t\t\t       pvec);\n\t}\n\n\tactive = false;\n\tif (pinned < 0) {\n\t\tpages = ERR_PTR(pinned);\n\t\tpinned = 0;\n\t} else if (pinned < num_pages) {\n\t\tpages = __i915_gem_userptr_get_pages_schedule(obj);\n\t\tactive = pages == ERR_PTR(-EAGAIN);\n\t} else {\n\t\tpages = __i915_gem_userptr_alloc_pages(obj, pvec, num_pages);\n\t\tactive = !IS_ERR(pages);\n\t}\n\tif (active)\n\t\t__i915_gem_userptr_set_active(obj, true);\n\n\tif (IS_ERR(pages))\n\t\trelease_pages(pvec, pinned);\n\tkvfree(pvec);\n\n\treturn PTR_ERR_OR_ZERO(pages);\n}",
        "func": "static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)\n{\n\tconst unsigned long num_pages = obj->base.size >> PAGE_SHIFT;\n\tstruct mm_struct *mm = obj->userptr.mm->mm;\n\tstruct page **pvec;\n\tstruct sg_table *pages;\n\tbool active;\n\tint pinned;\n\n\t/* If userspace should engineer that these pages are replaced in\n\t * the vma between us binding this page into the GTT and completion\n\t * of rendering... Their loss. If they change the mapping of their\n\t * pages they need to create a new bo to point to the new vma.\n\t *\n\t * However, that still leaves open the possibility of the vma\n\t * being copied upon fork. Which falls under the same userspace\n\t * synchronisation issue as a regular bo, except that this time\n\t * the process may not be expecting that a particular piece of\n\t * memory is tied to the GPU.\n\t *\n\t * Fortunately, we can hook into the mmu_notifier in order to\n\t * discard the page references prior to anything nasty happening\n\t * to the vma (discard or cloning) which should prevent the more\n\t * egregious cases from causing harm.\n\t */\n\n\tif (obj->userptr.work) {\n\t\t/* active flag should still be held for the pending work */\n\t\tif (IS_ERR(obj->userptr.work))\n\t\t\treturn PTR_ERR(obj->userptr.work);\n\t\telse\n\t\t\treturn -EAGAIN;\n\t}\n\n\tpvec = NULL;\n\tpinned = 0;\n\n\tif (mm == current->mm) {\n\t\tpvec = kvmalloc_array(num_pages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL |\n\t\t\t\t      __GFP_NORETRY |\n\t\t\t\t      __GFP_NOWARN);\n\t\t/*\n\t\t * Using __get_user_pages_fast() with a read-only\n\t\t * access is questionable. A read-only page may be\n\t\t * COW-broken, and then this might end up giving\n\t\t * the wrong side of the COW..\n\t\t *\n\t\t * We may or may not care.\n\t\t */\n\t\tif (pvec) /* defer to worker if malloc fails */\n\t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n\t\t\t\t\t\t       num_pages,\n\t\t\t\t\t\t       !i915_gem_object_is_readonly(obj),\n\t\t\t\t\t\t       pvec);\n\t}\n\n\tactive = false;\n\tif (pinned < 0) {\n\t\tpages = ERR_PTR(pinned);\n\t\tpinned = 0;\n\t} else if (pinned < num_pages) {\n\t\tpages = __i915_gem_userptr_get_pages_schedule(obj);\n\t\tactive = pages == ERR_PTR(-EAGAIN);\n\t} else {\n\t\tpages = __i915_gem_userptr_alloc_pages(obj, pvec, num_pages);\n\t\tactive = !IS_ERR(pages);\n\t}\n\tif (active)\n\t\t__i915_gem_userptr_set_active(obj, true);\n\n\tif (IS_ERR(pages))\n\t\trelease_pages(pvec, pinned);\n\tkvfree(pvec);\n\n\treturn PTR_ERR_OR_ZERO(pages);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,6 +40,14 @@\n \t\t\t\t      GFP_KERNEL |\n \t\t\t\t      __GFP_NORETRY |\n \t\t\t\t      __GFP_NOWARN);\n+\t\t/*\n+\t\t * Using __get_user_pages_fast() with a read-only\n+\t\t * access is questionable. A read-only page may be\n+\t\t * COW-broken, and then this might end up giving\n+\t\t * the wrong side of the COW..\n+\t\t *\n+\t\t * We may or may not care.\n+\t\t */\n \t\tif (pvec) /* defer to worker if malloc fails */\n \t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n \t\t\t\t\t\t       num_pages,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t/*",
                "\t\t * Using __get_user_pages_fast() with a read-only",
                "\t\t * access is questionable. A read-only page may be",
                "\t\t * COW-broken, and then this might end up giving",
                "\t\t * the wrong side of the COW..",
                "\t\t *",
                "\t\t * We may or may not care.",
                "\t\t */"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/__get_user_pages_fast",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "int __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages)\n{\n\tunsigned long len, end;\n\tunsigned long flags;\n\tint nr_pinned = 0;\n\t/*\n\t * Internally (within mm/gup.c), gup fast variants must set FOLL_GET,\n\t * because gup fast is always a \"pin with a +1 page refcount\" request.\n\t */\n\tunsigned int gup_flags = FOLL_GET;\n\n\tif (write)\n\t\tgup_flags |= FOLL_WRITE;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn 0;\n\n\t/*\n\t * Disable interrupts.  We use the nested form as we can already have\n\t * interrupts disabled by get_futex_key.\n\t *\n\t * With interrupts disabled, we block page table pages from being\n\t * freed from under us. See struct mmu_table_batch comments in\n\t * include/asm-generic/tlb.h for more details.\n\t *\n\t * We do not adopt an rcu_read_lock(.) here as we also want to\n\t * block IPIs that come from THPs splitting.\n\t */\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_save(flags);\n\t\tgup_pgd_range(start, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\treturn nr_pinned;\n}",
        "func": "int __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages)\n{\n\tunsigned long len, end;\n\tunsigned long flags;\n\tint nr_pinned = 0;\n\t/*\n\t * Internally (within mm/gup.c), gup fast variants must set FOLL_GET,\n\t * because gup fast is always a \"pin with a +1 page refcount\" request.\n\t */\n\tunsigned int gup_flags = FOLL_GET;\n\n\tif (write)\n\t\tgup_flags |= FOLL_WRITE;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn 0;\n\n\t/*\n\t * Disable interrupts.  We use the nested form as we can already have\n\t * interrupts disabled by get_futex_key.\n\t *\n\t * With interrupts disabled, we block page table pages from being\n\t * freed from under us. See struct mmu_table_batch comments in\n\t * include/asm-generic/tlb.h for more details.\n\t *\n\t * We do not adopt an rcu_read_lock(.) here as we also want to\n\t * block IPIs that come from THPs splitting.\n\t *\n\t * NOTE! We allow read-only gup_fast() here, but you'd better be\n\t * careful about possible COW pages. You'll get _a_ COW page, but\n\t * not necessarily the one you intended to get depending on what\n\t * COW event happens after this. COW may break the page copy in a\n\t * random direction.\n\t */\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_save(flags);\n\t\tgup_pgd_range(start, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\treturn nr_pinned;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -32,6 +32,12 @@\n \t *\n \t * We do not adopt an rcu_read_lock(.) here as we also want to\n \t * block IPIs that come from THPs splitting.\n+\t *\n+\t * NOTE! We allow read-only gup_fast() here, but you'd better be\n+\t * careful about possible COW pages. You'll get _a_ COW page, but\n+\t * not necessarily the one you intended to get depending on what\n+\t * COW event happens after this. COW may break the page copy in a\n+\t * random direction.\n \t */\n \n \tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t *",
                "\t * NOTE! We allow read-only gup_fast() here, but you'd better be",
                "\t * careful about possible COW pages. You'll get _a_ COW page, but",
                "\t * not necessarily the one you intended to get depending on what",
                "\t * COW event happens after this. COW may break the page copy in a",
                "\t * random direction."
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/internal_get_user_pages_fast",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static int internal_get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\t\t\tunsigned int gup_flags,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long addr, len, end;\n\tint nr_pinned = 0, ret = 0;\n\n\tif (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |\n\t\t\t\t       FOLL_FORCE | FOLL_PIN | FOLL_GET)))\n\t\treturn -EINVAL;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn -EFAULT;\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_disable();\n\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_enable();\n\t\tret = nr_pinned;\n\t}\n\n\tif (nr_pinned < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr_pinned << PAGE_SHIFT;\n\t\tpages += nr_pinned;\n\n\t\tret = __gup_longterm_unlocked(start, nr_pages - nr_pinned,\n\t\t\t\t\t      gup_flags, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr_pinned > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr_pinned;\n\t\t\telse\n\t\t\t\tret += nr_pinned;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "func": "static int internal_get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\t\t\tunsigned int gup_flags,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long addr, len, end;\n\tint nr_pinned = 0, ret = 0;\n\n\tif (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |\n\t\t\t\t       FOLL_FORCE | FOLL_PIN | FOLL_GET)))\n\t\treturn -EINVAL;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,\n\t * because get_user_pages() may need to cause an early COW in\n\t * order to avoid confusing the normal COW routines. So only\n\t * targets that are already writable are safe to do by just\n\t * looking at the page tables.\n\t */\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_disable();\n\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);\n\t\tlocal_irq_enable();\n\t\tret = nr_pinned;\n\t}\n\n\tif (nr_pinned < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr_pinned << PAGE_SHIFT;\n\t\tpages += nr_pinned;\n\n\t\tret = __gup_longterm_unlocked(start, nr_pages - nr_pinned,\n\t\t\t\t\t      gup_flags, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr_pinned > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr_pinned;\n\t\t\telse\n\t\t\t\tret += nr_pinned;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,10 +19,17 @@\n \tif (unlikely(!access_ok((void __user *)start, len)))\n \t\treturn -EFAULT;\n \n+\t/*\n+\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,\n+\t * because get_user_pages() may need to cause an early COW in\n+\t * order to avoid confusing the normal COW routines. So only\n+\t * targets that are already writable are safe to do by just\n+\t * looking at the page tables.\n+\t */\n \tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n \t    gup_fast_permitted(start, end)) {\n \t\tlocal_irq_disable();\n-\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);\n+\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);\n \t\tlocal_irq_enable();\n \t\tret = nr_pinned;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);"
            ],
            "added_lines": [
                "\t/*",
                "\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,",
                "\t * because get_user_pages() may need to cause an early COW in",
                "\t * order to avoid confusing the normal COW routines. So only",
                "\t * targets that are already writable are safe to do by just",
                "\t * looking at the page tables.",
                "\t */",
                "\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/__get_user_pages",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked)\n{\n\tlong ret = 0, i = 0;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct follow_page_context ctx = { NULL };\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tstart = untagged_addr(start);\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tctx.page_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tgup_flags, locked);\n\t\t\t\tif (locked && *locked == 0) {\n\t\t\t\t\t/*\n\t\t\t\t\t * We've got a VM_FAULT_RETRY\n\t\t\t\t\t * and we've lost mmap_sem.\n\t\t\t\t\t * We must stop here.\n\t\t\t\t\t */\n\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);\n\t\t\t\t\tBUG_ON(ret != 0);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tcond_resched();\n\n\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);\n\t\tif (!page) {\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\t   locked);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EBUSY:\n\t\t\t\tret = 0;\n\t\t\t\tfallthrough;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\tgoto out;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tctx.page_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tctx.page_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\nout:\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn i ? i : ret;\n}",
        "func": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked)\n{\n\tlong ret = 0, i = 0;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct follow_page_context ctx = { NULL };\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tstart = untagged_addr(start);\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tctx.page_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\tif (should_force_cow_break(vma, foll_flags))\n\t\t\t\t\tfoll_flags |= FOLL_WRITE;\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tfoll_flags, locked);\n\t\t\t\tif (locked && *locked == 0) {\n\t\t\t\t\t/*\n\t\t\t\t\t * We've got a VM_FAULT_RETRY\n\t\t\t\t\t * and we've lost mmap_sem.\n\t\t\t\t\t * We must stop here.\n\t\t\t\t\t */\n\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);\n\t\t\t\t\tBUG_ON(ret != 0);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (should_force_cow_break(vma, foll_flags))\n\t\t\tfoll_flags |= FOLL_WRITE;\n\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tcond_resched();\n\n\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);\n\t\tif (!page) {\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\t   locked);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EBUSY:\n\t\t\t\tret = 0;\n\t\t\t\tfallthrough;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\tgoto out;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tctx.page_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tctx.page_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\nout:\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn i ? i : ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -45,9 +45,11 @@\n \t\t\t\tgoto out;\n \t\t\t}\n \t\t\tif (is_vm_hugetlb_page(vma)) {\n+\t\t\t\tif (should_force_cow_break(vma, foll_flags))\n+\t\t\t\t\tfoll_flags |= FOLL_WRITE;\n \t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n \t\t\t\t\t\t&start, &nr_pages, i,\n-\t\t\t\t\t\tgup_flags, locked);\n+\t\t\t\t\t\tfoll_flags, locked);\n \t\t\t\tif (locked && *locked == 0) {\n \t\t\t\t\t/*\n \t\t\t\t\t * We've got a VM_FAULT_RETRY\n@@ -61,6 +63,10 @@\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t}\n+\n+\t\tif (should_force_cow_break(vma, foll_flags))\n+\t\t\tfoll_flags |= FOLL_WRITE;\n+\n retry:\n \t\t/*\n \t\t * If we have a pending SIGKILL, don't keep faulting pages and",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t\t\tgup_flags, locked);"
            ],
            "added_lines": [
                "\t\t\t\tif (should_force_cow_break(vma, foll_flags))",
                "\t\t\t\t\tfoll_flags |= FOLL_WRITE;",
                "\t\t\t\t\t\tfoll_flags, locked);",
                "",
                "\t\tif (should_force_cow_break(vma, foll_flags))",
                "\t\t\tfoll_flags |= FOLL_WRITE;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/can_follow_write_pte",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n{\n\treturn pte_write(pte) ||\n\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n}",
        "func": "static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n{\n\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,4 @@\n static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n {\n-\treturn pte_write(pte) ||\n-\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n+\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn pte_write(pte) ||",
                "\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));"
            ],
            "added_lines": [
                "\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3348",
        "func_name": "torvalds/linux/nbd_add_socket",
        "description": "nbd_add_socket in drivers/block/nbd.c in the Linux kernel through 5.10.12 has an ndb_queue_rq use-after-free that could be triggered by local attackers (with access to the nbd device) via an I/O request at a certain point during device setup, aka CID-b98e762e3d71.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b98e762e3d71e893b221f871825dc64694cfb258",
        "commit_title": "When setting up a device, we can krealloc the config->socks array to add",
        "commit_text": "new sockets to the configuration.  However if we happen to get a IO request in at this point even though we aren't setup we could hit a UAF, as we deref config->socks without any locking, assuming that the configuration was setup already and that ->socks is safe to access it as we have a reference on the configuration.  But there's nothing really preventing IO from occurring at this point of the device setup, we don't want to incur the overhead of a lock to access ->socks when it will never change while the device is running. To fix this UAF scenario simply freeze the queue if we are adding sockets.  This will protect us from this particular case without adding any additional overhead for the normal running case.  Cc: stable@vger.kernel.org ",
        "func_before": "static int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,\n\t\t\t  bool netlink)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock;\n\tstruct nbd_sock **socks;\n\tstruct nbd_sock *nsock;\n\tint err;\n\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\tif (!netlink && !nbd->task_setup &&\n\t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n\t\tnbd->task_setup = current;\n\n\tif (!netlink &&\n\t    (nbd->task_setup != current ||\n\t     test_bit(NBD_RT_BOUND, &config->runtime_flags))) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"Device being setup by another task\");\n\t\terr = -EBUSY;\n\t\tgoto put_socket;\n\t}\n\n\tnsock = kzalloc(sizeof(*nsock), GFP_KERNEL);\n\tif (!nsock) {\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tsocks = krealloc(config->socks, (config->num_connections + 1) *\n\t\t\t sizeof(struct nbd_sock *), GFP_KERNEL);\n\tif (!socks) {\n\t\tkfree(nsock);\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tconfig->socks = socks;\n\n\tnsock->fallback_index = -1;\n\tnsock->dead = false;\n\tmutex_init(&nsock->tx_lock);\n\tnsock->sock = sock;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\tnsock->cookie = 0;\n\tsocks[config->num_connections++] = nsock;\n\tatomic_inc(&config->live_connections);\n\n\treturn 0;\n\nput_socket:\n\tsockfd_put(sock);\n\treturn err;\n}",
        "func": "static int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,\n\t\t\t  bool netlink)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock;\n\tstruct nbd_sock **socks;\n\tstruct nbd_sock *nsock;\n\tint err;\n\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\t/*\n\t * We need to make sure we don't get any errant requests while we're\n\t * reallocating the ->socks array.\n\t */\n\tblk_mq_freeze_queue(nbd->disk->queue);\n\n\tif (!netlink && !nbd->task_setup &&\n\t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n\t\tnbd->task_setup = current;\n\n\tif (!netlink &&\n\t    (nbd->task_setup != current ||\n\t     test_bit(NBD_RT_BOUND, &config->runtime_flags))) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"Device being setup by another task\");\n\t\terr = -EBUSY;\n\t\tgoto put_socket;\n\t}\n\n\tnsock = kzalloc(sizeof(*nsock), GFP_KERNEL);\n\tif (!nsock) {\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tsocks = krealloc(config->socks, (config->num_connections + 1) *\n\t\t\t sizeof(struct nbd_sock *), GFP_KERNEL);\n\tif (!socks) {\n\t\tkfree(nsock);\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tconfig->socks = socks;\n\n\tnsock->fallback_index = -1;\n\tnsock->dead = false;\n\tmutex_init(&nsock->tx_lock);\n\tnsock->sock = sock;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\tnsock->cookie = 0;\n\tsocks[config->num_connections++] = nsock;\n\tatomic_inc(&config->live_connections);\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\n\treturn 0;\n\nput_socket:\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\tsockfd_put(sock);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,12 @@\n \tsock = nbd_get_socket(nbd, arg, &err);\n \tif (!sock)\n \t\treturn err;\n+\n+\t/*\n+\t * We need to make sure we don't get any errant requests while we're\n+\t * reallocating the ->socks array.\n+\t */\n+\tblk_mq_freeze_queue(nbd->disk->queue);\n \n \tif (!netlink && !nbd->task_setup &&\n \t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n@@ -49,10 +55,12 @@\n \tnsock->cookie = 0;\n \tsocks[config->num_connections++] = nsock;\n \tatomic_inc(&config->live_connections);\n+\tblk_mq_unfreeze_queue(nbd->disk->queue);\n \n \treturn 0;\n \n put_socket:\n+\tblk_mq_unfreeze_queue(nbd->disk->queue);\n \tsockfd_put(sock);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t/*",
                "\t * We need to make sure we don't get any errant requests while we're",
                "\t * reallocating the ->socks array.",
                "\t */",
                "\tblk_mq_freeze_queue(nbd->disk->queue);",
                "\tblk_mq_unfreeze_queue(nbd->disk->queue);",
                "\tblk_mq_unfreeze_queue(nbd->disk->queue);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-20261",
        "func_name": "torvalds/linux/lock_fdc",
        "description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a0c80efe5956ccce9fe7ae5c78542578c07bc20a",
        "commit_title": "floppy_revalidate() doesn't perform any error handling on lock_fdc()",
        "commit_text": "result. lock_fdc() might actually be interrupted by a signal (it waits for fdc becoming non-busy interruptibly). In such case, floppy_revalidate() proceeds as if it had claimed the lock, but it fact it doesn't.  In case of multiple threads trying to open(\"/dev/fdX\"), this leads to serious corruptions all over the place, because all of a sudden there is no critical section protection (that'd otherwise be guaranteed by locked fd) whatsoever.  While at this, fix the fact that the 'interruptible' parameter to lock_fdc() doesn't make any sense whatsoever, because we always wait interruptibly anyway.  Most of the lock_fdc() callsites do properly handle error (and propagate EINTR), but floppy_revalidate() and floppy_check_events() don't. Fix this.  Spotted by 'syzkaller' tool.  ",
        "func_before": "static int lock_fdc(int drive, bool interruptible)\n{\n\tif (WARN(atomic_read(&usage_count) == 0,\n\t\t \"Trying to lock fdc while usage count=0\\n\"))\n\t\treturn -1;\n\n\tif (wait_event_interruptible(fdc_wait, !test_and_set_bit(0, &fdc_busy)))\n\t\treturn -EINTR;\n\n\tcommand_status = FD_COMMAND_NONE;\n\n\treschedule_timeout(drive, \"lock fdc\");\n\tset_fdc(drive);\n\treturn 0;\n}",
        "func": "static int lock_fdc(int drive)\n{\n\tif (WARN(atomic_read(&usage_count) == 0,\n\t\t \"Trying to lock fdc while usage count=0\\n\"))\n\t\treturn -1;\n\n\tif (wait_event_interruptible(fdc_wait, !test_and_set_bit(0, &fdc_busy)))\n\t\treturn -EINTR;\n\n\tcommand_status = FD_COMMAND_NONE;\n\n\treschedule_timeout(drive, \"lock fdc\");\n\tset_fdc(drive);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-static int lock_fdc(int drive, bool interruptible)\n+static int lock_fdc(int drive)\n {\n \tif (WARN(atomic_read(&usage_count) == 0,\n \t\t \"Trying to lock fdc while usage count=0\\n\"))",
        "diff_line_info": {
            "deleted_lines": [
                "static int lock_fdc(int drive, bool interruptible)"
            ],
            "added_lines": [
                "static int lock_fdc(int drive)"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-20261",
        "func_name": "torvalds/linux/get_floppy_geometry",
        "description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a0c80efe5956ccce9fe7ae5c78542578c07bc20a",
        "commit_title": "floppy_revalidate() doesn't perform any error handling on lock_fdc()",
        "commit_text": "result. lock_fdc() might actually be interrupted by a signal (it waits for fdc becoming non-busy interruptibly). In such case, floppy_revalidate() proceeds as if it had claimed the lock, but it fact it doesn't.  In case of multiple threads trying to open(\"/dev/fdX\"), this leads to serious corruptions all over the place, because all of a sudden there is no critical section protection (that'd otherwise be guaranteed by locked fd) whatsoever.  While at this, fix the fact that the 'interruptible' parameter to lock_fdc() doesn't make any sense whatsoever, because we always wait interruptibly anyway.  Most of the lock_fdc() callsites do properly handle error (and propagate EINTR), but floppy_revalidate() and floppy_check_events() don't. Fix this.  Spotted by 'syzkaller' tool.  ",
        "func_before": "static int get_floppy_geometry(int drive, int type, struct floppy_struct **g)\n{\n\tif (type)\n\t\t*g = &floppy_type[type];\n\telse {\n\t\tif (lock_fdc(drive, false))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(false, 0) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t*g = current_type[drive];\n\t}\n\tif (!*g)\n\t\treturn -ENODEV;\n\treturn 0;\n}",
        "func": "static int get_floppy_geometry(int drive, int type, struct floppy_struct **g)\n{\n\tif (type)\n\t\t*g = &floppy_type[type];\n\telse {\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(false, 0) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t*g = current_type[drive];\n\t}\n\tif (!*g)\n\t\treturn -ENODEV;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n \tif (type)\n \t\t*g = &floppy_type[type];\n \telse {\n-\t\tif (lock_fdc(drive, false))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\tif (poll_drive(false, 0) == -EINTR)\n \t\t\treturn -EINTR;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (lock_fdc(drive, false))"
            ],
            "added_lines": [
                "\t\tif (lock_fdc(drive))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-20261",
        "func_name": "torvalds/linux/set_geometry",
        "description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a0c80efe5956ccce9fe7ae5c78542578c07bc20a",
        "commit_title": "floppy_revalidate() doesn't perform any error handling on lock_fdc()",
        "commit_text": "result. lock_fdc() might actually be interrupted by a signal (it waits for fdc becoming non-busy interruptibly). In such case, floppy_revalidate() proceeds as if it had claimed the lock, but it fact it doesn't.  In case of multiple threads trying to open(\"/dev/fdX\"), this leads to serious corruptions all over the place, because all of a sudden there is no critical section protection (that'd otherwise be guaranteed by locked fd) whatsoever.  While at this, fix the fact that the 'interruptible' parameter to lock_fdc() doesn't make any sense whatsoever, because we always wait interruptibly anyway.  Most of the lock_fdc() callsites do properly handle error (and propagate EINTR), but floppy_revalidate() and floppy_check_events() don't. Fix this.  Spotted by 'syzkaller' tool.  ",
        "func_before": "static int set_geometry(unsigned int cmd, struct floppy_struct *g,\n\t\t\t       int drive, int type, struct block_device *bdev)\n{\n\tint cnt;\n\n\t/* sanity checking for parameters. */\n\tif (g->sect <= 0 ||\n\t    g->head <= 0 ||\n\t    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||\n\t    /* check if reserved bits are set */\n\t    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)\n\t\treturn -EINVAL;\n\tif (type) {\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmutex_lock(&open_lock);\n\t\tif (lock_fdc(drive, true)) {\n\t\t\tmutex_unlock(&open_lock);\n\t\t\treturn -EINTR;\n\t\t}\n\t\tfloppy_type[type] = *g;\n\t\tfloppy_type[type].name = \"user format\";\n\t\tfor (cnt = type << 2; cnt < (type << 2) + 4; cnt++)\n\t\t\tfloppy_sizes[cnt] = floppy_sizes[cnt + 0x80] =\n\t\t\t    floppy_type[type].size + 1;\n\t\tprocess_fd_request();\n\t\tfor (cnt = 0; cnt < N_DRIVE; cnt++) {\n\t\t\tstruct block_device *bdev = opened_bdev[cnt];\n\t\t\tif (!bdev || ITYPE(drive_state[cnt].fd_device) != type)\n\t\t\t\tcontinue;\n\t\t\t__invalidate_device(bdev, true);\n\t\t}\n\t\tmutex_unlock(&open_lock);\n\t} else {\n\t\tint oldStretch;\n\n\t\tif (lock_fdc(drive, true))\n\t\t\treturn -EINTR;\n\t\tif (cmd != FDDEFPRM) {\n\t\t\t/* notice a disk change immediately, else\n\t\t\t * we lose our settings immediately*/\n\t\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\toldStretch = g->stretch;\n\t\tuser_params[drive] = *g;\n\t\tif (buffer_drive == drive)\n\t\t\tSUPBOUND(buffer_max, user_params[drive].sect);\n\t\tcurrent_type[drive] = &user_params[drive];\n\t\tfloppy_sizes[drive] = user_params[drive].size;\n\t\tif (cmd == FDDEFPRM)\n\t\t\tDRS->keep_data = -1;\n\t\telse\n\t\t\tDRS->keep_data = 1;\n\t\t/* invalidation. Invalidate only when needed, i.e.\n\t\t * when there are already sectors in the buffer cache\n\t\t * whose number will change. This is useful, because\n\t\t * mtools often changes the geometry of the disk after\n\t\t * looking at the boot block */\n\t\tif (DRS->maxblock > user_params[drive].sect ||\n\t\t    DRS->maxtrack ||\n\t\t    ((user_params[drive].sect ^ oldStretch) &\n\t\t     (FD_SWAPSIDES | FD_SECTBASEMASK)))\n\t\t\tinvalidate_drive(bdev);\n\t\telse\n\t\t\tprocess_fd_request();\n\t}\n\treturn 0;\n}",
        "func": "static int set_geometry(unsigned int cmd, struct floppy_struct *g,\n\t\t\t       int drive, int type, struct block_device *bdev)\n{\n\tint cnt;\n\n\t/* sanity checking for parameters. */\n\tif (g->sect <= 0 ||\n\t    g->head <= 0 ||\n\t    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||\n\t    /* check if reserved bits are set */\n\t    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)\n\t\treturn -EINVAL;\n\tif (type) {\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmutex_lock(&open_lock);\n\t\tif (lock_fdc(drive)) {\n\t\t\tmutex_unlock(&open_lock);\n\t\t\treturn -EINTR;\n\t\t}\n\t\tfloppy_type[type] = *g;\n\t\tfloppy_type[type].name = \"user format\";\n\t\tfor (cnt = type << 2; cnt < (type << 2) + 4; cnt++)\n\t\t\tfloppy_sizes[cnt] = floppy_sizes[cnt + 0x80] =\n\t\t\t    floppy_type[type].size + 1;\n\t\tprocess_fd_request();\n\t\tfor (cnt = 0; cnt < N_DRIVE; cnt++) {\n\t\t\tstruct block_device *bdev = opened_bdev[cnt];\n\t\t\tif (!bdev || ITYPE(drive_state[cnt].fd_device) != type)\n\t\t\t\tcontinue;\n\t\t\t__invalidate_device(bdev, true);\n\t\t}\n\t\tmutex_unlock(&open_lock);\n\t} else {\n\t\tint oldStretch;\n\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (cmd != FDDEFPRM) {\n\t\t\t/* notice a disk change immediately, else\n\t\t\t * we lose our settings immediately*/\n\t\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\toldStretch = g->stretch;\n\t\tuser_params[drive] = *g;\n\t\tif (buffer_drive == drive)\n\t\t\tSUPBOUND(buffer_max, user_params[drive].sect);\n\t\tcurrent_type[drive] = &user_params[drive];\n\t\tfloppy_sizes[drive] = user_params[drive].size;\n\t\tif (cmd == FDDEFPRM)\n\t\t\tDRS->keep_data = -1;\n\t\telse\n\t\t\tDRS->keep_data = 1;\n\t\t/* invalidation. Invalidate only when needed, i.e.\n\t\t * when there are already sectors in the buffer cache\n\t\t * whose number will change. This is useful, because\n\t\t * mtools often changes the geometry of the disk after\n\t\t * looking at the boot block */\n\t\tif (DRS->maxblock > user_params[drive].sect ||\n\t\t    DRS->maxtrack ||\n\t\t    ((user_params[drive].sect ^ oldStretch) &\n\t\t     (FD_SWAPSIDES | FD_SECTBASEMASK)))\n\t\t\tinvalidate_drive(bdev);\n\t\telse\n\t\t\tprocess_fd_request();\n\t}\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,7 @@\n \t\tif (!capable(CAP_SYS_ADMIN))\n \t\t\treturn -EPERM;\n \t\tmutex_lock(&open_lock);\n-\t\tif (lock_fdc(drive, true)) {\n+\t\tif (lock_fdc(drive)) {\n \t\t\tmutex_unlock(&open_lock);\n \t\t\treturn -EINTR;\n \t\t}\n@@ -34,7 +34,7 @@\n \t} else {\n \t\tint oldStretch;\n \n-\t\tif (lock_fdc(drive, true))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\tif (cmd != FDDEFPRM) {\n \t\t\t/* notice a disk change immediately, else",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (lock_fdc(drive, true)) {",
                "\t\tif (lock_fdc(drive, true))"
            ],
            "added_lines": [
                "\t\tif (lock_fdc(drive)) {",
                "\t\tif (lock_fdc(drive))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-20261",
        "func_name": "torvalds/linux/do_format",
        "description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a0c80efe5956ccce9fe7ae5c78542578c07bc20a",
        "commit_title": "floppy_revalidate() doesn't perform any error handling on lock_fdc()",
        "commit_text": "result. lock_fdc() might actually be interrupted by a signal (it waits for fdc becoming non-busy interruptibly). In such case, floppy_revalidate() proceeds as if it had claimed the lock, but it fact it doesn't.  In case of multiple threads trying to open(\"/dev/fdX\"), this leads to serious corruptions all over the place, because all of a sudden there is no critical section protection (that'd otherwise be guaranteed by locked fd) whatsoever.  While at this, fix the fact that the 'interruptible' parameter to lock_fdc() doesn't make any sense whatsoever, because we always wait interruptibly anyway.  Most of the lock_fdc() callsites do properly handle error (and propagate EINTR), but floppy_revalidate() and floppy_check_events() don't. Fix this.  Spotted by 'syzkaller' tool.  ",
        "func_before": "static int do_format(int drive, struct format_descr *tmp_format_req)\n{\n\tint ret;\n\n\tif (lock_fdc(drive, true))\n\t\treturn -EINTR;\n\n\tset_floppy(drive);\n\tif (!_floppy ||\n\t    _floppy->track > DP->tracks ||\n\t    tmp_format_req->track >= _floppy->track ||\n\t    tmp_format_req->head >= _floppy->head ||\n\t    (_floppy->sect << 2) % (1 << FD_SIZECODE(_floppy)) ||\n\t    !_floppy->fmt_gap) {\n\t\tprocess_fd_request();\n\t\treturn -EINVAL;\n\t}\n\tformat_req = *tmp_format_req;\n\tformat_errors = 0;\n\tcont = &format_cont;\n\terrors = &format_errors;\n\tret = wait_til_done(redo_format, true);\n\tif (ret == -EINTR)\n\t\treturn -EINTR;\n\tprocess_fd_request();\n\treturn ret;\n}",
        "func": "static int do_format(int drive, struct format_descr *tmp_format_req)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tset_floppy(drive);\n\tif (!_floppy ||\n\t    _floppy->track > DP->tracks ||\n\t    tmp_format_req->track >= _floppy->track ||\n\t    tmp_format_req->head >= _floppy->head ||\n\t    (_floppy->sect << 2) % (1 << FD_SIZECODE(_floppy)) ||\n\t    !_floppy->fmt_gap) {\n\t\tprocess_fd_request();\n\t\treturn -EINVAL;\n\t}\n\tformat_req = *tmp_format_req;\n\tformat_errors = 0;\n\tcont = &format_cont;\n\terrors = &format_errors;\n\tret = wait_til_done(redo_format, true);\n\tif (ret == -EINTR)\n\t\treturn -EINTR;\n\tprocess_fd_request();\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,7 @@\n {\n \tint ret;\n \n-\tif (lock_fdc(drive, true))\n+\tif (lock_fdc(drive))\n \t\treturn -EINTR;\n \n \tset_floppy(drive);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (lock_fdc(drive, true))"
            ],
            "added_lines": [
                "\tif (lock_fdc(drive))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-20261",
        "func_name": "torvalds/linux/fd_locked_ioctl",
        "description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a0c80efe5956ccce9fe7ae5c78542578c07bc20a",
        "commit_title": "floppy_revalidate() doesn't perform any error handling on lock_fdc()",
        "commit_text": "result. lock_fdc() might actually be interrupted by a signal (it waits for fdc becoming non-busy interruptibly). In such case, floppy_revalidate() proceeds as if it had claimed the lock, but it fact it doesn't.  In case of multiple threads trying to open(\"/dev/fdX\"), this leads to serious corruptions all over the place, because all of a sudden there is no critical section protection (that'd otherwise be guaranteed by locked fd) whatsoever.  While at this, fix the fact that the 'interruptible' parameter to lock_fdc() doesn't make any sense whatsoever, because we always wait interruptibly anyway.  Most of the lock_fdc() callsites do properly handle error (and propagate EINTR), but floppy_revalidate() and floppy_check_events() don't. Fix this.  Spotted by 'syzkaller' tool.  ",
        "func_before": "static int fd_locked_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,\n\t\t    unsigned long param)\n{\n\tint drive = (long)bdev->bd_disk->private_data;\n\tint type = ITYPE(UDRS->fd_device);\n\tint i;\n\tint ret;\n\tint size;\n\tunion inparam {\n\t\tstruct floppy_struct g;\t/* geometry */\n\t\tstruct format_descr f;\n\t\tstruct floppy_max_errors max_errors;\n\t\tstruct floppy_drive_params dp;\n\t} inparam;\t\t/* parameters coming from user space */\n\tconst void *outparam;\t/* parameters passed back to user space */\n\n\t/* convert compatibility eject ioctls into floppy eject ioctl.\n\t * We do this in order to provide a means to eject floppy disks before\n\t * installing the new fdutils package */\n\tif (cmd == CDROMEJECT ||\t/* CD-ROM eject */\n\t    cmd == 0x6470) {\t\t/* SunOS floppy eject */\n\t\tDPRINT(\"obsolete eject ioctl\\n\");\n\t\tDPRINT(\"please use floppycontrol --eject\\n\");\n\t\tcmd = FDEJECT;\n\t}\n\n\tif (!((cmd & 0xff00) == 0x0200))\n\t\treturn -EINVAL;\n\n\t/* convert the old style command into a new style command */\n\tret = normalize_ioctl(&cmd, &size);\n\tif (ret)\n\t\treturn ret;\n\n\t/* permission checks */\n\tif (((cmd & 0x40) && !(mode & (FMODE_WRITE | FMODE_WRITE_IOCTL))) ||\n\t    ((cmd & 0x80) && !capable(CAP_SYS_ADMIN)))\n\t\treturn -EPERM;\n\n\tif (WARN_ON(size < 0 || size > sizeof(inparam)))\n\t\treturn -EINVAL;\n\n\t/* copyin */\n\tmemset(&inparam, 0, sizeof(inparam));\n\tif (_IOC_DIR(cmd) & _IOC_WRITE) {\n\t\tret = fd_copyin((void __user *)param, &inparam, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tswitch (cmd) {\n\tcase FDEJECT:\n\t\tif (UDRS->fd_ref != 1)\n\t\t\t/* somebody else has this drive open */\n\t\t\treturn -EBUSY;\n\t\tif (lock_fdc(drive, true))\n\t\t\treturn -EINTR;\n\n\t\t/* do the actual eject. Fails on\n\t\t * non-Sparc architectures */\n\t\tret = fd_eject(UNIT(drive));\n\n\t\tset_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tset_bit(FD_VERIFY_BIT, &UDRS->flags);\n\t\tprocess_fd_request();\n\t\treturn ret;\n\tcase FDCLRPRM:\n\t\tif (lock_fdc(drive, true))\n\t\t\treturn -EINTR;\n\t\tcurrent_type[drive] = NULL;\n\t\tfloppy_sizes[drive] = MAX_DISK_SIZE << 1;\n\t\tUDRS->keep_data = 0;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETPRM:\n\tcase FDDEFPRM:\n\t\treturn set_geometry(cmd, &inparam.g, drive, type, bdev);\n\tcase FDGETPRM:\n\t\tret = get_floppy_geometry(drive, type,\n\t\t\t\t\t  (struct floppy_struct **)&outparam);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase FDMSGON:\n\t\tUDP->flags |= FTD_MSG;\n\t\treturn 0;\n\tcase FDMSGOFF:\n\t\tUDP->flags &= ~FTD_MSG;\n\t\treturn 0;\n\tcase FDFMTBEG:\n\t\tif (lock_fdc(drive, true))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tret = UDRS->flags;\n\t\tprocess_fd_request();\n\t\tif (ret & FD_VERIFY)\n\t\t\treturn -ENODEV;\n\t\tif (!(ret & FD_DISK_WRITABLE))\n\t\t\treturn -EROFS;\n\t\treturn 0;\n\tcase FDFMTTRK:\n\t\tif (UDRS->fd_ref != 1)\n\t\t\treturn -EBUSY;\n\t\treturn do_format(drive, &inparam.f);\n\tcase FDFMTEND:\n\tcase FDFLUSH:\n\t\tif (lock_fdc(drive, true))\n\t\t\treturn -EINTR;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETEMSGTRESH:\n\t\tUDP->max_errors.reporting = (unsigned short)(param & 0x0f);\n\t\treturn 0;\n\tcase FDGETMAXERRS:\n\t\toutparam = &UDP->max_errors;\n\t\tbreak;\n\tcase FDSETMAXERRS:\n\t\tUDP->max_errors = inparam.max_errors;\n\t\tbreak;\n\tcase FDGETDRVTYP:\n\t\toutparam = drive_name(type, drive);\n\t\tSUPBOUND(size, strlen((const char *)outparam) + 1);\n\t\tbreak;\n\tcase FDSETDRVPRM:\n\t\t*UDP = inparam.dp;\n\t\tbreak;\n\tcase FDGETDRVPRM:\n\t\toutparam = UDP;\n\t\tbreak;\n\tcase FDPOLLDRVSTAT:\n\t\tif (lock_fdc(drive, true))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t/* fall through */\n\tcase FDGETDRVSTAT:\n\t\toutparam = UDRS;\n\t\tbreak;\n\tcase FDRESET:\n\t\treturn user_reset_fdc(drive, (int)param, true);\n\tcase FDGETFDCSTAT:\n\t\toutparam = UFDCS;\n\t\tbreak;\n\tcase FDWERRORCLR:\n\t\tmemset(UDRWE, 0, sizeof(*UDRWE));\n\t\treturn 0;\n\tcase FDWERRORGET:\n\t\toutparam = UDRWE;\n\t\tbreak;\n\tcase FDRAWCMD:\n\t\tif (type)\n\t\t\treturn -EINVAL;\n\t\tif (lock_fdc(drive, true))\n\t\t\treturn -EINTR;\n\t\tset_floppy(drive);\n\t\ti = raw_cmd_ioctl(cmd, (void __user *)param);\n\t\tif (i == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\treturn i;\n\tcase FDTWADDLE:\n\t\tif (lock_fdc(drive, true))\n\t\t\treturn -EINTR;\n\t\ttwaddle();\n\t\tprocess_fd_request();\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (_IOC_DIR(cmd) & _IOC_READ)\n\t\treturn fd_copyout((void __user *)param, outparam, size);\n\n\treturn 0;\n}",
        "func": "static int fd_locked_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,\n\t\t    unsigned long param)\n{\n\tint drive = (long)bdev->bd_disk->private_data;\n\tint type = ITYPE(UDRS->fd_device);\n\tint i;\n\tint ret;\n\tint size;\n\tunion inparam {\n\t\tstruct floppy_struct g;\t/* geometry */\n\t\tstruct format_descr f;\n\t\tstruct floppy_max_errors max_errors;\n\t\tstruct floppy_drive_params dp;\n\t} inparam;\t\t/* parameters coming from user space */\n\tconst void *outparam;\t/* parameters passed back to user space */\n\n\t/* convert compatibility eject ioctls into floppy eject ioctl.\n\t * We do this in order to provide a means to eject floppy disks before\n\t * installing the new fdutils package */\n\tif (cmd == CDROMEJECT ||\t/* CD-ROM eject */\n\t    cmd == 0x6470) {\t\t/* SunOS floppy eject */\n\t\tDPRINT(\"obsolete eject ioctl\\n\");\n\t\tDPRINT(\"please use floppycontrol --eject\\n\");\n\t\tcmd = FDEJECT;\n\t}\n\n\tif (!((cmd & 0xff00) == 0x0200))\n\t\treturn -EINVAL;\n\n\t/* convert the old style command into a new style command */\n\tret = normalize_ioctl(&cmd, &size);\n\tif (ret)\n\t\treturn ret;\n\n\t/* permission checks */\n\tif (((cmd & 0x40) && !(mode & (FMODE_WRITE | FMODE_WRITE_IOCTL))) ||\n\t    ((cmd & 0x80) && !capable(CAP_SYS_ADMIN)))\n\t\treturn -EPERM;\n\n\tif (WARN_ON(size < 0 || size > sizeof(inparam)))\n\t\treturn -EINVAL;\n\n\t/* copyin */\n\tmemset(&inparam, 0, sizeof(inparam));\n\tif (_IOC_DIR(cmd) & _IOC_WRITE) {\n\t\tret = fd_copyin((void __user *)param, &inparam, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tswitch (cmd) {\n\tcase FDEJECT:\n\t\tif (UDRS->fd_ref != 1)\n\t\t\t/* somebody else has this drive open */\n\t\t\treturn -EBUSY;\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\n\t\t/* do the actual eject. Fails on\n\t\t * non-Sparc architectures */\n\t\tret = fd_eject(UNIT(drive));\n\n\t\tset_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tset_bit(FD_VERIFY_BIT, &UDRS->flags);\n\t\tprocess_fd_request();\n\t\treturn ret;\n\tcase FDCLRPRM:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tcurrent_type[drive] = NULL;\n\t\tfloppy_sizes[drive] = MAX_DISK_SIZE << 1;\n\t\tUDRS->keep_data = 0;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETPRM:\n\tcase FDDEFPRM:\n\t\treturn set_geometry(cmd, &inparam.g, drive, type, bdev);\n\tcase FDGETPRM:\n\t\tret = get_floppy_geometry(drive, type,\n\t\t\t\t\t  (struct floppy_struct **)&outparam);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase FDMSGON:\n\t\tUDP->flags |= FTD_MSG;\n\t\treturn 0;\n\tcase FDMSGOFF:\n\t\tUDP->flags &= ~FTD_MSG;\n\t\treturn 0;\n\tcase FDFMTBEG:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tret = UDRS->flags;\n\t\tprocess_fd_request();\n\t\tif (ret & FD_VERIFY)\n\t\t\treturn -ENODEV;\n\t\tif (!(ret & FD_DISK_WRITABLE))\n\t\t\treturn -EROFS;\n\t\treturn 0;\n\tcase FDFMTTRK:\n\t\tif (UDRS->fd_ref != 1)\n\t\t\treturn -EBUSY;\n\t\treturn do_format(drive, &inparam.f);\n\tcase FDFMTEND:\n\tcase FDFLUSH:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETEMSGTRESH:\n\t\tUDP->max_errors.reporting = (unsigned short)(param & 0x0f);\n\t\treturn 0;\n\tcase FDGETMAXERRS:\n\t\toutparam = &UDP->max_errors;\n\t\tbreak;\n\tcase FDSETMAXERRS:\n\t\tUDP->max_errors = inparam.max_errors;\n\t\tbreak;\n\tcase FDGETDRVTYP:\n\t\toutparam = drive_name(type, drive);\n\t\tSUPBOUND(size, strlen((const char *)outparam) + 1);\n\t\tbreak;\n\tcase FDSETDRVPRM:\n\t\t*UDP = inparam.dp;\n\t\tbreak;\n\tcase FDGETDRVPRM:\n\t\toutparam = UDP;\n\t\tbreak;\n\tcase FDPOLLDRVSTAT:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t/* fall through */\n\tcase FDGETDRVSTAT:\n\t\toutparam = UDRS;\n\t\tbreak;\n\tcase FDRESET:\n\t\treturn user_reset_fdc(drive, (int)param, true);\n\tcase FDGETFDCSTAT:\n\t\toutparam = UFDCS;\n\t\tbreak;\n\tcase FDWERRORCLR:\n\t\tmemset(UDRWE, 0, sizeof(*UDRWE));\n\t\treturn 0;\n\tcase FDWERRORGET:\n\t\toutparam = UDRWE;\n\t\tbreak;\n\tcase FDRAWCMD:\n\t\tif (type)\n\t\t\treturn -EINVAL;\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tset_floppy(drive);\n\t\ti = raw_cmd_ioctl(cmd, (void __user *)param);\n\t\tif (i == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\treturn i;\n\tcase FDTWADDLE:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\ttwaddle();\n\t\tprocess_fd_request();\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (_IOC_DIR(cmd) & _IOC_READ)\n\t\treturn fd_copyout((void __user *)param, outparam, size);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -53,7 +53,7 @@\n \t\tif (UDRS->fd_ref != 1)\n \t\t\t/* somebody else has this drive open */\n \t\t\treturn -EBUSY;\n-\t\tif (lock_fdc(drive, true))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \n \t\t/* do the actual eject. Fails on\n@@ -65,7 +65,7 @@\n \t\tprocess_fd_request();\n \t\treturn ret;\n \tcase FDCLRPRM:\n-\t\tif (lock_fdc(drive, true))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\tcurrent_type[drive] = NULL;\n \t\tfloppy_sizes[drive] = MAX_DISK_SIZE << 1;\n@@ -87,7 +87,7 @@\n \t\tUDP->flags &= ~FTD_MSG;\n \t\treturn 0;\n \tcase FDFMTBEG:\n-\t\tif (lock_fdc(drive, true))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n \t\t\treturn -EINTR;\n@@ -104,7 +104,7 @@\n \t\treturn do_format(drive, &inparam.f);\n \tcase FDFMTEND:\n \tcase FDFLUSH:\n-\t\tif (lock_fdc(drive, true))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\treturn invalidate_drive(bdev);\n \tcase FDSETEMSGTRESH:\n@@ -127,7 +127,7 @@\n \t\toutparam = UDP;\n \t\tbreak;\n \tcase FDPOLLDRVSTAT:\n-\t\tif (lock_fdc(drive, true))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n \t\t\treturn -EINTR;\n@@ -150,7 +150,7 @@\n \tcase FDRAWCMD:\n \t\tif (type)\n \t\t\treturn -EINVAL;\n-\t\tif (lock_fdc(drive, true))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\tset_floppy(drive);\n \t\ti = raw_cmd_ioctl(cmd, (void __user *)param);\n@@ -159,7 +159,7 @@\n \t\tprocess_fd_request();\n \t\treturn i;\n \tcase FDTWADDLE:\n-\t\tif (lock_fdc(drive, true))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\ttwaddle();\n \t\tprocess_fd_request();",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (lock_fdc(drive, true))",
                "\t\tif (lock_fdc(drive, true))",
                "\t\tif (lock_fdc(drive, true))",
                "\t\tif (lock_fdc(drive, true))",
                "\t\tif (lock_fdc(drive, true))",
                "\t\tif (lock_fdc(drive, true))",
                "\t\tif (lock_fdc(drive, true))"
            ],
            "added_lines": [
                "\t\tif (lock_fdc(drive))",
                "\t\tif (lock_fdc(drive))",
                "\t\tif (lock_fdc(drive))",
                "\t\tif (lock_fdc(drive))",
                "\t\tif (lock_fdc(drive))",
                "\t\tif (lock_fdc(drive))",
                "\t\tif (lock_fdc(drive))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-20261",
        "func_name": "torvalds/linux/user_reset_fdc",
        "description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a0c80efe5956ccce9fe7ae5c78542578c07bc20a",
        "commit_title": "floppy_revalidate() doesn't perform any error handling on lock_fdc()",
        "commit_text": "result. lock_fdc() might actually be interrupted by a signal (it waits for fdc becoming non-busy interruptibly). In such case, floppy_revalidate() proceeds as if it had claimed the lock, but it fact it doesn't.  In case of multiple threads trying to open(\"/dev/fdX\"), this leads to serious corruptions all over the place, because all of a sudden there is no critical section protection (that'd otherwise be guaranteed by locked fd) whatsoever.  While at this, fix the fact that the 'interruptible' parameter to lock_fdc() doesn't make any sense whatsoever, because we always wait interruptibly anyway.  Most of the lock_fdc() callsites do properly handle error (and propagate EINTR), but floppy_revalidate() and floppy_check_events() don't. Fix this.  Spotted by 'syzkaller' tool.  ",
        "func_before": "static int user_reset_fdc(int drive, int arg, bool interruptible)\n{\n\tint ret;\n\n\tif (lock_fdc(drive, interruptible))\n\t\treturn -EINTR;\n\n\tif (arg == FD_RESET_ALWAYS)\n\t\tFDCS->reset = 1;\n\tif (FDCS->reset) {\n\t\tcont = &reset_cont;\n\t\tret = wait_til_done(reset_fdc, interruptible);\n\t\tif (ret == -EINTR)\n\t\t\treturn -EINTR;\n\t}\n\tprocess_fd_request();\n\treturn 0;\n}",
        "func": "static int user_reset_fdc(int drive, int arg, bool interruptible)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tif (arg == FD_RESET_ALWAYS)\n\t\tFDCS->reset = 1;\n\tif (FDCS->reset) {\n\t\tcont = &reset_cont;\n\t\tret = wait_til_done(reset_fdc, interruptible);\n\t\tif (ret == -EINTR)\n\t\t\treturn -EINTR;\n\t}\n\tprocess_fd_request();\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,7 @@\n {\n \tint ret;\n \n-\tif (lock_fdc(drive, interruptible))\n+\tif (lock_fdc(drive))\n \t\treturn -EINTR;\n \n \tif (arg == FD_RESET_ALWAYS)",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (lock_fdc(drive, interruptible))"
            ],
            "added_lines": [
                "\tif (lock_fdc(drive))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-20261",
        "func_name": "torvalds/linux/floppy_revalidate",
        "description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a0c80efe5956ccce9fe7ae5c78542578c07bc20a",
        "commit_title": "floppy_revalidate() doesn't perform any error handling on lock_fdc()",
        "commit_text": "result. lock_fdc() might actually be interrupted by a signal (it waits for fdc becoming non-busy interruptibly). In such case, floppy_revalidate() proceeds as if it had claimed the lock, but it fact it doesn't.  In case of multiple threads trying to open(\"/dev/fdX\"), this leads to serious corruptions all over the place, because all of a sudden there is no critical section protection (that'd otherwise be guaranteed by locked fd) whatsoever.  While at this, fix the fact that the 'interruptible' parameter to lock_fdc() doesn't make any sense whatsoever, because we always wait interruptibly anyway.  Most of the lock_fdc() callsites do properly handle error (and propagate EINTR), but floppy_revalidate() and floppy_check_events() don't. Fix this.  Spotted by 'syzkaller' tool.  ",
        "func_before": "static int floppy_revalidate(struct gendisk *disk)\n{\n\tint drive = (long)disk->private_data;\n\tint cf;\n\tint res = 0;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive)) {\n\t\tif (WARN(atomic_read(&usage_count) == 0,\n\t\t\t \"VFS: revalidate called on non-open device.\\n\"))\n\t\t\treturn -EFAULT;\n\n\t\tlock_fdc(drive, false);\n\t\tcf = (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t\t      test_bit(FD_VERIFY_BIT, &UDRS->flags));\n\t\tif (!(cf || test_bit(drive, &fake_change) || drive_no_geom(drive))) {\n\t\t\tprocess_fd_request();\t/*already done by another thread */\n\t\t\treturn 0;\n\t\t}\n\t\tUDRS->maxblock = 0;\n\t\tUDRS->maxtrack = 0;\n\t\tif (buffer_drive == drive)\n\t\t\tbuffer_track = -1;\n\t\tclear_bit(drive, &fake_change);\n\t\tclear_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tif (cf)\n\t\t\tUDRS->generation++;\n\t\tif (drive_no_geom(drive)) {\n\t\t\t/* auto-sensing */\n\t\t\tres = __floppy_read_block_0(opened_bdev[drive], drive);\n\t\t} else {\n\t\t\tif (cf)\n\t\t\t\tpoll_drive(false, FD_RAW_NEED_DISK);\n\t\t\tprocess_fd_request();\n\t\t}\n\t}\n\tset_capacity(disk, floppy_sizes[UDRS->fd_device]);\n\treturn res;\n}",
        "func": "static int floppy_revalidate(struct gendisk *disk)\n{\n\tint drive = (long)disk->private_data;\n\tint cf;\n\tint res = 0;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive)) {\n\t\tif (WARN(atomic_read(&usage_count) == 0,\n\t\t\t \"VFS: revalidate called on non-open device.\\n\"))\n\t\t\treturn -EFAULT;\n\n\t\tres = lock_fdc(drive);\n\t\tif (res)\n\t\t\treturn res;\n\t\tcf = (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t\t      test_bit(FD_VERIFY_BIT, &UDRS->flags));\n\t\tif (!(cf || test_bit(drive, &fake_change) || drive_no_geom(drive))) {\n\t\t\tprocess_fd_request();\t/*already done by another thread */\n\t\t\treturn 0;\n\t\t}\n\t\tUDRS->maxblock = 0;\n\t\tUDRS->maxtrack = 0;\n\t\tif (buffer_drive == drive)\n\t\t\tbuffer_track = -1;\n\t\tclear_bit(drive, &fake_change);\n\t\tclear_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tif (cf)\n\t\t\tUDRS->generation++;\n\t\tif (drive_no_geom(drive)) {\n\t\t\t/* auto-sensing */\n\t\t\tres = __floppy_read_block_0(opened_bdev[drive], drive);\n\t\t} else {\n\t\t\tif (cf)\n\t\t\t\tpoll_drive(false, FD_RAW_NEED_DISK);\n\t\t\tprocess_fd_request();\n\t\t}\n\t}\n\tset_capacity(disk, floppy_sizes[UDRS->fd_device]);\n\treturn res;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,9 @@\n \t\t\t \"VFS: revalidate called on non-open device.\\n\"))\n \t\t\treturn -EFAULT;\n \n-\t\tlock_fdc(drive, false);\n+\t\tres = lock_fdc(drive);\n+\t\tif (res)\n+\t\t\treturn res;\n \t\tcf = (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n \t\t      test_bit(FD_VERIFY_BIT, &UDRS->flags));\n \t\tif (!(cf || test_bit(drive, &fake_change) || drive_no_geom(drive))) {",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tlock_fdc(drive, false);"
            ],
            "added_lines": [
                "\t\tres = lock_fdc(drive);",
                "\t\tif (res)",
                "\t\t\treturn res;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-20261",
        "func_name": "torvalds/linux/floppy_check_events",
        "description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a0c80efe5956ccce9fe7ae5c78542578c07bc20a",
        "commit_title": "floppy_revalidate() doesn't perform any error handling on lock_fdc()",
        "commit_text": "result. lock_fdc() might actually be interrupted by a signal (it waits for fdc becoming non-busy interruptibly). In such case, floppy_revalidate() proceeds as if it had claimed the lock, but it fact it doesn't.  In case of multiple threads trying to open(\"/dev/fdX\"), this leads to serious corruptions all over the place, because all of a sudden there is no critical section protection (that'd otherwise be guaranteed by locked fd) whatsoever.  While at this, fix the fact that the 'interruptible' parameter to lock_fdc() doesn't make any sense whatsoever, because we always wait interruptibly anyway.  Most of the lock_fdc() callsites do properly handle error (and propagate EINTR), but floppy_revalidate() and floppy_check_events() don't. Fix this.  Spotted by 'syzkaller' tool.  ",
        "func_before": "static unsigned int floppy_check_events(struct gendisk *disk,\n\t\t\t\t\tunsigned int clearing)\n{\n\tint drive = (long)disk->private_data;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\n\tif (time_after(jiffies, UDRS->last_checked + UDP->checkfreq)) {\n\t\tlock_fdc(drive, false);\n\t\tpoll_drive(false, 0);\n\t\tprocess_fd_request();\n\t}\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\treturn 0;\n}",
        "func": "static unsigned int floppy_check_events(struct gendisk *disk,\n\t\t\t\t\tunsigned int clearing)\n{\n\tint drive = (long)disk->private_data;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\n\tif (time_after(jiffies, UDRS->last_checked + UDP->checkfreq)) {\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tpoll_drive(false, 0);\n\t\tprocess_fd_request();\n\t}\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,8 @@\n \t\treturn DISK_EVENT_MEDIA_CHANGE;\n \n \tif (time_after(jiffies, UDRS->last_checked + UDP->checkfreq)) {\n-\t\tlock_fdc(drive, false);\n+\t\tif (lock_fdc(drive))\n+\t\t\treturn -EINTR;\n \t\tpoll_drive(false, 0);\n \t\tprocess_fd_request();\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tlock_fdc(drive, false);"
            ],
            "added_lines": [
                "\t\tif (lock_fdc(drive))",
                "\t\t\treturn -EINTR;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-28964",
        "func_name": "torvalds/linux/get_old_root",
        "description": "A race condition was discovered in get_old_root in fs/btrfs/ctree.c in the Linux kernel through 5.11.8. It allows attackers to cause a denial of service (BUG) because of a lack of locking on an extent buffer before a cloning operation, aka CID-dbcc7d57bffc.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=dbcc7d57bffc0c8cac9dac11bec548597d59a6a5",
        "commit_title": "While resolving backreferences, as part of a logical ino ioctl call or",
        "commit_text": "fiemap, we can end up hitting a BUG_ON() when replaying tree mod log operations of a root, triggering a stack trace like the following:    ------------[ cut here ]------------   kernel BUG at fs/btrfs/ctree.c:1210!   invalid opcode: 0000 [#1] SMP KASAN PTI   CPU: 1 PID: 19054 Comm: crawl_335 Tainted: G        W         5.11.0-2d11c0084b02-misc-next+ #89   Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014   RIP: 0010:__tree_mod_log_rewind+0x3b1/0x3c0   Code: 05 48 8d 74 10 (...)   RSP: 0018:ffffc90001eb70b8 EFLAGS: 00010297   RAX: 0000000000000000 RBX: ffff88812344e400 RCX: ffffffffb28933b6   RDX: 0000000000000007 RSI: dffffc0000000000 RDI: ffff88812344e42c   RBP: ffffc90001eb7108 R08: 1ffff11020b60a20 R09: ffffed1020b60a20   R10: ffff888105b050f9 R11: ffffed1020b60a1f R12: 00000000000000ee   R13: ffff8880195520c0 R14: ffff8881bc958500 R15: ffff88812344e42c   FS:  00007fd1955e8700(0000) GS:ffff8881f5600000(0000) knlGS:0000000000000000   CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033   CR2: 00007efdb7928718 CR3: 000000010103a006 CR4: 0000000000170ee0   Call Trace:    btrfs_search_old_slot+0x265/0x10d0    ? lock_acquired+0xbb/0x600    ? btrfs_search_slot+0x1090/0x1090    ? free_extent_buffer.part.61+0xd7/0x140    ? free_extent_buffer+0x13/0x20    resolve_indirect_refs+0x3e9/0xfc0    ? lock_downgrade+0x3d0/0x3d0    ? __kasan_check_read+0x11/0x20    ? add_prelim_ref.part.11+0x150/0x150    ? lock_downgrade+0x3d0/0x3d0    ? __kasan_check_read+0x11/0x20    ? lock_acquired+0xbb/0x600    ? __kasan_check_write+0x14/0x20    ? do_raw_spin_unlock+0xa8/0x140    ? rb_insert_color+0x30/0x360    ? prelim_ref_insert+0x12d/0x430    find_parent_nodes+0x5c3/0x1830    ? resolve_indirect_refs+0xfc0/0xfc0    ? lock_release+0xc8/0x620    ? fs_reclaim_acquire+0x67/0xf0    ? lock_acquire+0xc7/0x510    ? lock_downgrade+0x3d0/0x3d0    ? lockdep_hardirqs_on_prepare+0x160/0x210    ? lock_release+0xc8/0x620    ? fs_reclaim_acquire+0x67/0xf0    ? lock_acquire+0xc7/0x510    ? poison_range+0x38/0x40    ? unpoison_range+0x14/0x40    ? trace_hardirqs_on+0x55/0x120    btrfs_find_all_roots_safe+0x142/0x1e0    ? find_parent_nodes+0x1830/0x1830    ? btrfs_inode_flags_to_xflags+0x50/0x50    iterate_extent_inodes+0x20e/0x580    ? tree_backref_for_extent+0x230/0x230    ? lock_downgrade+0x3d0/0x3d0    ? read_extent_buffer+0xdd/0x110    ? lock_downgrade+0x3d0/0x3d0    ? __kasan_check_read+0x11/0x20    ? lock_acquired+0xbb/0x600    ? __kasan_check_write+0x14/0x20    ? _raw_spin_unlock+0x22/0x30    ? __kasan_check_write+0x14/0x20    iterate_inodes_from_logical+0x129/0x170    ? iterate_inodes_from_logical+0x129/0x170    ? btrfs_inode_flags_to_xflags+0x50/0x50    ? iterate_extent_inodes+0x580/0x580    ? __vmalloc_node+0x92/0xb0    ? init_data_container+0x34/0xb0    ? init_data_container+0x34/0xb0    ? kvmalloc_node+0x60/0x80    btrfs_ioctl_logical_to_ino+0x158/0x230    btrfs_ioctl+0x205e/0x4040    ? __might_sleep+0x71/0xe0    ? btrfs_ioctl_get_supported_features+0x30/0x30    ? getrusage+0x4b6/0x9c0    ? __kasan_check_read+0x11/0x20    ? lock_release+0xc8/0x620    ? __might_fault+0x64/0xd0    ? lock_acquire+0xc7/0x510    ? lock_downgrade+0x3d0/0x3d0    ? lockdep_hardirqs_on_prepare+0x210/0x210    ? lockdep_hardirqs_on_prepare+0x210/0x210    ? __kasan_check_read+0x11/0x20    ? do_vfs_ioctl+0xfc/0x9d0    ? ioctl_file_clone+0xe0/0xe0    ? lock_downgrade+0x3d0/0x3d0    ? lockdep_hardirqs_on_prepare+0x210/0x210    ? __kasan_check_read+0x11/0x20    ? lock_release+0xc8/0x620    ? __task_pid_nr_ns+0xd3/0x250    ? lock_acquire+0xc7/0x510    ? __fget_files+0x160/0x230    ? __fget_light+0xf2/0x110    __x64_sys_ioctl+0xc3/0x100    do_syscall_64+0x37/0x80    entry_SYSCALL_64_after_hwframe+0x44/0xa9   RIP: 0033:0x7fd1976e2427   Code: 00 00 90 48 8b 05 (...)   RSP: 002b:00007fd1955e5cf8 EFLAGS: 00000246 ORIG_RAX: 0000000000000010   RAX: ffffffffffffffda RBX: 00007fd1955e5f40 RCX: 00007fd1976e2427   RDX: 00007fd1955e5f48 RSI: 00000000c038943b RDI: 0000000000000004   RBP: 0000000001000000 R08: 0000000000000000 R09: 00007fd1955e6120   R10: 0000557835366b00 R11: 0000000000000246 R12: 0000000000000004   R13: 00007fd1955e5f48 R14: 00007fd1955e5f40 R15: 00007fd1955e5ef8   Modules linked in:   ---[ end trace ec8931a1c36e57be ]---    (gdb) l *(__tree_mod_log_rewind+0x3b1)   0xffffffff81893521 is in __tree_mod_log_rewind (fs/btrfs/ctree.c:1210).   1205                     * the modification. as we're going backwards, we do the   1206                     * opposite of each operation here.   1207                     */   1208                    switch (tm->op) {   1209                    case MOD_LOG_KEY_REMOVE_WHILE_FREEING:   1210                            BUG_ON(tm->slot < n);   1211                            fallthrough;   1212                    case MOD_LOG_KEY_REMOVE_WHILE_MOVING:   1213                    case MOD_LOG_KEY_REMOVE:   1214                            btrfs_set_node_key(eb, &tm->key, tm->slot);  Here's what happens to hit that BUG_ON():  1) We have one tree mod log user (through fiemap or the logical ino ioctl),    with a sequence number of 1, so we have fs_info->tree_mod_seq == 1;  2) Another task is at ctree.c:balance_level() and we have eb X currently as    the root of the tree, and we promote its single child, eb Y, as the new    root.     Then, at ctree.c:balance_level(), we call:        tree_mod_log_insert_root(eb X, eb Y, 1);  3) At tree_mod_log_insert_root() we create tree mod log elements for each    slot of eb X, of operation type MOD_LOG_KEY_REMOVE_WHILE_FREEING each    with a ->logical pointing to ebX->start. These are placed in an array    named tm_list.    Lets assume there are N elements (N pointers in eb X);  4) Then, still at tree_mod_log_insert_root(), we create a tree mod log    element of operation type MOD_LOG_ROOT_REPLACE, ->logical set to    ebY->start, ->old_root.logical set to ebX->start, ->old_root.level set    to the level of eb X and ->generation set to the generation of eb X;  5) Then tree_mod_log_insert_root() calls tree_mod_log_free_eb() with    tm_list as argument. After that, tree_mod_log_free_eb() calls    __tree_mod_log_insert() for each member of tm_list in reverse order,    from highest slot in eb X, slot N - 1, to slot 0 of eb X;  6) __tree_mod_log_insert() sets the sequence number of each given tree mod    log operation - it increments fs_info->tree_mod_seq and sets    fs_info->tree_mod_seq as the sequence number of the given tree mod log    operation.     This means that for the tm_list created at tree_mod_log_insert_root(),    the element corresponding to slot 0 of eb X has the highest sequence    number (1 + N), and the element corresponding to the last slot has the    lowest sequence number (2);  7) Then, after inserting tm_list's elements into the tree mod log rbtree,    the MOD_LOG_ROOT_REPLACE element is inserted, which gets the highest    sequence number, which is N + 2;  8) Back to ctree.c:balance_level(), we free eb X by calling    btrfs_free_tree_block() on it. Because eb X was created in the current    transaction, has no other references and writeback did not happen for    it, we add it back to the free space cache/tree;  9) Later some other task T allocates the metadata extent from eb X, since    it is marked as free space in the space cache/tree, and uses it as a    node for some other btree;  10) The tree mod log user task calls btrfs_search_old_slot(), which calls     get_old_root(), and finally that calls __tree_mod_log_oldest_root()     with time_seq == 1 and eb_root == eb Y;  11) First iteration of the while loop finds the tree mod log element with     sequence number N + 2, for the logical address of eb Y and of type     MOD_LOG_ROOT_REPLACE;  12) Because the operation type is MOD_LOG_ROOT_REPLACE, we don't break out     of the loop, and set root_logical to point to tm->old_root.logical     which corresponds to the logical address of eb X;  13) On the next iteration of the while loop, the call to     tree_mod_log_search_oldest() returns the smallest tree mod log element     for the logical address of eb X, which has a sequence number of 2, an     operation type of MOD_LOG_KEY_REMOVE_WHILE_FREEING and corresponds to     the old slot N - 1 of eb X (eb X had N items in it before being freed);  14) We then break out of the while loop and return the tree mod log operation     of type MOD_LOG_ROOT_REPLACE (eb Y), and not the one for slot N - 1 of     eb X, to get_old_root();  15) At get_old_root(), we process the MOD_LOG_ROOT_REPLACE operation     and set \"logical\" to the logical address of eb X, which was the old     root. We then call tree_mod_log_search() passing it the logical     address of eb X and time_seq == 1;  16) Then before calling tree_mod_log_search(), task T adds a key to eb X,     which results in adding a tree mod log operation of type     MOD_LOG_KEY_ADD to the tree mod log - this is done at     ctree.c:insert_ptr() - but after adding the tree mod log operation     and before updating the number of items in eb X from 0 to 1...  17) The task at get_old_root() calls tree_mod_log_search() and gets the     tree mod log operation of type MOD_LOG_KEY_ADD just added by task T.     Then it enters the following if branch:      if (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {        (...)     } (...)      Calls read_tree_block() for eb X, which gets a reference on eb X but     does not lock it - task T has it locked.     Then it clones eb X while it has nritems set to 0 in its header, before     task T sets nritems to 1 in eb X's header. From hereupon we use the     clone of eb X which no other task has access to;  18) Then we call __tree_mod_log_rewind(), passing it the MOD_LOG_KEY_ADD     mod log operation we just got from tree_mod_log_search() in the     previous step and the cloned version of eb X;  19) At __tree_mod_log_rewind(), we set the local variable \"n\" to the number     of items set in eb X's clone, which is 0. Then we enter the while loop,     and in its first iteration we process the MOD_LOG_KEY_ADD operation,     which just decrements \"n\" from 0 to (u32)-1, since \"n\" is declared with     a type of u32. At the end of this iteration we call rb_next() to find the     next tree mod log operation for eb X, that gives us the mod log operation     of type MOD_LOG_KEY_REMOVE_WHILE_FREEING, for slot 0, with a sequence     number of N + 1 (steps 3 to 6);  20) Then we go back to the top of the while loop and trigger the following     BUG_ON():          (...)         switch (tm->op) {         case MOD_LOG_KEY_REMOVE_WHILE_FREEING:                  BUG_ON(tm->slot < n);                  fallthrough;         (...)      Because \"n\" has a value of (u32)-1 (4294967295) and tm->slot is 0.  Fix this by taking a read lock on the extent buffer before cloning it at ctree.c:get_old_root(). This should be done regardless of the extent buffer having been freed and reused, as a concurrent task might be modifying it (while holding a write lock on it).  Link: https://lore.kernel.org/linux-btrfs/20210227155037.GN28049@hungrycats.org/ ",
        "func_before": "static inline struct extent_buffer *\nget_old_root(struct btrfs_root *root, u64 time_seq)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct tree_mod_elem *tm;\n\tstruct extent_buffer *eb = NULL;\n\tstruct extent_buffer *eb_root;\n\tu64 eb_root_owner = 0;\n\tstruct extent_buffer *old;\n\tstruct tree_mod_root *old_root = NULL;\n\tu64 old_generation = 0;\n\tu64 logical;\n\tint level;\n\n\teb_root = btrfs_read_lock_root_node(root);\n\ttm = __tree_mod_log_oldest_root(eb_root, time_seq);\n\tif (!tm)\n\t\treturn eb_root;\n\n\tif (tm->op == MOD_LOG_ROOT_REPLACE) {\n\t\told_root = &tm->old_root;\n\t\told_generation = tm->generation;\n\t\tlogical = old_root->logical;\n\t\tlevel = old_root->level;\n\t} else {\n\t\tlogical = eb_root->start;\n\t\tlevel = btrfs_header_level(eb_root);\n\t}\n\n\ttm = tree_mod_log_search(fs_info, logical, time_seq);\n\tif (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t\told = read_tree_block(fs_info, logical, root->root_key.objectid,\n\t\t\t\t      0, level, NULL);\n\t\tif (WARN_ON(IS_ERR(old) || !extent_buffer_uptodate(old))) {\n\t\t\tif (!IS_ERR(old))\n\t\t\t\tfree_extent_buffer(old);\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"failed to read tree block %llu from get_old_root\",\n\t\t\t\t   logical);\n\t\t} else {\n\t\t\teb = btrfs_clone_extent_buffer(old);\n\t\t\tfree_extent_buffer(old);\n\t\t}\n\t} else if (old_root) {\n\t\teb_root_owner = btrfs_header_owner(eb_root);\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t\teb = alloc_dummy_extent_buffer(fs_info, logical);\n\t} else {\n\t\teb = btrfs_clone_extent_buffer(eb_root);\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t}\n\n\tif (!eb)\n\t\treturn NULL;\n\tif (old_root) {\n\t\tbtrfs_set_header_bytenr(eb, eb->start);\n\t\tbtrfs_set_header_backref_rev(eb, BTRFS_MIXED_BACKREF_REV);\n\t\tbtrfs_set_header_owner(eb, eb_root_owner);\n\t\tbtrfs_set_header_level(eb, old_root->level);\n\t\tbtrfs_set_header_generation(eb, old_generation);\n\t}\n\tbtrfs_set_buffer_lockdep_class(btrfs_header_owner(eb), eb,\n\t\t\t\t       btrfs_header_level(eb));\n\tbtrfs_tree_read_lock(eb);\n\tif (tm)\n\t\t__tree_mod_log_rewind(fs_info, eb, time_seq, tm);\n\telse\n\t\tWARN_ON(btrfs_header_level(eb) != 0);\n\tWARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(fs_info));\n\n\treturn eb;\n}",
        "func": "static inline struct extent_buffer *\nget_old_root(struct btrfs_root *root, u64 time_seq)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct tree_mod_elem *tm;\n\tstruct extent_buffer *eb = NULL;\n\tstruct extent_buffer *eb_root;\n\tu64 eb_root_owner = 0;\n\tstruct extent_buffer *old;\n\tstruct tree_mod_root *old_root = NULL;\n\tu64 old_generation = 0;\n\tu64 logical;\n\tint level;\n\n\teb_root = btrfs_read_lock_root_node(root);\n\ttm = __tree_mod_log_oldest_root(eb_root, time_seq);\n\tif (!tm)\n\t\treturn eb_root;\n\n\tif (tm->op == MOD_LOG_ROOT_REPLACE) {\n\t\told_root = &tm->old_root;\n\t\told_generation = tm->generation;\n\t\tlogical = old_root->logical;\n\t\tlevel = old_root->level;\n\t} else {\n\t\tlogical = eb_root->start;\n\t\tlevel = btrfs_header_level(eb_root);\n\t}\n\n\ttm = tree_mod_log_search(fs_info, logical, time_seq);\n\tif (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t\told = read_tree_block(fs_info, logical, root->root_key.objectid,\n\t\t\t\t      0, level, NULL);\n\t\tif (WARN_ON(IS_ERR(old) || !extent_buffer_uptodate(old))) {\n\t\t\tif (!IS_ERR(old))\n\t\t\t\tfree_extent_buffer(old);\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"failed to read tree block %llu from get_old_root\",\n\t\t\t\t   logical);\n\t\t} else {\n\t\t\tbtrfs_tree_read_lock(old);\n\t\t\teb = btrfs_clone_extent_buffer(old);\n\t\t\tbtrfs_tree_read_unlock(old);\n\t\t\tfree_extent_buffer(old);\n\t\t}\n\t} else if (old_root) {\n\t\teb_root_owner = btrfs_header_owner(eb_root);\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t\teb = alloc_dummy_extent_buffer(fs_info, logical);\n\t} else {\n\t\teb = btrfs_clone_extent_buffer(eb_root);\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t}\n\n\tif (!eb)\n\t\treturn NULL;\n\tif (old_root) {\n\t\tbtrfs_set_header_bytenr(eb, eb->start);\n\t\tbtrfs_set_header_backref_rev(eb, BTRFS_MIXED_BACKREF_REV);\n\t\tbtrfs_set_header_owner(eb, eb_root_owner);\n\t\tbtrfs_set_header_level(eb, old_root->level);\n\t\tbtrfs_set_header_generation(eb, old_generation);\n\t}\n\tbtrfs_set_buffer_lockdep_class(btrfs_header_owner(eb), eb,\n\t\t\t\t       btrfs_header_level(eb));\n\tbtrfs_tree_read_lock(eb);\n\tif (tm)\n\t\t__tree_mod_log_rewind(fs_info, eb, time_seq, tm);\n\telse\n\t\tWARN_ON(btrfs_header_level(eb) != 0);\n\tWARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(fs_info));\n\n\treturn eb;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,7 +40,9 @@\n \t\t\t\t   \"failed to read tree block %llu from get_old_root\",\n \t\t\t\t   logical);\n \t\t} else {\n+\t\t\tbtrfs_tree_read_lock(old);\n \t\t\teb = btrfs_clone_extent_buffer(old);\n+\t\t\tbtrfs_tree_read_unlock(old);\n \t\t\tfree_extent_buffer(old);\n \t\t}\n \t} else if (old_root) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tbtrfs_tree_read_lock(old);",
                "\t\t\tbtrfs_tree_read_unlock(old);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-29265",
        "func_name": "torvalds/linux/usbip_sockfd_store",
        "description": "An issue was discovered in the Linux kernel before 5.11.7. usbip_sockfd_store in drivers/usb/usbip/stub_dev.c allows attackers to cause a denial of service (GPF) because the stub-up sequence has race conditions during an update of the local and shared status, aka CID-9380afd6df70.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=9380afd6df70e24eacbdbde33afc6a3950965d22",
        "commit_title": "usbip_sockfd_store() is invoked when user requests attach (import)",
        "commit_text": "detach (unimport) usb device from usbip host. vhci_hcd sends import request and usbip_sockfd_store() exports the device if it is free for export.  Export and unexport are governed by local state and shared state - Shared state (usbip device status, sockfd) - sockfd and Device   status are used to determine if stub should be brought up or shut   down. - Local state (tcp_socket, rx and tx thread task_struct ptrs)   A valid tcp_socket controls rx and tx thread operations while the   device is in exported state. - While the device is exported, device status is marked used and socket,   sockfd, and thread pointers are valid.  Export sequence (stub-up) includes validating the socket and creating receive (rx) and transmit (tx) threads to talk to the client to provide access to the exported device. rx and tx threads depends on local and shared state to be correct and in sync.  Unexport (stub-down) sequence shuts the socket down and stops the rx and tx threads. Stub-down sequence relies on local and shared states to be in sync.  There are races in updating the local and shared status in the current stub-up sequence resulting in crashes. These stem from starting rx and tx threads before local and global state is updated correctly to be in sync.  1. Doesn't handle kthread_create() error and saves invalid ptr in local    state that drives rx and tx threads. 2. Updates tcp_socket and sockfd,  starts stub_rx and stub_tx threads    before updating usbip_device status to SDEV_ST_USED. This opens up a    race condition between the threads and usbip_sockfd_store() stub up    and down handling.  Fix the above problems: - Stop using kthread_get_run() macro to create/start threads. - Create threads and get task struct reference. - Add kthread_create() failure handling and bail out. - Hold usbip_device lock to update local and shared states after   creating rx and tx threads. - Update usbip_device status to SDEV_ST_USED. - Update usbip_device tcp_socket, sockfd, tcp_rx, and tcp_tx - Start threads after usbip_device (tcp_socket, sockfd, tcp_rx, tcp_tx,   and status) is complete.  Credit goes to syzbot and Tetsuo Handa for finding and root-causing the kthread_get_run() improper error handling problem and others. This is a hard problem to find and debug since the races aren't seen in a normal case. Fuzzing forces the race window to be small enough for the kthread_get_run() error path bug and starting threads before updating the local and shared state bug in the stub-up sequence.  Tested with syzbot reproducer: - https://syzkaller.appspot.com/text?tag=ReproC&x=14801034d00000  Cc: stable@vger.kernel.org Link: https://lore.kernel.org/r/268a0668144d5ff36ec7d87fdfa90faf583b7ccc.1615171203.git.skhan@linuxfoundation.org ",
        "func_before": "static ssize_t usbip_sockfd_store(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct stub_device *sdev = dev_get_drvdata(dev);\n\tint sockfd = 0;\n\tstruct socket *socket;\n\tint rv;\n\n\tif (!sdev) {\n\t\tdev_err(dev, \"sdev is null\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\trv = sscanf(buf, \"%d\", &sockfd);\n\tif (rv != 1)\n\t\treturn -EINVAL;\n\n\tif (sockfd != -1) {\n\t\tint err;\n\n\t\tdev_info(dev, \"stub up\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\n\t\tif (sdev->ud.status != SDEV_ST_AVAILABLE) {\n\t\t\tdev_err(dev, \"not ready\\n\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tsocket = sockfd_lookup(sockfd, &err);\n\t\tif (!socket) {\n\t\t\tdev_err(dev, \"failed to lookup sock\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (socket->type != SOCK_STREAM) {\n\t\t\tdev_err(dev, \"Expecting SOCK_STREAM - found %d\",\n\t\t\t\tsocket->type);\n\t\t\tgoto sock_err;\n\t\t}\n\n\t\tsdev->ud.tcp_socket = socket;\n\t\tsdev->ud.sockfd = sockfd;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,\n\t\t\t\t\t\t  \"stub_rx\");\n\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,\n\t\t\t\t\t\t  \"stub_tx\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tsdev->ud.status = SDEV_ST_USED;\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t} else {\n\t\tdev_info(dev, \"stub down\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tif (sdev->ud.status != SDEV_ST_USED)\n\t\t\tgoto err;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_DOWN);\n\t}\n\n\treturn count;\n\nsock_err:\n\tsockfd_put(socket);\nerr:\n\tspin_unlock_irq(&sdev->ud.lock);\n\treturn -EINVAL;\n}",
        "func": "static ssize_t usbip_sockfd_store(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct stub_device *sdev = dev_get_drvdata(dev);\n\tint sockfd = 0;\n\tstruct socket *socket;\n\tint rv;\n\tstruct task_struct *tcp_rx = NULL;\n\tstruct task_struct *tcp_tx = NULL;\n\n\tif (!sdev) {\n\t\tdev_err(dev, \"sdev is null\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\trv = sscanf(buf, \"%d\", &sockfd);\n\tif (rv != 1)\n\t\treturn -EINVAL;\n\n\tif (sockfd != -1) {\n\t\tint err;\n\n\t\tdev_info(dev, \"stub up\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\n\t\tif (sdev->ud.status != SDEV_ST_AVAILABLE) {\n\t\t\tdev_err(dev, \"not ready\\n\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tsocket = sockfd_lookup(sockfd, &err);\n\t\tif (!socket) {\n\t\t\tdev_err(dev, \"failed to lookup sock\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (socket->type != SOCK_STREAM) {\n\t\t\tdev_err(dev, \"Expecting SOCK_STREAM - found %d\",\n\t\t\t\tsocket->type);\n\t\t\tgoto sock_err;\n\t\t}\n\n\t\t/* unlock and create threads and get tasks */\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");\n\t\tif (IS_ERR(tcp_rx)) {\n\t\t\tsockfd_put(socket);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");\n\t\tif (IS_ERR(tcp_tx)) {\n\t\t\tkthread_stop(tcp_rx);\n\t\t\tsockfd_put(socket);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* get task structs now */\n\t\tget_task_struct(tcp_rx);\n\t\tget_task_struct(tcp_tx);\n\n\t\t/* lock and update sdev->ud state */\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tsdev->ud.tcp_socket = socket;\n\t\tsdev->ud.sockfd = sockfd;\n\t\tsdev->ud.tcp_rx = tcp_rx;\n\t\tsdev->ud.tcp_tx = tcp_tx;\n\t\tsdev->ud.status = SDEV_ST_USED;\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\twake_up_process(sdev->ud.tcp_rx);\n\t\twake_up_process(sdev->ud.tcp_tx);\n\n\t} else {\n\t\tdev_info(dev, \"stub down\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tif (sdev->ud.status != SDEV_ST_USED)\n\t\t\tgoto err;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_DOWN);\n\t}\n\n\treturn count;\n\nsock_err:\n\tsockfd_put(socket);\nerr:\n\tspin_unlock_irq(&sdev->ud.lock);\n\treturn -EINVAL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,8 @@\n \tint sockfd = 0;\n \tstruct socket *socket;\n \tint rv;\n+\tstruct task_struct *tcp_rx = NULL;\n+\tstruct task_struct *tcp_tx = NULL;\n \n \tif (!sdev) {\n \t\tdev_err(dev, \"sdev is null\\n\");\n@@ -39,19 +41,35 @@\n \t\t\tgoto sock_err;\n \t\t}\n \n+\t\t/* unlock and create threads and get tasks */\n+\t\tspin_unlock_irq(&sdev->ud.lock);\n+\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");\n+\t\tif (IS_ERR(tcp_rx)) {\n+\t\t\tsockfd_put(socket);\n+\t\t\treturn -EINVAL;\n+\t\t}\n+\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");\n+\t\tif (IS_ERR(tcp_tx)) {\n+\t\t\tkthread_stop(tcp_rx);\n+\t\t\tsockfd_put(socket);\n+\t\t\treturn -EINVAL;\n+\t\t}\n+\n+\t\t/* get task structs now */\n+\t\tget_task_struct(tcp_rx);\n+\t\tget_task_struct(tcp_tx);\n+\n+\t\t/* lock and update sdev->ud state */\n+\t\tspin_lock_irq(&sdev->ud.lock);\n \t\tsdev->ud.tcp_socket = socket;\n \t\tsdev->ud.sockfd = sockfd;\n-\n+\t\tsdev->ud.tcp_rx = tcp_rx;\n+\t\tsdev->ud.tcp_tx = tcp_tx;\n+\t\tsdev->ud.status = SDEV_ST_USED;\n \t\tspin_unlock_irq(&sdev->ud.lock);\n \n-\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,\n-\t\t\t\t\t\t  \"stub_rx\");\n-\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,\n-\t\t\t\t\t\t  \"stub_tx\");\n-\n-\t\tspin_lock_irq(&sdev->ud.lock);\n-\t\tsdev->ud.status = SDEV_ST_USED;\n-\t\tspin_unlock_irq(&sdev->ud.lock);\n+\t\twake_up_process(sdev->ud.tcp_rx);\n+\t\twake_up_process(sdev->ud.tcp_tx);\n \n \t} else {\n \t\tdev_info(dev, \"stub down\\n\");",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,",
                "\t\t\t\t\t\t  \"stub_rx\");",
                "\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,",
                "\t\t\t\t\t\t  \"stub_tx\");",
                "",
                "\t\tspin_lock_irq(&sdev->ud.lock);",
                "\t\tsdev->ud.status = SDEV_ST_USED;",
                "\t\tspin_unlock_irq(&sdev->ud.lock);"
            ],
            "added_lines": [
                "\tstruct task_struct *tcp_rx = NULL;",
                "\tstruct task_struct *tcp_tx = NULL;",
                "\t\t/* unlock and create threads and get tasks */",
                "\t\tspin_unlock_irq(&sdev->ud.lock);",
                "\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");",
                "\t\tif (IS_ERR(tcp_rx)) {",
                "\t\t\tsockfd_put(socket);",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");",
                "\t\tif (IS_ERR(tcp_tx)) {",
                "\t\t\tkthread_stop(tcp_rx);",
                "\t\t\tsockfd_put(socket);",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "",
                "\t\t/* get task structs now */",
                "\t\tget_task_struct(tcp_rx);",
                "\t\tget_task_struct(tcp_tx);",
                "",
                "\t\t/* lock and update sdev->ud state */",
                "\t\tspin_lock_irq(&sdev->ud.lock);",
                "\t\tsdev->ud.tcp_rx = tcp_rx;",
                "\t\tsdev->ud.tcp_tx = tcp_tx;",
                "\t\tsdev->ud.status = SDEV_ST_USED;",
                "\t\twake_up_process(sdev->ud.tcp_rx);",
                "\t\twake_up_process(sdev->ud.tcp_tx);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-18224",
        "func_name": "torvalds/linux/ocfs2_direct_IO",
        "description": "In the Linux kernel before 4.15, fs/ocfs2/aops.c omits use of a semaphore and consequently has a race condition for access to the extent tree during read operations in DIRECT mode, which allows local users to cause a denial of service (BUG) by modifying a certain e_cpos field.",
        "git_url": "https://github.com/torvalds/linux/commit/3e4c56d41eef5595035872a2ec5a483f42e8917f",
        "commit_title": "ocfs2: ip_alloc_sem should be taken in ocfs2_get_block()",
        "commit_text": " ip_alloc_sem should be taken in ocfs2_get_block() when reading file in DIRECT mode to prevent concurrent access to extent tree with ocfs2_dio_end_io_write(), which may cause BUGON in the following situation:  read file 'A'                                  end_io of writing file 'A' vfs_read  __vfs_read   ocfs2_file_read_iter    generic_file_read_iter     ocfs2_direct_IO      __blockdev_direct_IO       do_blockdev_direct_IO        do_direct_IO         get_more_blocks          ocfs2_get_block           ocfs2_extent_map_get_blocks            ocfs2_get_clusters             ocfs2_get_clusters_nocache()              ocfs2_search_extent_list               return the index of record which               contains the v_cluster, that is               v_cluster > rec[i]->e_cpos.                                                 ocfs2_dio_end_io                                                  ocfs2_dio_end_io_write                                                   down_write(&oi->ip_alloc_sem);                                                   ocfs2_mark_extent_written                                                    ocfs2_change_extent_flag                                                     ocfs2_split_extent                                                      ...                                                  --> modify the rec[i]->e_cpos, resulting                                                      in v_cluster < rec[i]->e_cpos.              BUG_ON(v_cluster < le32_to_cpu(rec->e_cpos))  [alex.chen@huawei.com: v3]   Link: http://lkml.kernel.org/r/59EF3614.6050008@huawei.com Link: http://lkml.kernel.org/r/59EF3614.6050008@huawei.com Cc: Mark Fasheh <mfasheh@versity.com> Cc: Joel Becker <jlbec@evilplan.org> Cc: Junxiao Bi <junxiao.bi@oracle.com>",
        "func_before": "static ssize_t ocfs2_direct_IO(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct ocfs2_super *osb = OCFS2_SB(inode->i_sb);\n\tget_block_t *get_block;\n\n\t/*\n\t * Fallback to buffered I/O if we see an inode without\n\t * extents.\n\t */\n\tif (OCFS2_I(inode)->ip_dyn_features & OCFS2_INLINE_DATA_FL)\n\t\treturn 0;\n\n\t/* Fallback to buffered I/O if we do not support append dio. */\n\tif (iocb->ki_pos + iter->count > i_size_read(inode) &&\n\t    !ocfs2_supports_append_dio(osb))\n\t\treturn 0;\n\n\tif (iov_iter_rw(iter) == READ)\n\t\tget_block = ocfs2_get_block;\n\telse\n\t\tget_block = ocfs2_dio_get_block;\n\n\treturn __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,\n\t\t\t\t    iter, get_block,\n\t\t\t\t    ocfs2_dio_end_io, NULL, 0);\n}",
        "func": "static ssize_t ocfs2_direct_IO(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct ocfs2_super *osb = OCFS2_SB(inode->i_sb);\n\tget_block_t *get_block;\n\n\t/*\n\t * Fallback to buffered I/O if we see an inode without\n\t * extents.\n\t */\n\tif (OCFS2_I(inode)->ip_dyn_features & OCFS2_INLINE_DATA_FL)\n\t\treturn 0;\n\n\t/* Fallback to buffered I/O if we do not support append dio. */\n\tif (iocb->ki_pos + iter->count > i_size_read(inode) &&\n\t    !ocfs2_supports_append_dio(osb))\n\t\treturn 0;\n\n\tif (iov_iter_rw(iter) == READ)\n\t\tget_block = ocfs2_lock_get_block;\n\telse\n\t\tget_block = ocfs2_dio_wr_get_block;\n\n\treturn __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,\n\t\t\t\t    iter, get_block,\n\t\t\t\t    ocfs2_dio_end_io, NULL, 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,9 +18,9 @@\n \t\treturn 0;\n \n \tif (iov_iter_rw(iter) == READ)\n-\t\tget_block = ocfs2_get_block;\n+\t\tget_block = ocfs2_lock_get_block;\n \telse\n-\t\tget_block = ocfs2_dio_get_block;\n+\t\tget_block = ocfs2_dio_wr_get_block;\n \n \treturn __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,\n \t\t\t\t    iter, get_block,",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tget_block = ocfs2_get_block;",
                "\t\tget_block = ocfs2_dio_get_block;"
            ],
            "added_lines": [
                "\t\tget_block = ocfs2_lock_get_block;",
                "\t\tget_block = ocfs2_dio_wr_get_block;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-18249",
        "func_name": "torvalds/linux/add_free_nid",
        "description": "The add_free_nid function in fs/f2fs/node.c in the Linux kernel before 4.12 does not properly track an allocated nid, which allows local users to cause a denial of service (race condition) or possibly have unspecified other impact via concurrent threads.",
        "git_url": "https://github.com/torvalds/linux/commit/30a61ddf8117c26ac5b295e1233eaa9629a94ca3",
        "commit_title": "f2fs: fix race condition in between free nid allocator/initializer",
        "commit_text": " In below concurrent case, allocated nid can be loaded into free nid cache and be allocated again.  Thread A\t\t\t\tThread B - f2fs_create  - f2fs_new_inode   - alloc_nid    - __insert_nid_to_list(ALLOC_NID_LIST) \t\t\t\t\t- f2fs_balance_fs_bg \t\t\t\t\t - build_free_nids \t\t\t\t\t  - __build_free_nids \t\t\t\t\t   - scan_nat_page \t\t\t\t\t    - add_free_nid \t\t\t\t\t     - __lookup_nat_cache  - f2fs_add_link   - init_inode_metadata    - new_inode_page     - new_node_page      - set_node_addr  - alloc_nid_done   - __remove_nid_from_list(ALLOC_NID_LIST) \t\t\t\t\t     - __insert_nid_to_list(FREE_NID_LIST)  This patch makes nat cache lookup and free nid list operation being atomical to avoid this race condition. ",
        "func_before": "static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)\n{\n\tstruct f2fs_nm_info *nm_i = NM_I(sbi);\n\tstruct free_nid *i;\n\tstruct nat_entry *ne;\n\tint err;\n\n\t/* 0 nid should not be used */\n\tif (unlikely(nid == 0))\n\t\treturn false;\n\n\tif (build) {\n\t\t/* do not add allocated nids */\n\t\tne = __lookup_nat_cache(nm_i, nid);\n\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n\t\t\treturn false;\n\t}\n\n\ti = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);\n\ti->nid = nid;\n\ti->state = NID_NEW;\n\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\tkmem_cache_free(free_nid_slab, i);\n\t\treturn true;\n\t}\n\n\tspin_lock(&nm_i->nid_list_lock);\n\terr = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);\n\tspin_unlock(&nm_i->nid_list_lock);\n\tradix_tree_preload_end();\n\tif (err) {\n\t\tkmem_cache_free(free_nid_slab, i);\n\t\treturn true;\n\t}\n\treturn true;\n}",
        "func": "static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)\n{\n\tstruct f2fs_nm_info *nm_i = NM_I(sbi);\n\tstruct free_nid *i, *e;\n\tstruct nat_entry *ne;\n\tint err = -EINVAL;\n\tbool ret = false;\n\n\t/* 0 nid should not be used */\n\tif (unlikely(nid == 0))\n\t\treturn false;\n\n\ti = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);\n\ti->nid = nid;\n\ti->state = NID_NEW;\n\n\tif (radix_tree_preload(GFP_NOFS))\n\t\tgoto err;\n\n\tspin_lock(&nm_i->nid_list_lock);\n\n\tif (build) {\n\t\t/*\n\t\t *   Thread A             Thread B\n\t\t *  - f2fs_create\n\t\t *   - f2fs_new_inode\n\t\t *    - alloc_nid\n\t\t *     - __insert_nid_to_list(ALLOC_NID_LIST)\n\t\t *                     - f2fs_balance_fs_bg\n\t\t *                      - build_free_nids\n\t\t *                       - __build_free_nids\n\t\t *                        - scan_nat_page\n\t\t *                         - add_free_nid\n\t\t *                          - __lookup_nat_cache\n\t\t *  - f2fs_add_link\n\t\t *   - init_inode_metadata\n\t\t *    - new_inode_page\n\t\t *     - new_node_page\n\t\t *      - set_node_addr\n\t\t *  - alloc_nid_done\n\t\t *   - __remove_nid_from_list(ALLOC_NID_LIST)\n\t\t *                         - __insert_nid_to_list(FREE_NID_LIST)\n\t\t */\n\t\tne = __lookup_nat_cache(nm_i, nid);\n\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n\t\t\tgoto err_out;\n\n\t\te = __lookup_free_nid_list(nm_i, nid);\n\t\tif (e) {\n\t\t\tif (e->state == NID_NEW)\n\t\t\t\tret = true;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\tret = true;\n\terr = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);\nerr_out:\n\tspin_unlock(&nm_i->nid_list_lock);\n\tradix_tree_preload_end();\nerr:\n\tif (err)\n\t\tkmem_cache_free(free_nid_slab, i);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,38 +1,65 @@\n static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)\n {\n \tstruct f2fs_nm_info *nm_i = NM_I(sbi);\n-\tstruct free_nid *i;\n+\tstruct free_nid *i, *e;\n \tstruct nat_entry *ne;\n-\tint err;\n+\tint err = -EINVAL;\n+\tbool ret = false;\n \n \t/* 0 nid should not be used */\n \tif (unlikely(nid == 0))\n \t\treturn false;\n \n-\tif (build) {\n-\t\t/* do not add allocated nids */\n-\t\tne = __lookup_nat_cache(nm_i, nid);\n-\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n-\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n-\t\t\treturn false;\n-\t}\n-\n \ti = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);\n \ti->nid = nid;\n \ti->state = NID_NEW;\n \n-\tif (radix_tree_preload(GFP_NOFS)) {\n-\t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n+\tif (radix_tree_preload(GFP_NOFS))\n+\t\tgoto err;\n \n \tspin_lock(&nm_i->nid_list_lock);\n+\n+\tif (build) {\n+\t\t/*\n+\t\t *   Thread A             Thread B\n+\t\t *  - f2fs_create\n+\t\t *   - f2fs_new_inode\n+\t\t *    - alloc_nid\n+\t\t *     - __insert_nid_to_list(ALLOC_NID_LIST)\n+\t\t *                     - f2fs_balance_fs_bg\n+\t\t *                      - build_free_nids\n+\t\t *                       - __build_free_nids\n+\t\t *                        - scan_nat_page\n+\t\t *                         - add_free_nid\n+\t\t *                          - __lookup_nat_cache\n+\t\t *  - f2fs_add_link\n+\t\t *   - init_inode_metadata\n+\t\t *    - new_inode_page\n+\t\t *     - new_node_page\n+\t\t *      - set_node_addr\n+\t\t *  - alloc_nid_done\n+\t\t *   - __remove_nid_from_list(ALLOC_NID_LIST)\n+\t\t *                         - __insert_nid_to_list(FREE_NID_LIST)\n+\t\t */\n+\t\tne = __lookup_nat_cache(nm_i, nid);\n+\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n+\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n+\t\t\tgoto err_out;\n+\n+\t\te = __lookup_free_nid_list(nm_i, nid);\n+\t\tif (e) {\n+\t\t\tif (e->state == NID_NEW)\n+\t\t\t\tret = true;\n+\t\t\tgoto err_out;\n+\t\t}\n+\t}\n+\tret = true;\n \terr = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);\n+err_out:\n \tspin_unlock(&nm_i->nid_list_lock);\n \tradix_tree_preload_end();\n-\tif (err) {\n+err:\n+\tif (err)\n \t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n-\treturn true;\n+\treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct free_nid *i;",
                "\tint err;",
                "\tif (build) {",
                "\t\t/* do not add allocated nids */",
                "\t\tne = __lookup_nat_cache(nm_i, nid);",
                "\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||",
                "\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))",
                "\t\t\treturn false;",
                "\t}",
                "",
                "\tif (radix_tree_preload(GFP_NOFS)) {",
                "\t\tkmem_cache_free(free_nid_slab, i);",
                "\t\treturn true;",
                "\t}",
                "\tif (err) {",
                "\t\treturn true;",
                "\t}",
                "\treturn true;"
            ],
            "added_lines": [
                "\tstruct free_nid *i, *e;",
                "\tint err = -EINVAL;",
                "\tbool ret = false;",
                "\tif (radix_tree_preload(GFP_NOFS))",
                "\t\tgoto err;",
                "",
                "\tif (build) {",
                "\t\t/*",
                "\t\t *   Thread A             Thread B",
                "\t\t *  - f2fs_create",
                "\t\t *   - f2fs_new_inode",
                "\t\t *    - alloc_nid",
                "\t\t *     - __insert_nid_to_list(ALLOC_NID_LIST)",
                "\t\t *                     - f2fs_balance_fs_bg",
                "\t\t *                      - build_free_nids",
                "\t\t *                       - __build_free_nids",
                "\t\t *                        - scan_nat_page",
                "\t\t *                         - add_free_nid",
                "\t\t *                          - __lookup_nat_cache",
                "\t\t *  - f2fs_add_link",
                "\t\t *   - init_inode_metadata",
                "\t\t *    - new_inode_page",
                "\t\t *     - new_node_page",
                "\t\t *      - set_node_addr",
                "\t\t *  - alloc_nid_done",
                "\t\t *   - __remove_nid_from_list(ALLOC_NID_LIST)",
                "\t\t *                         - __insert_nid_to_list(FREE_NID_LIST)",
                "\t\t */",
                "\t\tne = __lookup_nat_cache(nm_i, nid);",
                "\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||",
                "\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))",
                "\t\t\tgoto err_out;",
                "",
                "\t\te = __lookup_free_nid_list(nm_i, nid);",
                "\t\tif (e) {",
                "\t\t\tif (e->state == NID_NEW)",
                "\t\t\t\tret = true;",
                "\t\t\tgoto err_out;",
                "\t\t}",
                "\t}",
                "\tret = true;",
                "err_out:",
                "err:",
                "\tif (err)",
                "\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-7566",
        "func_name": "torvalds/linux/snd_seq_write",
        "description": "The Linux kernel 4.15 has a Buffer Overflow via an SNDRV_SEQ_IOCTL_SET_CLIENT_POOL ioctl write operation to /dev/snd/seq by a local user.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=d15d662e89fc667b90cd294b0eb45694e33144da",
        "commit_title": "ALSA sequencer core initializes the event pool on demand by invoking",
        "commit_text": "snd_seq_pool_init() when the first write happens and the pool is empty.  Meanwhile user can reset the pool size manually via ioctl concurrently, and this may lead to UAF or out-of-bound accesses since the function tries to vmalloc / vfree the buffer.  A simple fix is to just wrap the snd_seq_pool_init() call with the recently introduced client->ioctl_mutex; as the calls for snd_seq_pool_init() from other side are always protected with this mutex, we can avoid the race.  Cc: <stable@vger.kernel.org> ",
        "func_before": "static ssize_t snd_seq_write(struct file *file, const char __user *buf,\n\t\t\t     size_t count, loff_t *offset)\n{\n\tstruct snd_seq_client *client = file->private_data;\n\tint written = 0, len;\n\tint err = -EINVAL;\n\tstruct snd_seq_event event;\n\n\tif (!(snd_seq_file_flags(file) & SNDRV_SEQ_LFLG_OUTPUT))\n\t\treturn -ENXIO;\n\n\t/* check client structures are in place */\n\tif (snd_BUG_ON(!client))\n\t\treturn -ENXIO;\n\t\t\n\tif (!client->accept_output || client->pool == NULL)\n\t\treturn -ENXIO;\n\n\t/* allocate the pool now if the pool is not allocated yet */ \n\tif (client->pool->size > 0 && !snd_seq_write_pool_allocated(client)) {\n\t\tif (snd_seq_pool_init(client->pool) < 0)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* only process whole events */\n\twhile (count >= sizeof(struct snd_seq_event)) {\n\t\t/* Read in the event header from the user */\n\t\tlen = sizeof(event);\n\t\tif (copy_from_user(&event, buf, len)) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tevent.source.client = client->number;\t/* fill in client number */\n\t\t/* Check for extension data length */\n\t\tif (check_event_type_and_length(&event)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* check for special events */\n\t\tif (event.type == SNDRV_SEQ_EVENT_NONE)\n\t\t\tgoto __skip_event;\n\t\telse if (snd_seq_ev_is_reserved(&event)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (snd_seq_ev_is_variable(&event)) {\n\t\t\tint extlen = event.data.ext.len & ~SNDRV_SEQ_EXT_MASK;\n\t\t\tif ((size_t)(extlen + len) > count) {\n\t\t\t\t/* back out, will get an error this time or next */\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* set user space pointer */\n\t\t\tevent.data.ext.len = extlen | SNDRV_SEQ_EXT_USRPTR;\n\t\t\tevent.data.ext.ptr = (char __force *)buf\n\t\t\t\t\t\t+ sizeof(struct snd_seq_event);\n\t\t\tlen += extlen; /* increment data length */\n\t\t} else {\n#ifdef CONFIG_COMPAT\n\t\t\tif (client->convert32 && snd_seq_ev_is_varusr(&event)) {\n\t\t\t\tvoid *ptr = (void __force *)compat_ptr(event.data.raw32.d[1]);\n\t\t\t\tevent.data.ext.ptr = ptr;\n\t\t\t}\n#endif\n\t\t}\n\n\t\t/* ok, enqueue it */\n\t\terr = snd_seq_client_enqueue_event(client, &event, file,\n\t\t\t\t\t\t   !(file->f_flags & O_NONBLOCK),\n\t\t\t\t\t\t   0, 0);\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t__skip_event:\n\t\t/* Update pointers and counts */\n\t\tcount -= len;\n\t\tbuf += len;\n\t\twritten += len;\n\t}\n\n\treturn written ? written : err;\n}",
        "func": "static ssize_t snd_seq_write(struct file *file, const char __user *buf,\n\t\t\t     size_t count, loff_t *offset)\n{\n\tstruct snd_seq_client *client = file->private_data;\n\tint written = 0, len;\n\tint err;\n\tstruct snd_seq_event event;\n\n\tif (!(snd_seq_file_flags(file) & SNDRV_SEQ_LFLG_OUTPUT))\n\t\treturn -ENXIO;\n\n\t/* check client structures are in place */\n\tif (snd_BUG_ON(!client))\n\t\treturn -ENXIO;\n\t\t\n\tif (!client->accept_output || client->pool == NULL)\n\t\treturn -ENXIO;\n\n\t/* allocate the pool now if the pool is not allocated yet */ \n\tif (client->pool->size > 0 && !snd_seq_write_pool_allocated(client)) {\n\t\tmutex_lock(&client->ioctl_mutex);\n\t\terr = snd_seq_pool_init(client->pool);\n\t\tmutex_unlock(&client->ioctl_mutex);\n\t\tif (err < 0)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* only process whole events */\n\terr = -EINVAL;\n\twhile (count >= sizeof(struct snd_seq_event)) {\n\t\t/* Read in the event header from the user */\n\t\tlen = sizeof(event);\n\t\tif (copy_from_user(&event, buf, len)) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tevent.source.client = client->number;\t/* fill in client number */\n\t\t/* Check for extension data length */\n\t\tif (check_event_type_and_length(&event)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* check for special events */\n\t\tif (event.type == SNDRV_SEQ_EVENT_NONE)\n\t\t\tgoto __skip_event;\n\t\telse if (snd_seq_ev_is_reserved(&event)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (snd_seq_ev_is_variable(&event)) {\n\t\t\tint extlen = event.data.ext.len & ~SNDRV_SEQ_EXT_MASK;\n\t\t\tif ((size_t)(extlen + len) > count) {\n\t\t\t\t/* back out, will get an error this time or next */\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* set user space pointer */\n\t\t\tevent.data.ext.len = extlen | SNDRV_SEQ_EXT_USRPTR;\n\t\t\tevent.data.ext.ptr = (char __force *)buf\n\t\t\t\t\t\t+ sizeof(struct snd_seq_event);\n\t\t\tlen += extlen; /* increment data length */\n\t\t} else {\n#ifdef CONFIG_COMPAT\n\t\t\tif (client->convert32 && snd_seq_ev_is_varusr(&event)) {\n\t\t\t\tvoid *ptr = (void __force *)compat_ptr(event.data.raw32.d[1]);\n\t\t\t\tevent.data.ext.ptr = ptr;\n\t\t\t}\n#endif\n\t\t}\n\n\t\t/* ok, enqueue it */\n\t\terr = snd_seq_client_enqueue_event(client, &event, file,\n\t\t\t\t\t\t   !(file->f_flags & O_NONBLOCK),\n\t\t\t\t\t\t   0, 0);\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t__skip_event:\n\t\t/* Update pointers and counts */\n\t\tcount -= len;\n\t\tbuf += len;\n\t\twritten += len;\n\t}\n\n\treturn written ? written : err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n {\n \tstruct snd_seq_client *client = file->private_data;\n \tint written = 0, len;\n-\tint err = -EINVAL;\n+\tint err;\n \tstruct snd_seq_event event;\n \n \tif (!(snd_seq_file_flags(file) & SNDRV_SEQ_LFLG_OUTPUT))\n@@ -18,11 +18,15 @@\n \n \t/* allocate the pool now if the pool is not allocated yet */ \n \tif (client->pool->size > 0 && !snd_seq_write_pool_allocated(client)) {\n-\t\tif (snd_seq_pool_init(client->pool) < 0)\n+\t\tmutex_lock(&client->ioctl_mutex);\n+\t\terr = snd_seq_pool_init(client->pool);\n+\t\tmutex_unlock(&client->ioctl_mutex);\n+\t\tif (err < 0)\n \t\t\treturn -ENOMEM;\n \t}\n \n \t/* only process whole events */\n+\terr = -EINVAL;\n \twhile (count >= sizeof(struct snd_seq_event)) {\n \t\t/* Read in the event header from the user */\n \t\tlen = sizeof(event);",
        "diff_line_info": {
            "deleted_lines": [
                "\tint err = -EINVAL;",
                "\t\tif (snd_seq_pool_init(client->pool) < 0)"
            ],
            "added_lines": [
                "\tint err;",
                "\t\tmutex_lock(&client->ioctl_mutex);",
                "\t\terr = snd_seq_pool_init(client->pool);",
                "\t\tmutex_unlock(&client->ioctl_mutex);",
                "\t\tif (err < 0)",
                "\terr = -EINVAL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-9016",
        "func_name": "torvalds/linux/flush_end_io",
        "description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "git_url": "https://github.com/torvalds/linux/commit/0048b4837affd153897ed1222283492070027aa9",
        "commit_title": "blk-mq: fix race between timeout and freeing request",
        "commit_text": " Inside timeout handler, blk_mq_tag_to_rq() is called to retrieve the request from one tag. This way is obviously wrong because the request can be freed any time and some fiedds of the request can't be trusted, then kernel oops might be triggered[1].  Currently wrt. blk_mq_tag_to_rq(), the only special case is that the flush request can share same tag with the request cloned from, and the two requests can't be active at the same time, so this patch fixes the above issue by updating tags->rqs[tag] with the active request(either flush rq or the request cloned from) of the tag.  Also blk_mq_tag_to_rq() gets much simplified with this patch.  Given blk_mq_tag_to_rq() is mainly for drivers and the caller must make sure the request can't be freed, so in bt_for_each() this helper is replaced with tags->rqs[tag].  [1] kernel oops log [  439.696220] BUG: unable to handle kernel NULL pointer dereference at 0000000000000158^M [  439.697162] IP: [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.700653] PGD 7ef765067 PUD 7ef764067 PMD 0 ^M [  439.700653] Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC ^M [  439.700653] Dumping ftrace buffer:^M [  439.700653]    (ftrace buffer empty)^M [  439.700653] Modules linked in: nbd ipv6 kvm_intel kvm serio_raw^M [  439.700653] CPU: 6 PID: 2779 Comm: stress-ng-sigfd Not tainted 4.2.0-rc5-next-20150805+ #265^M [  439.730500] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011^M [  439.730500] task: ffff880605308000 ti: ffff88060530c000 task.ti: ffff88060530c000^M [  439.730500] RIP: 0010:[<ffffffff812d89ba>]  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.730500] RSP: 0018:ffff880819203da0  EFLAGS: 00010283^M [  439.730500] RAX: ffff880811b0e000 RBX: ffff8800bb465f00 RCX: 0000000000000002^M [  439.730500] RDX: 0000000000000000 RSI: 0000000000000202 RDI: 0000000000000000^M [  439.730500] RBP: ffff880819203db0 R08: 0000000000000002 R09: 0000000000000000^M [  439.730500] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000202^M [  439.730500] R13: ffff880814104800 R14: 0000000000000002 R15: ffff880811a2ea00^M [  439.730500] FS:  00007f165b3f5740(0000) GS:ffff880819200000(0000) knlGS:0000000000000000^M [  439.730500] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b^M [  439.730500] CR2: 0000000000000158 CR3: 00000007ef766000 CR4: 00000000000006e0^M [  439.730500] Stack:^M [  439.730500]  0000000000000008 ffff8808114eed90 ffff880819203e00 ffffffff812dc104^M [  439.755663]  ffff880819203e40 ffffffff812d9f5e 0000020000000000 ffff8808114eed80^M [  439.755663] Call Trace:^M [  439.755663]  <IRQ> ^M [  439.755663]  [<ffffffff812dc104>] bt_for_each+0x6e/0xc8^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812dc1b3>] blk_mq_tag_busy_iter+0x55/0x5e^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff812d8911>] blk_mq_rq_timer+0x5d/0xd4^M [  439.755663]  [<ffffffff810a3e10>] call_timer_fn+0xf7/0x284^M [  439.755663]  [<ffffffff810a3d1e>] ? call_timer_fn+0x5/0x284^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff810a46d6>] run_timer_softirq+0x1ce/0x1f8^M [  439.755663]  [<ffffffff8104c367>] __do_softirq+0x181/0x3a4^M [  439.755663]  [<ffffffff8104c76e>] irq_exit+0x40/0x94^M [  439.755663]  [<ffffffff81031482>] smp_apic_timer_interrupt+0x33/0x3e^M [  439.755663]  [<ffffffff815559a4>] apic_timer_interrupt+0x84/0x90^M [  439.755663]  <EOI> ^M [  439.755663]  [<ffffffff81554350>] ? _raw_spin_unlock_irq+0x32/0x4a^M [  439.755663]  [<ffffffff8106a98b>] finish_task_switch+0xe0/0x163^M [  439.755663]  [<ffffffff8106a94d>] ? finish_task_switch+0xa2/0x163^M [  439.755663]  [<ffffffff81550066>] __schedule+0x469/0x6cd^M [  439.755663]  [<ffffffff8155039b>] schedule+0x82/0x9a^M [  439.789267]  [<ffffffff8119b28b>] signalfd_read+0x186/0x49a^M [  439.790911]  [<ffffffff8106d86a>] ? wake_up_q+0x47/0x47^M [  439.790911]  [<ffffffff811618c2>] __vfs_read+0x28/0x9f^M [  439.790911]  [<ffffffff8117a289>] ? __fget_light+0x4d/0x74^M [  439.790911]  [<ffffffff811620a7>] vfs_read+0x7a/0xc6^M [  439.790911]  [<ffffffff8116292b>] SyS_read+0x49/0x7f^M [  439.790911]  [<ffffffff81554c17>] entry_SYSCALL_64_fastpath+0x12/0x6f^M [  439.790911] Code: 48 89 e5 e8 a9 b8 e7 ff 5d c3 0f 1f 44 00 00 55 89 f2 48 89 e5 41 54 41 89 f4 53 48 8b 47 60 48 8b 1c d0 48 8b 7b 30 48 8b 53 38 <48> 8b 87 58 01 00 00 48 85 c0 75 09 48 8b 97 88 0c 00 00 eb 10 ^M [  439.790911] RIP  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.790911]  RSP <ffff880819203da0>^M [  439.790911] CR2: 0000000000000158^M [  439.790911] ---[ end trace d40af58949325661 ]---^M  Cc: <stable@vger.kernel.org>",
        "func_before": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
        "func": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\t/* release the tag's ownership to the req cloned from */\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,12 @@\n \tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n \n \tif (q->mq_ops) {\n+\t\tstruct blk_mq_hw_ctx *hctx;\n+\n+\t\t/* release the tag's ownership to the req cloned from */\n \t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n+\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n+\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n \t\tflush_rq->tag = -1;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tstruct blk_mq_hw_ctx *hctx;",
                "",
                "\t\t/* release the tag's ownership to the req cloned from */",
                "\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);",
                "\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-9016",
        "func_name": "torvalds/linux/blk_kick_flush",
        "description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "git_url": "https://github.com/torvalds/linux/commit/0048b4837affd153897ed1222283492070027aa9",
        "commit_title": "blk-mq: fix race between timeout and freeing request",
        "commit_text": " Inside timeout handler, blk_mq_tag_to_rq() is called to retrieve the request from one tag. This way is obviously wrong because the request can be freed any time and some fiedds of the request can't be trusted, then kernel oops might be triggered[1].  Currently wrt. blk_mq_tag_to_rq(), the only special case is that the flush request can share same tag with the request cloned from, and the two requests can't be active at the same time, so this patch fixes the above issue by updating tags->rqs[tag] with the active request(either flush rq or the request cloned from) of the tag.  Also blk_mq_tag_to_rq() gets much simplified with this patch.  Given blk_mq_tag_to_rq() is mainly for drivers and the caller must make sure the request can't be freed, so in bt_for_each() this helper is replaced with tags->rqs[tag].  [1] kernel oops log [  439.696220] BUG: unable to handle kernel NULL pointer dereference at 0000000000000158^M [  439.697162] IP: [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.700653] PGD 7ef765067 PUD 7ef764067 PMD 0 ^M [  439.700653] Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC ^M [  439.700653] Dumping ftrace buffer:^M [  439.700653]    (ftrace buffer empty)^M [  439.700653] Modules linked in: nbd ipv6 kvm_intel kvm serio_raw^M [  439.700653] CPU: 6 PID: 2779 Comm: stress-ng-sigfd Not tainted 4.2.0-rc5-next-20150805+ #265^M [  439.730500] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011^M [  439.730500] task: ffff880605308000 ti: ffff88060530c000 task.ti: ffff88060530c000^M [  439.730500] RIP: 0010:[<ffffffff812d89ba>]  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.730500] RSP: 0018:ffff880819203da0  EFLAGS: 00010283^M [  439.730500] RAX: ffff880811b0e000 RBX: ffff8800bb465f00 RCX: 0000000000000002^M [  439.730500] RDX: 0000000000000000 RSI: 0000000000000202 RDI: 0000000000000000^M [  439.730500] RBP: ffff880819203db0 R08: 0000000000000002 R09: 0000000000000000^M [  439.730500] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000202^M [  439.730500] R13: ffff880814104800 R14: 0000000000000002 R15: ffff880811a2ea00^M [  439.730500] FS:  00007f165b3f5740(0000) GS:ffff880819200000(0000) knlGS:0000000000000000^M [  439.730500] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b^M [  439.730500] CR2: 0000000000000158 CR3: 00000007ef766000 CR4: 00000000000006e0^M [  439.730500] Stack:^M [  439.730500]  0000000000000008 ffff8808114eed90 ffff880819203e00 ffffffff812dc104^M [  439.755663]  ffff880819203e40 ffffffff812d9f5e 0000020000000000 ffff8808114eed80^M [  439.755663] Call Trace:^M [  439.755663]  <IRQ> ^M [  439.755663]  [<ffffffff812dc104>] bt_for_each+0x6e/0xc8^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812dc1b3>] blk_mq_tag_busy_iter+0x55/0x5e^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff812d8911>] blk_mq_rq_timer+0x5d/0xd4^M [  439.755663]  [<ffffffff810a3e10>] call_timer_fn+0xf7/0x284^M [  439.755663]  [<ffffffff810a3d1e>] ? call_timer_fn+0x5/0x284^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff810a46d6>] run_timer_softirq+0x1ce/0x1f8^M [  439.755663]  [<ffffffff8104c367>] __do_softirq+0x181/0x3a4^M [  439.755663]  [<ffffffff8104c76e>] irq_exit+0x40/0x94^M [  439.755663]  [<ffffffff81031482>] smp_apic_timer_interrupt+0x33/0x3e^M [  439.755663]  [<ffffffff815559a4>] apic_timer_interrupt+0x84/0x90^M [  439.755663]  <EOI> ^M [  439.755663]  [<ffffffff81554350>] ? _raw_spin_unlock_irq+0x32/0x4a^M [  439.755663]  [<ffffffff8106a98b>] finish_task_switch+0xe0/0x163^M [  439.755663]  [<ffffffff8106a94d>] ? finish_task_switch+0xa2/0x163^M [  439.755663]  [<ffffffff81550066>] __schedule+0x469/0x6cd^M [  439.755663]  [<ffffffff8155039b>] schedule+0x82/0x9a^M [  439.789267]  [<ffffffff8119b28b>] signalfd_read+0x186/0x49a^M [  439.790911]  [<ffffffff8106d86a>] ? wake_up_q+0x47/0x47^M [  439.790911]  [<ffffffff811618c2>] __vfs_read+0x28/0x9f^M [  439.790911]  [<ffffffff8117a289>] ? __fget_light+0x4d/0x74^M [  439.790911]  [<ffffffff811620a7>] vfs_read+0x7a/0xc6^M [  439.790911]  [<ffffffff8116292b>] SyS_read+0x49/0x7f^M [  439.790911]  [<ffffffff81554c17>] entry_SYSCALL_64_fastpath+0x12/0x6f^M [  439.790911] Code: 48 89 e5 e8 a9 b8 e7 ff 5d c3 0f 1f 44 00 00 55 89 f2 48 89 e5 41 54 41 89 f4 53 48 8b 47 60 48 8b 1c d0 48 8b 7b 30 48 8b 53 38 <48> 8b 87 58 01 00 00 48 85 c0 75 09 48 8b 97 88 0c 00 00 eb 10 ^M [  439.790911] RIP  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.790911]  RSP <ffff880819203da0>^M [  439.790911] CR2: 0000000000000158^M [  439.790911] ---[ end trace d40af58949325661 ]---^M  Cc: <stable@vger.kernel.org>",
        "func_before": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time.\n\t */\n\tif (q->mq_ops) {\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
        "func": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time. And acquire the tag's\n\t * ownership for flush req.\n\t */\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t\tfq->orig_rq = first_rq;\n\n\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,11 +25,18 @@\n \n \t/*\n \t * Borrow tag from the first request since they can't\n-\t * be in flight at the same time.\n+\t * be in flight at the same time. And acquire the tag's\n+\t * ownership for flush req.\n \t */\n \tif (q->mq_ops) {\n+\t\tstruct blk_mq_hw_ctx *hctx;\n+\n \t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n \t\tflush_rq->tag = first_rq->tag;\n+\t\tfq->orig_rq = first_rq;\n+\n+\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n+\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n \t}\n \n \tflush_rq->cmd_type = REQ_TYPE_FS;",
        "diff_line_info": {
            "deleted_lines": [
                "\t * be in flight at the same time."
            ],
            "added_lines": [
                "\t * be in flight at the same time. And acquire the tag's",
                "\t * ownership for flush req.",
                "\t\tstruct blk_mq_hw_ctx *hctx;",
                "",
                "\t\tfq->orig_rq = first_rq;",
                "",
                "\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);",
                "\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-9016",
        "func_name": "torvalds/linux/blk_mq_tag_to_rq",
        "description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "git_url": "https://github.com/torvalds/linux/commit/0048b4837affd153897ed1222283492070027aa9",
        "commit_title": "blk-mq: fix race between timeout and freeing request",
        "commit_text": " Inside timeout handler, blk_mq_tag_to_rq() is called to retrieve the request from one tag. This way is obviously wrong because the request can be freed any time and some fiedds of the request can't be trusted, then kernel oops might be triggered[1].  Currently wrt. blk_mq_tag_to_rq(), the only special case is that the flush request can share same tag with the request cloned from, and the two requests can't be active at the same time, so this patch fixes the above issue by updating tags->rqs[tag] with the active request(either flush rq or the request cloned from) of the tag.  Also blk_mq_tag_to_rq() gets much simplified with this patch.  Given blk_mq_tag_to_rq() is mainly for drivers and the caller must make sure the request can't be freed, so in bt_for_each() this helper is replaced with tags->rqs[tag].  [1] kernel oops log [  439.696220] BUG: unable to handle kernel NULL pointer dereference at 0000000000000158^M [  439.697162] IP: [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.700653] PGD 7ef765067 PUD 7ef764067 PMD 0 ^M [  439.700653] Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC ^M [  439.700653] Dumping ftrace buffer:^M [  439.700653]    (ftrace buffer empty)^M [  439.700653] Modules linked in: nbd ipv6 kvm_intel kvm serio_raw^M [  439.700653] CPU: 6 PID: 2779 Comm: stress-ng-sigfd Not tainted 4.2.0-rc5-next-20150805+ #265^M [  439.730500] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011^M [  439.730500] task: ffff880605308000 ti: ffff88060530c000 task.ti: ffff88060530c000^M [  439.730500] RIP: 0010:[<ffffffff812d89ba>]  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.730500] RSP: 0018:ffff880819203da0  EFLAGS: 00010283^M [  439.730500] RAX: ffff880811b0e000 RBX: ffff8800bb465f00 RCX: 0000000000000002^M [  439.730500] RDX: 0000000000000000 RSI: 0000000000000202 RDI: 0000000000000000^M [  439.730500] RBP: ffff880819203db0 R08: 0000000000000002 R09: 0000000000000000^M [  439.730500] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000202^M [  439.730500] R13: ffff880814104800 R14: 0000000000000002 R15: ffff880811a2ea00^M [  439.730500] FS:  00007f165b3f5740(0000) GS:ffff880819200000(0000) knlGS:0000000000000000^M [  439.730500] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b^M [  439.730500] CR2: 0000000000000158 CR3: 00000007ef766000 CR4: 00000000000006e0^M [  439.730500] Stack:^M [  439.730500]  0000000000000008 ffff8808114eed90 ffff880819203e00 ffffffff812dc104^M [  439.755663]  ffff880819203e40 ffffffff812d9f5e 0000020000000000 ffff8808114eed80^M [  439.755663] Call Trace:^M [  439.755663]  <IRQ> ^M [  439.755663]  [<ffffffff812dc104>] bt_for_each+0x6e/0xc8^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812dc1b3>] blk_mq_tag_busy_iter+0x55/0x5e^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff812d8911>] blk_mq_rq_timer+0x5d/0xd4^M [  439.755663]  [<ffffffff810a3e10>] call_timer_fn+0xf7/0x284^M [  439.755663]  [<ffffffff810a3d1e>] ? call_timer_fn+0x5/0x284^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff810a46d6>] run_timer_softirq+0x1ce/0x1f8^M [  439.755663]  [<ffffffff8104c367>] __do_softirq+0x181/0x3a4^M [  439.755663]  [<ffffffff8104c76e>] irq_exit+0x40/0x94^M [  439.755663]  [<ffffffff81031482>] smp_apic_timer_interrupt+0x33/0x3e^M [  439.755663]  [<ffffffff815559a4>] apic_timer_interrupt+0x84/0x90^M [  439.755663]  <EOI> ^M [  439.755663]  [<ffffffff81554350>] ? _raw_spin_unlock_irq+0x32/0x4a^M [  439.755663]  [<ffffffff8106a98b>] finish_task_switch+0xe0/0x163^M [  439.755663]  [<ffffffff8106a94d>] ? finish_task_switch+0xa2/0x163^M [  439.755663]  [<ffffffff81550066>] __schedule+0x469/0x6cd^M [  439.755663]  [<ffffffff8155039b>] schedule+0x82/0x9a^M [  439.789267]  [<ffffffff8119b28b>] signalfd_read+0x186/0x49a^M [  439.790911]  [<ffffffff8106d86a>] ? wake_up_q+0x47/0x47^M [  439.790911]  [<ffffffff811618c2>] __vfs_read+0x28/0x9f^M [  439.790911]  [<ffffffff8117a289>] ? __fget_light+0x4d/0x74^M [  439.790911]  [<ffffffff811620a7>] vfs_read+0x7a/0xc6^M [  439.790911]  [<ffffffff8116292b>] SyS_read+0x49/0x7f^M [  439.790911]  [<ffffffff81554c17>] entry_SYSCALL_64_fastpath+0x12/0x6f^M [  439.790911] Code: 48 89 e5 e8 a9 b8 e7 ff 5d c3 0f 1f 44 00 00 55 89 f2 48 89 e5 41 54 41 89 f4 53 48 8b 47 60 48 8b 1c d0 48 8b 7b 30 48 8b 53 38 <48> 8b 87 58 01 00 00 48 85 c0 75 09 48 8b 97 88 0c 00 00 eb 10 ^M [  439.790911] RIP  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.790911]  RSP <ffff880819203da0>^M [  439.790911] CR2: 0000000000000158^M [  439.790911] ---[ end trace d40af58949325661 ]---^M  Cc: <stable@vger.kernel.org>",
        "func_before": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}",
        "func": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,4 @@\n struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n {\n-\tstruct request *rq = tags->rqs[tag];\n-\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n-\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n-\n-\tif (!is_flush_request(rq, fq, tag))\n-\t\treturn rq;\n-\n-\treturn fq->flush_rq;\n+\treturn tags->rqs[tag];\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct request *rq = tags->rqs[tag];",
                "\t/* mq_ctx of flush rq is always cloned from the corresponding req */",
                "\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);",
                "",
                "\tif (!is_flush_request(rq, fq, tag))",
                "\t\treturn rq;",
                "",
                "\treturn fq->flush_rq;"
            ],
            "added_lines": [
                "\treturn tags->rqs[tag];"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-9016",
        "func_name": "torvalds/linux/bt_for_each",
        "description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "git_url": "https://github.com/torvalds/linux/commit/0048b4837affd153897ed1222283492070027aa9",
        "commit_title": "blk-mq: fix race between timeout and freeing request",
        "commit_text": " Inside timeout handler, blk_mq_tag_to_rq() is called to retrieve the request from one tag. This way is obviously wrong because the request can be freed any time and some fiedds of the request can't be trusted, then kernel oops might be triggered[1].  Currently wrt. blk_mq_tag_to_rq(), the only special case is that the flush request can share same tag with the request cloned from, and the two requests can't be active at the same time, so this patch fixes the above issue by updating tags->rqs[tag] with the active request(either flush rq or the request cloned from) of the tag.  Also blk_mq_tag_to_rq() gets much simplified with this patch.  Given blk_mq_tag_to_rq() is mainly for drivers and the caller must make sure the request can't be freed, so in bt_for_each() this helper is replaced with tags->rqs[tag].  [1] kernel oops log [  439.696220] BUG: unable to handle kernel NULL pointer dereference at 0000000000000158^M [  439.697162] IP: [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.700653] PGD 7ef765067 PUD 7ef764067 PMD 0 ^M [  439.700653] Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC ^M [  439.700653] Dumping ftrace buffer:^M [  439.700653]    (ftrace buffer empty)^M [  439.700653] Modules linked in: nbd ipv6 kvm_intel kvm serio_raw^M [  439.700653] CPU: 6 PID: 2779 Comm: stress-ng-sigfd Not tainted 4.2.0-rc5-next-20150805+ #265^M [  439.730500] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011^M [  439.730500] task: ffff880605308000 ti: ffff88060530c000 task.ti: ffff88060530c000^M [  439.730500] RIP: 0010:[<ffffffff812d89ba>]  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.730500] RSP: 0018:ffff880819203da0  EFLAGS: 00010283^M [  439.730500] RAX: ffff880811b0e000 RBX: ffff8800bb465f00 RCX: 0000000000000002^M [  439.730500] RDX: 0000000000000000 RSI: 0000000000000202 RDI: 0000000000000000^M [  439.730500] RBP: ffff880819203db0 R08: 0000000000000002 R09: 0000000000000000^M [  439.730500] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000202^M [  439.730500] R13: ffff880814104800 R14: 0000000000000002 R15: ffff880811a2ea00^M [  439.730500] FS:  00007f165b3f5740(0000) GS:ffff880819200000(0000) knlGS:0000000000000000^M [  439.730500] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b^M [  439.730500] CR2: 0000000000000158 CR3: 00000007ef766000 CR4: 00000000000006e0^M [  439.730500] Stack:^M [  439.730500]  0000000000000008 ffff8808114eed90 ffff880819203e00 ffffffff812dc104^M [  439.755663]  ffff880819203e40 ffffffff812d9f5e 0000020000000000 ffff8808114eed80^M [  439.755663] Call Trace:^M [  439.755663]  <IRQ> ^M [  439.755663]  [<ffffffff812dc104>] bt_for_each+0x6e/0xc8^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812dc1b3>] blk_mq_tag_busy_iter+0x55/0x5e^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff812d8911>] blk_mq_rq_timer+0x5d/0xd4^M [  439.755663]  [<ffffffff810a3e10>] call_timer_fn+0xf7/0x284^M [  439.755663]  [<ffffffff810a3d1e>] ? call_timer_fn+0x5/0x284^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff810a46d6>] run_timer_softirq+0x1ce/0x1f8^M [  439.755663]  [<ffffffff8104c367>] __do_softirq+0x181/0x3a4^M [  439.755663]  [<ffffffff8104c76e>] irq_exit+0x40/0x94^M [  439.755663]  [<ffffffff81031482>] smp_apic_timer_interrupt+0x33/0x3e^M [  439.755663]  [<ffffffff815559a4>] apic_timer_interrupt+0x84/0x90^M [  439.755663]  <EOI> ^M [  439.755663]  [<ffffffff81554350>] ? _raw_spin_unlock_irq+0x32/0x4a^M [  439.755663]  [<ffffffff8106a98b>] finish_task_switch+0xe0/0x163^M [  439.755663]  [<ffffffff8106a94d>] ? finish_task_switch+0xa2/0x163^M [  439.755663]  [<ffffffff81550066>] __schedule+0x469/0x6cd^M [  439.755663]  [<ffffffff8155039b>] schedule+0x82/0x9a^M [  439.789267]  [<ffffffff8119b28b>] signalfd_read+0x186/0x49a^M [  439.790911]  [<ffffffff8106d86a>] ? wake_up_q+0x47/0x47^M [  439.790911]  [<ffffffff811618c2>] __vfs_read+0x28/0x9f^M [  439.790911]  [<ffffffff8117a289>] ? __fget_light+0x4d/0x74^M [  439.790911]  [<ffffffff811620a7>] vfs_read+0x7a/0xc6^M [  439.790911]  [<ffffffff8116292b>] SyS_read+0x49/0x7f^M [  439.790911]  [<ffffffff81554c17>] entry_SYSCALL_64_fastpath+0x12/0x6f^M [  439.790911] Code: 48 89 e5 e8 a9 b8 e7 ff 5d c3 0f 1f 44 00 00 55 89 f2 48 89 e5 41 54 41 89 f4 53 48 8b 47 60 48 8b 1c d0 48 8b 7b 30 48 8b 53 38 <48> 8b 87 58 01 00 00 48 85 c0 75 09 48 8b 97 88 0c 00 00 eb 10 ^M [  439.790911] RIP  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.790911]  RSP <ffff880819203da0>^M [  439.790911] CR2: 0000000000000158^M [  439.790911] ---[ end trace d40af58949325661 ]---^M  Cc: <stable@vger.kernel.org>",
        "func_before": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "func": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = hctx->tags->rqs[off + bit];\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n \t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n \t\t     bit < bm->depth;\n \t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n-\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n+\t\t\trq = hctx->tags->rqs[off + bit];\n \t\t\tif (rq->q == hctx->queue)\n \t\t\t\tfn(hctx, rq, data, reserved);\n \t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);"
            ],
            "added_lines": [
                "\t\t\trq = hctx->tags->rqs[off + bit];"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-9016",
        "func_name": "torvalds/linux/bt_tags_for_each",
        "description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "git_url": "https://github.com/torvalds/linux/commit/0048b4837affd153897ed1222283492070027aa9",
        "commit_title": "blk-mq: fix race between timeout and freeing request",
        "commit_text": " Inside timeout handler, blk_mq_tag_to_rq() is called to retrieve the request from one tag. This way is obviously wrong because the request can be freed any time and some fiedds of the request can't be trusted, then kernel oops might be triggered[1].  Currently wrt. blk_mq_tag_to_rq(), the only special case is that the flush request can share same tag with the request cloned from, and the two requests can't be active at the same time, so this patch fixes the above issue by updating tags->rqs[tag] with the active request(either flush rq or the request cloned from) of the tag.  Also blk_mq_tag_to_rq() gets much simplified with this patch.  Given blk_mq_tag_to_rq() is mainly for drivers and the caller must make sure the request can't be freed, so in bt_for_each() this helper is replaced with tags->rqs[tag].  [1] kernel oops log [  439.696220] BUG: unable to handle kernel NULL pointer dereference at 0000000000000158^M [  439.697162] IP: [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.700653] PGD 7ef765067 PUD 7ef764067 PMD 0 ^M [  439.700653] Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC ^M [  439.700653] Dumping ftrace buffer:^M [  439.700653]    (ftrace buffer empty)^M [  439.700653] Modules linked in: nbd ipv6 kvm_intel kvm serio_raw^M [  439.700653] CPU: 6 PID: 2779 Comm: stress-ng-sigfd Not tainted 4.2.0-rc5-next-20150805+ #265^M [  439.730500] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011^M [  439.730500] task: ffff880605308000 ti: ffff88060530c000 task.ti: ffff88060530c000^M [  439.730500] RIP: 0010:[<ffffffff812d89ba>]  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.730500] RSP: 0018:ffff880819203da0  EFLAGS: 00010283^M [  439.730500] RAX: ffff880811b0e000 RBX: ffff8800bb465f00 RCX: 0000000000000002^M [  439.730500] RDX: 0000000000000000 RSI: 0000000000000202 RDI: 0000000000000000^M [  439.730500] RBP: ffff880819203db0 R08: 0000000000000002 R09: 0000000000000000^M [  439.730500] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000202^M [  439.730500] R13: ffff880814104800 R14: 0000000000000002 R15: ffff880811a2ea00^M [  439.730500] FS:  00007f165b3f5740(0000) GS:ffff880819200000(0000) knlGS:0000000000000000^M [  439.730500] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b^M [  439.730500] CR2: 0000000000000158 CR3: 00000007ef766000 CR4: 00000000000006e0^M [  439.730500] Stack:^M [  439.730500]  0000000000000008 ffff8808114eed90 ffff880819203e00 ffffffff812dc104^M [  439.755663]  ffff880819203e40 ffffffff812d9f5e 0000020000000000 ffff8808114eed80^M [  439.755663] Call Trace:^M [  439.755663]  <IRQ> ^M [  439.755663]  [<ffffffff812dc104>] bt_for_each+0x6e/0xc8^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M [  439.755663]  [<ffffffff812dc1b3>] blk_mq_tag_busy_iter+0x55/0x5e^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff812d8911>] blk_mq_rq_timer+0x5d/0xd4^M [  439.755663]  [<ffffffff810a3e10>] call_timer_fn+0xf7/0x284^M [  439.755663]  [<ffffffff810a3d1e>] ? call_timer_fn+0x5/0x284^M [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M [  439.755663]  [<ffffffff810a46d6>] run_timer_softirq+0x1ce/0x1f8^M [  439.755663]  [<ffffffff8104c367>] __do_softirq+0x181/0x3a4^M [  439.755663]  [<ffffffff8104c76e>] irq_exit+0x40/0x94^M [  439.755663]  [<ffffffff81031482>] smp_apic_timer_interrupt+0x33/0x3e^M [  439.755663]  [<ffffffff815559a4>] apic_timer_interrupt+0x84/0x90^M [  439.755663]  <EOI> ^M [  439.755663]  [<ffffffff81554350>] ? _raw_spin_unlock_irq+0x32/0x4a^M [  439.755663]  [<ffffffff8106a98b>] finish_task_switch+0xe0/0x163^M [  439.755663]  [<ffffffff8106a94d>] ? finish_task_switch+0xa2/0x163^M [  439.755663]  [<ffffffff81550066>] __schedule+0x469/0x6cd^M [  439.755663]  [<ffffffff8155039b>] schedule+0x82/0x9a^M [  439.789267]  [<ffffffff8119b28b>] signalfd_read+0x186/0x49a^M [  439.790911]  [<ffffffff8106d86a>] ? wake_up_q+0x47/0x47^M [  439.790911]  [<ffffffff811618c2>] __vfs_read+0x28/0x9f^M [  439.790911]  [<ffffffff8117a289>] ? __fget_light+0x4d/0x74^M [  439.790911]  [<ffffffff811620a7>] vfs_read+0x7a/0xc6^M [  439.790911]  [<ffffffff8116292b>] SyS_read+0x49/0x7f^M [  439.790911]  [<ffffffff81554c17>] entry_SYSCALL_64_fastpath+0x12/0x6f^M [  439.790911] Code: 48 89 e5 e8 a9 b8 e7 ff 5d c3 0f 1f 44 00 00 55 89 f2 48 89 e5 41 54 41 89 f4 53 48 8b 47 60 48 8b 1c d0 48 8b 7b 30 48 8b 53 38 <48> 8b 87 58 01 00 00 48 85 c0 75 09 48 8b 97 88 0c 00 00 eb 10 ^M [  439.790911] RIP  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M [  439.790911]  RSP <ffff880819203da0>^M [  439.790911] CR2: 0000000000000158^M [  439.790911] ---[ end trace d40af58949325661 ]---^M  Cc: <stable@vger.kernel.org>",
        "func_before": "static void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "func": "static void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = tags->rqs[off + bit];\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n \t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n \t\t     bit < bm->depth;\n \t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n-\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);\n+\t\t\trq = tags->rqs[off + bit];\n \t\t\tfn(rq, data, reserved);\n \t\t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);"
            ],
            "added_lines": [
                "\t\t\trq = tags->rqs[off + bit];"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8897",
        "func_name": "xen-project/xen/do_debug",
        "description": "A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",
        "git_url": "https://github.com/xen-project/xen/commit/f7d457524c7d4fbf8fba635452e12c057fe71e60",
        "commit_title": "x86/traps: Fix %dr6 handing in #DB handler",
        "commit_text": " Most bits in %dr6 accumulate, rather than being set directly based on the current source of #DB.  Have the handler follow the manuals guidance, which avoids leaking hypervisor debugging activities into guest context.  This is part of XSA-260 / CVE-2018-8897 ",
        "func_before": "void do_debug(struct cpu_user_regs *regs)\n{\n    struct vcpu *v = current;\n\n    if ( debugger_trap_entry(TRAP_debug, regs) )\n        return;\n\n    if ( !guest_mode(regs) )\n    {\n        if ( regs->eflags & X86_EFLAGS_TF )\n        {\n            /* In SYSENTER entry path we can't zap TF until EFLAGS is saved. */\n            if ( (regs->rip >= (unsigned long)sysenter_entry) &&\n                 (regs->rip <= (unsigned long)sysenter_eflags_saved) )\n            {\n                if ( regs->rip == (unsigned long)sysenter_eflags_saved )\n                    regs->eflags &= ~X86_EFLAGS_TF;\n                goto out;\n            }\n            if ( !debugger_trap_fatal(TRAP_debug, regs) )\n            {\n                WARN();\n                regs->eflags &= ~X86_EFLAGS_TF;\n            }\n        }\n        else\n        {\n            /*\n             * We ignore watchpoints when they trigger within Xen. This may\n             * happen when a buffer is passed to us which previously had a\n             * watchpoint set on it. No need to bump EIP; the only faulting\n             * trap is an instruction breakpoint, which can't happen to us.\n             */\n            WARN_ON(!search_exception_table(regs));\n        }\n        goto out;\n    }\n\n    /* Save debug status register where guest OS can peek at it */\n    v->arch.debugreg[6] = read_debugreg(6);\n\n    ler_enable();\n    pv_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n    return;\n\n out:\n    ler_enable();\n    return;\n}",
        "func": "void do_debug(struct cpu_user_regs *regs)\n{\n    unsigned long dr6;\n    struct vcpu *v = current;\n\n    /* Stash dr6 as early as possible. */\n    dr6 = read_debugreg(6);\n\n    if ( debugger_trap_entry(TRAP_debug, regs) )\n        return;\n\n    /*\n     * At the time of writing (March 2018), on the subject of %dr6:\n     *\n     * The Intel manual says:\n     *   Certain debug exceptions may clear bits 0-3. The remaining contents\n     *   of the DR6 register are never cleared by the processor. To avoid\n     *   confusion in identifying debug exceptions, debug handlers should\n     *   clear the register (except bit 16, which they should set) before\n     *   returning to the interrupted task.\n     *\n     * The AMD manual says:\n     *   Bits 15:13 of the DR6 register are not cleared by the processor and\n     *   must be cleared by software after the contents have been read.\n     *\n     * Some bits are reserved set, some are reserved clear, and some bits\n     * which were previously reserved set are reused and cleared by hardware.\n     * For future compatibility, reset to the default value, which will allow\n     * us to spot any bit being changed by hardware to its non-default value.\n     */\n    write_debugreg(6, X86_DR6_DEFAULT);\n\n    if ( !guest_mode(regs) )\n    {\n        if ( regs->eflags & X86_EFLAGS_TF )\n        {\n            /* In SYSENTER entry path we can't zap TF until EFLAGS is saved. */\n            if ( (regs->rip >= (unsigned long)sysenter_entry) &&\n                 (regs->rip <= (unsigned long)sysenter_eflags_saved) )\n            {\n                if ( regs->rip == (unsigned long)sysenter_eflags_saved )\n                    regs->eflags &= ~X86_EFLAGS_TF;\n                goto out;\n            }\n            if ( !debugger_trap_fatal(TRAP_debug, regs) )\n            {\n                WARN();\n                regs->eflags &= ~X86_EFLAGS_TF;\n            }\n        }\n        else\n        {\n            /*\n             * We ignore watchpoints when they trigger within Xen. This may\n             * happen when a buffer is passed to us which previously had a\n             * watchpoint set on it. No need to bump EIP; the only faulting\n             * trap is an instruction breakpoint, which can't happen to us.\n             */\n            WARN_ON(!search_exception_table(regs));\n        }\n        goto out;\n    }\n\n    /* Save debug status register where guest OS can peek at it */\n    v->arch.debugreg[6] |= (dr6 & ~X86_DR6_DEFAULT);\n    v->arch.debugreg[6] &= (dr6 | ~X86_DR6_DEFAULT);\n\n    ler_enable();\n    pv_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n    return;\n\n out:\n    ler_enable();\n    return;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,34 @@\n void do_debug(struct cpu_user_regs *regs)\n {\n+    unsigned long dr6;\n     struct vcpu *v = current;\n+\n+    /* Stash dr6 as early as possible. */\n+    dr6 = read_debugreg(6);\n \n     if ( debugger_trap_entry(TRAP_debug, regs) )\n         return;\n+\n+    /*\n+     * At the time of writing (March 2018), on the subject of %dr6:\n+     *\n+     * The Intel manual says:\n+     *   Certain debug exceptions may clear bits 0-3. The remaining contents\n+     *   of the DR6 register are never cleared by the processor. To avoid\n+     *   confusion in identifying debug exceptions, debug handlers should\n+     *   clear the register (except bit 16, which they should set) before\n+     *   returning to the interrupted task.\n+     *\n+     * The AMD manual says:\n+     *   Bits 15:13 of the DR6 register are not cleared by the processor and\n+     *   must be cleared by software after the contents have been read.\n+     *\n+     * Some bits are reserved set, some are reserved clear, and some bits\n+     * which were previously reserved set are reused and cleared by hardware.\n+     * For future compatibility, reset to the default value, which will allow\n+     * us to spot any bit being changed by hardware to its non-default value.\n+     */\n+    write_debugreg(6, X86_DR6_DEFAULT);\n \n     if ( !guest_mode(regs) )\n     {\n@@ -37,7 +62,8 @@\n     }\n \n     /* Save debug status register where guest OS can peek at it */\n-    v->arch.debugreg[6] = read_debugreg(6);\n+    v->arch.debugreg[6] |= (dr6 & ~X86_DR6_DEFAULT);\n+    v->arch.debugreg[6] &= (dr6 | ~X86_DR6_DEFAULT);\n \n     ler_enable();\n     pv_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);",
        "diff_line_info": {
            "deleted_lines": [
                "    v->arch.debugreg[6] = read_debugreg(6);"
            ],
            "added_lines": [
                "    unsigned long dr6;",
                "",
                "    /* Stash dr6 as early as possible. */",
                "    dr6 = read_debugreg(6);",
                "",
                "    /*",
                "     * At the time of writing (March 2018), on the subject of %dr6:",
                "     *",
                "     * The Intel manual says:",
                "     *   Certain debug exceptions may clear bits 0-3. The remaining contents",
                "     *   of the DR6 register are never cleared by the processor. To avoid",
                "     *   confusion in identifying debug exceptions, debug handlers should",
                "     *   clear the register (except bit 16, which they should set) before",
                "     *   returning to the interrupted task.",
                "     *",
                "     * The AMD manual says:",
                "     *   Bits 15:13 of the DR6 register are not cleared by the processor and",
                "     *   must be cleared by software after the contents have been read.",
                "     *",
                "     * Some bits are reserved set, some are reserved clear, and some bits",
                "     * which were previously reserved set are reused and cleared by hardware.",
                "     * For future compatibility, reset to the default value, which will allow",
                "     * us to spot any bit being changed by hardware to its non-default value.",
                "     */",
                "    write_debugreg(6, X86_DR6_DEFAULT);",
                "    v->arch.debugreg[6] |= (dr6 & ~X86_DR6_DEFAULT);",
                "    v->arch.debugreg[6] &= (dr6 | ~X86_DR6_DEFAULT);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8897",
        "func_name": "xen-project/xen/do_debug",
        "description": "A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",
        "git_url": "https://github.com/xen-project/xen/commit/75d6828bc2146d0eea16adc92376951a310d94a7",
        "commit_title": "x86/traps: Fix handling of #DB exceptions in hypervisor context",
        "commit_text": " The WARN_ON() can be triggered by guest activities, and emits a full stack trace without rate limiting.  Swap it out for a ratelimited printk with just enough information to work out what is going on.  Not all #DB exceptions are traps, so blindly continuing is not a safe action to take.  We don't let PV guests select these settings in the real %dr7 to begin with, but for added safety against unexpected situations, detect the fault cases and crash in an obvious manner.  This is part of XSA-260 / CVE-2018-8897 ",
        "func_before": "void do_debug(struct cpu_user_regs *regs)\n{\n    unsigned long dr6;\n    struct vcpu *v = current;\n\n    /* Stash dr6 as early as possible. */\n    dr6 = read_debugreg(6);\n\n    if ( debugger_trap_entry(TRAP_debug, regs) )\n        return;\n\n    /*\n     * At the time of writing (March 2018), on the subject of %dr6:\n     *\n     * The Intel manual says:\n     *   Certain debug exceptions may clear bits 0-3. The remaining contents\n     *   of the DR6 register are never cleared by the processor. To avoid\n     *   confusion in identifying debug exceptions, debug handlers should\n     *   clear the register (except bit 16, which they should set) before\n     *   returning to the interrupted task.\n     *\n     * The AMD manual says:\n     *   Bits 15:13 of the DR6 register are not cleared by the processor and\n     *   must be cleared by software after the contents have been read.\n     *\n     * Some bits are reserved set, some are reserved clear, and some bits\n     * which were previously reserved set are reused and cleared by hardware.\n     * For future compatibility, reset to the default value, which will allow\n     * us to spot any bit being changed by hardware to its non-default value.\n     */\n    write_debugreg(6, X86_DR6_DEFAULT);\n\n    if ( !guest_mode(regs) )\n    {\n        if ( regs->eflags & X86_EFLAGS_TF )\n        {\n            /* In SYSENTER entry path we can't zap TF until EFLAGS is saved. */\n            if ( (regs->rip >= (unsigned long)sysenter_entry) &&\n                 (regs->rip <= (unsigned long)sysenter_eflags_saved) )\n            {\n                if ( regs->rip == (unsigned long)sysenter_eflags_saved )\n                    regs->eflags &= ~X86_EFLAGS_TF;\n                goto out;\n            }\n            if ( !debugger_trap_fatal(TRAP_debug, regs) )\n            {\n                WARN();\n                regs->eflags &= ~X86_EFLAGS_TF;\n            }\n        }\n        else\n        {\n            /*\n             * We ignore watchpoints when they trigger within Xen. This may\n             * happen when a buffer is passed to us which previously had a\n             * watchpoint set on it. No need to bump EIP; the only faulting\n             * trap is an instruction breakpoint, which can't happen to us.\n             */\n            WARN_ON(!search_exception_table(regs));\n        }\n        goto out;\n    }\n\n    /* Save debug status register where guest OS can peek at it */\n    v->arch.debugreg[6] |= (dr6 & ~X86_DR6_DEFAULT);\n    v->arch.debugreg[6] &= (dr6 | ~X86_DR6_DEFAULT);\n\n    ler_enable();\n    pv_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n    return;\n\n out:\n    ler_enable();\n    return;\n}",
        "func": "void do_debug(struct cpu_user_regs *regs)\n{\n    unsigned long dr6;\n    struct vcpu *v = current;\n\n    /* Stash dr6 as early as possible. */\n    dr6 = read_debugreg(6);\n\n    if ( debugger_trap_entry(TRAP_debug, regs) )\n        return;\n\n    /*\n     * At the time of writing (March 2018), on the subject of %dr6:\n     *\n     * The Intel manual says:\n     *   Certain debug exceptions may clear bits 0-3. The remaining contents\n     *   of the DR6 register are never cleared by the processor. To avoid\n     *   confusion in identifying debug exceptions, debug handlers should\n     *   clear the register (except bit 16, which they should set) before\n     *   returning to the interrupted task.\n     *\n     * The AMD manual says:\n     *   Bits 15:13 of the DR6 register are not cleared by the processor and\n     *   must be cleared by software after the contents have been read.\n     *\n     * Some bits are reserved set, some are reserved clear, and some bits\n     * which were previously reserved set are reused and cleared by hardware.\n     * For future compatibility, reset to the default value, which will allow\n     * us to spot any bit being changed by hardware to its non-default value.\n     */\n    write_debugreg(6, X86_DR6_DEFAULT);\n\n    if ( !guest_mode(regs) )\n    {\n        if ( regs->eflags & X86_EFLAGS_TF )\n        {\n            /* In SYSENTER entry path we can't zap TF until EFLAGS is saved. */\n            if ( (regs->rip >= (unsigned long)sysenter_entry) &&\n                 (regs->rip <= (unsigned long)sysenter_eflags_saved) )\n            {\n                if ( regs->rip == (unsigned long)sysenter_eflags_saved )\n                    regs->eflags &= ~X86_EFLAGS_TF;\n                goto out;\n            }\n            if ( !debugger_trap_fatal(TRAP_debug, regs) )\n            {\n                WARN();\n                regs->eflags &= ~X86_EFLAGS_TF;\n            }\n        }\n\n        /*\n         * Check for fault conditions.  General Detect, and instruction\n         * breakpoints are faults rather than traps, at which point attempting\n         * to ignore and continue will result in a livelock.\n         */\n        if ( dr6 & DR_GENERAL_DETECT )\n        {\n            printk(XENLOG_ERR \"Hit General Detect in Xen context\\n\");\n            fatal_trap(regs, 0);\n        }\n\n        if ( dr6 & (DR_TRAP3 | DR_TRAP2 | DR_TRAP1 | DR_TRAP0) )\n        {\n            unsigned int bp, dr7 = read_debugreg(7) >> DR_CONTROL_SHIFT;\n\n            for ( bp = 0; bp < 4; ++bp )\n            {\n                if ( (dr6 & (1u << bp)) && /* Breakpoint triggered? */\n                     ((dr7 & (3u << (bp * DR_CONTROL_SIZE))) == 0) /* Insn? */ )\n                {\n                    printk(XENLOG_ERR\n                           \"Hit instruction breakpoint in Xen context\\n\");\n                    fatal_trap(regs, 0);\n                }\n            }\n        }\n\n        /*\n         * Whatever caused this #DB should be a trap.  Note it and continue.\n         * Guests can trigger this in certain corner cases, so ensure the\n         * message is ratelimited.\n         */\n        gprintk(XENLOG_WARNING,\n                \"Hit #DB in Xen context: %04x:%p [%ps], stk %04x:%p, dr6 %lx\\n\",\n                regs->cs, _p(regs->rip), _p(regs->rip),\n                regs->ss, _p(regs->rsp), dr6);\n\n        goto out;\n    }\n\n    /* Save debug status register where guest OS can peek at it */\n    v->arch.debugreg[6] |= (dr6 & ~X86_DR6_DEFAULT);\n    v->arch.debugreg[6] &= (dr6 | ~X86_DR6_DEFAULT);\n\n    ler_enable();\n    pv_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n    return;\n\n out:\n    ler_enable();\n    return;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -48,16 +48,44 @@\n                 regs->eflags &= ~X86_EFLAGS_TF;\n             }\n         }\n-        else\n+\n+        /*\n+         * Check for fault conditions.  General Detect, and instruction\n+         * breakpoints are faults rather than traps, at which point attempting\n+         * to ignore and continue will result in a livelock.\n+         */\n+        if ( dr6 & DR_GENERAL_DETECT )\n         {\n-            /*\n-             * We ignore watchpoints when they trigger within Xen. This may\n-             * happen when a buffer is passed to us which previously had a\n-             * watchpoint set on it. No need to bump EIP; the only faulting\n-             * trap is an instruction breakpoint, which can't happen to us.\n-             */\n-            WARN_ON(!search_exception_table(regs));\n+            printk(XENLOG_ERR \"Hit General Detect in Xen context\\n\");\n+            fatal_trap(regs, 0);\n         }\n+\n+        if ( dr6 & (DR_TRAP3 | DR_TRAP2 | DR_TRAP1 | DR_TRAP0) )\n+        {\n+            unsigned int bp, dr7 = read_debugreg(7) >> DR_CONTROL_SHIFT;\n+\n+            for ( bp = 0; bp < 4; ++bp )\n+            {\n+                if ( (dr6 & (1u << bp)) && /* Breakpoint triggered? */\n+                     ((dr7 & (3u << (bp * DR_CONTROL_SIZE))) == 0) /* Insn? */ )\n+                {\n+                    printk(XENLOG_ERR\n+                           \"Hit instruction breakpoint in Xen context\\n\");\n+                    fatal_trap(regs, 0);\n+                }\n+            }\n+        }\n+\n+        /*\n+         * Whatever caused this #DB should be a trap.  Note it and continue.\n+         * Guests can trigger this in certain corner cases, so ensure the\n+         * message is ratelimited.\n+         */\n+        gprintk(XENLOG_WARNING,\n+                \"Hit #DB in Xen context: %04x:%p [%ps], stk %04x:%p, dr6 %lx\\n\",\n+                regs->cs, _p(regs->rip), _p(regs->rip),\n+                regs->ss, _p(regs->rsp), dr6);\n+\n         goto out;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "        else",
                "            /*",
                "             * We ignore watchpoints when they trigger within Xen. This may",
                "             * happen when a buffer is passed to us which previously had a",
                "             * watchpoint set on it. No need to bump EIP; the only faulting",
                "             * trap is an instruction breakpoint, which can't happen to us.",
                "             */",
                "            WARN_ON(!search_exception_table(regs));"
            ],
            "added_lines": [
                "",
                "        /*",
                "         * Check for fault conditions.  General Detect, and instruction",
                "         * breakpoints are faults rather than traps, at which point attempting",
                "         * to ignore and continue will result in a livelock.",
                "         */",
                "        if ( dr6 & DR_GENERAL_DETECT )",
                "            printk(XENLOG_ERR \"Hit General Detect in Xen context\\n\");",
                "            fatal_trap(regs, 0);",
                "",
                "        if ( dr6 & (DR_TRAP3 | DR_TRAP2 | DR_TRAP1 | DR_TRAP0) )",
                "        {",
                "            unsigned int bp, dr7 = read_debugreg(7) >> DR_CONTROL_SHIFT;",
                "",
                "            for ( bp = 0; bp < 4; ++bp )",
                "            {",
                "                if ( (dr6 & (1u << bp)) && /* Breakpoint triggered? */",
                "                     ((dr7 & (3u << (bp * DR_CONTROL_SIZE))) == 0) /* Insn? */ )",
                "                {",
                "                    printk(XENLOG_ERR",
                "                           \"Hit instruction breakpoint in Xen context\\n\");",
                "                    fatal_trap(regs, 0);",
                "                }",
                "            }",
                "        }",
                "",
                "        /*",
                "         * Whatever caused this #DB should be a trap.  Note it and continue.",
                "         * Guests can trigger this in certain corner cases, so ensure the",
                "         * message is ratelimited.",
                "         */",
                "        gprintk(XENLOG_WARNING,",
                "                \"Hit #DB in Xen context: %04x:%p [%ps], stk %04x:%p, dr6 %lx\\n\",",
                "                regs->cs, _p(regs->rip), _p(regs->rip),",
                "                regs->ss, _p(regs->rsp), dr6);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8897",
        "func_name": "xen-project/xen/enable_each_ist",
        "description": "A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",
        "git_url": "https://github.com/xen-project/xen/commit/5d37af364dc158aa387f7c8b2a05c90325c63dce",
        "commit_title": "x86/traps: Use an Interrupt Stack Table for #DB",
        "commit_text": " PV guests can use architectural corner cases to cause #DB to be raised after transitioning into supervisor mode.  Use an interrupt stack table for #DB to prevent the exception being taken with a guest controlled stack pointer.  This is part of XSA-260 / CVE-2018-8897 ",
        "func_before": "static inline void enable_each_ist(idt_entry_t *idt)\n{\n    set_ist(&idt[TRAP_double_fault],  IST_DF);\n    set_ist(&idt[TRAP_nmi],           IST_NMI);\n    set_ist(&idt[TRAP_machine_check], IST_MCE);\n}",
        "func": "static inline void enable_each_ist(idt_entry_t *idt)\n{\n    set_ist(&idt[TRAP_double_fault],  IST_DF);\n    set_ist(&idt[TRAP_nmi],           IST_NMI);\n    set_ist(&idt[TRAP_machine_check], IST_MCE);\n    set_ist(&idt[TRAP_debug],         IST_DB);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,4 +3,5 @@\n     set_ist(&idt[TRAP_double_fault],  IST_DF);\n     set_ist(&idt[TRAP_nmi],           IST_NMI);\n     set_ist(&idt[TRAP_machine_check], IST_MCE);\n+    set_ist(&idt[TRAP_debug],         IST_DB);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    set_ist(&idt[TRAP_debug],         IST_DB);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8897",
        "func_name": "xen-project/xen/disable_each_ist",
        "description": "A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",
        "git_url": "https://github.com/xen-project/xen/commit/5d37af364dc158aa387f7c8b2a05c90325c63dce",
        "commit_title": "x86/traps: Use an Interrupt Stack Table for #DB",
        "commit_text": " PV guests can use architectural corner cases to cause #DB to be raised after transitioning into supervisor mode.  Use an interrupt stack table for #DB to prevent the exception being taken with a guest controlled stack pointer.  This is part of XSA-260 / CVE-2018-8897 ",
        "func_before": "static inline void disable_each_ist(idt_entry_t *idt)\n{\n    set_ist(&idt[TRAP_double_fault],  IST_NONE);\n    set_ist(&idt[TRAP_nmi],           IST_NONE);\n    set_ist(&idt[TRAP_machine_check], IST_NONE);\n}",
        "func": "static inline void disable_each_ist(idt_entry_t *idt)\n{\n    set_ist(&idt[TRAP_double_fault],  IST_NONE);\n    set_ist(&idt[TRAP_nmi],           IST_NONE);\n    set_ist(&idt[TRAP_machine_check], IST_NONE);\n    set_ist(&idt[TRAP_debug],         IST_NONE);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,4 +3,5 @@\n     set_ist(&idt[TRAP_double_fault],  IST_NONE);\n     set_ist(&idt[TRAP_nmi],           IST_NONE);\n     set_ist(&idt[TRAP_machine_check], IST_NONE);\n+    set_ist(&idt[TRAP_debug],         IST_NONE);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    set_ist(&idt[TRAP_debug],         IST_NONE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8897",
        "func_name": "xen-project/xen/get_stack_dump_bottom",
        "description": "A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",
        "git_url": "https://github.com/xen-project/xen/commit/5d37af364dc158aa387f7c8b2a05c90325c63dce",
        "commit_title": "x86/traps: Use an Interrupt Stack Table for #DB",
        "commit_text": " PV guests can use architectural corner cases to cause #DB to be raised after transitioning into supervisor mode.  Use an interrupt stack table for #DB to prevent the exception being taken with a guest controlled stack pointer.  This is part of XSA-260 / CVE-2018-8897 ",
        "func_before": "unsigned long get_stack_dump_bottom(unsigned long sp)\n{\n    switch ( get_stack_page(sp) )\n    {\n    case 0 ... 2:\n        return ROUNDUP(sp, PAGE_SIZE) - sizeof(unsigned long);\n\n#ifndef MEMORY_GUARD\n    case 3 ... 5:\n#endif\n    case 6 ... 7:\n        return ROUNDUP(sp, STACK_SIZE) - sizeof(unsigned long);\n\n    default:\n        return sp - sizeof(unsigned long);\n    }\n}",
        "func": "unsigned long get_stack_dump_bottom(unsigned long sp)\n{\n    switch ( get_stack_page(sp) )\n    {\n    case 0 ... 3:\n        return ROUNDUP(sp, PAGE_SIZE) - sizeof(unsigned long);\n\n#ifndef MEMORY_GUARD\n    case 4 ... 5:\n#endif\n    case 6 ... 7:\n        return ROUNDUP(sp, STACK_SIZE) - sizeof(unsigned long);\n\n    default:\n        return sp - sizeof(unsigned long);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,11 +2,11 @@\n {\n     switch ( get_stack_page(sp) )\n     {\n-    case 0 ... 2:\n+    case 0 ... 3:\n         return ROUNDUP(sp, PAGE_SIZE) - sizeof(unsigned long);\n \n #ifndef MEMORY_GUARD\n-    case 3 ... 5:\n+    case 4 ... 5:\n #endif\n     case 6 ... 7:\n         return ROUNDUP(sp, STACK_SIZE) - sizeof(unsigned long);",
        "diff_line_info": {
            "deleted_lines": [
                "    case 0 ... 2:",
                "    case 3 ... 5:"
            ],
            "added_lines": [
                "    case 0 ... 3:",
                "    case 4 ... 5:"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8897",
        "func_name": "xen-project/xen/get_stack_trace_bottom",
        "description": "A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",
        "git_url": "https://github.com/xen-project/xen/commit/5d37af364dc158aa387f7c8b2a05c90325c63dce",
        "commit_title": "x86/traps: Use an Interrupt Stack Table for #DB",
        "commit_text": " PV guests can use architectural corner cases to cause #DB to be raised after transitioning into supervisor mode.  Use an interrupt stack table for #DB to prevent the exception being taken with a guest controlled stack pointer.  This is part of XSA-260 / CVE-2018-8897 ",
        "func_before": "unsigned long get_stack_trace_bottom(unsigned long sp)\n{\n    switch ( get_stack_page(sp) )\n    {\n    case 0 ... 2:\n        return ROUNDUP(sp, PAGE_SIZE) -\n            offsetof(struct cpu_user_regs, es) - sizeof(unsigned long);\n\n#ifndef MEMORY_GUARD\n    case 3 ... 5:\n#endif\n    case 6 ... 7:\n        return ROUNDUP(sp, STACK_SIZE) -\n            sizeof(struct cpu_info) - sizeof(unsigned long);\n\n    default:\n        return sp - sizeof(unsigned long);\n    }\n}",
        "func": "unsigned long get_stack_trace_bottom(unsigned long sp)\n{\n    switch ( get_stack_page(sp) )\n    {\n    case 0 ... 3:\n        return ROUNDUP(sp, PAGE_SIZE) -\n            offsetof(struct cpu_user_regs, es) - sizeof(unsigned long);\n\n#ifndef MEMORY_GUARD\n    case 4 ... 5:\n#endif\n    case 6 ... 7:\n        return ROUNDUP(sp, STACK_SIZE) -\n            sizeof(struct cpu_info) - sizeof(unsigned long);\n\n    default:\n        return sp - sizeof(unsigned long);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,12 +2,12 @@\n {\n     switch ( get_stack_page(sp) )\n     {\n-    case 0 ... 2:\n+    case 0 ... 3:\n         return ROUNDUP(sp, PAGE_SIZE) -\n             offsetof(struct cpu_user_regs, es) - sizeof(unsigned long);\n \n #ifndef MEMORY_GUARD\n-    case 3 ... 5:\n+    case 4 ... 5:\n #endif\n     case 6 ... 7:\n         return ROUNDUP(sp, STACK_SIZE) -",
        "diff_line_info": {
            "deleted_lines": [
                "    case 0 ... 2:",
                "    case 3 ... 5:"
            ],
            "added_lines": [
                "    case 0 ... 3:",
                "    case 4 ... 5:"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8897",
        "func_name": "xen-project/xen/load_system_tables",
        "description": "A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",
        "git_url": "https://github.com/xen-project/xen/commit/5d37af364dc158aa387f7c8b2a05c90325c63dce",
        "commit_title": "x86/traps: Use an Interrupt Stack Table for #DB",
        "commit_text": " PV guests can use architectural corner cases to cause #DB to be raised after transitioning into supervisor mode.  Use an interrupt stack table for #DB to prevent the exception being taken with a guest controlled stack pointer.  This is part of XSA-260 / CVE-2018-8897 ",
        "func_before": "void load_system_tables(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tunsigned long stack_bottom = get_stack_bottom(),\n\t\tstack_top = stack_bottom & ~(STACK_SIZE - 1);\n\n\tstruct tss_struct *tss = &this_cpu(init_tss);\n\tstruct desc_struct *gdt =\n\t\tthis_cpu(gdt_table) - FIRST_RESERVED_GDT_ENTRY;\n\tstruct desc_struct *compat_gdt =\n\t\tthis_cpu(compat_gdt_table) - FIRST_RESERVED_GDT_ENTRY;\n\n\tconst struct desc_ptr gdtr = {\n\t\t.base = (unsigned long)gdt,\n\t\t.limit = LAST_RESERVED_GDT_BYTE,\n\t};\n\tconst struct desc_ptr idtr = {\n\t\t.base = (unsigned long)idt_tables[cpu],\n\t\t.limit = (IDT_ENTRIES * sizeof(idt_entry_t)) - 1,\n\t};\n\n\t*tss = (struct tss_struct){\n\t\t/* Main stack for interrupts/exceptions. */\n\t\t.rsp0 = stack_bottom,\n\n\t\t/* Ring 1 and 2 stacks poisoned. */\n\t\t.rsp1 = 0x8600111111111111ul,\n\t\t.rsp2 = 0x8600111111111111ul,\n\n\t\t/*\n\t\t * MCE, NMI and Double Fault handlers get their own stacks.\n\t\t * All others poisoned.\n\t\t */\n\t\t.ist = {\n\t\t\t[IST_MCE - 1] = stack_top + IST_MCE * PAGE_SIZE,\n\t\t\t[IST_DF  - 1] = stack_top + IST_DF  * PAGE_SIZE,\n\t\t\t[IST_NMI - 1] = stack_top + IST_NMI * PAGE_SIZE,\n\n\t\t\t[IST_MAX ... ARRAY_SIZE(tss->ist) - 1] =\n\t\t\t\t0x8600111111111111ul,\n\t\t},\n\n\t\t.bitmap = IOBMP_INVALID_OFFSET,\n\t};\n\n\t_set_tssldt_desc(\n\t\tgdt + TSS_ENTRY,\n\t\t(unsigned long)tss,\n\t\toffsetof(struct tss_struct, __cacheline_filler) - 1,\n\t\tSYS_DESC_tss_avail);\n\t_set_tssldt_desc(\n\t\tcompat_gdt + TSS_ENTRY,\n\t\t(unsigned long)tss,\n\t\toffsetof(struct tss_struct, __cacheline_filler) - 1,\n\t\tSYS_DESC_tss_busy);\n\n\tlgdt(&gdtr);\n\tlidt(&idtr);\n\tltr(TSS_ENTRY << 3);\n\tlldt(0);\n\n\tenable_each_ist(idt_tables[cpu]);\n\n\t/*\n\t * Bottom-of-stack must be 16-byte aligned!\n\t *\n\t * Defer checks until exception support is sufficiently set up.\n\t */\n\tBUILD_BUG_ON((sizeof(struct cpu_info) -\n\t\t      offsetof(struct cpu_info, guest_cpu_user_regs.es)) & 0xf);\n\tBUG_ON(system_state != SYS_STATE_early_boot && (stack_bottom & 0xf));\n}",
        "func": "void load_system_tables(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tunsigned long stack_bottom = get_stack_bottom(),\n\t\tstack_top = stack_bottom & ~(STACK_SIZE - 1);\n\n\tstruct tss_struct *tss = &this_cpu(init_tss);\n\tstruct desc_struct *gdt =\n\t\tthis_cpu(gdt_table) - FIRST_RESERVED_GDT_ENTRY;\n\tstruct desc_struct *compat_gdt =\n\t\tthis_cpu(compat_gdt_table) - FIRST_RESERVED_GDT_ENTRY;\n\n\tconst struct desc_ptr gdtr = {\n\t\t.base = (unsigned long)gdt,\n\t\t.limit = LAST_RESERVED_GDT_BYTE,\n\t};\n\tconst struct desc_ptr idtr = {\n\t\t.base = (unsigned long)idt_tables[cpu],\n\t\t.limit = (IDT_ENTRIES * sizeof(idt_entry_t)) - 1,\n\t};\n\n\t*tss = (struct tss_struct){\n\t\t/* Main stack for interrupts/exceptions. */\n\t\t.rsp0 = stack_bottom,\n\n\t\t/* Ring 1 and 2 stacks poisoned. */\n\t\t.rsp1 = 0x8600111111111111ul,\n\t\t.rsp2 = 0x8600111111111111ul,\n\n\t\t/*\n\t\t * MCE, NMI and Double Fault handlers get their own stacks.\n\t\t * All others poisoned.\n\t\t */\n\t\t.ist = {\n\t\t\t[IST_MCE - 1] = stack_top + IST_MCE * PAGE_SIZE,\n\t\t\t[IST_DF  - 1] = stack_top + IST_DF  * PAGE_SIZE,\n\t\t\t[IST_NMI - 1] = stack_top + IST_NMI * PAGE_SIZE,\n\t\t\t[IST_DB  - 1] = stack_top + IST_DB  * PAGE_SIZE,\n\n\t\t\t[IST_MAX ... ARRAY_SIZE(tss->ist) - 1] =\n\t\t\t\t0x8600111111111111ul,\n\t\t},\n\n\t\t.bitmap = IOBMP_INVALID_OFFSET,\n\t};\n\n\t_set_tssldt_desc(\n\t\tgdt + TSS_ENTRY,\n\t\t(unsigned long)tss,\n\t\toffsetof(struct tss_struct, __cacheline_filler) - 1,\n\t\tSYS_DESC_tss_avail);\n\t_set_tssldt_desc(\n\t\tcompat_gdt + TSS_ENTRY,\n\t\t(unsigned long)tss,\n\t\toffsetof(struct tss_struct, __cacheline_filler) - 1,\n\t\tSYS_DESC_tss_busy);\n\n\tlgdt(&gdtr);\n\tlidt(&idtr);\n\tltr(TSS_ENTRY << 3);\n\tlldt(0);\n\n\tenable_each_ist(idt_tables[cpu]);\n\n\t/*\n\t * Bottom-of-stack must be 16-byte aligned!\n\t *\n\t * Defer checks until exception support is sufficiently set up.\n\t */\n\tBUILD_BUG_ON((sizeof(struct cpu_info) -\n\t\t      offsetof(struct cpu_info, guest_cpu_user_regs.es)) & 0xf);\n\tBUG_ON(system_state != SYS_STATE_early_boot && (stack_bottom & 0xf));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -35,6 +35,7 @@\n \t\t\t[IST_MCE - 1] = stack_top + IST_MCE * PAGE_SIZE,\n \t\t\t[IST_DF  - 1] = stack_top + IST_DF  * PAGE_SIZE,\n \t\t\t[IST_NMI - 1] = stack_top + IST_NMI * PAGE_SIZE,\n+\t\t\t[IST_DB  - 1] = stack_top + IST_DB  * PAGE_SIZE,\n \n \t\t\t[IST_MAX ... ARRAY_SIZE(tss->ist) - 1] =\n \t\t\t\t0x8600111111111111ul,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\t[IST_DB  - 1] = stack_top + IST_DB  * PAGE_SIZE,"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8897",
        "func_name": "torvalds/linux/do_int3",
        "description": "A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",
        "git_url": "https://github.com/torvalds/linux/commit/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9",
        "commit_title": "x86/entry/64: Don't use IST entry for #BP stack",
        "commit_text": " There's nothing IST-worthy about #BP/int3.  We don't allow kprobes in the small handful of places in the kernel that run at CPL0 with an invalid stack, and 32-bit kernels have used normal interrupt gates for #BP forever.  Furthermore, we don't allow kprobes in places that have usergs while in kernel mode, so \"paranoid\" is also unnecessary.  Cc: stable@vger.kernel.org",
        "func_before": "notrace do_int3(struct pt_regs *regs, long error_code)\n{\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t/*\n\t * ftrace must be first, everything else may cause a recursive crash.\n\t * See note by declaration of modifying_ftrace_code in ftrace.c\n\t */\n\tif (unlikely(atomic_read(&modifying_ftrace_code)) &&\n\t    ftrace_int3_handler(regs))\n\t\treturn;\n#endif\n\tif (poke_int3_handler(regs))\n\t\treturn;\n\n\tist_enter(regs);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n\n#ifdef CONFIG_KPROBES\n\tif (kprobe_int3_handler(regs))\n\t\tgoto exit;\n#endif\n\n\tif (notify_die(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n\n\t/*\n\t * Let others (NMI) know that the debug stack is in use\n\t * as we may switch to the interrupt stack.\n\t */\n\tdebug_stack_usage_inc();\n\tcond_local_irq_enable(regs);\n\tdo_trap(X86_TRAP_BP, SIGTRAP, \"int3\", regs, error_code, NULL);\n\tcond_local_irq_disable(regs);\n\tdebug_stack_usage_dec();\nexit:\n\tist_exit(regs);\n}",
        "func": "notrace do_int3(struct pt_regs *regs, long error_code)\n{\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t/*\n\t * ftrace must be first, everything else may cause a recursive crash.\n\t * See note by declaration of modifying_ftrace_code in ftrace.c\n\t */\n\tif (unlikely(atomic_read(&modifying_ftrace_code)) &&\n\t    ftrace_int3_handler(regs))\n\t\treturn;\n#endif\n\tif (poke_int3_handler(regs))\n\t\treturn;\n\n\t/*\n\t * Use ist_enter despite the fact that we don't use an IST stack.\n\t * We can be called from a kprobe in non-CONTEXT_KERNEL kernel\n\t * mode or even during context tracking state changes.\n\t *\n\t * This means that we can't schedule.  That's okay.\n\t */\n\tist_enter(regs);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n\n#ifdef CONFIG_KPROBES\n\tif (kprobe_int3_handler(regs))\n\t\tgoto exit;\n#endif\n\n\tif (notify_die(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n\n\tcond_local_irq_enable(regs);\n\tdo_trap(X86_TRAP_BP, SIGTRAP, \"int3\", regs, error_code, NULL);\n\tcond_local_irq_disable(regs);\n\nexit:\n\tist_exit(regs);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,13 @@\n \tif (poke_int3_handler(regs))\n \t\treturn;\n \n+\t/*\n+\t * Use ist_enter despite the fact that we don't use an IST stack.\n+\t * We can be called from a kprobe in non-CONTEXT_KERNEL kernel\n+\t * mode or even during context tracking state changes.\n+\t *\n+\t * This means that we can't schedule.  That's okay.\n+\t */\n \tist_enter(regs);\n \tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n@@ -29,15 +36,10 @@\n \t\t\tSIGTRAP) == NOTIFY_STOP)\n \t\tgoto exit;\n \n-\t/*\n-\t * Let others (NMI) know that the debug stack is in use\n-\t * as we may switch to the interrupt stack.\n-\t */\n-\tdebug_stack_usage_inc();\n \tcond_local_irq_enable(regs);\n \tdo_trap(X86_TRAP_BP, SIGTRAP, \"int3\", regs, error_code, NULL);\n \tcond_local_irq_disable(regs);\n-\tdebug_stack_usage_dec();\n+\n exit:\n \tist_exit(regs);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t/*",
                "\t * Let others (NMI) know that the debug stack is in use",
                "\t * as we may switch to the interrupt stack.",
                "\t */",
                "\tdebug_stack_usage_inc();",
                "\tdebug_stack_usage_dec();"
            ],
            "added_lines": [
                "\t/*",
                "\t * Use ist_enter despite the fact that we don't use an IST stack.",
                "\t * We can be called from a kprobe in non-CONTEXT_KERNEL kernel",
                "\t * mode or even during context tracking state changes.",
                "\t *",
                "\t * This means that we can't schedule.  That's okay.",
                "\t */",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-12232",
        "func_name": "torvalds/linux/sockfs_setattr",
        "description": "In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",
        "git_url": "https://github.com/torvalds/linux/commit/6d8c50dcb029872b298eea68cc6209c866fd3e14",
        "commit_title": "socket: close race condition between sock_close() and sockfs_setattr()",
        "commit_text": " fchownat() doesn't even hold refcnt of fd until it figures out fd is really needed (otherwise is ignored) and releases it after it resolves the path. This means sock_close() could race with sockfs_setattr(), which leads to a NULL pointer dereference since typically we set sock->sk to NULL in ->release().  As pointed out by Al, this is unique to sockfs. So we can fix this in socket layer by acquiring inode_lock in sock_close() and checking against NULL in sockfs_setattr().  sock_release() is called in many places, only the sock_close() path matters here. And fortunately, this should not affect normal sock_close() as it is only called when the last fd refcnt is gone. It only affects sock_close() with a parallel sockfs_setattr() in progress, which is not common.  Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp> Cc: Lorenzo Colitti <lorenzo@google.com> Cc: Al Viro <viro@zeniv.linux.org.uk>",
        "func_before": "static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n{\n\tint err = simple_setattr(dentry, iattr);\n\n\tif (!err && (iattr->ia_valid & ATTR_UID)) {\n\t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n\n\t\tsock->sk->sk_uid = iattr->ia_uid;\n\t}\n\n\treturn err;\n}",
        "func": "static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n{\n\tint err = simple_setattr(dentry, iattr);\n\n\tif (!err && (iattr->ia_valid & ATTR_UID)) {\n\t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n\n\t\tif (sock->sk)\n\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n\t\telse\n\t\t\terr = -ENOENT;\n\t}\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,10 @@\n \tif (!err && (iattr->ia_valid & ATTR_UID)) {\n \t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n \n-\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\tif (sock->sk)\n+\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\telse\n+\t\t\terr = -ENOENT;\n \t}\n \n \treturn err;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tsock->sk->sk_uid = iattr->ia_uid;"
            ],
            "added_lines": [
                "\t\tif (sock->sk)",
                "\t\t\tsock->sk->sk_uid = iattr->ia_uid;",
                "\t\telse",
                "\t\t\terr = -ENOENT;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-12232",
        "func_name": "torvalds/linux/sock_release",
        "description": "In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",
        "git_url": "https://github.com/torvalds/linux/commit/6d8c50dcb029872b298eea68cc6209c866fd3e14",
        "commit_title": "socket: close race condition between sock_close() and sockfs_setattr()",
        "commit_text": " fchownat() doesn't even hold refcnt of fd until it figures out fd is really needed (otherwise is ignored) and releases it after it resolves the path. This means sock_close() could race with sockfs_setattr(), which leads to a NULL pointer dereference since typically we set sock->sk to NULL in ->release().  As pointed out by Al, this is unique to sockfs. So we can fix this in socket layer by acquiring inode_lock in sock_close() and checking against NULL in sockfs_setattr().  sock_release() is called in many places, only the sock_close() path matters here. And fortunately, this should not affect normal sock_close() as it is only called when the last fd refcnt is gone. It only affects sock_close() with a parallel sockfs_setattr() in progress, which is not common.  Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp> Cc: Lorenzo Colitti <lorenzo@google.com> Cc: Al Viro <viro@zeniv.linux.org.uk>",
        "func_before": "void sock_release(struct socket *sock)\n{\n\tif (sock->ops) {\n\t\tstruct module *owner = sock->ops->owner;\n\n\t\tsock->ops->release(sock);\n\t\tsock->ops = NULL;\n\t\tmodule_put(owner);\n\t}\n\n\tif (rcu_dereference_protected(sock->wq, 1)->fasync_list)\n\t\tpr_err(\"%s: fasync list not empty!\\n\", __func__);\n\n\tif (!sock->file) {\n\t\tiput(SOCK_INODE(sock));\n\t\treturn;\n\t}\n\tsock->file = NULL;\n}",
        "func": "void sock_release(struct socket *sock)\n{\n\t__sock_release(sock, NULL);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,19 +1,4 @@\n void sock_release(struct socket *sock)\n {\n-\tif (sock->ops) {\n-\t\tstruct module *owner = sock->ops->owner;\n-\n-\t\tsock->ops->release(sock);\n-\t\tsock->ops = NULL;\n-\t\tmodule_put(owner);\n-\t}\n-\n-\tif (rcu_dereference_protected(sock->wq, 1)->fasync_list)\n-\t\tpr_err(\"%s: fasync list not empty!\\n\", __func__);\n-\n-\tif (!sock->file) {\n-\t\tiput(SOCK_INODE(sock));\n-\t\treturn;\n-\t}\n-\tsock->file = NULL;\n+\t__sock_release(sock, NULL);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (sock->ops) {",
                "\t\tstruct module *owner = sock->ops->owner;",
                "",
                "\t\tsock->ops->release(sock);",
                "\t\tsock->ops = NULL;",
                "\t\tmodule_put(owner);",
                "\t}",
                "",
                "\tif (rcu_dereference_protected(sock->wq, 1)->fasync_list)",
                "\t\tpr_err(\"%s: fasync list not empty!\\n\", __func__);",
                "",
                "\tif (!sock->file) {",
                "\t\tiput(SOCK_INODE(sock));",
                "\t\treturn;",
                "\t}",
                "\tsock->file = NULL;"
            ],
            "added_lines": [
                "\t__sock_release(sock, NULL);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-12232",
        "func_name": "torvalds/linux/sock_close",
        "description": "In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",
        "git_url": "https://github.com/torvalds/linux/commit/6d8c50dcb029872b298eea68cc6209c866fd3e14",
        "commit_title": "socket: close race condition between sock_close() and sockfs_setattr()",
        "commit_text": " fchownat() doesn't even hold refcnt of fd until it figures out fd is really needed (otherwise is ignored) and releases it after it resolves the path. This means sock_close() could race with sockfs_setattr(), which leads to a NULL pointer dereference since typically we set sock->sk to NULL in ->release().  As pointed out by Al, this is unique to sockfs. So we can fix this in socket layer by acquiring inode_lock in sock_close() and checking against NULL in sockfs_setattr().  sock_release() is called in many places, only the sock_close() path matters here. And fortunately, this should not affect normal sock_close() as it is only called when the last fd refcnt is gone. It only affects sock_close() with a parallel sockfs_setattr() in progress, which is not common.  Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp> Cc: Lorenzo Colitti <lorenzo@google.com> Cc: Al Viro <viro@zeniv.linux.org.uk>",
        "func_before": "static int sock_close(struct inode *inode, struct file *filp)\n{\n\tsock_release(SOCKET_I(inode));\n\treturn 0;\n}",
        "func": "static int sock_close(struct inode *inode, struct file *filp)\n{\n\t__sock_release(SOCKET_I(inode), inode);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n static int sock_close(struct inode *inode, struct file *filp)\n {\n-\tsock_release(SOCKET_I(inode));\n+\t__sock_release(SOCKET_I(inode), inode);\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tsock_release(SOCKET_I(inode));"
            ],
            "added_lines": [
                "\t__sock_release(SOCKET_I(inode), inode);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/put_busid_priv",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=c171654caa875919be3c533d3518da8be5be966e",
        "commit_title": "stub_probe() calls put_busid_priv() in an error path when device isn't",
        "commit_text": "found in the busid_table. Fix it by making put_busid_priv() safe to be called with null struct bus_id_priv pointer.  This problem happens when \"usbip bind\" is run without loading usbip_host driver and then running modprobe. The first failed bind attempt unbinds the device from the original driver and when usbip_host is modprobed, stub_probe() runs and doesn't find the device in its busid table and calls put_busid_priv(0 with null bus_id_priv pointer.  usbip-host 3-10.2: 3-10.2 is not in match_busid table...  skip!  [  367.359679] ===================================== [  367.359681] WARNING: bad unlock balance detected! [  367.359683] 4.17.0-rc4+ #5 Not tainted [  367.359685] ------------------------------------- [  367.359688] modprobe/2768 is trying to release lock ( [  367.359689] ================================================================== [  367.359696] BUG: KASAN: null-ptr-deref in print_unlock_imbalance_bug+0x99/0x110 [  367.359699] Read of size 8 at addr 0000000000000058 by task modprobe/2768  [  367.359705] CPU: 4 PID: 2768 Comm: modprobe Not tainted 4.17.0-rc4+ #5  Cc: stable <stable@kernel.org> ",
        "func_before": "void put_busid_priv(struct bus_id_priv *bid)\n{\n\tspin_unlock(&bid->busid_lock);\n}",
        "func": "void put_busid_priv(struct bus_id_priv *bid)\n{\n\tif (bid)\n\t\tspin_unlock(&bid->busid_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n void put_busid_priv(struct bus_id_priv *bid)\n {\n-\tspin_unlock(&bid->busid_lock);\n+\tif (bid)\n+\t\tspin_unlock(&bid->busid_lock);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tspin_unlock(&bid->busid_lock);"
            ],
            "added_lines": [
                "\tif (bid)",
                "\t\tspin_unlock(&bid->busid_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/del_match_busid",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "int del_match_busid(char *busid)\n{\n\tint idx;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx < 0)\n\t\tgoto out;\n\n\t/* found */\n\tret = 0;\n\n\tif (busid_table[idx].status == STUB_BUSID_OTHER)\n\t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n\n\tif ((busid_table[idx].status != STUB_BUSID_OTHER) &&\n\t    (busid_table[idx].status != STUB_BUSID_ADDED))\n\t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "func": "int del_match_busid(char *busid)\n{\n\tint idx;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx < 0)\n\t\tgoto out;\n\n\t/* found */\n\tret = 0;\n\n\tspin_lock(&busid_table[idx].busid_lock);\n\n\tif (busid_table[idx].status == STUB_BUSID_OTHER)\n\t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n\n\tif ((busid_table[idx].status != STUB_BUSID_OTHER) &&\n\t    (busid_table[idx].status != STUB_BUSID_ADDED))\n\t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n\n\tspin_unlock(&busid_table[idx].busid_lock);\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,6 +11,8 @@\n \t/* found */\n \tret = 0;\n \n+\tspin_lock(&busid_table[idx].busid_lock);\n+\n \tif (busid_table[idx].status == STUB_BUSID_OTHER)\n \t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n \n@@ -18,6 +20,7 @@\n \t    (busid_table[idx].status != STUB_BUSID_ADDED))\n \t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n \n+\tspin_unlock(&busid_table[idx].busid_lock);\n out:\n \tspin_unlock(&busid_table_lock);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tspin_lock(&busid_table[idx].busid_lock);",
                "",
                "\tspin_unlock(&busid_table[idx].busid_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/stub_device_rebind",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "static void stub_device_rebind(void)\n{\n#if IS_MODULE(CONFIG_USBIP_HOST)\n\tstruct bus_id_priv *busid_priv;\n\tint i;\n\n\t/* update status to STUB_BUSID_OTHER so probe ignores the device */\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tbusid_priv->status = STUB_BUSID_OTHER;\n\t\t}\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\t/* now run rebind */\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tdo_rebind(busid_table[i].name, busid_priv);\n\t\t}\n\t}\n#endif\n}",
        "func": "static void stub_device_rebind(void)\n{\n#if IS_MODULE(CONFIG_USBIP_HOST)\n\tstruct bus_id_priv *busid_priv;\n\tint i;\n\n\t/* update status to STUB_BUSID_OTHER so probe ignores the device */\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tbusid_priv->status = STUB_BUSID_OTHER;\n\t\t}\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\t/* now run rebind - no need to hold locks. driver files are removed */\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tdo_rebind(busid_table[i].name, busid_priv);\n\t\t}\n\t}\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,7 +15,7 @@\n \t}\n \tspin_unlock(&busid_table_lock);\n \n-\t/* now run rebind */\n+\t/* now run rebind - no need to hold locks. driver files are removed */\n \tfor (i = 0; i < MAX_BUSID; i++) {\n \t\tif (busid_table[i].name[0] &&\n \t\t    busid_table[i].shutdown_busid) {",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* now run rebind */"
            ],
            "added_lines": [
                "\t/* now run rebind - no need to hold locks. driver files are removed */"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/init_busid_table",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "static void init_busid_table(void)\n{\n\t/*\n\t * This also sets the bus_table[i].status to\n\t * STUB_BUSID_OTHER, which is 0.\n\t */\n\tmemset(busid_table, 0, sizeof(busid_table));\n\n\tspin_lock_init(&busid_table_lock);\n}",
        "func": "static void init_busid_table(void)\n{\n\tint i;\n\n\t/*\n\t * This also sets the bus_table[i].status to\n\t * STUB_BUSID_OTHER, which is 0.\n\t */\n\tmemset(busid_table, 0, sizeof(busid_table));\n\n\tspin_lock_init(&busid_table_lock);\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tspin_lock_init(&busid_table[i].busid_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,7 @@\n static void init_busid_table(void)\n {\n+\tint i;\n+\n \t/*\n \t * This also sets the bus_table[i].status to\n \t * STUB_BUSID_OTHER, which is 0.\n@@ -7,4 +9,7 @@\n \tmemset(busid_table, 0, sizeof(busid_table));\n \n \tspin_lock_init(&busid_table_lock);\n+\n+\tfor (i = 0; i < MAX_BUSID; i++)\n+\t\tspin_lock_init(&busid_table[i].busid_lock);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tint i;",
                "",
                "",
                "\tfor (i = 0; i < MAX_BUSID; i++)",
                "\t\tspin_lock_init(&busid_table[i].busid_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/add_match_busid",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "static int add_match_busid(char *busid)\n{\n\tint i;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\t/* already registered? */\n\tif (get_busid_idx(busid) >= 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (!busid_table[i].name[0]) {\n\t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n\t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n\t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n\t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "func": "static int add_match_busid(char *busid)\n{\n\tint i;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\t/* already registered? */\n\tif (get_busid_idx(busid) >= 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (!busid_table[i].name[0]) {\n\t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n\t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n\t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n\t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n\t\t\tret = 0;\n\t\t\tspin_unlock(&busid_table[i].busid_lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,15 +10,19 @@\n \t\tgoto out;\n \t}\n \n-\tfor (i = 0; i < MAX_BUSID; i++)\n+\tfor (i = 0; i < MAX_BUSID; i++) {\n+\t\tspin_lock(&busid_table[i].busid_lock);\n \t\tif (!busid_table[i].name[0]) {\n \t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n \t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n \t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n \t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n \t\t\tret = 0;\n+\t\t\tspin_unlock(&busid_table[i].busid_lock);\n \t\t\tbreak;\n \t\t}\n+\t\tspin_unlock(&busid_table[i].busid_lock);\n+\t}\n \n out:\n \tspin_unlock(&busid_table_lock);",
        "diff_line_info": {
            "deleted_lines": [
                "\tfor (i = 0; i < MAX_BUSID; i++)"
            ],
            "added_lines": [
                "\tfor (i = 0; i < MAX_BUSID; i++) {",
                "\t\tspin_lock(&busid_table[i].busid_lock);",
                "\t\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/rebind_store",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "static ssize_t rebind_store(struct device_driver *dev, const char *buf,\n\t\t\t\t size_t count)\n{\n\tint ret;\n\tint len;\n\tstruct bus_id_priv *bid;\n\n\t/* buf length should be less that BUSID_SIZE */\n\tlen = strnlen(buf, BUSID_SIZE);\n\n\tif (!(len < BUSID_SIZE))\n\t\treturn -EINVAL;\n\n\tbid = get_busid_priv(buf);\n\tif (!bid)\n\t\treturn -ENODEV;\n\n\t/* mark the device for deletion so probe ignores it during rescan */\n\tbid->status = STUB_BUSID_OTHER;\n\n\tret = do_rebind((char *) buf, bid);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* delete device from busid_table */\n\tdel_match_busid((char *) buf);\n\n\treturn count;\n}",
        "func": "static ssize_t rebind_store(struct device_driver *dev, const char *buf,\n\t\t\t\t size_t count)\n{\n\tint ret;\n\tint len;\n\tstruct bus_id_priv *bid;\n\n\t/* buf length should be less that BUSID_SIZE */\n\tlen = strnlen(buf, BUSID_SIZE);\n\n\tif (!(len < BUSID_SIZE))\n\t\treturn -EINVAL;\n\n\tbid = get_busid_priv(buf);\n\tif (!bid)\n\t\treturn -ENODEV;\n\n\t/* mark the device for deletion so probe ignores it during rescan */\n\tbid->status = STUB_BUSID_OTHER;\n\t/* release the busid lock */\n\tput_busid_priv(bid);\n\n\tret = do_rebind((char *) buf, bid);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* delete device from busid_table */\n\tdel_match_busid((char *) buf);\n\n\treturn count;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,6 +17,8 @@\n \n \t/* mark the device for deletion so probe ignores it during rescan */\n \tbid->status = STUB_BUSID_OTHER;\n+\t/* release the busid lock */\n+\tput_busid_priv(bid);\n \n \tret = do_rebind((char *) buf, bid);\n \tif (ret < 0)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t/* release the busid lock */",
                "\tput_busid_priv(bid);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/match_busid_show",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "static ssize_t match_busid_show(struct device_driver *drv, char *buf)\n{\n\tint i;\n\tchar *out = buf;\n\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (busid_table[i].name[0])\n\t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n\tspin_unlock(&busid_table_lock);\n\tout += sprintf(out, \"\\n\");\n\n\treturn out - buf;\n}",
        "func": "static ssize_t match_busid_show(struct device_driver *drv, char *buf)\n{\n\tint i;\n\tchar *out = buf;\n\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (busid_table[i].name[0])\n\t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\tspin_unlock(&busid_table_lock);\n\tout += sprintf(out, \"\\n\");\n\n\treturn out - buf;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,9 +4,12 @@\n \tchar *out = buf;\n \n \tspin_lock(&busid_table_lock);\n-\tfor (i = 0; i < MAX_BUSID; i++)\n+\tfor (i = 0; i < MAX_BUSID; i++) {\n+\t\tspin_lock(&busid_table[i].busid_lock);\n \t\tif (busid_table[i].name[0])\n \t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n+\t\tspin_unlock(&busid_table[i].busid_lock);\n+\t}\n \tspin_unlock(&busid_table_lock);\n \tout += sprintf(out, \"\\n\");\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tfor (i = 0; i < MAX_BUSID; i++)"
            ],
            "added_lines": [
                "\tfor (i = 0; i < MAX_BUSID; i++) {",
                "\t\tspin_lock(&busid_table[i].busid_lock);",
                "\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/get_busid_priv",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "struct bus_id_priv *get_busid_priv(const char *busid)\n{\n\tint idx;\n\tstruct bus_id_priv *bid = NULL;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx >= 0)\n\t\tbid = &(busid_table[idx]);\n\tspin_unlock(&busid_table_lock);\n\n\treturn bid;\n}",
        "func": "struct bus_id_priv *get_busid_priv(const char *busid)\n{\n\tint idx;\n\tstruct bus_id_priv *bid = NULL;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx >= 0) {\n\t\tbid = &(busid_table[idx]);\n\t\t/* get busid_lock before returning */\n\t\tspin_lock(&bid->busid_lock);\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\treturn bid;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,8 +5,11 @@\n \n \tspin_lock(&busid_table_lock);\n \tidx = get_busid_idx(busid);\n-\tif (idx >= 0)\n+\tif (idx >= 0) {\n \t\tbid = &(busid_table[idx]);\n+\t\t/* get busid_lock before returning */\n+\t\tspin_lock(&bid->busid_lock);\n+\t}\n \tspin_unlock(&busid_table_lock);\n \n \treturn bid;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (idx >= 0)"
            ],
            "added_lines": [
                "\tif (idx >= 0) {",
                "\t\t/* get busid_lock before returning */",
                "\t\tspin_lock(&bid->busid_lock);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/get_busid_idx",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "static int get_busid_idx(const char *busid)\n{\n\tint i;\n\tint idx = -1;\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (busid_table[i].name[0])\n\t\t\tif (!strncmp(busid_table[i].name, busid, BUSID_SIZE)) {\n\t\t\t\tidx = i;\n\t\t\t\tbreak;\n\t\t\t}\n\treturn idx;\n}",
        "func": "static int get_busid_idx(const char *busid)\n{\n\tint i;\n\tint idx = -1;\n\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (busid_table[i].name[0])\n\t\t\tif (!strncmp(busid_table[i].name, busid, BUSID_SIZE)) {\n\t\t\t\tidx = i;\n\t\t\t\tspin_unlock(&busid_table[i].busid_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\treturn idx;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,11 +3,15 @@\n \tint i;\n \tint idx = -1;\n \n-\tfor (i = 0; i < MAX_BUSID; i++)\n+\tfor (i = 0; i < MAX_BUSID; i++) {\n+\t\tspin_lock(&busid_table[i].busid_lock);\n \t\tif (busid_table[i].name[0])\n \t\t\tif (!strncmp(busid_table[i].name, busid, BUSID_SIZE)) {\n \t\t\t\tidx = i;\n+\t\t\t\tspin_unlock(&busid_table[i].busid_lock);\n \t\t\t\tbreak;\n \t\t\t}\n+\t\tspin_unlock(&busid_table[i].busid_lock);\n+\t}\n \treturn idx;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tfor (i = 0; i < MAX_BUSID; i++)"
            ],
            "added_lines": [
                "\tfor (i = 0; i < MAX_BUSID; i++) {",
                "\t\tspin_lock(&busid_table[i].busid_lock);",
                "\t\t\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/stub_probe",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "static int stub_probe(struct usb_device *udev)\n{\n\tstruct stub_device *sdev = NULL;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc;\n\n\tdev_dbg(&udev->dev, \"Enter probe\\n\");\n\n\t/* check we should claim or not by busid_table */\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv || (busid_priv->status == STUB_BUSID_REMOV) ||\n\t    (busid_priv->status == STUB_BUSID_OTHER)) {\n\t\tdev_info(&udev->dev,\n\t\t\t\"%s is not in match_busid table... skip!\\n\",\n\t\t\tudev_busid);\n\n\t\t/*\n\t\t * Return value should be ENODEV or ENOXIO to continue trying\n\t\t * other matched drivers by the driver core.\n\t\t * See driver_probe_device() in driver/base/dd.c\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\tif (udev->descriptor.bDeviceClass == USB_CLASS_HUB) {\n\t\tdev_dbg(&udev->dev, \"%s is a usb hub device... skip!\\n\",\n\t\t\t udev_busid);\n\t\treturn -ENODEV;\n\t}\n\n\tif (!strcmp(udev->bus->bus_name, \"vhci_hcd\")) {\n\t\tdev_dbg(&udev->dev,\n\t\t\t\"%s is attached on vhci_hcd... skip!\\n\",\n\t\t\tudev_busid);\n\n\t\treturn -ENODEV;\n\t}\n\n\t/* ok, this is my device */\n\tsdev = stub_device_alloc(udev);\n\tif (!sdev)\n\t\treturn -ENOMEM;\n\n\tdev_info(&udev->dev,\n\t\t\"usbip-host: register new device (bus %u dev %u)\\n\",\n\t\tudev->bus->busnum, udev->devnum);\n\n\tbusid_priv->shutdown_busid = 0;\n\n\t/* set private data to usb_device */\n\tdev_set_drvdata(&udev->dev, sdev);\n\tbusid_priv->sdev = sdev;\n\tbusid_priv->udev = udev;\n\n\t/*\n\t * Claim this hub port.\n\t * It doesn't matter what value we pass as owner\n\t * (struct dev_state) as long as it is unique.\n\t */\n\trc = usb_hub_claim_port(udev->parent, udev->portnum,\n\t\t\t(struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to claim port\\n\");\n\t\tgoto err_port;\n\t}\n\n\trc = stub_add_files(&udev->dev);\n\tif (rc) {\n\t\tdev_err(&udev->dev, \"stub_add_files for %s\\n\", udev_busid);\n\t\tgoto err_files;\n\t}\n\tbusid_priv->status = STUB_BUSID_ALLOC;\n\n\treturn 0;\nerr_files:\n\tusb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t     (struct usb_dev_state *) udev);\nerr_port:\n\tdev_set_drvdata(&udev->dev, NULL);\n\tusb_put_dev(udev);\n\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\treturn rc;\n}",
        "func": "static int stub_probe(struct usb_device *udev)\n{\n\tstruct stub_device *sdev = NULL;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc = 0;\n\n\tdev_dbg(&udev->dev, \"Enter probe\\n\");\n\n\t/* check we should claim or not by busid_table */\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv || (busid_priv->status == STUB_BUSID_REMOV) ||\n\t    (busid_priv->status == STUB_BUSID_OTHER)) {\n\t\tdev_info(&udev->dev,\n\t\t\t\"%s is not in match_busid table... skip!\\n\",\n\t\t\tudev_busid);\n\n\t\t/*\n\t\t * Return value should be ENODEV or ENOXIO to continue trying\n\t\t * other matched drivers by the driver core.\n\t\t * See driver_probe_device() in driver/base/dd.c\n\t\t */\n\t\trc = -ENODEV;\n\t\tgoto call_put_busid_priv;\n\t}\n\n\tif (udev->descriptor.bDeviceClass == USB_CLASS_HUB) {\n\t\tdev_dbg(&udev->dev, \"%s is a usb hub device... skip!\\n\",\n\t\t\t udev_busid);\n\t\trc = -ENODEV;\n\t\tgoto call_put_busid_priv;\n\t}\n\n\tif (!strcmp(udev->bus->bus_name, \"vhci_hcd\")) {\n\t\tdev_dbg(&udev->dev,\n\t\t\t\"%s is attached on vhci_hcd... skip!\\n\",\n\t\t\tudev_busid);\n\n\t\trc = -ENODEV;\n\t\tgoto call_put_busid_priv;\n\t}\n\n\t/* ok, this is my device */\n\tsdev = stub_device_alloc(udev);\n\tif (!sdev) {\n\t\trc = -ENOMEM;\n\t\tgoto call_put_busid_priv;\n\t}\n\n\tdev_info(&udev->dev,\n\t\t\"usbip-host: register new device (bus %u dev %u)\\n\",\n\t\tudev->bus->busnum, udev->devnum);\n\n\tbusid_priv->shutdown_busid = 0;\n\n\t/* set private data to usb_device */\n\tdev_set_drvdata(&udev->dev, sdev);\n\tbusid_priv->sdev = sdev;\n\tbusid_priv->udev = udev;\n\n\t/*\n\t * Claim this hub port.\n\t * It doesn't matter what value we pass as owner\n\t * (struct dev_state) as long as it is unique.\n\t */\n\trc = usb_hub_claim_port(udev->parent, udev->portnum,\n\t\t\t(struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to claim port\\n\");\n\t\tgoto err_port;\n\t}\n\n\trc = stub_add_files(&udev->dev);\n\tif (rc) {\n\t\tdev_err(&udev->dev, \"stub_add_files for %s\\n\", udev_busid);\n\t\tgoto err_files;\n\t}\n\tbusid_priv->status = STUB_BUSID_ALLOC;\n\n\trc = 0;\n\tgoto call_put_busid_priv;\n\nerr_files:\n\tusb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t     (struct usb_dev_state *) udev);\nerr_port:\n\tdev_set_drvdata(&udev->dev, NULL);\n\tusb_put_dev(udev);\n\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\ncall_put_busid_priv:\n\tput_busid_priv(busid_priv);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n \tstruct stub_device *sdev = NULL;\n \tconst char *udev_busid = dev_name(&udev->dev);\n \tstruct bus_id_priv *busid_priv;\n-\tint rc;\n+\tint rc = 0;\n \n \tdev_dbg(&udev->dev, \"Enter probe\\n\");\n \n@@ -20,13 +20,15 @@\n \t\t * other matched drivers by the driver core.\n \t\t * See driver_probe_device() in driver/base/dd.c\n \t\t */\n-\t\treturn -ENODEV;\n+\t\trc = -ENODEV;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \tif (udev->descriptor.bDeviceClass == USB_CLASS_HUB) {\n \t\tdev_dbg(&udev->dev, \"%s is a usb hub device... skip!\\n\",\n \t\t\t udev_busid);\n-\t\treturn -ENODEV;\n+\t\trc = -ENODEV;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \tif (!strcmp(udev->bus->bus_name, \"vhci_hcd\")) {\n@@ -34,13 +36,16 @@\n \t\t\t\"%s is attached on vhci_hcd... skip!\\n\",\n \t\t\tudev_busid);\n \n-\t\treturn -ENODEV;\n+\t\trc = -ENODEV;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \t/* ok, this is my device */\n \tsdev = stub_device_alloc(udev);\n-\tif (!sdev)\n-\t\treturn -ENOMEM;\n+\tif (!sdev) {\n+\t\trc = -ENOMEM;\n+\t\tgoto call_put_busid_priv;\n+\t}\n \n \tdev_info(&udev->dev,\n \t\t\"usbip-host: register new device (bus %u dev %u)\\n\",\n@@ -72,7 +77,9 @@\n \t}\n \tbusid_priv->status = STUB_BUSID_ALLOC;\n \n-\treturn 0;\n+\trc = 0;\n+\tgoto call_put_busid_priv;\n+\n err_files:\n \tusb_hub_release_port(udev->parent, udev->portnum,\n \t\t\t     (struct usb_dev_state *) udev);\n@@ -82,5 +89,8 @@\n \n \tbusid_priv->sdev = NULL;\n \tstub_device_free(sdev);\n+\n+call_put_busid_priv:\n+\tput_busid_priv(busid_priv);\n \treturn rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tint rc;",
                "\t\treturn -ENODEV;",
                "\t\treturn -ENODEV;",
                "\t\treturn -ENODEV;",
                "\tif (!sdev)",
                "\t\treturn -ENOMEM;",
                "\treturn 0;"
            ],
            "added_lines": [
                "\tint rc = 0;",
                "\t\trc = -ENODEV;",
                "\t\tgoto call_put_busid_priv;",
                "\t\trc = -ENODEV;",
                "\t\tgoto call_put_busid_priv;",
                "\t\trc = -ENODEV;",
                "\t\tgoto call_put_busid_priv;",
                "\tif (!sdev) {",
                "\t\trc = -ENOMEM;",
                "\t\tgoto call_put_busid_priv;",
                "\t}",
                "\trc = 0;",
                "\tgoto call_put_busid_priv;",
                "",
                "",
                "call_put_busid_priv:",
                "\tput_busid_priv(busid_priv);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5814",
        "func_name": "torvalds/linux/stub_disconnect",
        "description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=22076557b07c12086eeb16b8ce2b0b735f7a27e7",
        "commit_title": "usbip_host updates device status without holding lock from stub probe,",
        "commit_text": "disconnect and rebind code paths. When multiple requests to import a device are received, these unprotected code paths step all over each other and drive fails with NULL-ptr deref and use-after-free errors.  The driver uses a table lock to protect the busid array for adding and deleting busids to the table. However, the probe, disconnect and rebind paths get the busid table entry and update the status without holding the busid table lock. Add a new finer grain lock to protect the busid entry. This new lock will be held to search and update the busid entry fields from get_busid_idx(), add_match_busid() and del_match_busid().  match_busid_show() does the same to access the busid entry fields.  get_busid_priv() changed to return the pointer to the busid entry holding the busid lock. stub_probe(), stub_disconnect() and stub_device_rebind() call put_busid_priv() to release the busid lock before returning. This changes fixes the unprotected code paths eliminating the race conditions in updating the busid entries.  Cc: stable <stable@kernel.org> ",
        "func_before": "static void stub_disconnect(struct usb_device *udev)\n{\n\tstruct stub_device *sdev;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc;\n\n\tdev_dbg(&udev->dev, \"Enter disconnect\\n\");\n\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv) {\n\t\tBUG();\n\t\treturn;\n\t}\n\n\tsdev = dev_get_drvdata(&udev->dev);\n\n\t/* get stub_device */\n\tif (!sdev) {\n\t\tdev_err(&udev->dev, \"could not get device\");\n\t\treturn;\n\t}\n\n\tdev_set_drvdata(&udev->dev, NULL);\n\n\t/*\n\t * NOTE: rx/tx threads are invoked for each usb_device.\n\t */\n\tstub_remove_files(&udev->dev);\n\n\t/* release port */\n\trc = usb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t\t  (struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to release port\\n\");\n\t\treturn;\n\t}\n\n\t/* If usb reset is called from event handler */\n\tif (usbip_in_eh(current))\n\t\treturn;\n\n\t/* shutdown the current connection */\n\tshutdown_busid(busid_priv);\n\n\tusb_put_dev(sdev->udev);\n\n\t/* free sdev */\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\n\tif (busid_priv->status == STUB_BUSID_ALLOC)\n\t\tbusid_priv->status = STUB_BUSID_ADDED;\n}",
        "func": "static void stub_disconnect(struct usb_device *udev)\n{\n\tstruct stub_device *sdev;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc;\n\n\tdev_dbg(&udev->dev, \"Enter disconnect\\n\");\n\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv) {\n\t\tBUG();\n\t\treturn;\n\t}\n\n\tsdev = dev_get_drvdata(&udev->dev);\n\n\t/* get stub_device */\n\tif (!sdev) {\n\t\tdev_err(&udev->dev, \"could not get device\");\n\t\tgoto call_put_busid_priv;\n\t}\n\n\tdev_set_drvdata(&udev->dev, NULL);\n\n\t/*\n\t * NOTE: rx/tx threads are invoked for each usb_device.\n\t */\n\tstub_remove_files(&udev->dev);\n\n\t/* release port */\n\trc = usb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t\t  (struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to release port\\n\");\n\t\tgoto call_put_busid_priv;\n\t}\n\n\t/* If usb reset is called from event handler */\n\tif (usbip_in_eh(current))\n\t\tgoto call_put_busid_priv;\n\n\t/* shutdown the current connection */\n\tshutdown_busid(busid_priv);\n\n\tusb_put_dev(sdev->udev);\n\n\t/* free sdev */\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\n\tif (busid_priv->status == STUB_BUSID_ALLOC)\n\t\tbusid_priv->status = STUB_BUSID_ADDED;\n\ncall_put_busid_priv:\n\tput_busid_priv(busid_priv);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,7 +18,7 @@\n \t/* get stub_device */\n \tif (!sdev) {\n \t\tdev_err(&udev->dev, \"could not get device\");\n-\t\treturn;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \tdev_set_drvdata(&udev->dev, NULL);\n@@ -33,12 +33,12 @@\n \t\t\t\t  (struct usb_dev_state *) udev);\n \tif (rc) {\n \t\tdev_dbg(&udev->dev, \"unable to release port\\n\");\n-\t\treturn;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \t/* If usb reset is called from event handler */\n \tif (usbip_in_eh(current))\n-\t\treturn;\n+\t\tgoto call_put_busid_priv;\n \n \t/* shutdown the current connection */\n \tshutdown_busid(busid_priv);\n@@ -51,4 +51,7 @@\n \n \tif (busid_priv->status == STUB_BUSID_ALLOC)\n \t\tbusid_priv->status = STUB_BUSID_ADDED;\n+\n+call_put_busid_priv:\n+\tput_busid_priv(busid_priv);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn;",
                "\t\treturn;",
                "\t\treturn;"
            ],
            "added_lines": [
                "\t\tgoto call_put_busid_priv;",
                "\t\tgoto call_put_busid_priv;",
                "\t\tgoto call_put_busid_priv;",
                "",
                "call_put_busid_priv:",
                "\tput_busid_priv(busid_priv);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-12633",
        "func_name": "torvalds/linux/vbg_misc_device_ioctl",
        "description": "An issue was discovered in the Linux kernel through 4.17.2. vbg_misc_device_ioctl() in drivers/virt/vboxguest/vboxguest_linux.c reads the same user data twice with copy_from_user. The header part of the user data is double-fetched, and a malicious user thread can tamper with the critical variables (hdr.size_in and hdr.size_out) in the header between the two fetches because of a race condition, leading to severe kernel errors, such as buffer over-accesses. This bug can cause a local denial of service and information leakage.",
        "git_url": "https://github.com/torvalds/linux/commit/bd23a7269834dc7c1f93e83535d16ebc44b75eba",
        "commit_title": "virt: vbox: Only copy_from_user the request-header once",
        "commit_text": " In vbg_misc_device_ioctl(), the header of the ioctl argument is copied from the userspace pointer 'arg' and saved to the kernel object 'hdr'. Then the 'version', 'size_in', and 'size_out' fields of 'hdr' are verified.  Before this commit, after the checks a buffer for the entire request would be allocated and then all data including the verified header would be copied from the userspace 'arg' pointer again.  Given that the 'arg' pointer resides in userspace, a malicious userspace process can race to change the data pointed to by 'arg' between the two copies. By doing so, the user can bypass the verifications on the ioctl argument.  This commit fixes this by using the already checked copy of the header to fill the header part of the allocated buffer and only copying the remainder of the data from userspace. ",
        "func_before": "static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct vbg_session *session = filp->private_data;\n\tsize_t returned_size, size;\n\tstruct vbg_ioctl_hdr hdr;\n\tbool is_vmmdev_req;\n\tint ret = 0;\n\tvoid *buf;\n\n\tif (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))\n\t\treturn -EFAULT;\n\n\tif (hdr.version != VBG_IOCTL_HDR_VERSION)\n\t\treturn -EINVAL;\n\n\tif (hdr.size_in < sizeof(hdr) ||\n\t    (hdr.size_out && hdr.size_out < sizeof(hdr)))\n\t\treturn -EINVAL;\n\n\tsize = max(hdr.size_in, hdr.size_out);\n\tif (_IOC_SIZE(req) && _IOC_SIZE(req) != size)\n\t\treturn -EINVAL;\n\tif (size > SZ_16M)\n\t\treturn -E2BIG;\n\n\t/*\n\t * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid\n\t * the need for a bounce-buffer and another copy later on.\n\t */\n\tis_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||\n\t\t\t req == VBG_IOCTL_VMMDEV_REQUEST_BIG;\n\n\tif (is_vmmdev_req)\n\t\tbuf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);\n\telse\n\t\tbuf = kmalloc(size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(buf, (void *)arg, hdr.size_in)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\tif (hdr.size_in < size)\n\t\tmemset(buf + hdr.size_in, 0, size -  hdr.size_in);\n\n\tret = vbg_core_ioctl(session, req, buf);\n\tif (ret)\n\t\tgoto out;\n\n\treturned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;\n\tif (returned_size > size) {\n\t\tvbg_debug(\"%s: too much output data %zu > %zu\\n\",\n\t\t\t  __func__, returned_size, size);\n\t\treturned_size = size;\n\t}\n\tif (copy_to_user((void *)arg, buf, returned_size) != 0)\n\t\tret = -EFAULT;\n\nout:\n\tif (is_vmmdev_req)\n\t\tvbg_req_free(buf, size);\n\telse\n\t\tkfree(buf);\n\n\treturn ret;\n}",
        "func": "static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct vbg_session *session = filp->private_data;\n\tsize_t returned_size, size;\n\tstruct vbg_ioctl_hdr hdr;\n\tbool is_vmmdev_req;\n\tint ret = 0;\n\tvoid *buf;\n\n\tif (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))\n\t\treturn -EFAULT;\n\n\tif (hdr.version != VBG_IOCTL_HDR_VERSION)\n\t\treturn -EINVAL;\n\n\tif (hdr.size_in < sizeof(hdr) ||\n\t    (hdr.size_out && hdr.size_out < sizeof(hdr)))\n\t\treturn -EINVAL;\n\n\tsize = max(hdr.size_in, hdr.size_out);\n\tif (_IOC_SIZE(req) && _IOC_SIZE(req) != size)\n\t\treturn -EINVAL;\n\tif (size > SZ_16M)\n\t\treturn -E2BIG;\n\n\t/*\n\t * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid\n\t * the need for a bounce-buffer and another copy later on.\n\t */\n\tis_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||\n\t\t\t req == VBG_IOCTL_VMMDEV_REQUEST_BIG;\n\n\tif (is_vmmdev_req)\n\t\tbuf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);\n\telse\n\t\tbuf = kmalloc(size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\t*((struct vbg_ioctl_hdr *)buf) = hdr;\n\tif (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),\n\t\t\t   hdr.size_in - sizeof(hdr))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\tif (hdr.size_in < size)\n\t\tmemset(buf + hdr.size_in, 0, size -  hdr.size_in);\n\n\tret = vbg_core_ioctl(session, req, buf);\n\tif (ret)\n\t\tgoto out;\n\n\treturned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;\n\tif (returned_size > size) {\n\t\tvbg_debug(\"%s: too much output data %zu > %zu\\n\",\n\t\t\t  __func__, returned_size, size);\n\t\treturned_size = size;\n\t}\n\tif (copy_to_user((void *)arg, buf, returned_size) != 0)\n\t\tret = -EFAULT;\n\nout:\n\tif (is_vmmdev_req)\n\t\tvbg_req_free(buf, size);\n\telse\n\t\tkfree(buf);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,7 +38,9 @@\n \tif (!buf)\n \t\treturn -ENOMEM;\n \n-\tif (copy_from_user(buf, (void *)arg, hdr.size_in)) {\n+\t*((struct vbg_ioctl_hdr *)buf) = hdr;\n+\tif (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),\n+\t\t\t   hdr.size_in - sizeof(hdr))) {\n \t\tret = -EFAULT;\n \t\tgoto out;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (copy_from_user(buf, (void *)arg, hdr.size_in)) {"
            ],
            "added_lines": [
                "\t*((struct vbg_ioctl_hdr *)buf) = hdr;",
                "\tif (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),",
                "\t\t\t   hdr.size_in - sizeof(hdr))) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36557",
        "func_name": "torvalds/linux/vt_disallocate_all",
        "description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=ca4463bf8438b403596edd0ec961ca0d4fbe0220",
        "commit_title": "The VT_DISALLOCATE ioctl can free a virtual console while tty_release()",
        "commit_text": "is still running, causing a use-after-free in con_shutdown().  This occurs because VT_DISALLOCATE considers a virtual console's 'struct vc_data' to be unused as soon as the corresponding tty's refcount hits 0.  But actually it may be still being closed.  Fix this by making vc_data be reference-counted via the embedded 'struct tty_port'.  A newly allocated virtual console has refcount 1. Opening it for the first time increments the refcount to 2.  Closing it for the last time decrements the refcount (in tty_operations::cleanup() so that it happens late enough), as does VT_DISALLOCATE.  Reproducer: \t#include <fcntl.h> \t#include <linux/vt.h> \t#include <sys/ioctl.h> \t#include <unistd.h>  \tint main() \t{ \t\tif (fork()) { \t\t\tfor (;;) \t\t\t\tclose(open(\"/dev/tty5\", O_RDWR)); \t\t} else { \t\t\tint fd = open(\"/dev/tty10\", O_RDWR);  \t\t\tfor (;;) \t\t\t\tioctl(fd, VT_DISALLOCATE, 5); \t\t} \t}  KASAN report: \tBUG: KASAN: use-after-free in con_shutdown+0x76/0x80 drivers/tty/vt/vt.c:3278 \tWrite of size 8 at addr ffff88806a4ec108 by task syz_vt/129  \tCPU: 0 PID: 129 Comm: syz_vt Not tainted 5.6.0-rc2 #11 \tHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20191223_100556-anatol 04/01/2014 \tCall Trace: \t [...] \t con_shutdown+0x76/0x80 drivers/tty/vt/vt.c:3278 \t release_tty+0xa8/0x410 drivers/tty/tty_io.c:1514 \t tty_release_struct+0x34/0x50 drivers/tty/tty_io.c:1629 \t tty_release+0x984/0xed0 drivers/tty/tty_io.c:1789 \t [...]  \tAllocated by task 129: \t [...] \t kzalloc include/linux/slab.h:669 [inline] \t vc_allocate drivers/tty/vt/vt.c:1085 [inline] \t vc_allocate+0x1ac/0x680 drivers/tty/vt/vt.c:1066 \t con_install+0x4d/0x3f0 drivers/tty/vt/vt.c:3229 \t tty_driver_install_tty drivers/tty/tty_io.c:1228 [inline] \t tty_init_dev+0x94/0x350 drivers/tty/tty_io.c:1341 \t tty_open_by_driver drivers/tty/tty_io.c:1987 [inline] \t tty_open+0x3ca/0xb30 drivers/tty/tty_io.c:2035 \t [...]  \tFreed by task 130: \t [...] \t kfree+0xbf/0x1e0 mm/slab.c:3757 \t vt_disallocate drivers/tty/vt/vt_ioctl.c:300 [inline] \t vt_ioctl+0x16dc/0x1e30 drivers/tty/vt/vt_ioctl.c:818 \t tty_ioctl+0x9db/0x11b0 drivers/tty/tty_io.c:2660 \t [...]  Cc: <stable@vger.kernel.org> # v3.4+ Link: https://lore.kernel.org/r/20200322034305.210082-2-ebiggers@kernel.org ",
        "func_before": "static void vt_disallocate_all(void)\n{\n\tstruct vc_data *vc[MAX_NR_CONSOLES];\n\tint i;\n\n\tconsole_lock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++)\n\t\tif (!vt_busy(i))\n\t\t\tvc[i] = vc_deallocate(i);\n\t\telse\n\t\t\tvc[i] = NULL;\n\tconsole_unlock();\n\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {\n\t\t\ttty_port_destroy(&vc[i]->port);\n\t\t\tkfree(vc[i]);\n\t\t}\n\t}\n}",
        "func": "static void vt_disallocate_all(void)\n{\n\tstruct vc_data *vc[MAX_NR_CONSOLES];\n\tint i;\n\n\tconsole_lock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++)\n\t\tif (!vt_busy(i))\n\t\t\tvc[i] = vc_deallocate(i);\n\t\telse\n\t\t\tvc[i] = NULL;\n\tconsole_unlock();\n\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n\t\tif (vc[i] && i >= MIN_NR_CONSOLES)\n\t\t\ttty_port_put(&vc[i]->port);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,9 +12,7 @@\n \tconsole_unlock();\n \n \tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n-\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {\n-\t\t\ttty_port_destroy(&vc[i]->port);\n-\t\t\tkfree(vc[i]);\n-\t\t}\n+\t\tif (vc[i] && i >= MIN_NR_CONSOLES)\n+\t\t\ttty_port_put(&vc[i]->port);\n \t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {",
                "\t\t\ttty_port_destroy(&vc[i]->port);",
                "\t\t\tkfree(vc[i]);",
                "\t\t}"
            ],
            "added_lines": [
                "\t\tif (vc[i] && i >= MIN_NR_CONSOLES)",
                "\t\t\ttty_port_put(&vc[i]->port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36557",
        "func_name": "torvalds/linux/vt_disallocate",
        "description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=ca4463bf8438b403596edd0ec961ca0d4fbe0220",
        "commit_title": "The VT_DISALLOCATE ioctl can free a virtual console while tty_release()",
        "commit_text": "is still running, causing a use-after-free in con_shutdown().  This occurs because VT_DISALLOCATE considers a virtual console's 'struct vc_data' to be unused as soon as the corresponding tty's refcount hits 0.  But actually it may be still being closed.  Fix this by making vc_data be reference-counted via the embedded 'struct tty_port'.  A newly allocated virtual console has refcount 1. Opening it for the first time increments the refcount to 2.  Closing it for the last time decrements the refcount (in tty_operations::cleanup() so that it happens late enough), as does VT_DISALLOCATE.  Reproducer: \t#include <fcntl.h> \t#include <linux/vt.h> \t#include <sys/ioctl.h> \t#include <unistd.h>  \tint main() \t{ \t\tif (fork()) { \t\t\tfor (;;) \t\t\t\tclose(open(\"/dev/tty5\", O_RDWR)); \t\t} else { \t\t\tint fd = open(\"/dev/tty10\", O_RDWR);  \t\t\tfor (;;) \t\t\t\tioctl(fd, VT_DISALLOCATE, 5); \t\t} \t}  KASAN report: \tBUG: KASAN: use-after-free in con_shutdown+0x76/0x80 drivers/tty/vt/vt.c:3278 \tWrite of size 8 at addr ffff88806a4ec108 by task syz_vt/129  \tCPU: 0 PID: 129 Comm: syz_vt Not tainted 5.6.0-rc2 #11 \tHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20191223_100556-anatol 04/01/2014 \tCall Trace: \t [...] \t con_shutdown+0x76/0x80 drivers/tty/vt/vt.c:3278 \t release_tty+0xa8/0x410 drivers/tty/tty_io.c:1514 \t tty_release_struct+0x34/0x50 drivers/tty/tty_io.c:1629 \t tty_release+0x984/0xed0 drivers/tty/tty_io.c:1789 \t [...]  \tAllocated by task 129: \t [...] \t kzalloc include/linux/slab.h:669 [inline] \t vc_allocate drivers/tty/vt/vt.c:1085 [inline] \t vc_allocate+0x1ac/0x680 drivers/tty/vt/vt.c:1066 \t con_install+0x4d/0x3f0 drivers/tty/vt/vt.c:3229 \t tty_driver_install_tty drivers/tty/tty_io.c:1228 [inline] \t tty_init_dev+0x94/0x350 drivers/tty/tty_io.c:1341 \t tty_open_by_driver drivers/tty/tty_io.c:1987 [inline] \t tty_open+0x3ca/0xb30 drivers/tty/tty_io.c:2035 \t [...]  \tFreed by task 130: \t [...] \t kfree+0xbf/0x1e0 mm/slab.c:3757 \t vt_disallocate drivers/tty/vt/vt_ioctl.c:300 [inline] \t vt_ioctl+0x16dc/0x1e30 drivers/tty/vt/vt_ioctl.c:818 \t tty_ioctl+0x9db/0x11b0 drivers/tty/tty_io.c:2660 \t [...]  Cc: <stable@vger.kernel.org> # v3.4+ Link: https://lore.kernel.org/r/20200322034305.210082-2-ebiggers@kernel.org ",
        "func_before": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\n\tif (vc && vc_num >= MIN_NR_CONSOLES) {\n\t\ttty_port_destroy(&vc->port);\n\t\tkfree(vc);\n\t}\n\n\treturn ret;\n}",
        "func": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\n\tif (vc && vc_num >= MIN_NR_CONSOLES)\n\t\ttty_port_put(&vc->port);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,10 +10,8 @@\n \t\tvc = vc_deallocate(vc_num);\n \tconsole_unlock();\n \n-\tif (vc && vc_num >= MIN_NR_CONSOLES) {\n-\t\ttty_port_destroy(&vc->port);\n-\t\tkfree(vc);\n-\t}\n+\tif (vc && vc_num >= MIN_NR_CONSOLES)\n+\t\ttty_port_put(&vc->port);\n \n \treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (vc && vc_num >= MIN_NR_CONSOLES) {",
                "\t\ttty_port_destroy(&vc->port);",
                "\t\tkfree(vc);",
                "\t}"
            ],
            "added_lines": [
                "\tif (vc && vc_num >= MIN_NR_CONSOLES)",
                "\t\ttty_port_put(&vc->port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36557",
        "func_name": "torvalds/linux/vc_allocate",
        "description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=ca4463bf8438b403596edd0ec961ca0d4fbe0220",
        "commit_title": "The VT_DISALLOCATE ioctl can free a virtual console while tty_release()",
        "commit_text": "is still running, causing a use-after-free in con_shutdown().  This occurs because VT_DISALLOCATE considers a virtual console's 'struct vc_data' to be unused as soon as the corresponding tty's refcount hits 0.  But actually it may be still being closed.  Fix this by making vc_data be reference-counted via the embedded 'struct tty_port'.  A newly allocated virtual console has refcount 1. Opening it for the first time increments the refcount to 2.  Closing it for the last time decrements the refcount (in tty_operations::cleanup() so that it happens late enough), as does VT_DISALLOCATE.  Reproducer: \t#include <fcntl.h> \t#include <linux/vt.h> \t#include <sys/ioctl.h> \t#include <unistd.h>  \tint main() \t{ \t\tif (fork()) { \t\t\tfor (;;) \t\t\t\tclose(open(\"/dev/tty5\", O_RDWR)); \t\t} else { \t\t\tint fd = open(\"/dev/tty10\", O_RDWR);  \t\t\tfor (;;) \t\t\t\tioctl(fd, VT_DISALLOCATE, 5); \t\t} \t}  KASAN report: \tBUG: KASAN: use-after-free in con_shutdown+0x76/0x80 drivers/tty/vt/vt.c:3278 \tWrite of size 8 at addr ffff88806a4ec108 by task syz_vt/129  \tCPU: 0 PID: 129 Comm: syz_vt Not tainted 5.6.0-rc2 #11 \tHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20191223_100556-anatol 04/01/2014 \tCall Trace: \t [...] \t con_shutdown+0x76/0x80 drivers/tty/vt/vt.c:3278 \t release_tty+0xa8/0x410 drivers/tty/tty_io.c:1514 \t tty_release_struct+0x34/0x50 drivers/tty/tty_io.c:1629 \t tty_release+0x984/0xed0 drivers/tty/tty_io.c:1789 \t [...]  \tAllocated by task 129: \t [...] \t kzalloc include/linux/slab.h:669 [inline] \t vc_allocate drivers/tty/vt/vt.c:1085 [inline] \t vc_allocate+0x1ac/0x680 drivers/tty/vt/vt.c:1066 \t con_install+0x4d/0x3f0 drivers/tty/vt/vt.c:3229 \t tty_driver_install_tty drivers/tty/tty_io.c:1228 [inline] \t tty_init_dev+0x94/0x350 drivers/tty/tty_io.c:1341 \t tty_open_by_driver drivers/tty/tty_io.c:1987 [inline] \t tty_open+0x3ca/0xb30 drivers/tty/tty_io.c:2035 \t [...]  \tFreed by task 130: \t [...] \t kfree+0xbf/0x1e0 mm/slab.c:3757 \t vt_disallocate drivers/tty/vt/vt_ioctl.c:300 [inline] \t vt_ioctl+0x16dc/0x1e30 drivers/tty/vt/vt_ioctl.c:818 \t tty_ioctl+0x9db/0x11b0 drivers/tty/tty_io.c:2660 \t [...]  Cc: <stable@vger.kernel.org> # v3.4+ Link: https://lore.kernel.org/r/20200322034305.210082-2-ebiggers@kernel.org ",
        "func_before": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\n\tWARN_CONSOLE_UNLOCKED();\n\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\n\tvisual_init(vc, currcons, 1);\n\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
        "func": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\n\tWARN_CONSOLE_UNLOCKED();\n\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tvc->port.ops = &vc_port_ops;\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\n\tvisual_init(vc, currcons, 1);\n\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -23,6 +23,7 @@\n \n \tvc_cons[currcons].d = vc;\n \ttty_port_init(&vc->port);\n+\tvc->port.ops = &vc_port_ops;\n \tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n \n \tvisual_init(vc, currcons, 1);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tvc->port.ops = &vc_port_ops;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36557",
        "func_name": "torvalds/linux/con_install",
        "description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=ca4463bf8438b403596edd0ec961ca0d4fbe0220",
        "commit_title": "The VT_DISALLOCATE ioctl can free a virtual console while tty_release()",
        "commit_text": "is still running, causing a use-after-free in con_shutdown().  This occurs because VT_DISALLOCATE considers a virtual console's 'struct vc_data' to be unused as soon as the corresponding tty's refcount hits 0.  But actually it may be still being closed.  Fix this by making vc_data be reference-counted via the embedded 'struct tty_port'.  A newly allocated virtual console has refcount 1. Opening it for the first time increments the refcount to 2.  Closing it for the last time decrements the refcount (in tty_operations::cleanup() so that it happens late enough), as does VT_DISALLOCATE.  Reproducer: \t#include <fcntl.h> \t#include <linux/vt.h> \t#include <sys/ioctl.h> \t#include <unistd.h>  \tint main() \t{ \t\tif (fork()) { \t\t\tfor (;;) \t\t\t\tclose(open(\"/dev/tty5\", O_RDWR)); \t\t} else { \t\t\tint fd = open(\"/dev/tty10\", O_RDWR);  \t\t\tfor (;;) \t\t\t\tioctl(fd, VT_DISALLOCATE, 5); \t\t} \t}  KASAN report: \tBUG: KASAN: use-after-free in con_shutdown+0x76/0x80 drivers/tty/vt/vt.c:3278 \tWrite of size 8 at addr ffff88806a4ec108 by task syz_vt/129  \tCPU: 0 PID: 129 Comm: syz_vt Not tainted 5.6.0-rc2 #11 \tHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20191223_100556-anatol 04/01/2014 \tCall Trace: \t [...] \t con_shutdown+0x76/0x80 drivers/tty/vt/vt.c:3278 \t release_tty+0xa8/0x410 drivers/tty/tty_io.c:1514 \t tty_release_struct+0x34/0x50 drivers/tty/tty_io.c:1629 \t tty_release+0x984/0xed0 drivers/tty/tty_io.c:1789 \t [...]  \tAllocated by task 129: \t [...] \t kzalloc include/linux/slab.h:669 [inline] \t vc_allocate drivers/tty/vt/vt.c:1085 [inline] \t vc_allocate+0x1ac/0x680 drivers/tty/vt/vt.c:1066 \t con_install+0x4d/0x3f0 drivers/tty/vt/vt.c:3229 \t tty_driver_install_tty drivers/tty/tty_io.c:1228 [inline] \t tty_init_dev+0x94/0x350 drivers/tty/tty_io.c:1341 \t tty_open_by_driver drivers/tty/tty_io.c:1987 [inline] \t tty_open+0x3ca/0xb30 drivers/tty/tty_io.c:2035 \t [...]  \tFreed by task 130: \t [...] \t kfree+0xbf/0x1e0 mm/slab.c:3757 \t vt_disallocate drivers/tty/vt/vt_ioctl.c:300 [inline] \t vt_ioctl+0x16dc/0x1e30 drivers/tty/vt/vt_ioctl.c:818 \t tty_ioctl+0x9db/0x11b0 drivers/tty/tty_io.c:2660 \t [...]  Cc: <stable@vger.kernel.org> # v3.4+ Link: https://lore.kernel.org/r/20200322034305.210082-2-ebiggers@kernel.org ",
        "func_before": "static int con_install(struct tty_driver *driver, struct tty_struct *tty)\n{\n\tunsigned int currcons = tty->index;\n\tstruct vc_data *vc;\n\tint ret;\n\n\tconsole_lock();\n\tret = vc_allocate(currcons);\n\tif (ret)\n\t\tgoto unlock;\n\n\tvc = vc_cons[currcons].d;\n\n\t/* Still being freed */\n\tif (vc->port.tty) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto unlock;\n\t}\n\n\tret = tty_port_install(&vc->port, driver, tty);\n\tif (ret)\n\t\tgoto unlock;\n\n\ttty->driver_data = vc;\n\tvc->port.tty = tty;\n\n\tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n\t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;\n\t\ttty->winsize.ws_col = vc_cons[currcons].d->vc_cols;\n\t}\n\tif (vc->vc_utf)\n\t\ttty->termios.c_iflag |= IUTF8;\n\telse\n\t\ttty->termios.c_iflag &= ~IUTF8;\nunlock:\n\tconsole_unlock();\n\treturn ret;\n}",
        "func": "static int con_install(struct tty_driver *driver, struct tty_struct *tty)\n{\n\tunsigned int currcons = tty->index;\n\tstruct vc_data *vc;\n\tint ret;\n\n\tconsole_lock();\n\tret = vc_allocate(currcons);\n\tif (ret)\n\t\tgoto unlock;\n\n\tvc = vc_cons[currcons].d;\n\n\t/* Still being freed */\n\tif (vc->port.tty) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto unlock;\n\t}\n\n\tret = tty_port_install(&vc->port, driver, tty);\n\tif (ret)\n\t\tgoto unlock;\n\n\ttty->driver_data = vc;\n\tvc->port.tty = tty;\n\ttty_port_get(&vc->port);\n\n\tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n\t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;\n\t\ttty->winsize.ws_col = vc_cons[currcons].d->vc_cols;\n\t}\n\tif (vc->vc_utf)\n\t\ttty->termios.c_iflag |= IUTF8;\n\telse\n\t\ttty->termios.c_iflag &= ~IUTF8;\nunlock:\n\tconsole_unlock();\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -23,6 +23,7 @@\n \n \ttty->driver_data = vc;\n \tvc->port.tty = tty;\n+\ttty_port_get(&vc->port);\n \n \tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n \t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\ttty_port_get(&vc->port);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36558",
        "func_name": "torvalds/linux/vt_ioctl",
        "description": "A race condition in the Linux kernel before 5.5.7 involving VT_RESIZEX could lead to a NULL pointer dereference and general protection fault.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=6cd1ed50efd88261298577cd92a14f2768eddeeb",
        "commit_title": "We need to make sure vc_cons[i].d is not NULL after grabbing",
        "commit_text": "console_lock(), or risk a crash.  general protection fault, probably for non-canonical address 0xdffffc0000000068: 0000 [#1] PREEMPT SMP KASAN KASAN: null-ptr-deref in range [0x0000000000000340-0x0000000000000347] CPU: 1 PID: 19462 Comm: syz-executor.5 Not tainted 5.5.0-syzkaller #0 Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011 RIP: 0010:vt_ioctl+0x1f96/0x26d0 drivers/tty/vt/vt_ioctl.c:883 Code: 74 41 e8 bd a6 84 fd 48 89 d8 48 c1 e8 03 42 80 3c 28 00 0f 85 e4 04 00 00 48 8b 03 48 8d b8 40 03 00 00 48 89 fa 48 c1 ea 03 <42> 0f b6 14 2a 84 d2 74 09 80 fa 03 0f 8e b1 05 00 00 44 89 b8 40 RSP: 0018:ffffc900086d7bb0 EFLAGS: 00010202 RAX: 0000000000000000 RBX: ffffffff8c34ee88 RCX: ffffc9001415c000 RDX: 0000000000000068 RSI: ffffffff83f0e6e3 RDI: 0000000000000340 RBP: ffffc900086d7cd0 R08: ffff888054ce0100 R09: fffffbfff16a2f6d R10: ffff888054ce0998 R11: ffff888054ce0100 R12: 000000000000001d R13: dffffc0000000000 R14: 1ffff920010daf79 R15: 000000000000ff7f FS:  00007f7d13c12700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000 CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 CR2: 00007ffd477e3c38 CR3: 0000000095d0a000 CR4: 00000000001406e0 DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 Call Trace:  tty_ioctl+0xa37/0x14f0 drivers/tty/tty_io.c:2660  vfs_ioctl fs/ioctl.c:47 [inline]  ksys_ioctl+0x123/0x180 fs/ioctl.c:763  __do_sys_ioctl fs/ioctl.c:772 [inline]  __se_sys_ioctl fs/ioctl.c:770 [inline]  __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:770  do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294  entry_SYSCALL_64_after_hwframe+0x49/0xbe RIP: 0033:0x45b399 Code: ad b6 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b6 fb ff c3 66 2e 0f 1f 84 00 00 00 00 RSP: 002b:00007f7d13c11c78 EFLAGS: 00000246 ORIG_RAX: 0000000000000010 RAX: ffffffffffffffda RBX: 00007f7d13c126d4 RCX: 000000000045b399 RDX: 0000000020000080 RSI: 000000000000560a RDI: 0000000000000003 RBP: 000000000075bf20 R08: 0000000000000000 R09: 0000000000000000 R10: 0000000000000000 R11: 0000000000000246 R12: 00000000ffffffff R13: 0000000000000666 R14: 00000000004c7f04 R15: 000000000075bf2c Modules linked in: ---[ end trace 80970faf7a67eb77 ]--- RIP: 0010:vt_ioctl+0x1f96/0x26d0 drivers/tty/vt/vt_ioctl.c:883 Code: 74 41 e8 bd a6 84 fd 48 89 d8 48 c1 e8 03 42 80 3c 28 00 0f 85 e4 04 00 00 48 8b 03 48 8d b8 40 03 00 00 48 89 fa 48 c1 ea 03 <42> 0f b6 14 2a 84 d2 74 09 80 fa 03 0f 8e b1 05 00 00 44 89 b8 40 RSP: 0018:ffffc900086d7bb0 EFLAGS: 00010202 RAX: 0000000000000000 RBX: ffffffff8c34ee88 RCX: ffffc9001415c000 RDX: 0000000000000068 RSI: ffffffff83f0e6e3 RDI: 0000000000000340 RBP: ffffc900086d7cd0 R08: ffff888054ce0100 R09: fffffbfff16a2f6d R10: ffff888054ce0998 R11: ffff888054ce0100 R12: 000000000000001d R13: dffffc0000000000 R14: 1ffff920010daf79 R15: 000000000000ff7f FS:  00007f7d13c12700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000 CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 CR2: 00007ffd477e3c38 CR3: 0000000095d0a000 CR4: 00000000001406e0 DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400  Cc: stable <stable@vger.kernel.org> Link: https://lore.kernel.org/r/20200210190721.200418-1-edumazet@google.com ",
        "func_before": "int vt_ioctl(struct tty_struct *tty,\n\t     unsigned int cmd, unsigned long arg)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tstruct console_font_op op;\t/* used in multiple places here */\n\tunsigned int console;\n\tunsigned char ucval;\n\tunsigned int uival;\n\tvoid __user *up = (void __user *)arg;\n\tint i, perm;\n\tint ret = 0;\n\n\tconsole = vc->vc_num;\n\n\n\tif (!vc_cons_allocated(console)) { \t/* impossible? */\n\t\tret = -ENOIOCTLCMD;\n\t\tgoto out;\n\t}\n\n\n\t/*\n\t * To have permissions to do most of the vt ioctls, we either have\n\t * to be the owner of the tty, or have CAP_SYS_TTY_CONFIG.\n\t */\n\tperm = 0;\n\tif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 1;\n \n\tswitch (cmd) {\n\tcase TIOCLINUX:\n\t\tret = tioclinux(tty, arg);\n\t\tbreak;\n\tcase KIOCSOUND:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\t/*\n\t\t * The use of PIT_TICK_RATE is historic, it used to be\n\t\t * the platform-dependent CLOCK_TICK_RATE between 2.6.12\n\t\t * and 2.6.36, which was a minor but unfortunate ABI\n\t\t * change. kd_mksound is locked by the input layer.\n\t\t */\n\t\tif (arg)\n\t\t\targ = PIT_TICK_RATE / arg;\n\t\tkd_mksound(arg, 0);\n\t\tbreak;\n\n\tcase KDMKTONE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t{\n\t\tunsigned int ticks, count;\n\t\t\n\t\t/*\n\t\t * Generate the tone for the appropriate number of ticks.\n\t\t * If the time is zero, turn off sound ourselves.\n\t\t */\n\t\tticks = msecs_to_jiffies((arg >> 16) & 0xffff);\n\t\tcount = ticks ? (arg & 0xffff) : 0;\n\t\tif (count)\n\t\t\tcount = PIT_TICK_RATE / count;\n\t\tkd_mksound(count, ticks);\n\t\tbreak;\n\t}\n\n\tcase KDGKBTYPE:\n\t\t/*\n\t\t * this is naïve.\n\t\t */\n\t\tucval = KB_101;\n\t\tret = put_user(ucval, (char __user *)arg);\n\t\tbreak;\n\n\t\t/*\n\t\t * These cannot be implemented on any machine that implements\n\t\t * ioperm() in user level (such as Alpha PCs) or not at all.\n\t\t *\n\t\t * XXX: you should never use these, just call ioperm directly..\n\t\t */\n#ifdef CONFIG_X86\n\tcase KDADDIO:\n\tcase KDDELIO:\n\t\t/*\n\t\t * KDADDIO and KDDELIO may be able to add ports beyond what\n\t\t * we reject here, but to be safe...\n\t\t *\n\t\t * These are locked internally via sys_ioperm\n\t\t */\n\t\tif (arg < GPFIRST || arg > GPLAST) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tret = ksys_ioperm(arg, 1, (cmd == KDADDIO)) ? -ENXIO : 0;\n\t\tbreak;\n\n\tcase KDENABIO:\n\tcase KDDISABIO:\n\t\tret = ksys_ioperm(GPFIRST, GPNUM,\n\t\t\t\t  (cmd == KDENABIO)) ? -ENXIO : 0;\n\t\tbreak;\n#endif\n\n\t/* Linux m68k/i386 interface for setting the keyboard delay/repeat rate */\n\t\t\n\tcase KDKBDREP:\n\t{\n\t\tstruct kbd_repeat kbrep;\n\t\t\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&kbrep, up, sizeof(struct kbd_repeat))) {\n\t\t\tret =  -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tret = kbd_rate(&kbrep);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &kbrep, sizeof(struct kbd_repeat)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase KDSETMODE:\n\t\t/*\n\t\t * currently, setting the mode from KD_TEXT to KD_GRAPHICS\n\t\t * doesn't do a whole lot. i'm not sure if it should do any\n\t\t * restoration of modes or what...\n\t\t *\n\t\t * XXX It should at least call into the driver, fbdev's definitely\n\t\t * need to restore their engine state. --BenH\n\t\t */\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tswitch (arg) {\n\t\tcase KD_GRAPHICS:\n\t\t\tbreak;\n\t\tcase KD_TEXT0:\n\t\tcase KD_TEXT1:\n\t\t\targ = KD_TEXT;\n\t\tcase KD_TEXT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\t/* FIXME: this needs the console lock extending */\n\t\tif (vc->vc_mode == (unsigned char) arg)\n\t\t\tbreak;\n\t\tvc->vc_mode = (unsigned char) arg;\n\t\tif (console != fg_console)\n\t\t\tbreak;\n\t\t/*\n\t\t * explicitly blank/unblank the screen if switching modes\n\t\t */\n\t\tconsole_lock();\n\t\tif (arg == KD_TEXT)\n\t\t\tdo_unblank_screen(1);\n\t\telse\n\t\t\tdo_blank_screen(1);\n\t\tconsole_unlock();\n\t\tbreak;\n\n\tcase KDGETMODE:\n\t\tuival = vc->vc_mode;\n\t\tgoto setint;\n\n\tcase KDMAPDISP:\n\tcase KDUNMAPDISP:\n\t\t/*\n\t\t * these work like a combination of mmap and KDENABIO.\n\t\t * this could be easily finished.\n\t\t */\n\t\tret = -EINVAL;\n\t\tbreak;\n\n\tcase KDSKBMODE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tret = vt_do_kdskbmode(console, arg);\n\t\tif (ret == 0)\n\t\t\ttty_ldisc_flush(tty);\n\t\tbreak;\n\n\tcase KDGKBMODE:\n\t\tuival = vt_do_kdgkbmode(console);\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\t/* this could be folded into KDSKBMODE, but for compatibility\n\t   reasons it is not so easy to fold KDGKBMETA into KDGKBMODE */\n\tcase KDSKBMETA:\n\t\tret = vt_do_kdskbmeta(console, arg);\n\t\tbreak;\n\n\tcase KDGKBMETA:\n\t\t/* FIXME: should review whether this is worth locking */\n\t\tuival = vt_do_kdgkbmeta(console);\n\tsetint:\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\tcase KDGETKEYCODE:\n\tcase KDSETKEYCODE:\n\t\tif(!capable(CAP_SYS_TTY_CONFIG))\n\t\t\tperm = 0;\n\t\tret = vt_do_kbkeycode_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\tcase KDGKBENT:\n\tcase KDSKBENT:\n\t\tret = vt_do_kdsk_ioctl(cmd, up, perm, console);\n\t\tbreak;\n\n\tcase KDGKBSENT:\n\tcase KDSKBSENT:\n\t\tret = vt_do_kdgkb_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\t/* Diacritical processing. Handled in keyboard.c as it has\n\t   to operate on the keyboard locks and structures */\n\tcase KDGKBDIACR:\n\tcase KDGKBDIACRUC:\n\tcase KDSKBDIACR:\n\tcase KDSKBDIACRUC:\n\t\tret = vt_do_diacrit(cmd, up, perm);\n\t\tbreak;\n\n\t/* the ioctls below read/set the flags usually shown in the leds */\n\t/* don't use them - they will go away without warning */\n\tcase KDGKBLED:\n\tcase KDSKBLED:\n\tcase KDGETLED:\n\tcase KDSETLED:\n\t\tret = vt_do_kdskled(console, cmd, arg, perm);\n\t\tbreak;\n\n\t/*\n\t * A process can indicate its willingness to accept signals\n\t * generated by pressing an appropriate key combination.\n\t * Thus, one can have a daemon that e.g. spawns a new console\n\t * upon a keypress and then changes to it.\n\t * See also the kbrequest field of inittab(5).\n\t */\n\tcase KDSIGACCEPT:\n\t{\n\t\tif (!perm || !capable(CAP_KILL))\n\t\t\treturn -EPERM;\n\t\tif (!valid_signal(arg) || arg < 1 || arg == SIGKILL)\n\t\t\tret = -EINVAL;\n\t\telse {\n\t\t\tspin_lock_irq(&vt_spawn_con.lock);\n\t\t\tput_pid(vt_spawn_con.pid);\n\t\t\tvt_spawn_con.pid = get_pid(task_pid(current));\n\t\t\tvt_spawn_con.sig = arg;\n\t\t\tspin_unlock_irq(&vt_spawn_con.lock);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_SETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&tmp, up, sizeof(struct vt_mode))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tmp.mode != VT_AUTO && tmp.mode != VT_PROCESS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tconsole_lock();\n\t\tvc->vt_mode = tmp;\n\t\t/* the frsig is ignored, so we set it to 0 */\n\t\tvc->vt_mode.frsig = 0;\n\t\tput_pid(vc->vt_pid);\n\t\tvc->vt_pid = get_pid(task_pid(current));\n\t\t/* no switch is required -- saw@shade.msu.ru */\n\t\tvc->vt_newvt = -1;\n\t\tconsole_unlock();\n\t\tbreak;\n\t}\n\n\tcase VT_GETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\t\tint rc;\n\n\t\tconsole_lock();\n\t\tmemcpy(&tmp, &vc->vt_mode, sizeof(struct vt_mode));\n\t\tconsole_unlock();\n\n\t\trc = copy_to_user(up, &tmp, sizeof(struct vt_mode));\n\t\tif (rc)\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns global vt state. Note that VT 0 is always open, since\n\t * it's an alias for the current VT, and people can't use it here.\n\t * We cannot return state for more than 16 VTs, since v_state is short.\n\t */\n\tcase VT_GETSTATE:\n\t{\n\t\tstruct vt_stat __user *vtstat = up;\n\t\tunsigned short state, mask;\n\n\t\t/* Review: FIXME: Console lock ? */\n\t\tif (put_user(fg_console + 1, &vtstat->v_active))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tstate = 1;\t/* /dev/tty0 is always open */\n\t\t\tfor (i = 0, mask = 2; i < MAX_NR_CONSOLES && mask;\n\t\t\t\t\t\t\t++i, mask <<= 1)\n\t\t\t\tif (VT_IS_IN_USE(i))\n\t\t\t\t\tstate |= mask;\n\t\t\tret = put_user(state, &vtstat->v_state);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns the first available (non-opened) console.\n\t */\n\tcase VT_OPENQRY:\n\t\t/* FIXME: locking ? - but then this is a stupid API */\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; ++i)\n\t\t\tif (! VT_IS_IN_USE(i))\n\t\t\t\tbreak;\n\t\tuival = i < MAX_NR_CONSOLES ? (i+1) : -1;\n\t\tgoto setint;\t\t \n\n\t/*\n\t * ioctl(fd, VT_ACTIVATE, num) will cause us to switch to vt # num,\n\t * with num >= 1 (switches to vt 0, our console, are not allowed, just\n\t * to preserve sanity).\n\t */\n\tcase VT_ACTIVATE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret =  -ENXIO;\n\t\telse {\n\t\t\targ--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(arg);\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tset_console(arg);\n\t\t}\n\t\tbreak;\n\n\tcase VT_SETACTIVATE:\n\t{\n\t\tstruct vt_setactivate vsa;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&vsa, (struct vt_setactivate __user *)arg,\n\t\t\t\t\tsizeof(struct vt_setactivate))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (vsa.console == 0 || vsa.console > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse {\n\t\t\tvsa.console = array_index_nospec(vsa.console,\n\t\t\t\t\t\t\t MAX_NR_CONSOLES + 1);\n\t\t\tvsa.console--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(vsa.console);\n\t\t\tif (ret == 0) {\n\t\t\t\tstruct vc_data *nvc;\n\t\t\t\t/* This is safe providing we don't drop the\n\t\t\t\t   console sem between vc_allocate and\n\t\t\t\t   finishing referencing nvc */\n\t\t\t\tnvc = vc_cons[vsa.console].d;\n\t\t\t\tnvc->vt_mode = vsa.mode;\n\t\t\t\tnvc->vt_mode.frsig = 0;\n\t\t\t\tput_pid(nvc->vt_pid);\n\t\t\t\tnvc->vt_pid = get_pid(task_pid(current));\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\t/* Commence switch and lock */\n\t\t\t/* Review set_console locks */\n\t\t\tset_console(vsa.console);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * wait until the specified VT has been activated\n\t */\n\tcase VT_WAITACTIVE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse\n\t\t\tret = vt_waitactive(arg);\n\t\tbreak;\n\n\t/*\n\t * If a vt is under process control, the kernel will not switch to it\n\t * immediately, but postpone the operation until the process calls this\n\t * ioctl, allowing the switch to complete.\n\t *\n\t * According to the X sources this is the behavior:\n\t *\t0:\tpending switch-from not OK\n\t *\t1:\tpending switch-from OK\n\t *\t2:\tcompleted switch-to OK\n\t */\n\tcase VT_RELDISP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tconsole_lock();\n\t\tif (vc->vt_mode.mode != VT_PROCESS) {\n\t\t\tconsole_unlock();\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * Switching-from response\n\t\t */\n\t\tif (vc->vt_newvt >= 0) {\n\t\t\tif (arg == 0)\n\t\t\t\t/*\n\t\t\t\t * Switch disallowed, so forget we were trying\n\t\t\t\t * to do it.\n\t\t\t\t */\n\t\t\t\tvc->vt_newvt = -1;\n\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * The current vt has been released, so\n\t\t\t\t * complete the switch.\n\t\t\t\t */\n\t\t\t\tint newvt;\n\t\t\t\tnewvt = vc->vt_newvt;\n\t\t\t\tvc->vt_newvt = -1;\n\t\t\t\tret = vc_allocate(newvt);\n\t\t\t\tif (ret) {\n\t\t\t\t\tconsole_unlock();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * When we actually do the console switch,\n\t\t\t\t * make sure we are atomic with respect to\n\t\t\t\t * other console switches..\n\t\t\t\t */\n\t\t\t\tcomplete_change_console(vc_cons[newvt].d);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Switched-to response\n\t\t\t */\n\t\t\t/*\n\t\t\t * If it's just an ACK, ignore it\n\t\t\t */\n\t\t\tif (arg != VT_ACKACQ)\n\t\t\t\tret = -EINVAL;\n\t\t}\n\t\tconsole_unlock();\n\t\tbreak;\n\n\t /*\n\t  * Disallocate memory associated to VT (but leave VT1)\n\t  */\n\t case VT_DISALLOCATE:\n\t\tif (arg > MAX_NR_CONSOLES) {\n\t\t\tret = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (arg == 0)\n\t\t\tvt_disallocate_all();\n\t\telse\n\t\t\tret = vt_disallocate(--arg);\n\t\tbreak;\n\n\tcase VT_RESIZE:\n\t{\n\t\tstruct vt_sizes __user *vtsizes = up;\n\t\tstruct vc_data *vc;\n\n\t\tushort ll,cc;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (get_user(ll, &vtsizes->v_rows) ||\n\t\t    get_user(cc, &vtsizes->v_cols))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tconsole_lock();\n\t\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\t\tvc = vc_cons[i].d;\n\n\t\t\t\tif (vc) {\n\t\t\t\t\tvc->vc_resize_user = 1;\n\t\t\t\t\t/* FIXME: review v tty lock */\n\t\t\t\t\tvc_resize(vc_cons[i].d, cc, ll);\n\t\t\t\t}\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_RESIZEX:\n\t{\n\t\tstruct vt_consize v;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&v, up, sizeof(struct vt_consize)))\n\t\t\treturn -EFAULT;\n\t\t/* FIXME: Should check the copies properly */\n\t\tif (!v.v_vlin)\n\t\t\tv.v_vlin = vc->vc_scan_lines;\n\t\tif (v.v_clin) {\n\t\t\tint rows = v.v_vlin/v.v_clin;\n\t\t\tif (v.v_rows != rows) {\n\t\t\t\tif (v.v_rows) /* Parameters don't add up */\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_rows = rows;\n\t\t\t}\n\t\t}\n\t\tif (v.v_vcol && v.v_ccol) {\n\t\t\tint cols = v.v_vcol/v.v_ccol;\n\t\t\tif (v.v_cols != cols) {\n\t\t\t\tif (v.v_cols)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_cols = cols;\n\t\t\t}\n\t\t}\n\n\t\tif (v.v_clin > 32)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\tif (!vc_cons[i].d)\n\t\t\t\tcontinue;\n\t\t\tconsole_lock();\n\t\t\tif (v.v_vlin)\n\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;\n\t\t\tif (v.v_clin)\n\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;\n\t\t\tvc_cons[i].d->vc_resize_user = 1;\n\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase PIO_FONT: {\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\top.op = KD_FONT_OP_SET;\n\t\top.flags = KD_FONT_FLAG_OLD | KD_FONT_FLAG_DONT_RECALC;\t/* Compatibility */\n\t\top.width = 8;\n\t\top.height = 0;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase GIO_FONT: {\n\t\top.op = KD_FONT_OP_GET;\n\t\top.flags = KD_FONT_FLAG_OLD;\n\t\top.width = 8;\n\t\top.height = 32;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase PIO_CMAP:\n                if (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t                ret = con_set_cmap(up);\n\t\tbreak;\n\n\tcase GIO_CMAP:\n                ret = con_get_cmap(up);\n\t\tbreak;\n\n\tcase PIO_FONTX:\n\tcase GIO_FONTX:\n\t\tret = do_fontx_ioctl(cmd, up, perm, &op);\n\t\tbreak;\n\n\tcase PIO_FONTRESET:\n\t{\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n#ifdef BROKEN_GRAPHICS_PROGRAMS\n\t\t/* With BROKEN_GRAPHICS_PROGRAMS defined, the default\n\t\t   font is not saved. */\n\t\tret = -ENOSYS;\n\t\tbreak;\n#else\n\t\t{\n\t\top.op = KD_FONT_OP_SET_DEFAULT;\n\t\top.data = NULL;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tconsole_lock();\n\t\tcon_set_default_unimap(vc_cons[fg_console].d);\n\t\tconsole_unlock();\n\t\tbreak;\n\t\t}\n#endif\n\t}\n\n\tcase KDFONTOP: {\n\t\tif (copy_from_user(&op, up, sizeof(op))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!perm && op.op != KD_FONT_OP_GET)\n\t\t\treturn -EPERM;\n\t\tret = con_font_op(vc, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &op, sizeof(op)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase PIO_SCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_old(up);\n\t\tbreak;\n\n\tcase GIO_SCRNMAP:\n\t\tret = con_get_trans_old(up);\n\t\tbreak;\n\n\tcase PIO_UNISCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_new(up);\n\t\tbreak;\n\n\tcase GIO_UNISCRNMAP:\n\t\tret = con_get_trans_new(up);\n\t\tbreak;\n\n\tcase PIO_UNIMAPCLR:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tcon_clear_unimap(vc);\n\t\tbreak;\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\tret = do_unimap_ioctl(cmd, up, perm, vc);\n\t\tbreak;\n\n\tcase VT_LOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 1;\n\t\tbreak;\n\tcase VT_UNLOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 0;\n\t\tbreak;\n\tcase VT_GETHIFONTMASK:\n\t\tret = put_user(vc->vc_hi_font_mask,\n\t\t\t\t\t(unsigned short __user *)arg);\n\t\tbreak;\n\tcase VT_WAITEVENT:\n\t\tret = vt_event_wait_ioctl((struct vt_event __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t}\nout:\n\treturn ret;\n}",
        "func": "int vt_ioctl(struct tty_struct *tty,\n\t     unsigned int cmd, unsigned long arg)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tstruct console_font_op op;\t/* used in multiple places here */\n\tunsigned int console;\n\tunsigned char ucval;\n\tunsigned int uival;\n\tvoid __user *up = (void __user *)arg;\n\tint i, perm;\n\tint ret = 0;\n\n\tconsole = vc->vc_num;\n\n\n\tif (!vc_cons_allocated(console)) { \t/* impossible? */\n\t\tret = -ENOIOCTLCMD;\n\t\tgoto out;\n\t}\n\n\n\t/*\n\t * To have permissions to do most of the vt ioctls, we either have\n\t * to be the owner of the tty, or have CAP_SYS_TTY_CONFIG.\n\t */\n\tperm = 0;\n\tif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 1;\n \n\tswitch (cmd) {\n\tcase TIOCLINUX:\n\t\tret = tioclinux(tty, arg);\n\t\tbreak;\n\tcase KIOCSOUND:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\t/*\n\t\t * The use of PIT_TICK_RATE is historic, it used to be\n\t\t * the platform-dependent CLOCK_TICK_RATE between 2.6.12\n\t\t * and 2.6.36, which was a minor but unfortunate ABI\n\t\t * change. kd_mksound is locked by the input layer.\n\t\t */\n\t\tif (arg)\n\t\t\targ = PIT_TICK_RATE / arg;\n\t\tkd_mksound(arg, 0);\n\t\tbreak;\n\n\tcase KDMKTONE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t{\n\t\tunsigned int ticks, count;\n\t\t\n\t\t/*\n\t\t * Generate the tone for the appropriate number of ticks.\n\t\t * If the time is zero, turn off sound ourselves.\n\t\t */\n\t\tticks = msecs_to_jiffies((arg >> 16) & 0xffff);\n\t\tcount = ticks ? (arg & 0xffff) : 0;\n\t\tif (count)\n\t\t\tcount = PIT_TICK_RATE / count;\n\t\tkd_mksound(count, ticks);\n\t\tbreak;\n\t}\n\n\tcase KDGKBTYPE:\n\t\t/*\n\t\t * this is naïve.\n\t\t */\n\t\tucval = KB_101;\n\t\tret = put_user(ucval, (char __user *)arg);\n\t\tbreak;\n\n\t\t/*\n\t\t * These cannot be implemented on any machine that implements\n\t\t * ioperm() in user level (such as Alpha PCs) or not at all.\n\t\t *\n\t\t * XXX: you should never use these, just call ioperm directly..\n\t\t */\n#ifdef CONFIG_X86\n\tcase KDADDIO:\n\tcase KDDELIO:\n\t\t/*\n\t\t * KDADDIO and KDDELIO may be able to add ports beyond what\n\t\t * we reject here, but to be safe...\n\t\t *\n\t\t * These are locked internally via sys_ioperm\n\t\t */\n\t\tif (arg < GPFIRST || arg > GPLAST) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tret = ksys_ioperm(arg, 1, (cmd == KDADDIO)) ? -ENXIO : 0;\n\t\tbreak;\n\n\tcase KDENABIO:\n\tcase KDDISABIO:\n\t\tret = ksys_ioperm(GPFIRST, GPNUM,\n\t\t\t\t  (cmd == KDENABIO)) ? -ENXIO : 0;\n\t\tbreak;\n#endif\n\n\t/* Linux m68k/i386 interface for setting the keyboard delay/repeat rate */\n\t\t\n\tcase KDKBDREP:\n\t{\n\t\tstruct kbd_repeat kbrep;\n\t\t\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&kbrep, up, sizeof(struct kbd_repeat))) {\n\t\t\tret =  -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tret = kbd_rate(&kbrep);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &kbrep, sizeof(struct kbd_repeat)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase KDSETMODE:\n\t\t/*\n\t\t * currently, setting the mode from KD_TEXT to KD_GRAPHICS\n\t\t * doesn't do a whole lot. i'm not sure if it should do any\n\t\t * restoration of modes or what...\n\t\t *\n\t\t * XXX It should at least call into the driver, fbdev's definitely\n\t\t * need to restore their engine state. --BenH\n\t\t */\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tswitch (arg) {\n\t\tcase KD_GRAPHICS:\n\t\t\tbreak;\n\t\tcase KD_TEXT0:\n\t\tcase KD_TEXT1:\n\t\t\targ = KD_TEXT;\n\t\tcase KD_TEXT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\t/* FIXME: this needs the console lock extending */\n\t\tif (vc->vc_mode == (unsigned char) arg)\n\t\t\tbreak;\n\t\tvc->vc_mode = (unsigned char) arg;\n\t\tif (console != fg_console)\n\t\t\tbreak;\n\t\t/*\n\t\t * explicitly blank/unblank the screen if switching modes\n\t\t */\n\t\tconsole_lock();\n\t\tif (arg == KD_TEXT)\n\t\t\tdo_unblank_screen(1);\n\t\telse\n\t\t\tdo_blank_screen(1);\n\t\tconsole_unlock();\n\t\tbreak;\n\n\tcase KDGETMODE:\n\t\tuival = vc->vc_mode;\n\t\tgoto setint;\n\n\tcase KDMAPDISP:\n\tcase KDUNMAPDISP:\n\t\t/*\n\t\t * these work like a combination of mmap and KDENABIO.\n\t\t * this could be easily finished.\n\t\t */\n\t\tret = -EINVAL;\n\t\tbreak;\n\n\tcase KDSKBMODE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tret = vt_do_kdskbmode(console, arg);\n\t\tif (ret == 0)\n\t\t\ttty_ldisc_flush(tty);\n\t\tbreak;\n\n\tcase KDGKBMODE:\n\t\tuival = vt_do_kdgkbmode(console);\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\t/* this could be folded into KDSKBMODE, but for compatibility\n\t   reasons it is not so easy to fold KDGKBMETA into KDGKBMODE */\n\tcase KDSKBMETA:\n\t\tret = vt_do_kdskbmeta(console, arg);\n\t\tbreak;\n\n\tcase KDGKBMETA:\n\t\t/* FIXME: should review whether this is worth locking */\n\t\tuival = vt_do_kdgkbmeta(console);\n\tsetint:\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\tcase KDGETKEYCODE:\n\tcase KDSETKEYCODE:\n\t\tif(!capable(CAP_SYS_TTY_CONFIG))\n\t\t\tperm = 0;\n\t\tret = vt_do_kbkeycode_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\tcase KDGKBENT:\n\tcase KDSKBENT:\n\t\tret = vt_do_kdsk_ioctl(cmd, up, perm, console);\n\t\tbreak;\n\n\tcase KDGKBSENT:\n\tcase KDSKBSENT:\n\t\tret = vt_do_kdgkb_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\t/* Diacritical processing. Handled in keyboard.c as it has\n\t   to operate on the keyboard locks and structures */\n\tcase KDGKBDIACR:\n\tcase KDGKBDIACRUC:\n\tcase KDSKBDIACR:\n\tcase KDSKBDIACRUC:\n\t\tret = vt_do_diacrit(cmd, up, perm);\n\t\tbreak;\n\n\t/* the ioctls below read/set the flags usually shown in the leds */\n\t/* don't use them - they will go away without warning */\n\tcase KDGKBLED:\n\tcase KDSKBLED:\n\tcase KDGETLED:\n\tcase KDSETLED:\n\t\tret = vt_do_kdskled(console, cmd, arg, perm);\n\t\tbreak;\n\n\t/*\n\t * A process can indicate its willingness to accept signals\n\t * generated by pressing an appropriate key combination.\n\t * Thus, one can have a daemon that e.g. spawns a new console\n\t * upon a keypress and then changes to it.\n\t * See also the kbrequest field of inittab(5).\n\t */\n\tcase KDSIGACCEPT:\n\t{\n\t\tif (!perm || !capable(CAP_KILL))\n\t\t\treturn -EPERM;\n\t\tif (!valid_signal(arg) || arg < 1 || arg == SIGKILL)\n\t\t\tret = -EINVAL;\n\t\telse {\n\t\t\tspin_lock_irq(&vt_spawn_con.lock);\n\t\t\tput_pid(vt_spawn_con.pid);\n\t\t\tvt_spawn_con.pid = get_pid(task_pid(current));\n\t\t\tvt_spawn_con.sig = arg;\n\t\t\tspin_unlock_irq(&vt_spawn_con.lock);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_SETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&tmp, up, sizeof(struct vt_mode))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tmp.mode != VT_AUTO && tmp.mode != VT_PROCESS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tconsole_lock();\n\t\tvc->vt_mode = tmp;\n\t\t/* the frsig is ignored, so we set it to 0 */\n\t\tvc->vt_mode.frsig = 0;\n\t\tput_pid(vc->vt_pid);\n\t\tvc->vt_pid = get_pid(task_pid(current));\n\t\t/* no switch is required -- saw@shade.msu.ru */\n\t\tvc->vt_newvt = -1;\n\t\tconsole_unlock();\n\t\tbreak;\n\t}\n\n\tcase VT_GETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\t\tint rc;\n\n\t\tconsole_lock();\n\t\tmemcpy(&tmp, &vc->vt_mode, sizeof(struct vt_mode));\n\t\tconsole_unlock();\n\n\t\trc = copy_to_user(up, &tmp, sizeof(struct vt_mode));\n\t\tif (rc)\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns global vt state. Note that VT 0 is always open, since\n\t * it's an alias for the current VT, and people can't use it here.\n\t * We cannot return state for more than 16 VTs, since v_state is short.\n\t */\n\tcase VT_GETSTATE:\n\t{\n\t\tstruct vt_stat __user *vtstat = up;\n\t\tunsigned short state, mask;\n\n\t\t/* Review: FIXME: Console lock ? */\n\t\tif (put_user(fg_console + 1, &vtstat->v_active))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tstate = 1;\t/* /dev/tty0 is always open */\n\t\t\tfor (i = 0, mask = 2; i < MAX_NR_CONSOLES && mask;\n\t\t\t\t\t\t\t++i, mask <<= 1)\n\t\t\t\tif (VT_IS_IN_USE(i))\n\t\t\t\t\tstate |= mask;\n\t\t\tret = put_user(state, &vtstat->v_state);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns the first available (non-opened) console.\n\t */\n\tcase VT_OPENQRY:\n\t\t/* FIXME: locking ? - but then this is a stupid API */\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; ++i)\n\t\t\tif (! VT_IS_IN_USE(i))\n\t\t\t\tbreak;\n\t\tuival = i < MAX_NR_CONSOLES ? (i+1) : -1;\n\t\tgoto setint;\t\t \n\n\t/*\n\t * ioctl(fd, VT_ACTIVATE, num) will cause us to switch to vt # num,\n\t * with num >= 1 (switches to vt 0, our console, are not allowed, just\n\t * to preserve sanity).\n\t */\n\tcase VT_ACTIVATE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret =  -ENXIO;\n\t\telse {\n\t\t\targ--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(arg);\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tset_console(arg);\n\t\t}\n\t\tbreak;\n\n\tcase VT_SETACTIVATE:\n\t{\n\t\tstruct vt_setactivate vsa;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&vsa, (struct vt_setactivate __user *)arg,\n\t\t\t\t\tsizeof(struct vt_setactivate))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (vsa.console == 0 || vsa.console > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse {\n\t\t\tvsa.console = array_index_nospec(vsa.console,\n\t\t\t\t\t\t\t MAX_NR_CONSOLES + 1);\n\t\t\tvsa.console--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(vsa.console);\n\t\t\tif (ret == 0) {\n\t\t\t\tstruct vc_data *nvc;\n\t\t\t\t/* This is safe providing we don't drop the\n\t\t\t\t   console sem between vc_allocate and\n\t\t\t\t   finishing referencing nvc */\n\t\t\t\tnvc = vc_cons[vsa.console].d;\n\t\t\t\tnvc->vt_mode = vsa.mode;\n\t\t\t\tnvc->vt_mode.frsig = 0;\n\t\t\t\tput_pid(nvc->vt_pid);\n\t\t\t\tnvc->vt_pid = get_pid(task_pid(current));\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\t/* Commence switch and lock */\n\t\t\t/* Review set_console locks */\n\t\t\tset_console(vsa.console);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * wait until the specified VT has been activated\n\t */\n\tcase VT_WAITACTIVE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse\n\t\t\tret = vt_waitactive(arg);\n\t\tbreak;\n\n\t/*\n\t * If a vt is under process control, the kernel will not switch to it\n\t * immediately, but postpone the operation until the process calls this\n\t * ioctl, allowing the switch to complete.\n\t *\n\t * According to the X sources this is the behavior:\n\t *\t0:\tpending switch-from not OK\n\t *\t1:\tpending switch-from OK\n\t *\t2:\tcompleted switch-to OK\n\t */\n\tcase VT_RELDISP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tconsole_lock();\n\t\tif (vc->vt_mode.mode != VT_PROCESS) {\n\t\t\tconsole_unlock();\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * Switching-from response\n\t\t */\n\t\tif (vc->vt_newvt >= 0) {\n\t\t\tif (arg == 0)\n\t\t\t\t/*\n\t\t\t\t * Switch disallowed, so forget we were trying\n\t\t\t\t * to do it.\n\t\t\t\t */\n\t\t\t\tvc->vt_newvt = -1;\n\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * The current vt has been released, so\n\t\t\t\t * complete the switch.\n\t\t\t\t */\n\t\t\t\tint newvt;\n\t\t\t\tnewvt = vc->vt_newvt;\n\t\t\t\tvc->vt_newvt = -1;\n\t\t\t\tret = vc_allocate(newvt);\n\t\t\t\tif (ret) {\n\t\t\t\t\tconsole_unlock();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * When we actually do the console switch,\n\t\t\t\t * make sure we are atomic with respect to\n\t\t\t\t * other console switches..\n\t\t\t\t */\n\t\t\t\tcomplete_change_console(vc_cons[newvt].d);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Switched-to response\n\t\t\t */\n\t\t\t/*\n\t\t\t * If it's just an ACK, ignore it\n\t\t\t */\n\t\t\tif (arg != VT_ACKACQ)\n\t\t\t\tret = -EINVAL;\n\t\t}\n\t\tconsole_unlock();\n\t\tbreak;\n\n\t /*\n\t  * Disallocate memory associated to VT (but leave VT1)\n\t  */\n\t case VT_DISALLOCATE:\n\t\tif (arg > MAX_NR_CONSOLES) {\n\t\t\tret = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (arg == 0)\n\t\t\tvt_disallocate_all();\n\t\telse\n\t\t\tret = vt_disallocate(--arg);\n\t\tbreak;\n\n\tcase VT_RESIZE:\n\t{\n\t\tstruct vt_sizes __user *vtsizes = up;\n\t\tstruct vc_data *vc;\n\n\t\tushort ll,cc;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (get_user(ll, &vtsizes->v_rows) ||\n\t\t    get_user(cc, &vtsizes->v_cols))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tconsole_lock();\n\t\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\t\tvc = vc_cons[i].d;\n\n\t\t\t\tif (vc) {\n\t\t\t\t\tvc->vc_resize_user = 1;\n\t\t\t\t\t/* FIXME: review v tty lock */\n\t\t\t\t\tvc_resize(vc_cons[i].d, cc, ll);\n\t\t\t\t}\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_RESIZEX:\n\t{\n\t\tstruct vt_consize v;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&v, up, sizeof(struct vt_consize)))\n\t\t\treturn -EFAULT;\n\t\t/* FIXME: Should check the copies properly */\n\t\tif (!v.v_vlin)\n\t\t\tv.v_vlin = vc->vc_scan_lines;\n\t\tif (v.v_clin) {\n\t\t\tint rows = v.v_vlin/v.v_clin;\n\t\t\tif (v.v_rows != rows) {\n\t\t\t\tif (v.v_rows) /* Parameters don't add up */\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_rows = rows;\n\t\t\t}\n\t\t}\n\t\tif (v.v_vcol && v.v_ccol) {\n\t\t\tint cols = v.v_vcol/v.v_ccol;\n\t\t\tif (v.v_cols != cols) {\n\t\t\t\tif (v.v_cols)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_cols = cols;\n\t\t\t}\n\t\t}\n\n\t\tif (v.v_clin > 32)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\tstruct vc_data *vcp;\n\n\t\t\tif (!vc_cons[i].d)\n\t\t\t\tcontinue;\n\t\t\tconsole_lock();\n\t\t\tvcp = vc_cons[i].d;\n\t\t\tif (vcp) {\n\t\t\t\tif (v.v_vlin)\n\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;\n\t\t\t\tif (v.v_clin)\n\t\t\t\t\tvcp->vc_font.height = v.v_clin;\n\t\t\t\tvcp->vc_resize_user = 1;\n\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase PIO_FONT: {\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\top.op = KD_FONT_OP_SET;\n\t\top.flags = KD_FONT_FLAG_OLD | KD_FONT_FLAG_DONT_RECALC;\t/* Compatibility */\n\t\top.width = 8;\n\t\top.height = 0;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase GIO_FONT: {\n\t\top.op = KD_FONT_OP_GET;\n\t\top.flags = KD_FONT_FLAG_OLD;\n\t\top.width = 8;\n\t\top.height = 32;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase PIO_CMAP:\n                if (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t                ret = con_set_cmap(up);\n\t\tbreak;\n\n\tcase GIO_CMAP:\n                ret = con_get_cmap(up);\n\t\tbreak;\n\n\tcase PIO_FONTX:\n\tcase GIO_FONTX:\n\t\tret = do_fontx_ioctl(cmd, up, perm, &op);\n\t\tbreak;\n\n\tcase PIO_FONTRESET:\n\t{\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n#ifdef BROKEN_GRAPHICS_PROGRAMS\n\t\t/* With BROKEN_GRAPHICS_PROGRAMS defined, the default\n\t\t   font is not saved. */\n\t\tret = -ENOSYS;\n\t\tbreak;\n#else\n\t\t{\n\t\top.op = KD_FONT_OP_SET_DEFAULT;\n\t\top.data = NULL;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tconsole_lock();\n\t\tcon_set_default_unimap(vc_cons[fg_console].d);\n\t\tconsole_unlock();\n\t\tbreak;\n\t\t}\n#endif\n\t}\n\n\tcase KDFONTOP: {\n\t\tif (copy_from_user(&op, up, sizeof(op))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!perm && op.op != KD_FONT_OP_GET)\n\t\t\treturn -EPERM;\n\t\tret = con_font_op(vc, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &op, sizeof(op)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase PIO_SCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_old(up);\n\t\tbreak;\n\n\tcase GIO_SCRNMAP:\n\t\tret = con_get_trans_old(up);\n\t\tbreak;\n\n\tcase PIO_UNISCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_new(up);\n\t\tbreak;\n\n\tcase GIO_UNISCRNMAP:\n\t\tret = con_get_trans_new(up);\n\t\tbreak;\n\n\tcase PIO_UNIMAPCLR:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tcon_clear_unimap(vc);\n\t\tbreak;\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\tret = do_unimap_ioctl(cmd, up, perm, vc);\n\t\tbreak;\n\n\tcase VT_LOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 1;\n\t\tbreak;\n\tcase VT_UNLOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 0;\n\t\tbreak;\n\tcase VT_GETHIFONTMASK:\n\t\tret = put_user(vc->vc_hi_font_mask,\n\t\t\t\t\t(unsigned short __user *)arg);\n\t\tbreak;\n\tcase VT_WAITEVENT:\n\t\tret = vt_event_wait_ioctl((struct vt_event __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t}\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -544,15 +544,20 @@\n \t\t\treturn -EINVAL;\n \n \t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n+\t\t\tstruct vc_data *vcp;\n+\n \t\t\tif (!vc_cons[i].d)\n \t\t\t\tcontinue;\n \t\t\tconsole_lock();\n-\t\t\tif (v.v_vlin)\n-\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;\n-\t\t\tif (v.v_clin)\n-\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;\n-\t\t\tvc_cons[i].d->vc_resize_user = 1;\n-\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);\n+\t\t\tvcp = vc_cons[i].d;\n+\t\t\tif (vcp) {\n+\t\t\t\tif (v.v_vlin)\n+\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;\n+\t\t\t\tif (v.v_clin)\n+\t\t\t\t\tvcp->vc_font.height = v.v_clin;\n+\t\t\t\tvcp->vc_resize_user = 1;\n+\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);\n+\t\t\t}\n \t\t\tconsole_unlock();\n \t\t}\n \t\tbreak;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tif (v.v_vlin)",
                "\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;",
                "\t\t\tif (v.v_clin)",
                "\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;",
                "\t\t\tvc_cons[i].d->vc_resize_user = 1;",
                "\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);"
            ],
            "added_lines": [
                "\t\t\tstruct vc_data *vcp;",
                "",
                "\t\t\tvcp = vc_cons[i].d;",
                "\t\t\tif (vcp) {",
                "\t\t\t\tif (v.v_vlin)",
                "\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;",
                "\t\t\t\tif (v.v_clin)",
                "\t\t\t\t\tvcp->vc_font.height = v.v_clin;",
                "\t\t\t\tvcp->vc_resize_user = 1;",
                "\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);",
                "\t\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11739",
        "func_name": "xen-project/xen/_write_trylock",
        "description": "An issue was discovered in Xen through 4.13.x, allowing guest OS users to cause a denial of service or possibly gain privileges because of missing memory barriers in read-write unlock paths. The read-write unlock paths don't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceding ones. In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section. As a consequence, it may be possible to have a writer executing a critical section at the same time as readers or another writer. In other words, many of the assumptions (e.g., a variable cannot be modified after a check) in the critical sections are not safe anymore. The read-write locks are used in hypercalls (such as grant-table ones), so a malicious guest could exploit the race. For instance, there is a small window where Xen can leak memory if XENMAPSPACE_grant_table is used concurrently. A malicious guest may be able to leak memory, or cause a hypervisor crash resulting in a Denial of Service (DoS). Information leak and privilege escalation cannot be excluded.",
        "git_url": "https://github.com/xen-project/xen/commit/6890a04072e664c25447a297fe663b45ecfd6398",
        "commit_title": "xen/rwlock: Add missing memory barrier in the unlock path of rwlock",
        "commit_text": " The rwlock unlock paths are using atomic_sub() to release the lock. However the implementation of atomic_sub() rightfully doesn't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceeding access.  In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section.  The rwlock paths already contains barrier indirectly, but they are not very useful without the counterpart in the unlock paths.  The memory barriers are not necessary on x86 because loads/stores are not re-ordered with lock instructions.  So add arch_lock_release_barrier() in the unlock paths that will only add memory barrier on Arm.  Take the opportunity to document each lock paths explaining why a barrier is not necessary.  This is XSA-314. ",
        "func_before": "static inline int _write_trylock(rwlock_t *lock)\n{\n    u32 cnts;\n\n    preempt_disable();\n    cnts = atomic_read(&lock->cnts);\n    if ( unlikely(cnts) ||\n         unlikely(atomic_cmpxchg(&lock->cnts, 0, _write_lock_val()) != 0) )\n    {\n        preempt_enable();\n        return 0;\n    }\n\n    return 1;\n}",
        "func": "static inline int _write_trylock(rwlock_t *lock)\n{\n    u32 cnts;\n\n    preempt_disable();\n    cnts = atomic_read(&lock->cnts);\n    if ( unlikely(cnts) ||\n         unlikely(atomic_cmpxchg(&lock->cnts, 0, _write_lock_val()) != 0) )\n    {\n        preempt_enable();\n        return 0;\n    }\n\n    /*\n     * atomic_cmpxchg() is a full barrier so no need for an\n     * arch_lock_acquire_barrier().\n     */\n    return 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,5 +11,9 @@\n         return 0;\n     }\n \n+    /*\n+     * atomic_cmpxchg() is a full barrier so no need for an\n+     * arch_lock_acquire_barrier().\n+     */\n     return 1;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    /*",
                "     * atomic_cmpxchg() is a full barrier so no need for an",
                "     * arch_lock_acquire_barrier().",
                "     */"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11739",
        "func_name": "xen-project/xen/_read_trylock",
        "description": "An issue was discovered in Xen through 4.13.x, allowing guest OS users to cause a denial of service or possibly gain privileges because of missing memory barriers in read-write unlock paths. The read-write unlock paths don't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceding ones. In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section. As a consequence, it may be possible to have a writer executing a critical section at the same time as readers or another writer. In other words, many of the assumptions (e.g., a variable cannot be modified after a check) in the critical sections are not safe anymore. The read-write locks are used in hypercalls (such as grant-table ones), so a malicious guest could exploit the race. For instance, there is a small window where Xen can leak memory if XENMAPSPACE_grant_table is used concurrently. A malicious guest may be able to leak memory, or cause a hypervisor crash resulting in a Denial of Service (DoS). Information leak and privilege escalation cannot be excluded.",
        "git_url": "https://github.com/xen-project/xen/commit/6890a04072e664c25447a297fe663b45ecfd6398",
        "commit_title": "xen/rwlock: Add missing memory barrier in the unlock path of rwlock",
        "commit_text": " The rwlock unlock paths are using atomic_sub() to release the lock. However the implementation of atomic_sub() rightfully doesn't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceeding access.  In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section.  The rwlock paths already contains barrier indirectly, but they are not very useful without the counterpart in the unlock paths.  The memory barriers are not necessary on x86 because loads/stores are not re-ordered with lock instructions.  So add arch_lock_release_barrier() in the unlock paths that will only add memory barrier on Arm.  Take the opportunity to document each lock paths explaining why a barrier is not necessary.  This is XSA-314. ",
        "func_before": "static inline int _read_trylock(rwlock_t *lock)\n{\n    u32 cnts;\n\n    preempt_disable();\n    cnts = atomic_read(&lock->cnts);\n    if ( likely(_can_read_lock(cnts)) )\n    {\n        cnts = (u32)atomic_add_return(_QR_BIAS, &lock->cnts);\n        if ( likely(_can_read_lock(cnts)) )\n            return 1;\n        atomic_sub(_QR_BIAS, &lock->cnts);\n    }\n    preempt_enable();\n    return 0;\n}",
        "func": "static inline int _read_trylock(rwlock_t *lock)\n{\n    u32 cnts;\n\n    preempt_disable();\n    cnts = atomic_read(&lock->cnts);\n    if ( likely(_can_read_lock(cnts)) )\n    {\n        cnts = (u32)atomic_add_return(_QR_BIAS, &lock->cnts);\n        /*\n         * atomic_add_return() is a full barrier so no need for an\n         * arch_lock_acquire_barrier().\n         */\n        if ( likely(_can_read_lock(cnts)) )\n            return 1;\n        atomic_sub(_QR_BIAS, &lock->cnts);\n    }\n    preempt_enable();\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,10 @@\n     if ( likely(_can_read_lock(cnts)) )\n     {\n         cnts = (u32)atomic_add_return(_QR_BIAS, &lock->cnts);\n+        /*\n+         * atomic_add_return() is a full barrier so no need for an\n+         * arch_lock_acquire_barrier().\n+         */\n         if ( likely(_can_read_lock(cnts)) )\n             return 1;\n         atomic_sub(_QR_BIAS, &lock->cnts);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        /*",
                "         * atomic_add_return() is a full barrier so no need for an",
                "         * arch_lock_acquire_barrier().",
                "         */"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11739",
        "func_name": "xen-project/xen/_write_lock",
        "description": "An issue was discovered in Xen through 4.13.x, allowing guest OS users to cause a denial of service or possibly gain privileges because of missing memory barriers in read-write unlock paths. The read-write unlock paths don't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceding ones. In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section. As a consequence, it may be possible to have a writer executing a critical section at the same time as readers or another writer. In other words, many of the assumptions (e.g., a variable cannot be modified after a check) in the critical sections are not safe anymore. The read-write locks are used in hypercalls (such as grant-table ones), so a malicious guest could exploit the race. For instance, there is a small window where Xen can leak memory if XENMAPSPACE_grant_table is used concurrently. A malicious guest may be able to leak memory, or cause a hypervisor crash resulting in a Denial of Service (DoS). Information leak and privilege escalation cannot be excluded.",
        "git_url": "https://github.com/xen-project/xen/commit/6890a04072e664c25447a297fe663b45ecfd6398",
        "commit_title": "xen/rwlock: Add missing memory barrier in the unlock path of rwlock",
        "commit_text": " The rwlock unlock paths are using atomic_sub() to release the lock. However the implementation of atomic_sub() rightfully doesn't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceeding access.  In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section.  The rwlock paths already contains barrier indirectly, but they are not very useful without the counterpart in the unlock paths.  The memory barriers are not necessary on x86 because loads/stores are not re-ordered with lock instructions.  So add arch_lock_release_barrier() in the unlock paths that will only add memory barrier on Arm.  Take the opportunity to document each lock paths explaining why a barrier is not necessary.  This is XSA-314. ",
        "func_before": "static inline void _write_lock(rwlock_t *lock)\n{\n    /* Optimize for the unfair lock case where the fair flag is 0. */\n    preempt_disable();\n    if ( atomic_cmpxchg(&lock->cnts, 0, _write_lock_val()) == 0 )\n        return;\n\n    queue_write_lock_slowpath(lock);\n}",
        "func": "static inline void _write_lock(rwlock_t *lock)\n{\n    preempt_disable();\n    /*\n     * Optimize for the unfair lock case where the fair flag is 0.\n     *\n     * atomic_cmpxchg() is a full barrier so no need for an\n     * arch_lock_acquire_barrier().\n     */\n    if ( atomic_cmpxchg(&lock->cnts, 0, _write_lock_val()) == 0 )\n        return;\n\n    queue_write_lock_slowpath(lock);\n    /*\n     * queue_write_lock_slowpath() is using spinlock and therefore is a\n     * full barrier. So no need for an arch_lock_acquire_barrier().\n     */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,18 @@\n static inline void _write_lock(rwlock_t *lock)\n {\n-    /* Optimize for the unfair lock case where the fair flag is 0. */\n     preempt_disable();\n+    /*\n+     * Optimize for the unfair lock case where the fair flag is 0.\n+     *\n+     * atomic_cmpxchg() is a full barrier so no need for an\n+     * arch_lock_acquire_barrier().\n+     */\n     if ( atomic_cmpxchg(&lock->cnts, 0, _write_lock_val()) == 0 )\n         return;\n \n     queue_write_lock_slowpath(lock);\n+    /*\n+     * queue_write_lock_slowpath() is using spinlock and therefore is a\n+     * full barrier. So no need for an arch_lock_acquire_barrier().\n+     */\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    /* Optimize for the unfair lock case where the fair flag is 0. */"
            ],
            "added_lines": [
                "    /*",
                "     * Optimize for the unfair lock case where the fair flag is 0.",
                "     *",
                "     * atomic_cmpxchg() is a full barrier so no need for an",
                "     * arch_lock_acquire_barrier().",
                "     */",
                "    /*",
                "     * queue_write_lock_slowpath() is using spinlock and therefore is a",
                "     * full barrier. So no need for an arch_lock_acquire_barrier().",
                "     */"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11739",
        "func_name": "xen-project/xen/_write_unlock",
        "description": "An issue was discovered in Xen through 4.13.x, allowing guest OS users to cause a denial of service or possibly gain privileges because of missing memory barriers in read-write unlock paths. The read-write unlock paths don't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceding ones. In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section. As a consequence, it may be possible to have a writer executing a critical section at the same time as readers or another writer. In other words, many of the assumptions (e.g., a variable cannot be modified after a check) in the critical sections are not safe anymore. The read-write locks are used in hypercalls (such as grant-table ones), so a malicious guest could exploit the race. For instance, there is a small window where Xen can leak memory if XENMAPSPACE_grant_table is used concurrently. A malicious guest may be able to leak memory, or cause a hypervisor crash resulting in a Denial of Service (DoS). Information leak and privilege escalation cannot be excluded.",
        "git_url": "https://github.com/xen-project/xen/commit/6890a04072e664c25447a297fe663b45ecfd6398",
        "commit_title": "xen/rwlock: Add missing memory barrier in the unlock path of rwlock",
        "commit_text": " The rwlock unlock paths are using atomic_sub() to release the lock. However the implementation of atomic_sub() rightfully doesn't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceeding access.  In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section.  The rwlock paths already contains barrier indirectly, but they are not very useful without the counterpart in the unlock paths.  The memory barriers are not necessary on x86 because loads/stores are not re-ordered with lock instructions.  So add arch_lock_release_barrier() in the unlock paths that will only add memory barrier on Arm.  Take the opportunity to document each lock paths explaining why a barrier is not necessary.  This is XSA-314. ",
        "func_before": "static inline void _write_unlock(rwlock_t *lock)\n{\n    ASSERT(_is_write_locked_by_me(atomic_read(&lock->cnts)));\n    atomic_and(~(_QW_CPUMASK | _QW_WMASK), &lock->cnts);\n    preempt_enable();\n}",
        "func": "static inline void _write_unlock(rwlock_t *lock)\n{\n    ASSERT(_is_write_locked_by_me(atomic_read(&lock->cnts)));\n    arch_lock_release_barrier();\n    atomic_and(~(_QW_CPUMASK | _QW_WMASK), &lock->cnts);\n    preempt_enable();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n static inline void _write_unlock(rwlock_t *lock)\n {\n     ASSERT(_is_write_locked_by_me(atomic_read(&lock->cnts)));\n+    arch_lock_release_barrier();\n     atomic_and(~(_QW_CPUMASK | _QW_WMASK), &lock->cnts);\n     preempt_enable();\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    arch_lock_release_barrier();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11739",
        "func_name": "xen-project/xen/_read_lock",
        "description": "An issue was discovered in Xen through 4.13.x, allowing guest OS users to cause a denial of service or possibly gain privileges because of missing memory barriers in read-write unlock paths. The read-write unlock paths don't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceding ones. In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section. As a consequence, it may be possible to have a writer executing a critical section at the same time as readers or another writer. In other words, many of the assumptions (e.g., a variable cannot be modified after a check) in the critical sections are not safe anymore. The read-write locks are used in hypercalls (such as grant-table ones), so a malicious guest could exploit the race. For instance, there is a small window where Xen can leak memory if XENMAPSPACE_grant_table is used concurrently. A malicious guest may be able to leak memory, or cause a hypervisor crash resulting in a Denial of Service (DoS). Information leak and privilege escalation cannot be excluded.",
        "git_url": "https://github.com/xen-project/xen/commit/6890a04072e664c25447a297fe663b45ecfd6398",
        "commit_title": "xen/rwlock: Add missing memory barrier in the unlock path of rwlock",
        "commit_text": " The rwlock unlock paths are using atomic_sub() to release the lock. However the implementation of atomic_sub() rightfully doesn't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceeding access.  In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section.  The rwlock paths already contains barrier indirectly, but they are not very useful without the counterpart in the unlock paths.  The memory barriers are not necessary on x86 because loads/stores are not re-ordered with lock instructions.  So add arch_lock_release_barrier() in the unlock paths that will only add memory barrier on Arm.  Take the opportunity to document each lock paths explaining why a barrier is not necessary.  This is XSA-314. ",
        "func_before": "static inline void _read_lock(rwlock_t *lock)\n{\n    u32 cnts;\n\n    preempt_disable();\n    cnts = atomic_add_return(_QR_BIAS, &lock->cnts);\n    if ( likely(_can_read_lock(cnts)) )\n        return;\n\n    /* The slowpath will decrement the reader count, if necessary. */\n    queue_read_lock_slowpath(lock);\n}",
        "func": "static inline void _read_lock(rwlock_t *lock)\n{\n    u32 cnts;\n\n    preempt_disable();\n    cnts = atomic_add_return(_QR_BIAS, &lock->cnts);\n    /*\n     * atomic_add_return() is a full barrier so no need for an\n     * arch_lock_acquire_barrier().\n     */\n    if ( likely(_can_read_lock(cnts)) )\n        return;\n\n    /* The slowpath will decrement the reader count, if necessary. */\n    queue_read_lock_slowpath(lock);\n    /*\n     * queue_read_lock_slowpath() is using spinlock and therefore is a\n     * full barrier. So no need for an arch_lock_acquire_barrier().\n     */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,9 +4,17 @@\n \n     preempt_disable();\n     cnts = atomic_add_return(_QR_BIAS, &lock->cnts);\n+    /*\n+     * atomic_add_return() is a full barrier so no need for an\n+     * arch_lock_acquire_barrier().\n+     */\n     if ( likely(_can_read_lock(cnts)) )\n         return;\n \n     /* The slowpath will decrement the reader count, if necessary. */\n     queue_read_lock_slowpath(lock);\n+    /*\n+     * queue_read_lock_slowpath() is using spinlock and therefore is a\n+     * full barrier. So no need for an arch_lock_acquire_barrier().\n+     */\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    /*",
                "     * atomic_add_return() is a full barrier so no need for an",
                "     * arch_lock_acquire_barrier().",
                "     */",
                "    /*",
                "     * queue_read_lock_slowpath() is using spinlock and therefore is a",
                "     * full barrier. So no need for an arch_lock_acquire_barrier().",
                "     */"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11739",
        "func_name": "xen-project/xen/_read_unlock",
        "description": "An issue was discovered in Xen through 4.13.x, allowing guest OS users to cause a denial of service or possibly gain privileges because of missing memory barriers in read-write unlock paths. The read-write unlock paths don't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceding ones. In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section. As a consequence, it may be possible to have a writer executing a critical section at the same time as readers or another writer. In other words, many of the assumptions (e.g., a variable cannot be modified after a check) in the critical sections are not safe anymore. The read-write locks are used in hypercalls (such as grant-table ones), so a malicious guest could exploit the race. For instance, there is a small window where Xen can leak memory if XENMAPSPACE_grant_table is used concurrently. A malicious guest may be able to leak memory, or cause a hypervisor crash resulting in a Denial of Service (DoS). Information leak and privilege escalation cannot be excluded.",
        "git_url": "https://github.com/xen-project/xen/commit/6890a04072e664c25447a297fe663b45ecfd6398",
        "commit_title": "xen/rwlock: Add missing memory barrier in the unlock path of rwlock",
        "commit_text": " The rwlock unlock paths are using atomic_sub() to release the lock. However the implementation of atomic_sub() rightfully doesn't contain a memory barrier. On Arm, this means a processor is allowed to re-order the memory access with the preceeding access.  In other words, the unlock may be seen by another processor before all the memory accesses within the \"critical\" section.  The rwlock paths already contains barrier indirectly, but they are not very useful without the counterpart in the unlock paths.  The memory barriers are not necessary on x86 because loads/stores are not re-ordered with lock instructions.  So add arch_lock_release_barrier() in the unlock paths that will only add memory barrier on Arm.  Take the opportunity to document each lock paths explaining why a barrier is not necessary.  This is XSA-314. ",
        "func_before": "static inline void _read_unlock(rwlock_t *lock)\n{\n    /*\n     * Atomically decrement the reader count\n     */\n    atomic_sub(_QR_BIAS, &lock->cnts);\n    preempt_enable();\n}",
        "func": "static inline void _read_unlock(rwlock_t *lock)\n{\n    arch_lock_release_barrier();\n    /*\n     * Atomically decrement the reader count\n     */\n    atomic_sub(_QR_BIAS, &lock->cnts);\n    preempt_enable();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n static inline void _read_unlock(rwlock_t *lock)\n {\n+    arch_lock_release_barrier();\n     /*\n      * Atomically decrement the reader count\n      */",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    arch_lock_release_barrier();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11810",
        "func_name": "OpenVPN/openvpn/multi_process_incoming_link",
        "description": "An issue was discovered in OpenVPN 2.4.x before 2.4.9. An attacker can inject a data channel v2 (P_DATA_V2) packet using a victim's peer-id. Normally such packets are dropped, but if this packet arrives before the data channel crypto parameters have been initialized, the victim's connection will be dropped. This requires careful timing due to the small time window (usually within a few seconds) between the victim client connection starting and the server PUSH_REPLY response back to the client. This attack will only work if Negotiable Cipher Parameters (NCP) is in use.",
        "git_url": "https://github.com/OpenVPN/openvpn/commit/37bc691e7d26ea4eb61a8a434ebd7a9ae76225ab",
        "commit_title": "Fix illegal client float (CVE-2020-11810)",
        "commit_text": " There is a time frame between allocating peer-id and initializing data channel key (which is performed on receiving push request or on async push-reply) in which the existing peer-id float checks do not work right.  If a \"rogue\" data channel packet arrives during that time frame from another address and  with same peer-id, this would cause client to float to that new address. This is because:   - tls_pre_decrypt() sets packet length to zero if    data channel key has not been initialized, which leads to   - openvpn_decrypt() returns true if packet length is zero,    which leads to   - process_incoming_link_part1() returns true, which    calls multi_process_float(), which commits float  Note that problem doesn't happen when data channel key is initialized, since in this case openvpn_decrypt() returns false.  The net effect of this behaviour is that the VPN session for the \"victim client\" is broken.  Since the \"attacker client\" does not have suitable keys, it can not inject or steal VPN traffic from the other session.  The time window is small and it can not be used to attack a specific client's session, unless some other way is found to make it disconnect and reconnect first.  CVE-2020-11810 has been assigned to acknowledge this risk.  Fix illegal float by adding buffer length check (\"is this packet still considered valid\") before calling multi_process_float().  Trac: #1272 CVE: 2020-11810  Message-Id: <20200415073017.22839-1-lstipakov@gmail.com> URL: https://www.mail-archive.com/openvpn-devel@lists.sourceforge.net/msg19720.html",
        "func_before": "bool\nmulti_process_incoming_link(struct multi_context *m, struct multi_instance *instance, const unsigned int mpp_flags)\n{\n    struct gc_arena gc = gc_new();\n\n    struct context *c;\n    struct mroute_addr src, dest;\n    unsigned int mroute_flags;\n    struct multi_instance *mi;\n    bool ret = true;\n    bool floated = false;\n\n    if (m->pending)\n    {\n        return true;\n    }\n\n    if (!instance)\n    {\n#ifdef MULTI_DEBUG_EVENT_LOOP\n        printf(\"TCP/UDP -> TUN [%d]\\n\", BLEN(&m->top.c2.buf));\n#endif\n        multi_set_pending(m, multi_get_create_instance_udp(m, &floated));\n    }\n    else\n    {\n        multi_set_pending(m, instance);\n    }\n\n    if (m->pending)\n    {\n        set_prefix(m->pending);\n\n        /* get instance context */\n        c = &m->pending->context;\n\n        if (!instance)\n        {\n            /* transfer packet pointer from top-level context buffer to instance */\n            c->c2.buf = m->top.c2.buf;\n\n            /* transfer from-addr from top-level context buffer to instance */\n            if (!floated)\n            {\n                c->c2.from = m->top.c2.from;\n            }\n        }\n\n        if (BLEN(&c->c2.buf) > 0)\n        {\n            struct link_socket_info *lsi;\n            const uint8_t *orig_buf;\n\n            /* decrypt in instance context */\n\n            perf_push(PERF_PROC_IN_LINK);\n            lsi = get_link_socket_info(c);\n            orig_buf = c->c2.buf.data;\n            if (process_incoming_link_part1(c, lsi, floated))\n            {\n                if (floated)\n                {\n                    multi_process_float(m, m->pending);\n                }\n\n                process_incoming_link_part2(c, lsi, orig_buf);\n            }\n            perf_pop();\n\n            if (TUNNEL_TYPE(m->top.c1.tuntap) == DEV_TYPE_TUN)\n            {\n                /* extract packet source and dest addresses */\n                mroute_flags = mroute_extract_addr_from_packet(&src,\n                                                               &dest,\n                                                               NULL,\n                                                               NULL,\n                                                               0,\n                                                               &c->c2.to_tun,\n                                                               DEV_TYPE_TUN);\n\n                /* drop packet if extract failed */\n                if (!(mroute_flags & MROUTE_EXTRACT_SUCCEEDED))\n                {\n                    c->c2.to_tun.len = 0;\n                }\n                /* make sure that source address is associated with this client */\n                else if (multi_get_instance_by_virtual_addr(m, &src, true) != m->pending)\n                {\n                    /* IPv6 link-local address (fe80::xxx)? */\n                    if ( (src.type & MR_ADDR_MASK) == MR_ADDR_IPV6\n                         && IN6_IS_ADDR_LINKLOCAL(&src.v6.addr) )\n                    {\n                        /* do nothing, for now.  TODO: add address learning */\n                    }\n                    else\n                    {\n                        msg(D_MULTI_DROPPED, \"MULTI: bad source address from client [%s], packet dropped\",\n                            mroute_addr_print(&src, &gc));\n                    }\n                    c->c2.to_tun.len = 0;\n                }\n                /* client-to-client communication enabled? */\n                else if (m->enable_c2c)\n                {\n                    /* multicast? */\n                    if (mroute_flags & MROUTE_EXTRACT_MCAST)\n                    {\n                        /* for now, treat multicast as broadcast */\n                        multi_bcast(m, &c->c2.to_tun, m->pending, NULL, 0);\n                    }\n                    else /* possible client to client routing */\n                    {\n                        ASSERT(!(mroute_flags & MROUTE_EXTRACT_BCAST));\n                        mi = multi_get_instance_by_virtual_addr(m, &dest, true);\n\n                        /* if dest addr is a known client, route to it */\n                        if (mi)\n                        {\n#ifdef ENABLE_PF\n                            if (!pf_c2c_test(&c->c2.pf, c->c2.tls_multi,\n                                             &mi->context.c2.pf,\n                                             mi->context.c2.tls_multi,\n                                             \"tun_c2c\"))\n                            {\n                                msg(D_PF_DROPPED, \"PF: client -> client[%s] packet dropped by TUN packet filter\",\n                                    mi_prefix(mi));\n                            }\n                            else\n#endif\n                            {\n                                multi_unicast(m, &c->c2.to_tun, mi);\n                                register_activity(c, BLEN(&c->c2.to_tun));\n                            }\n                            c->c2.to_tun.len = 0;\n                        }\n                    }\n                }\n#ifdef ENABLE_PF\n                if (c->c2.to_tun.len && !pf_addr_test(&c->c2.pf, c, &dest,\n                                                      \"tun_dest_addr\"))\n                {\n                    msg(D_PF_DROPPED, \"PF: client -> addr[%s] packet dropped by TUN packet filter\",\n                        mroute_addr_print_ex(&dest, MAPF_SHOW_ARP, &gc));\n                    c->c2.to_tun.len = 0;\n                }\n#endif\n            }\n            else if (TUNNEL_TYPE(m->top.c1.tuntap) == DEV_TYPE_TAP)\n            {\n                uint16_t vid = 0;\n#ifdef ENABLE_PF\n                struct mroute_addr edest;\n                mroute_addr_reset(&edest);\n#endif\n\n                if (m->top.options.vlan_tagging)\n                {\n                    if (vlan_is_tagged(&c->c2.to_tun))\n                    {\n                        /* Drop VLAN-tagged frame. */\n                        msg(D_VLAN_DEBUG, \"dropping incoming VLAN-tagged frame\");\n                        c->c2.to_tun.len = 0;\n                    }\n                    else\n                    {\n                        vid = c->options.vlan_pvid;\n                    }\n                }\n                /* extract packet source and dest addresses */\n                mroute_flags = mroute_extract_addr_from_packet(&src,\n                                                               &dest,\n                                                               NULL,\n#ifdef ENABLE_PF\n                                                               &edest,\n#else\n                                                               NULL,\n#endif\n                                                               vid,\n                                                               &c->c2.to_tun,\n                                                               DEV_TYPE_TAP);\n\n                if (mroute_flags & MROUTE_EXTRACT_SUCCEEDED)\n                {\n                    if (multi_learn_addr(m, m->pending, &src, 0) == m->pending)\n                    {\n                        /* check for broadcast */\n                        if (m->enable_c2c)\n                        {\n                            if (mroute_flags & (MROUTE_EXTRACT_BCAST|MROUTE_EXTRACT_MCAST))\n                            {\n                                multi_bcast(m, &c->c2.to_tun, m->pending, NULL,\n                                            vid);\n                            }\n                            else /* try client-to-client routing */\n                            {\n                                mi = multi_get_instance_by_virtual_addr(m, &dest, false);\n\n                                /* if dest addr is a known client, route to it */\n                                if (mi)\n                                {\n#ifdef ENABLE_PF\n                                    if (!pf_c2c_test(&c->c2.pf, c->c2.tls_multi,\n                                                     &mi->context.c2.pf,\n                                                     mi->context.c2.tls_multi,\n                                                     \"tap_c2c\"))\n                                    {\n                                        msg(D_PF_DROPPED, \"PF: client -> client[%s] packet dropped by TAP packet filter\",\n                                            mi_prefix(mi));\n                                    }\n                                    else\n#endif\n                                    {\n                                        multi_unicast(m, &c->c2.to_tun, mi);\n                                        register_activity(c, BLEN(&c->c2.to_tun));\n                                    }\n                                    c->c2.to_tun.len = 0;\n                                }\n                            }\n                        }\n#ifdef ENABLE_PF\n                        if (c->c2.to_tun.len && !pf_addr_test(&c->c2.pf, c,\n                                                              &edest,\n                                                              \"tap_dest_addr\"))\n                        {\n                            msg(D_PF_DROPPED, \"PF: client -> addr[%s] packet dropped by TAP packet filter\",\n                                mroute_addr_print_ex(&edest, MAPF_SHOW_ARP, &gc));\n                            c->c2.to_tun.len = 0;\n                        }\n#endif\n                    }\n                    else\n                    {\n                        msg(D_MULTI_DROPPED, \"MULTI: bad source address from client [%s], packet dropped\",\n                            mroute_addr_print(&src, &gc));\n                        c->c2.to_tun.len = 0;\n                    }\n                }\n                else\n                {\n                    c->c2.to_tun.len = 0;\n                }\n            }\n        }\n\n        /* postprocess and set wakeup */\n        ret = multi_process_post(m, m->pending, mpp_flags);\n\n        clear_prefix();\n    }\n\n    gc_free(&gc);\n    return ret;\n}",
        "func": "bool\nmulti_process_incoming_link(struct multi_context *m, struct multi_instance *instance, const unsigned int mpp_flags)\n{\n    struct gc_arena gc = gc_new();\n\n    struct context *c;\n    struct mroute_addr src, dest;\n    unsigned int mroute_flags;\n    struct multi_instance *mi;\n    bool ret = true;\n    bool floated = false;\n\n    if (m->pending)\n    {\n        return true;\n    }\n\n    if (!instance)\n    {\n#ifdef MULTI_DEBUG_EVENT_LOOP\n        printf(\"TCP/UDP -> TUN [%d]\\n\", BLEN(&m->top.c2.buf));\n#endif\n        multi_set_pending(m, multi_get_create_instance_udp(m, &floated));\n    }\n    else\n    {\n        multi_set_pending(m, instance);\n    }\n\n    if (m->pending)\n    {\n        set_prefix(m->pending);\n\n        /* get instance context */\n        c = &m->pending->context;\n\n        if (!instance)\n        {\n            /* transfer packet pointer from top-level context buffer to instance */\n            c->c2.buf = m->top.c2.buf;\n\n            /* transfer from-addr from top-level context buffer to instance */\n            if (!floated)\n            {\n                c->c2.from = m->top.c2.from;\n            }\n        }\n\n        if (BLEN(&c->c2.buf) > 0)\n        {\n            struct link_socket_info *lsi;\n            const uint8_t *orig_buf;\n\n            /* decrypt in instance context */\n\n            perf_push(PERF_PROC_IN_LINK);\n            lsi = get_link_socket_info(c);\n            orig_buf = c->c2.buf.data;\n            if (process_incoming_link_part1(c, lsi, floated))\n            {\n                /* nonzero length means that we have a valid, decrypted packed */\n                if (floated && c->c2.buf.len > 0)\n                {\n                    multi_process_float(m, m->pending);\n                }\n\n                process_incoming_link_part2(c, lsi, orig_buf);\n            }\n            perf_pop();\n\n            if (TUNNEL_TYPE(m->top.c1.tuntap) == DEV_TYPE_TUN)\n            {\n                /* extract packet source and dest addresses */\n                mroute_flags = mroute_extract_addr_from_packet(&src,\n                                                               &dest,\n                                                               NULL,\n                                                               NULL,\n                                                               0,\n                                                               &c->c2.to_tun,\n                                                               DEV_TYPE_TUN);\n\n                /* drop packet if extract failed */\n                if (!(mroute_flags & MROUTE_EXTRACT_SUCCEEDED))\n                {\n                    c->c2.to_tun.len = 0;\n                }\n                /* make sure that source address is associated with this client */\n                else if (multi_get_instance_by_virtual_addr(m, &src, true) != m->pending)\n                {\n                    /* IPv6 link-local address (fe80::xxx)? */\n                    if ( (src.type & MR_ADDR_MASK) == MR_ADDR_IPV6\n                         && IN6_IS_ADDR_LINKLOCAL(&src.v6.addr) )\n                    {\n                        /* do nothing, for now.  TODO: add address learning */\n                    }\n                    else\n                    {\n                        msg(D_MULTI_DROPPED, \"MULTI: bad source address from client [%s], packet dropped\",\n                            mroute_addr_print(&src, &gc));\n                    }\n                    c->c2.to_tun.len = 0;\n                }\n                /* client-to-client communication enabled? */\n                else if (m->enable_c2c)\n                {\n                    /* multicast? */\n                    if (mroute_flags & MROUTE_EXTRACT_MCAST)\n                    {\n                        /* for now, treat multicast as broadcast */\n                        multi_bcast(m, &c->c2.to_tun, m->pending, NULL, 0);\n                    }\n                    else /* possible client to client routing */\n                    {\n                        ASSERT(!(mroute_flags & MROUTE_EXTRACT_BCAST));\n                        mi = multi_get_instance_by_virtual_addr(m, &dest, true);\n\n                        /* if dest addr is a known client, route to it */\n                        if (mi)\n                        {\n#ifdef ENABLE_PF\n                            if (!pf_c2c_test(&c->c2.pf, c->c2.tls_multi,\n                                             &mi->context.c2.pf,\n                                             mi->context.c2.tls_multi,\n                                             \"tun_c2c\"))\n                            {\n                                msg(D_PF_DROPPED, \"PF: client -> client[%s] packet dropped by TUN packet filter\",\n                                    mi_prefix(mi));\n                            }\n                            else\n#endif\n                            {\n                                multi_unicast(m, &c->c2.to_tun, mi);\n                                register_activity(c, BLEN(&c->c2.to_tun));\n                            }\n                            c->c2.to_tun.len = 0;\n                        }\n                    }\n                }\n#ifdef ENABLE_PF\n                if (c->c2.to_tun.len && !pf_addr_test(&c->c2.pf, c, &dest,\n                                                      \"tun_dest_addr\"))\n                {\n                    msg(D_PF_DROPPED, \"PF: client -> addr[%s] packet dropped by TUN packet filter\",\n                        mroute_addr_print_ex(&dest, MAPF_SHOW_ARP, &gc));\n                    c->c2.to_tun.len = 0;\n                }\n#endif\n            }\n            else if (TUNNEL_TYPE(m->top.c1.tuntap) == DEV_TYPE_TAP)\n            {\n                uint16_t vid = 0;\n#ifdef ENABLE_PF\n                struct mroute_addr edest;\n                mroute_addr_reset(&edest);\n#endif\n\n                if (m->top.options.vlan_tagging)\n                {\n                    if (vlan_is_tagged(&c->c2.to_tun))\n                    {\n                        /* Drop VLAN-tagged frame. */\n                        msg(D_VLAN_DEBUG, \"dropping incoming VLAN-tagged frame\");\n                        c->c2.to_tun.len = 0;\n                    }\n                    else\n                    {\n                        vid = c->options.vlan_pvid;\n                    }\n                }\n                /* extract packet source and dest addresses */\n                mroute_flags = mroute_extract_addr_from_packet(&src,\n                                                               &dest,\n                                                               NULL,\n#ifdef ENABLE_PF\n                                                               &edest,\n#else\n                                                               NULL,\n#endif\n                                                               vid,\n                                                               &c->c2.to_tun,\n                                                               DEV_TYPE_TAP);\n\n                if (mroute_flags & MROUTE_EXTRACT_SUCCEEDED)\n                {\n                    if (multi_learn_addr(m, m->pending, &src, 0) == m->pending)\n                    {\n                        /* check for broadcast */\n                        if (m->enable_c2c)\n                        {\n                            if (mroute_flags & (MROUTE_EXTRACT_BCAST|MROUTE_EXTRACT_MCAST))\n                            {\n                                multi_bcast(m, &c->c2.to_tun, m->pending, NULL,\n                                            vid);\n                            }\n                            else /* try client-to-client routing */\n                            {\n                                mi = multi_get_instance_by_virtual_addr(m, &dest, false);\n\n                                /* if dest addr is a known client, route to it */\n                                if (mi)\n                                {\n#ifdef ENABLE_PF\n                                    if (!pf_c2c_test(&c->c2.pf, c->c2.tls_multi,\n                                                     &mi->context.c2.pf,\n                                                     mi->context.c2.tls_multi,\n                                                     \"tap_c2c\"))\n                                    {\n                                        msg(D_PF_DROPPED, \"PF: client -> client[%s] packet dropped by TAP packet filter\",\n                                            mi_prefix(mi));\n                                    }\n                                    else\n#endif\n                                    {\n                                        multi_unicast(m, &c->c2.to_tun, mi);\n                                        register_activity(c, BLEN(&c->c2.to_tun));\n                                    }\n                                    c->c2.to_tun.len = 0;\n                                }\n                            }\n                        }\n#ifdef ENABLE_PF\n                        if (c->c2.to_tun.len && !pf_addr_test(&c->c2.pf, c,\n                                                              &edest,\n                                                              \"tap_dest_addr\"))\n                        {\n                            msg(D_PF_DROPPED, \"PF: client -> addr[%s] packet dropped by TAP packet filter\",\n                                mroute_addr_print_ex(&edest, MAPF_SHOW_ARP, &gc));\n                            c->c2.to_tun.len = 0;\n                        }\n#endif\n                    }\n                    else\n                    {\n                        msg(D_MULTI_DROPPED, \"MULTI: bad source address from client [%s], packet dropped\",\n                            mroute_addr_print(&src, &gc));\n                        c->c2.to_tun.len = 0;\n                    }\n                }\n                else\n                {\n                    c->c2.to_tun.len = 0;\n                }\n            }\n        }\n\n        /* postprocess and set wakeup */\n        ret = multi_process_post(m, m->pending, mpp_flags);\n\n        clear_prefix();\n    }\n\n    gc_free(&gc);\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -58,7 +58,8 @@\n             orig_buf = c->c2.buf.data;\n             if (process_incoming_link_part1(c, lsi, floated))\n             {\n-                if (floated)\n+                /* nonzero length means that we have a valid, decrypted packed */\n+                if (floated && c->c2.buf.len > 0)\n                 {\n                     multi_process_float(m, m->pending);\n                 }",
        "diff_line_info": {
            "deleted_lines": [
                "                if (floated)"
            ],
            "added_lines": [
                "                /* nonzero length means that we have a valid, decrypted packed */",
                "                if (floated && c->c2.buf.len > 0)"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11884",
        "func_name": "kernel/git/tip/tip/enable_sacf_uaccess",
        "description": "In the Linux kernel 4.19 through 5.6.7 on the s390 platform, code execution may occur because of a race condition, as demonstrated by code in enable_sacf_uaccess in arch/s390/lib/uaccess.c that fails to protect against a concurrent page table upgrade, aka CID-3f777e19d171. A crash could also occur.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/commit/?h=215d1f3928713d6eaec67244bcda72105b898000",
        "commit_title": "commit 316ec154810960052d4586b634156c54d0778f74 upstream.",
        "commit_text": " A page table upgrade in a kernel section that uses secondary address mode will mess up the kernel instructions as follows:  Consider the following scenario: two threads are sharing memory. On CPU1 thread 1 does e.g. strnlen_user().  That gets to         old_fs = enable_sacf_uaccess();         len = strnlen_user_srst(src, size); and                 \"   la    %2,0(%1)\\n\"                 \"   la    %3,0(%0,%1)\\n\"                 \"   slgr  %0,%0\\n\"                 \"   sacf  256\\n\"                 \"0: srst  %3,%2\\n\" in strnlen_user_srst().  At that point we are in secondary space mode, control register 1 points to kernel page table and instruction fetching happens via c1, rather than usual c13.  Interrupts are not disabled, for obvious reasons.  On CPU2 thread 2 does MAP_FIXED mmap(), forcing the upgrade of page table from 3-level to e.g. 4-level one.  We'd allocated new top-level table, set it up and now we hit this:                 notify = 1;                 spin_unlock_bh(&mm->page_table_lock);         }         if (notify)                 on_each_cpu(__crst_table_upgrade, mm, 0); OK, we need to actually change over to use of new page table and we need that to happen in all threads that are currently running.  Which happens to include the thread 1.  IPI is delivered and we have static void __crst_table_upgrade(void *arg) {         struct mm_struct *mm = arg;          if (current->active_mm == mm)                 set_user_asce(mm);         __tlb_flush_local(); } run on CPU1.  That does static inline void set_user_asce(struct mm_struct *mm) {         S390_lowcore.user_asce = mm->context.asce; OK, user page table address updated...         __ctl_load(S390_lowcore.user_asce, 1, 1); ... and control register 1 set to it.         clear_cpu_flag(CIF_ASCE_PRIMARY); }  IPI is run in home space mode, so it's fine - insns are fetched using c13, which always points to kernel page table.  But as soon as we return from the interrupt, previous PSW is restored, putting CPU1 back into secondary space mode, at which point we no longer get the kernel instructions from the kernel mapping.  The fix is to only fixup the control registers that are currently in use for user processes during the page table update.  We must also disable interrupts in enable_sacf_uaccess to synchronize the cr and thread.mm_segment updates against the on_each-cpu.  Cc: stable@vger.kernel.org # 4.15+ References: CVE-2020-11884  ",
        "func_before": "mm_segment_t enable_sacf_uaccess(void)\n{\n\tmm_segment_t old_fs;\n\tunsigned long asce, cr;\n\n\told_fs = current->thread.mm_segment;\n\tif (old_fs & 1)\n\t\treturn old_fs;\n\tcurrent->thread.mm_segment |= 1;\n\tasce = S390_lowcore.kernel_asce;\n\tif (likely(old_fs == USER_DS)) {\n\t\t__ctl_store(cr, 1, 1);\n\t\tif (cr != S390_lowcore.kernel_asce) {\n\t\t\t__ctl_load(S390_lowcore.kernel_asce, 1, 1);\n\t\t\tset_cpu_flag(CIF_ASCE_PRIMARY);\n\t\t}\n\t\tasce = S390_lowcore.user_asce;\n\t}\n\t__ctl_store(cr, 7, 7);\n\tif (cr != asce) {\n\t\t__ctl_load(asce, 7, 7);\n\t\tset_cpu_flag(CIF_ASCE_SECONDARY);\n\t}\n\treturn old_fs;\n}",
        "func": "mm_segment_t enable_sacf_uaccess(void)\n{\n\tmm_segment_t old_fs;\n\tunsigned long asce, cr;\n\tunsigned long flags;\n\n\told_fs = current->thread.mm_segment;\n\tif (old_fs & 1)\n\t\treturn old_fs;\n\t/* protect against a concurrent page table upgrade */\n\tlocal_irq_save(flags);\n\tcurrent->thread.mm_segment |= 1;\n\tasce = S390_lowcore.kernel_asce;\n\tif (likely(old_fs == USER_DS)) {\n\t\t__ctl_store(cr, 1, 1);\n\t\tif (cr != S390_lowcore.kernel_asce) {\n\t\t\t__ctl_load(S390_lowcore.kernel_asce, 1, 1);\n\t\t\tset_cpu_flag(CIF_ASCE_PRIMARY);\n\t\t}\n\t\tasce = S390_lowcore.user_asce;\n\t}\n\t__ctl_store(cr, 7, 7);\n\tif (cr != asce) {\n\t\t__ctl_load(asce, 7, 7);\n\t\tset_cpu_flag(CIF_ASCE_SECONDARY);\n\t}\n\tlocal_irq_restore(flags);\n\treturn old_fs;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,10 +2,13 @@\n {\n \tmm_segment_t old_fs;\n \tunsigned long asce, cr;\n+\tunsigned long flags;\n \n \told_fs = current->thread.mm_segment;\n \tif (old_fs & 1)\n \t\treturn old_fs;\n+\t/* protect against a concurrent page table upgrade */\n+\tlocal_irq_save(flags);\n \tcurrent->thread.mm_segment |= 1;\n \tasce = S390_lowcore.kernel_asce;\n \tif (likely(old_fs == USER_DS)) {\n@@ -21,5 +24,6 @@\n \t\t__ctl_load(asce, 7, 7);\n \t\tset_cpu_flag(CIF_ASCE_SECONDARY);\n \t}\n+\tlocal_irq_restore(flags);\n \treturn old_fs;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tunsigned long flags;",
                "\t/* protect against a concurrent page table upgrade */",
                "\tlocal_irq_save(flags);",
                "\tlocal_irq_restore(flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11884",
        "func_name": "kernel/git/tip/tip/__crst_table_upgrade",
        "description": "In the Linux kernel 4.19 through 5.6.7 on the s390 platform, code execution may occur because of a race condition, as demonstrated by code in enable_sacf_uaccess in arch/s390/lib/uaccess.c that fails to protect against a concurrent page table upgrade, aka CID-3f777e19d171. A crash could also occur.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/commit/?h=215d1f3928713d6eaec67244bcda72105b898000",
        "commit_title": "commit 316ec154810960052d4586b634156c54d0778f74 upstream.",
        "commit_text": " A page table upgrade in a kernel section that uses secondary address mode will mess up the kernel instructions as follows:  Consider the following scenario: two threads are sharing memory. On CPU1 thread 1 does e.g. strnlen_user().  That gets to         old_fs = enable_sacf_uaccess();         len = strnlen_user_srst(src, size); and                 \"   la    %2,0(%1)\\n\"                 \"   la    %3,0(%0,%1)\\n\"                 \"   slgr  %0,%0\\n\"                 \"   sacf  256\\n\"                 \"0: srst  %3,%2\\n\" in strnlen_user_srst().  At that point we are in secondary space mode, control register 1 points to kernel page table and instruction fetching happens via c1, rather than usual c13.  Interrupts are not disabled, for obvious reasons.  On CPU2 thread 2 does MAP_FIXED mmap(), forcing the upgrade of page table from 3-level to e.g. 4-level one.  We'd allocated new top-level table, set it up and now we hit this:                 notify = 1;                 spin_unlock_bh(&mm->page_table_lock);         }         if (notify)                 on_each_cpu(__crst_table_upgrade, mm, 0); OK, we need to actually change over to use of new page table and we need that to happen in all threads that are currently running.  Which happens to include the thread 1.  IPI is delivered and we have static void __crst_table_upgrade(void *arg) {         struct mm_struct *mm = arg;          if (current->active_mm == mm)                 set_user_asce(mm);         __tlb_flush_local(); } run on CPU1.  That does static inline void set_user_asce(struct mm_struct *mm) {         S390_lowcore.user_asce = mm->context.asce; OK, user page table address updated...         __ctl_load(S390_lowcore.user_asce, 1, 1); ... and control register 1 set to it.         clear_cpu_flag(CIF_ASCE_PRIMARY); }  IPI is run in home space mode, so it's fine - insns are fetched using c13, which always points to kernel page table.  But as soon as we return from the interrupt, previous PSW is restored, putting CPU1 back into secondary space mode, at which point we no longer get the kernel instructions from the kernel mapping.  The fix is to only fixup the control registers that are currently in use for user processes during the page table update.  We must also disable interrupts in enable_sacf_uaccess to synchronize the cr and thread.mm_segment updates against the on_each-cpu.  Cc: stable@vger.kernel.org # 4.15+ References: CVE-2020-11884  ",
        "func_before": "static void __crst_table_upgrade(void *arg)\n{\n\tstruct mm_struct *mm = arg;\n\n\tif (current->active_mm == mm)\n\t\tset_user_asce(mm);\n\t__tlb_flush_local();\n}",
        "func": "static void __crst_table_upgrade(void *arg)\n{\n\tstruct mm_struct *mm = arg;\n\n\t/* we must change all active ASCEs to avoid the creation of new TLBs */\n\tif (current->active_mm == mm) {\n\t\tS390_lowcore.user_asce = mm->context.asce;\n\t\tif (current->thread.mm_segment == USER_DS) {\n\t\t\t__ctl_load(S390_lowcore.user_asce, 1, 1);\n\t\t\t/* Mark user-ASCE present in CR1 */\n\t\t\tclear_cpu_flag(CIF_ASCE_PRIMARY);\n\t\t}\n\t\tif (current->thread.mm_segment == USER_DS_SACF) {\n\t\t\t__ctl_load(S390_lowcore.user_asce, 7, 7);\n\t\t\t/* enable_sacf_uaccess does all or nothing */\n\t\t\tWARN_ON(!test_cpu_flag(CIF_ASCE_SECONDARY));\n\t\t}\n\t}\n\t__tlb_flush_local();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,19 @@\n {\n \tstruct mm_struct *mm = arg;\n \n-\tif (current->active_mm == mm)\n-\t\tset_user_asce(mm);\n+\t/* we must change all active ASCEs to avoid the creation of new TLBs */\n+\tif (current->active_mm == mm) {\n+\t\tS390_lowcore.user_asce = mm->context.asce;\n+\t\tif (current->thread.mm_segment == USER_DS) {\n+\t\t\t__ctl_load(S390_lowcore.user_asce, 1, 1);\n+\t\t\t/* Mark user-ASCE present in CR1 */\n+\t\t\tclear_cpu_flag(CIF_ASCE_PRIMARY);\n+\t\t}\n+\t\tif (current->thread.mm_segment == USER_DS_SACF) {\n+\t\t\t__ctl_load(S390_lowcore.user_asce, 7, 7);\n+\t\t\t/* enable_sacf_uaccess does all or nothing */\n+\t\t\tWARN_ON(!test_cpu_flag(CIF_ASCE_SECONDARY));\n+\t\t}\n+\t}\n \t__tlb_flush_local();\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (current->active_mm == mm)",
                "\t\tset_user_asce(mm);"
            ],
            "added_lines": [
                "\t/* we must change all active ASCEs to avoid the creation of new TLBs */",
                "\tif (current->active_mm == mm) {",
                "\t\tS390_lowcore.user_asce = mm->context.asce;",
                "\t\tif (current->thread.mm_segment == USER_DS) {",
                "\t\t\t__ctl_load(S390_lowcore.user_asce, 1, 1);",
                "\t\t\t/* Mark user-ASCE present in CR1 */",
                "\t\t\tclear_cpu_flag(CIF_ASCE_PRIMARY);",
                "\t\t}",
                "\t\tif (current->thread.mm_segment == USER_DS_SACF) {",
                "\t\t\t__ctl_load(S390_lowcore.user_asce, 7, 7);",
                "\t\t\t/* enable_sacf_uaccess does all or nothing */",
                "\t\t\tWARN_ON(!test_cpu_flag(CIF_ASCE_SECONDARY));",
                "\t\t}",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-14416",
        "func_name": "torvalds/linux/slip_write_wakeup",
        "description": "In the Linux kernel before 5.4.16, a race condition in tty->disc_data handling in the slip and slcan line discipline could lead to a use-after-free, aka CID-0ace17d56824. This affects drivers/net/slip/slip.c and drivers/net/can/slcan.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=0ace17d56824165c7f4c68785d6b58971db954dd",
        "commit_title": "write_wakeup can happen in parallel with close/hangup where tty->disc_data",
        "commit_text": "is set to NULL and the netdevice is freed thus also freeing disc_data. write_wakeup accesses disc_data so we must prevent close from freeing the netdev while write_wakeup has a non-NULL view of tty->disc_data.  We also need to make sure that accesses to disc_data are atomic. Which can all be done with RCU.  This problem was found by Syzkaller on SLCAN, but the same issue is reproducible with the SLIP line discipline using an LTP test based on the Syzkaller reproducer.  A fix which didn't use RCU was posted by Hillf Danton.  Cc: Wolfgang Grandegger <wg@grandegger.com> Cc: Marc Kleine-Budde <mkl@pengutronix.de> Cc: \"David S. Miller\" <davem@davemloft.net> Cc: Tyler Hall <tylerwhall@gmail.com> Cc: linux-can@vger.kernel.org Cc: netdev@vger.kernel.org Cc: linux-kernel@vger.kernel.org Cc: syzkaller@googlegroups.com ",
        "func_before": "static void slip_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slip *sl = tty->disc_data;\n\n\tschedule_work(&sl->tx_work);\n}",
        "func": "static void slip_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slip *sl;\n\n\trcu_read_lock();\n\tsl = rcu_dereference(tty->disc_data);\n\tif (!sl)\n\t\tgoto out;\n\n\tschedule_work(&sl->tx_work);\nout:\n\trcu_read_unlock();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,13 @@\n static void slip_write_wakeup(struct tty_struct *tty)\n {\n-\tstruct slip *sl = tty->disc_data;\n+\tstruct slip *sl;\n+\n+\trcu_read_lock();\n+\tsl = rcu_dereference(tty->disc_data);\n+\tif (!sl)\n+\t\tgoto out;\n \n \tschedule_work(&sl->tx_work);\n+out:\n+\trcu_read_unlock();\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct slip *sl = tty->disc_data;"
            ],
            "added_lines": [
                "\tstruct slip *sl;",
                "",
                "\trcu_read_lock();",
                "\tsl = rcu_dereference(tty->disc_data);",
                "\tif (!sl)",
                "\t\tgoto out;",
                "out:",
                "\trcu_read_unlock();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-14416",
        "func_name": "torvalds/linux/slip_close",
        "description": "In the Linux kernel before 5.4.16, a race condition in tty->disc_data handling in the slip and slcan line discipline could lead to a use-after-free, aka CID-0ace17d56824. This affects drivers/net/slip/slip.c and drivers/net/can/slcan.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=0ace17d56824165c7f4c68785d6b58971db954dd",
        "commit_title": "write_wakeup can happen in parallel with close/hangup where tty->disc_data",
        "commit_text": "is set to NULL and the netdevice is freed thus also freeing disc_data. write_wakeup accesses disc_data so we must prevent close from freeing the netdev while write_wakeup has a non-NULL view of tty->disc_data.  We also need to make sure that accesses to disc_data are atomic. Which can all be done with RCU.  This problem was found by Syzkaller on SLCAN, but the same issue is reproducible with the SLIP line discipline using an LTP test based on the Syzkaller reproducer.  A fix which didn't use RCU was posted by Hillf Danton.  Cc: Wolfgang Grandegger <wg@grandegger.com> Cc: Marc Kleine-Budde <mkl@pengutronix.de> Cc: \"David S. Miller\" <davem@davemloft.net> Cc: Tyler Hall <tylerwhall@gmail.com> Cc: linux-can@vger.kernel.org Cc: netdev@vger.kernel.org Cc: linux-kernel@vger.kernel.org Cc: syzkaller@googlegroups.com ",
        "func_before": "static void slip_close(struct tty_struct *tty)\n{\n\tstruct slip *sl = tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLIP_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\ttty->disc_data = NULL;\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tflush_work(&sl->tx_work);\n\n\t/* VSV = very important to remove timers */\n#ifdef CONFIG_SLIP_SMART\n\tdel_timer_sync(&sl->keepalive_timer);\n\tdel_timer_sync(&sl->outfill_timer);\n#endif\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "func": "static void slip_close(struct tty_struct *tty)\n{\n\tstruct slip *sl = tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLIP_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\trcu_assign_pointer(tty->disc_data, NULL);\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tsynchronize_rcu();\n\tflush_work(&sl->tx_work);\n\n\t/* VSV = very important to remove timers */\n#ifdef CONFIG_SLIP_SMART\n\tdel_timer_sync(&sl->keepalive_timer);\n\tdel_timer_sync(&sl->outfill_timer);\n#endif\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,10 +7,11 @@\n \t\treturn;\n \n \tspin_lock_bh(&sl->lock);\n-\ttty->disc_data = NULL;\n+\trcu_assign_pointer(tty->disc_data, NULL);\n \tsl->tty = NULL;\n \tspin_unlock_bh(&sl->lock);\n \n+\tsynchronize_rcu();\n \tflush_work(&sl->tx_work);\n \n \t/* VSV = very important to remove timers */",
        "diff_line_info": {
            "deleted_lines": [
                "\ttty->disc_data = NULL;"
            ],
            "added_lines": [
                "\trcu_assign_pointer(tty->disc_data, NULL);",
                "\tsynchronize_rcu();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-14416",
        "func_name": "torvalds/linux/slcan_write_wakeup",
        "description": "In the Linux kernel before 5.4.16, a race condition in tty->disc_data handling in the slip and slcan line discipline could lead to a use-after-free, aka CID-0ace17d56824. This affects drivers/net/slip/slip.c and drivers/net/can/slcan.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=0ace17d56824165c7f4c68785d6b58971db954dd",
        "commit_title": "write_wakeup can happen in parallel with close/hangup where tty->disc_data",
        "commit_text": "is set to NULL and the netdevice is freed thus also freeing disc_data. write_wakeup accesses disc_data so we must prevent close from freeing the netdev while write_wakeup has a non-NULL view of tty->disc_data.  We also need to make sure that accesses to disc_data are atomic. Which can all be done with RCU.  This problem was found by Syzkaller on SLCAN, but the same issue is reproducible with the SLIP line discipline using an LTP test based on the Syzkaller reproducer.  A fix which didn't use RCU was posted by Hillf Danton.  Cc: Wolfgang Grandegger <wg@grandegger.com> Cc: Marc Kleine-Budde <mkl@pengutronix.de> Cc: \"David S. Miller\" <davem@davemloft.net> Cc: Tyler Hall <tylerwhall@gmail.com> Cc: linux-can@vger.kernel.org Cc: netdev@vger.kernel.org Cc: linux-kernel@vger.kernel.org Cc: syzkaller@googlegroups.com ",
        "func_before": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl = tty->disc_data;\n\n\tschedule_work(&sl->tx_work);\n}",
        "func": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl;\n\n\trcu_read_lock();\n\tsl = rcu_dereference(tty->disc_data);\n\tif (!sl)\n\t\tgoto out;\n\n\tschedule_work(&sl->tx_work);\nout:\n\trcu_read_unlock();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,13 @@\n static void slcan_write_wakeup(struct tty_struct *tty)\n {\n-\tstruct slcan *sl = tty->disc_data;\n+\tstruct slcan *sl;\n+\n+\trcu_read_lock();\n+\tsl = rcu_dereference(tty->disc_data);\n+\tif (!sl)\n+\t\tgoto out;\n \n \tschedule_work(&sl->tx_work);\n+out:\n+\trcu_read_unlock();\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct slcan *sl = tty->disc_data;"
            ],
            "added_lines": [
                "\tstruct slcan *sl;",
                "",
                "\trcu_read_lock();",
                "\tsl = rcu_dereference(tty->disc_data);",
                "\tif (!sl)",
                "\t\tgoto out;",
                "out:",
                "\trcu_read_unlock();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-14416",
        "func_name": "torvalds/linux/slcan_close",
        "description": "In the Linux kernel before 5.4.16, a race condition in tty->disc_data handling in the slip and slcan line discipline could lead to a use-after-free, aka CID-0ace17d56824. This affects drivers/net/slip/slip.c and drivers/net/can/slcan.c.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=0ace17d56824165c7f4c68785d6b58971db954dd",
        "commit_title": "write_wakeup can happen in parallel with close/hangup where tty->disc_data",
        "commit_text": "is set to NULL and the netdevice is freed thus also freeing disc_data. write_wakeup accesses disc_data so we must prevent close from freeing the netdev while write_wakeup has a non-NULL view of tty->disc_data.  We also need to make sure that accesses to disc_data are atomic. Which can all be done with RCU.  This problem was found by Syzkaller on SLCAN, but the same issue is reproducible with the SLIP line discipline using an LTP test based on the Syzkaller reproducer.  A fix which didn't use RCU was posted by Hillf Danton.  Cc: Wolfgang Grandegger <wg@grandegger.com> Cc: Marc Kleine-Budde <mkl@pengutronix.de> Cc: \"David S. Miller\" <davem@davemloft.net> Cc: Tyler Hall <tylerwhall@gmail.com> Cc: linux-can@vger.kernel.org Cc: netdev@vger.kernel.org Cc: linux-kernel@vger.kernel.org Cc: syzkaller@googlegroups.com ",
        "func_before": "static void slcan_close(struct tty_struct *tty)\n{\n\tstruct slcan *sl = (struct slcan *) tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLCAN_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\ttty->disc_data = NULL;\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tflush_work(&sl->tx_work);\n\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "func": "static void slcan_close(struct tty_struct *tty)\n{\n\tstruct slcan *sl = (struct slcan *) tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLCAN_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\trcu_assign_pointer(tty->disc_data, NULL);\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tsynchronize_rcu();\n\tflush_work(&sl->tx_work);\n\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,10 +7,11 @@\n \t\treturn;\n \n \tspin_lock_bh(&sl->lock);\n-\ttty->disc_data = NULL;\n+\trcu_assign_pointer(tty->disc_data, NULL);\n \tsl->tty = NULL;\n \tspin_unlock_bh(&sl->lock);\n \n+\tsynchronize_rcu();\n \tflush_work(&sl->tx_work);\n \n \t/* Flush network side */",
        "diff_line_info": {
            "deleted_lines": [
                "\ttty->disc_data = NULL;"
            ],
            "added_lines": [
                "\trcu_assign_pointer(tty->disc_data, NULL);",
                "\tsynchronize_rcu();"
            ]
        }
    }
]