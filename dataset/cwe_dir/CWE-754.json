[
    {
        "cve_id": "CVE-2018-20840",
        "func_name": "google/google-api-cpp-client/OAuth2Credential::UpdateFromString",
        "description": "An unhandled exception vulnerability exists during Google Sign-In with Google API C++ Client before 2019-04-10. It potentially causes an outage of third-party services that were not designed to recover from exceptions. On the client, ID token handling can cause an unhandled exception because of misinterpretation of an integer as a string, resulting in denial-of-service and then other users can no longer login/sign-in to the affected third-party service. Once this third-party service uses Google Sign-In with google-api-cpp-client, a malicious user can trigger this client/auth/oauth2_authorization.cc vulnerability by requesting the client to receive the ID token from a Google authentication server.",
        "git_url": "https://github.com/google/google-api-cpp-client/commit/c62e7bcb3ae9d8ef1a94c2fbb143019d647f6304",
        "commit_title": "fixed 'exp' handling in ID token",
        "commit_text": "",
        "func_before": "util::Status OAuth2Credential::UpdateFromString(const string& json) {\n  OAuth2AuthorizationFlow::SimpleJsonData data;\n  googleapis::util::Status status = data.Init(json);\n  if (!status.ok()) return status;\n\n  string str_value;\n  int int_value;\n\n  if (data.GetString(\"refresh_token\", &str_value)) {\n    VLOG(1) << \"Updating refresh token\";\n    refresh_token_.set(str_value);\n  }\n  if (data.GetString(\"access_token\", &str_value)) {\n    access_token_.set(str_value);\n    VLOG(1) << \"Updating access token\";\n  }\n  if (data.GetString(\"expires_at\", &str_value)\n      || data.GetString(\"exp\", &str_value)) {\n    int64 timestamp;\n    if (!safe_strto64(str_value.c_str(), &timestamp)) {\n      LOG(ERROR) << \"Invalid timestamp=[\" << str_value << \"]\";\n    } else {\n      expiration_timestamp_secs_.set(timestamp);\n      VLOG(1) << \"Updating access token expiration\";\n    }\n  } else if (data.GetScalar(\"expires_in\", &int_value)) {\n    int64 now = DateTime().ToEpochTime();\n    int64 expiration = now + int_value;\n    expiration_timestamp_secs_.set(expiration);\n    VLOG(1) << \"Updating access token expiration\";\n  }\n  if (data.GetString(\"email\", &str_value)) {\n    string bool_str;\n    // Read this as a string because OAuth2 server returns it as\n    // a true/false string.\n    data.GetString(\"email_verified\", &bool_str);\n\n    email_ = str_value;\n    email_verified_ = bool_str == \"true\";\n  }\n\n  if (data.GetString(\"id_token\", &str_value)) {\n    // Extract additional stuff from the JWT claims.\n    // We dont need to verify the signature since we already know\n    // this is coming from the OAuth2 server and is secure with https.\n    // see https://developers.google.com/accounts/docs/OAuth2Login\n    //     #validatinganidtoken\n    int dot_positions[3];\n    int n_dots = 0;\n    for (size_t i = 0; i < str_value.size(); ++i) {\n      if (str_value[i] == '.') {\n        dot_positions[n_dots] = i;\n        ++n_dots;\n        if (n_dots == 3) break;\n      }\n    }\n    if (n_dots != 2) {\n      return StatusUnknown(\"Invalid id_token attribute - not a JWT\");\n    }\n    string claims;\n    const char *claims_start = str_value.data() + dot_positions[0] + 1;\n    size_t claims_len = dot_positions[1] - dot_positions[0] - 1;\n    if (!googleapis_util::Base64Unescape(claims_start, claims_len, &claims)) {\n      return StatusUnknown(\"id_token claims not base-64 encoded\");\n    }\n    return UpdateFromString(claims);\n  }\n\n  return StatusOk();\n}",
        "func": "util::Status OAuth2Credential::UpdateFromString(const string& json) {\n  OAuth2AuthorizationFlow::SimpleJsonData data;\n  googleapis::util::Status status = data.Init(json);\n  if (!status.ok()) return status;\n\n  string str_value;\n  int int_value;\n\n  if (data.GetString(\"refresh_token\", &str_value)) {\n    VLOG(1) << \"Updating refresh token\";\n    refresh_token_.set(str_value);\n  }\n  if (data.GetString(\"access_token\", &str_value)) {\n    access_token_.set(str_value);\n    VLOG(1) << \"Updating access token\";\n  }\n  if (data.GetString(\"expires_at\", &str_value)) {\n    int64 timestamp;\n    if (!safe_strto64(str_value.c_str(), &timestamp)) {\n      LOG(ERROR) << \"Invalid timestamp=[\" << str_value << \"]\";\n    } else {\n      expiration_timestamp_secs_.set(timestamp);\n      VLOG(1) << \"Updating access token expiration\";\n    }\n  } else if (data.GetScalar(\"exp\", &int_value)) {\n    int64 timestamp = int_value;\n    expiration_timestamp_secs_.set(timestamp);\n    VLOG(1) << \"Updating access token expiration\";\n  } else if (data.GetScalar(\"expires_in\", &int_value)) {\n    int64 now = DateTime().ToEpochTime();\n    int64 expiration = now + int_value;\n    expiration_timestamp_secs_.set(expiration);\n    VLOG(1) << \"Updating access token expiration\";\n  }\n  if (data.GetString(\"email\", &str_value)) {\n    string bool_str;\n    // Read this as a string because OAuth2 server returns it as\n    // a true/false string.\n    data.GetString(\"email_verified\", &bool_str);\n\n    email_ = str_value;\n    email_verified_ = bool_str == \"true\";\n  }\n\n  if (data.GetString(\"id_token\", &str_value)) {\n    // Extract additional stuff from the JWT claims.\n    // We dont need to verify the signature since we already know\n    // this is coming from the OAuth2 server and is secure with https.\n    // see https://developers.google.com/accounts/docs/OAuth2Login\n    //     #validatinganidtoken\n    int dot_positions[3];\n    int n_dots = 0;\n    for (size_t i = 0; i < str_value.size(); ++i) {\n      if (str_value[i] == '.') {\n        dot_positions[n_dots] = i;\n        ++n_dots;\n        if (n_dots == 3) break;\n      }\n    }\n    if (n_dots != 2) {\n      return StatusUnknown(\"Invalid id_token attribute - not a JWT\");\n    }\n    string claims;\n    const char *claims_start = str_value.data() + dot_positions[0] + 1;\n    size_t claims_len = dot_positions[1] - dot_positions[0] - 1;\n    if (!googleapis_util::Base64Unescape(claims_start, claims_len, &claims)) {\n      return StatusUnknown(\"id_token claims not base-64 encoded\");\n    }\n    return UpdateFromString(claims);\n  }\n\n  return StatusOk();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,8 +14,7 @@\n     access_token_.set(str_value);\n     VLOG(1) << \"Updating access token\";\n   }\n-  if (data.GetString(\"expires_at\", &str_value)\n-      || data.GetString(\"exp\", &str_value)) {\n+  if (data.GetString(\"expires_at\", &str_value)) {\n     int64 timestamp;\n     if (!safe_strto64(str_value.c_str(), &timestamp)) {\n       LOG(ERROR) << \"Invalid timestamp=[\" << str_value << \"]\";\n@@ -23,6 +22,10 @@\n       expiration_timestamp_secs_.set(timestamp);\n       VLOG(1) << \"Updating access token expiration\";\n     }\n+  } else if (data.GetScalar(\"exp\", &int_value)) {\n+    int64 timestamp = int_value;\n+    expiration_timestamp_secs_.set(timestamp);\n+    VLOG(1) << \"Updating access token expiration\";\n   } else if (data.GetScalar(\"expires_in\", &int_value)) {\n     int64 now = DateTime().ToEpochTime();\n     int64 expiration = now + int_value;",
        "diff_line_info": {
            "deleted_lines": [
                "  if (data.GetString(\"expires_at\", &str_value)",
                "      || data.GetString(\"exp\", &str_value)) {"
            ],
            "added_lines": [
                "  if (data.GetString(\"expires_at\", &str_value)) {",
                "  } else if (data.GetScalar(\"exp\", &int_value)) {",
                "    int64 timestamp = int_value;",
                "    expiration_timestamp_secs_.set(timestamp);",
                "    VLOG(1) << \"Updating access token expiration\";"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-1010239",
        "func_name": "DaveGamble/cJSON/get_object_item",
        "description": "DaveGamble/cJSON cJSON 1.7.8 is affected by: Improper Check for Unusual or Exceptional Conditions. The impact is: Null dereference, so attack can cause denial of service. The component is: cJSON_GetObjectItemCaseSensitive() function. The attack vector is: crafted json file. The fixed version is: 1.7.9 and later.",
        "git_url": "https://github.com/DaveGamble/cJSON/commit/be749d7efa7c9021da746e685bd6dec79f9dd99b",
        "commit_title": "Fix crash of cJSON_GetObjectItemCaseSensitive when calling it on arrays",
        "commit_text": "",
        "func_before": "static cJSON *get_object_item(const cJSON * const object, const char * const name, const cJSON_bool case_sensitive)\n{\n    cJSON *current_element = NULL;\n\n    if ((object == NULL) || (name == NULL))\n    {\n        return NULL;\n    }\n\n    current_element = object->child;\n    if (case_sensitive)\n    {\n        while ((current_element != NULL) && (strcmp(name, current_element->string) != 0))\n        {\n            current_element = current_element->next;\n        }\n    }\n    else\n    {\n        while ((current_element != NULL) && (case_insensitive_strcmp((const unsigned char*)name, (const unsigned char*)(current_element->string)) != 0))\n        {\n            current_element = current_element->next;\n        }\n    }\n\n    return current_element;\n}",
        "func": "static cJSON *get_object_item(const cJSON * const object, const char * const name, const cJSON_bool case_sensitive)\n{\n    cJSON *current_element = NULL;\n\n    if ((object == NULL) || (name == NULL))\n    {\n        return NULL;\n    }\n\n    current_element = object->child;\n    if (case_sensitive)\n    {\n        while ((current_element != NULL) && (current_element->string != NULL) && (strcmp(name, current_element->string) != 0))\n        {\n            current_element = current_element->next;\n        }\n    }\n    else\n    {\n        while ((current_element != NULL) && (case_insensitive_strcmp((const unsigned char*)name, (const unsigned char*)(current_element->string)) != 0))\n        {\n            current_element = current_element->next;\n        }\n    }\n\n    if ((current_element == NULL) || (current_element->string == NULL)) {\n        return NULL;\n    }\n\n    return current_element;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,7 +10,7 @@\n     current_element = object->child;\n     if (case_sensitive)\n     {\n-        while ((current_element != NULL) && (strcmp(name, current_element->string) != 0))\n+        while ((current_element != NULL) && (current_element->string != NULL) && (strcmp(name, current_element->string) != 0))\n         {\n             current_element = current_element->next;\n         }\n@@ -23,5 +23,9 @@\n         }\n     }\n \n+    if ((current_element == NULL) || (current_element->string == NULL)) {\n+        return NULL;\n+    }\n+\n     return current_element;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "        while ((current_element != NULL) && (strcmp(name, current_element->string) != 0))"
            ],
            "added_lines": [
                "        while ((current_element != NULL) && (current_element->string != NULL) && (strcmp(name, current_element->string) != 0))",
                "    if ((current_element == NULL) || (current_element->string == NULL)) {",
                "        return NULL;",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15900",
        "func_name": "slicer69/doas/parsegid",
        "description": "An issue was discovered in slicer69 doas before 6.2 on certain platforms other than OpenBSD. On platforms without strtonum(3), sscanf was used without checking for error cases. Instead, the uninitialized variable errstr was checked and in some cases returned success even if sscanf failed. The result was that, instead of reporting that the supplied username or group name did not exist, it would execute the command as root.",
        "git_url": "https://github.com/slicer69/doas/commit/2f83222829448e5bc4c9391d607ec265a1e06531",
        "commit_title": "Added optimization to Makefile (can be set/overruled using OPT).",
        "commit_text": "Added flag to display all warnings during compiling. Added status checks when parsing user/group IDs for Linux. Make sure Linux drops original user's groups when running as another user.",
        "func_before": "static int\nparsegid(const char *s, gid_t *gid)\n{\n\tstruct group *gr;\n\tconst char *errstr;\n\n\tif ((gr = getgrnam(s)) != NULL) {\n\t\t*gid = gr->gr_gid;\n\t\treturn 0;\n\t}\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\t*gid = strtonum(s, 0, GID_MAX, &errstr);\n\t#else\n\tsscanf(s, \"%d\", gid);\n\t#endif\n\tif (errstr)\n\t\treturn -1;\n\treturn 0;\n}",
        "func": "static int\nparsegid(const char *s, gid_t *gid)\n{\n\tstruct group *gr;\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\tconst char *errstr = NULL;\n        #else\n        int status;\n        #endif\n\n\tif ((gr = getgrnam(s)) != NULL) {\n\t\t*gid = gr->gr_gid;\n\t\treturn 0;\n\t}\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\t*gid = strtonum(s, 0, GID_MAX, &errstr);\n\tif (errstr)\n\t\treturn -1;\n\t#else\n\tstatus = sscanf(s, \"%d\", gid);\n        if (status != 1)\n            return -1;\n\t#endif\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,11 @@\n parsegid(const char *s, gid_t *gid)\n {\n \tstruct group *gr;\n-\tconst char *errstr;\n+\t#if !defined(__linux__) && !defined(__NetBSD__)\n+\tconst char *errstr = NULL;\n+        #else\n+        int status;\n+        #endif\n \n \tif ((gr = getgrnam(s)) != NULL) {\n \t\t*gid = gr->gr_gid;\n@@ -10,10 +14,12 @@\n \t}\n \t#if !defined(__linux__) && !defined(__NetBSD__)\n \t*gid = strtonum(s, 0, GID_MAX, &errstr);\n-\t#else\n-\tsscanf(s, \"%d\", gid);\n-\t#endif\n \tif (errstr)\n \t\treturn -1;\n+\t#else\n+\tstatus = sscanf(s, \"%d\", gid);\n+        if (status != 1)\n+            return -1;\n+\t#endif\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tconst char *errstr;",
                "\t#else",
                "\tsscanf(s, \"%d\", gid);",
                "\t#endif"
            ],
            "added_lines": [
                "\t#if !defined(__linux__) && !defined(__NetBSD__)",
                "\tconst char *errstr = NULL;",
                "        #else",
                "        int status;",
                "        #endif",
                "\t#else",
                "\tstatus = sscanf(s, \"%d\", gid);",
                "        if (status != 1)",
                "            return -1;",
                "\t#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15900",
        "func_name": "slicer69/doas/parseuid",
        "description": "An issue was discovered in slicer69 doas before 6.2 on certain platforms other than OpenBSD. On platforms without strtonum(3), sscanf was used without checking for error cases. Instead, the uninitialized variable errstr was checked and in some cases returned success even if sscanf failed. The result was that, instead of reporting that the supplied username or group name did not exist, it would execute the command as root.",
        "git_url": "https://github.com/slicer69/doas/commit/2f83222829448e5bc4c9391d607ec265a1e06531",
        "commit_title": "Added optimization to Makefile (can be set/overruled using OPT).",
        "commit_text": "Added flag to display all warnings during compiling. Added status checks when parsing user/group IDs for Linux. Make sure Linux drops original user's groups when running as another user.",
        "func_before": "static int\nparseuid(const char *s, uid_t *uid)\n{\n\tstruct passwd *pw;\n\tconst char *errstr;\n\n\tif ((pw = getpwnam(s)) != NULL) {\n\t\t*uid = pw->pw_uid;\n\t\treturn 0;\n\t}\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\t*uid = strtonum(s, 0, UID_MAX, &errstr);\n\t#else\n\tsscanf(s, \"%d\", uid);\n\t#endif\n\tif (errstr)\n\t\treturn -1;\n\treturn 0;\n}",
        "func": "static int\nparseuid(const char *s, uid_t *uid)\n{\n\tstruct passwd *pw;\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\tconst char *errstr = NULL;\n        #else\n        int status;\n        #endif\n\n\tif ((pw = getpwnam(s)) != NULL) {\n\t\t*uid = pw->pw_uid;\n\t\treturn 0;\n\t}\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\t*uid = strtonum(s, 0, UID_MAX, &errstr);\n\tif (errstr)\n\t\treturn -1;\n\t#else\n\tstatus = sscanf(s, \"%d\", uid);\n        if (status != 1)\n           return -1;\n\t#endif\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,11 @@\n parseuid(const char *s, uid_t *uid)\n {\n \tstruct passwd *pw;\n-\tconst char *errstr;\n+\t#if !defined(__linux__) && !defined(__NetBSD__)\n+\tconst char *errstr = NULL;\n+        #else\n+        int status;\n+        #endif\n \n \tif ((pw = getpwnam(s)) != NULL) {\n \t\t*uid = pw->pw_uid;\n@@ -10,10 +14,12 @@\n \t}\n \t#if !defined(__linux__) && !defined(__NetBSD__)\n \t*uid = strtonum(s, 0, UID_MAX, &errstr);\n-\t#else\n-\tsscanf(s, \"%d\", uid);\n-\t#endif\n \tif (errstr)\n \t\treturn -1;\n+\t#else\n+\tstatus = sscanf(s, \"%d\", uid);\n+        if (status != 1)\n+           return -1;\n+\t#endif\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tconst char *errstr;",
                "\t#else",
                "\tsscanf(s, \"%d\", uid);",
                "\t#endif"
            ],
            "added_lines": [
                "\t#if !defined(__linux__) && !defined(__NetBSD__)",
                "\tconst char *errstr = NULL;",
                "        #else",
                "        int status;",
                "        #endif",
                "\t#else",
                "\tstatus = sscanf(s, \"%d\", uid);",
                "        if (status != 1)",
                "           return -1;",
                "\t#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21737",
        "func_name": "tensorflow/Compute",
        "description": "Tensorflow is an Open Source Machine Learning Framework. The implementation of `*Bincount` operations allows malicious users to cause denial of service by passing in arguments which would trigger a `CHECK`-fail. There are several conditions that the input arguments must satisfy. Some are not caught during shape inference and others are not caught during kernel implementation. This results in `CHECK` failures later when the output tensors get allocated. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/7019ce4f68925fd01cdafde26f8d8c938f47e6f9",
        "commit_title": "Fix check-fail when bincount ops are passed invalid values.",
        "commit_text": " PiperOrigin-RevId: 415063028",
        "func_before": "void Compute(OpKernelContext* ctx) override {\n    const auto splits = ctx->input(0).flat<int64_t>();\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& size_t = ctx->input(2);\n    const auto weights = ctx->input(3).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    int num_rows = splits.size() - 1;\n    int num_values = values.size();\n    int batch_idx = 0;\n\n    OP_REQUIRES(ctx, splits(0) == 0,\n                errors::InvalidArgument(\"Splits must start with 0, not with \",\n                                        splits(0)));\n\n    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n                errors::InvalidArgument(\n                    \"Splits must end with the number of values, got \",\n                    splits(num_rows), \" instead of \", num_values));\n\n    Tensor* out_t;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n    functor::SetZeroFunctor<Device, T> fill;\n    fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n    const auto out = out_t->matrix<T>();\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      while (idx >= splits(batch_idx)) {\n        batch_idx++;\n      }\n      Tidx bin = values(idx);\n      OP_REQUIRES(ctx, bin >= 0,\n                  errors::InvalidArgument(\"Input must be non-negative\"));\n      if (bin < size) {\n        if (binary_output_) {\n          out(batch_idx - 1, bin) = T(1);\n        } else {\n          T value = (weights_size > 0) ? weights(idx) : T(1);\n          out(batch_idx - 1, bin) += value;\n        }\n      }\n    }\n  }",
        "func": "void Compute(OpKernelContext* ctx) override {\n    const auto splits = ctx->input(0).flat<int64_t>();\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& size_t = ctx->input(2);\n    const auto weights = ctx->input(3).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    int num_rows = splits.size() - 1;\n    int num_values = values.size();\n    int batch_idx = 0;\n\n    OP_REQUIRES(ctx, splits(0) == 0,\n                errors::InvalidArgument(\"Splits must start with 0, not with \",\n                                        splits(0)));\n\n    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n                errors::InvalidArgument(\n                    \"Splits must end with the number of values, got \",\n                    splits(num_rows), \" instead of \", num_values));\n\n    Tensor* out_t;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n    functor::SetZeroFunctor<Device, T> fill;\n    fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n    const auto out = out_t->matrix<T>();\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      while (idx >= splits(batch_idx)) {\n        batch_idx++;\n      }\n      Tidx bin = values(idx);\n      OP_REQUIRES(ctx, bin >= 0,\n                  errors::InvalidArgument(\"Input must be non-negative\"));\n      if (bin < size) {\n        if (binary_output_) {\n          out(batch_idx - 1, bin) = T(1);\n        } else {\n          T value = (weights_size > 0) ? weights(idx) : T(1);\n          out(batch_idx - 1, bin) += value;\n        }\n      }\n    }\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,9 @@\n     const auto weights = ctx->input(3).flat<T>();\n     const int64_t weights_size = weights.size();\n \n+    OP_REQUIRES(ctx, size_t.dims() == 0,\n+                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n+                                        size_t.dims()));\n     Tidx size = size_t.scalar<Tidx>()();\n     OP_REQUIRES(\n         ctx, size >= 0,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    OP_REQUIRES(ctx, size_t.dims() == 0,",
                "                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",",
                "                                        size_t.dims()));"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-23590",
        "func_name": "tensorflow/Graph::AddNode",
        "description": "Tensorflow is an Open Source Machine Learning Framework. A `GraphDef` from a TensorFlow `SavedModel` can be maliciously altered to cause a TensorFlow process to crash due to encountering a `StatusOr` value that is an error and forcibly extracting the value from it. We have patched the issue in multiple GitHub commits and these will be included in TensorFlow 2.8.0 and TensorFlow 2.7.1, as both are affected.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/955059813cc325dc1db5e2daa6221271406d4439",
        "commit_title": "Check for type inference error on node construction.",
        "commit_text": " PiperOrigin-RevId: 409415804",
        "func_before": "Node* Graph::AddNode(NodeDef node_def, Status* status) {\n  const OpRegistrationData* op_reg_data;\n  status->Update(ops_.LookUp(node_def.op(), &op_reg_data));\n  if (!status->ok()) return nullptr;\n\n  DataTypeVector inputs;\n  DataTypeVector outputs;\n  status->Update(\n      InOutTypesForNode(node_def, op_reg_data->op_def, &inputs, &outputs));\n  if (!status->ok()) {\n    *status = AttachDef(*status, node_def);\n    return nullptr;\n  }\n\n  Node::NodeClass node_class = op_reg_data->is_function_op\n                                   ? Node::NC_FUNCTION_OP\n                                   : Node::GetNodeClassForOp(node_def.op());\n\n  if (op_reg_data->type_ctor != nullptr) {\n    VLOG(3) << \"AddNode: found type constructor for \" << node_def.name();\n    const auto ctor_type =\n        full_type::SpecializeType(AttrSlice(node_def), op_reg_data->op_def);\n    const FullTypeDef ctor_typedef = ctor_type.ValueOrDie();\n    if (ctor_typedef.type_id() != TFT_UNSET) {\n      *(node_def.mutable_experimental_type()) = ctor_typedef;\n    }\n  } else {\n    VLOG(3) << \"AddNode: no type constructor for \" << node_def.name();\n  }\n\n  Node* node = AllocateNode(std::make_shared<NodeProperties>(\n                                &op_reg_data->op_def, std::move(node_def),\n                                inputs, outputs, op_reg_data->fwd_type_fn),\n                            nullptr, node_class);\n  return node;\n}",
        "func": "Node* Graph::AddNode(NodeDef node_def, Status* status) {\n  const OpRegistrationData* op_reg_data;\n  status->Update(ops_.LookUp(node_def.op(), &op_reg_data));\n  if (!status->ok()) return nullptr;\n\n  DataTypeVector inputs;\n  DataTypeVector outputs;\n  status->Update(\n      InOutTypesForNode(node_def, op_reg_data->op_def, &inputs, &outputs));\n  if (!status->ok()) {\n    *status = AttachDef(*status, node_def);\n    return nullptr;\n  }\n\n  Node::NodeClass node_class = op_reg_data->is_function_op\n                                   ? Node::NC_FUNCTION_OP\n                                   : Node::GetNodeClassForOp(node_def.op());\n\n  if (op_reg_data->type_ctor != nullptr) {\n    VLOG(3) << \"AddNode: found type constructor for \" << node_def.name();\n    const auto ctor_type =\n        full_type::SpecializeType(AttrSlice(node_def), op_reg_data->op_def);\n    if (!ctor_type.ok()) {\n      *status = errors::InvalidArgument(\"type error: \",\n                                        ctor_type.status().ToString());\n      return nullptr;\n    }\n    const FullTypeDef ctor_typedef = ctor_type.ValueOrDie();\n    if (ctor_typedef.type_id() != TFT_UNSET) {\n      *(node_def.mutable_experimental_type()) = ctor_typedef;\n    }\n  } else {\n    VLOG(3) << \"AddNode: no type constructor for \" << node_def.name();\n  }\n\n  Node* node = AllocateNode(std::make_shared<NodeProperties>(\n                                &op_reg_data->op_def, std::move(node_def),\n                                inputs, outputs, op_reg_data->fwd_type_fn),\n                            nullptr, node_class);\n  return node;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,6 +20,11 @@\n     VLOG(3) << \"AddNode: found type constructor for \" << node_def.name();\n     const auto ctor_type =\n         full_type::SpecializeType(AttrSlice(node_def), op_reg_data->op_def);\n+    if (!ctor_type.ok()) {\n+      *status = errors::InvalidArgument(\"type error: \",\n+                                        ctor_type.status().ToString());\n+      return nullptr;\n+    }\n     const FullTypeDef ctor_typedef = ctor_type.ValueOrDie();\n     if (ctor_typedef.type_id() != TFT_UNSET) {\n       *(node_def.mutable_experimental_type()) = ctor_typedef;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (!ctor_type.ok()) {",
                "      *status = errors::InvalidArgument(\"type error: \",",
                "                                        ctor_type.status().ToString());",
                "      return nullptr;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-23593",
        "func_name": "tensorflow/simplifyBroadcast",
        "description": "Tensorflow is an Open Source Machine Learning Framework. The `simplifyBroadcast` function in the MLIR-TFRT infrastructure in TensorFlow is vulnerable to a segfault (hence, denial of service), if called with scalar shapes. If all shapes are scalar, then `maxRank` is 0, so we build an empty `SmallVector`. The fix will be included in TensorFlow 2.8.0. This is the only affected version.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/35f0fabb4c178253a964d7aabdbb15c6a398b69a",
        "commit_title": "Avoid Segfault for scalar shapes.",
        "commit_text": " Calling tensor::FromElementsOp with an empty vector of elements and no type causes a segfault. We need to let the FromElementsOp know which scalar type it should have. Also add back the DynamicBroadcastInDimOp canonicalization patterns, which previously prevented this bug from happening. Add a regression test that demonstrates the bug.  PiperOrigin-RevId: 417561444",
        "func_before": "llvm::Optional<Value> simplifyBroadcast(ShapeComponentAnalysis& analysis,\n                                        ValueRange shapes, Location loc,\n                                        OpBuilder* builder) {\n  // First find the input shape with the largest rank.\n  SmallVector<ArrayRef<ShapeComponentAnalysis::SymbolicExpr>> shapes_found;\n  size_t maxRank = 0;\n  for (const auto &shape : llvm::enumerate(shapes)) {\n    auto found_shape = analysis.GetValueInfo(shape.value());\n    if (!found_shape) return {};\n    shapes_found.push_back(*found_shape);\n    maxRank = std::max(maxRank, found_shape->size());\n  }\n\n  SmallVector<const ShapeComponentAnalysis::SymbolicExpr*> joined_dimensions(\n      maxRank);\n  SmallVector<std::pair<Value, int64_t>> shape_and_rank_for_dim(maxRank);\n  for (const auto &shape : llvm::enumerate(shapes_found)) {\n    for (const auto &dim : llvm::enumerate(llvm::reverse(shape.value()))) {\n      // 1 dimensions don't contribute to the final result.\n      if (dim.value().isConstant(1)) continue;\n      // If it's not a 1 dimension it will be present in the result. Remember\n      // where it came from.\n      auto index = maxRank - dim.index() - 1;\n      if (!joined_dimensions[index]) {\n        joined_dimensions[index] = &dim.value();\n        shape_and_rank_for_dim[index] =\n            std::make_pair(shapes[shape.index()], shape.value().size());\n        continue;\n      }\n      // Bail if the dimensions are neither equal nor 1.\n      if (*joined_dimensions[index] != dim.value()) return {};\n    }\n  }\n  // If the output is the same as one of the inputs just return that.\n  if (llvm::is_splat(shape_and_rank_for_dim) &&\n      shape_and_rank_for_dim[0].first) {\n    return shape_and_rank_for_dim[0].first;\n  }\n  // Otherwise rematerialize the shape from the pieces we have.\n  SmallVector<Value> elements;\n  for (int i = 0; i != maxRank; ++i) {\n    // 1 dimensions are filtered above, recreate the constant.\n    if (!shape_and_rank_for_dim[i].first) {\n      auto one = builder->getIntegerAttr(\n          shapes[0].getType().cast<RankedTensorType>().getElementType(), 1);\n      elements.push_back(builder->create<ConstantOp>(loc, one));\n      continue;\n    }\n    // Extract from one of the shapes, accounting for the reverse indexing\n    // performed by broadcast.\n    Value index = builder->create<ConstantIndexOp>(\n        loc, i - maxRank + shape_and_rank_for_dim[i].second);\n    elements.push_back(builder->create<tensor::ExtractOp>(\n        loc, shape_and_rank_for_dim[i].first, index));\n  }\n  return Value(builder->create<tensor::FromElementsOp>(loc, elements));\n}",
        "func": "llvm::Optional<Value> simplifyBroadcast(ShapeComponentAnalysis& analysis,\n                                        ValueRange shapes, Location loc,\n                                        OpBuilder* builder) {\n  // First find the input shape with the largest rank.\n  SmallVector<ArrayRef<ShapeComponentAnalysis::SymbolicExpr>> shapes_found;\n  size_t maxRank = 0;\n  for (const auto &shape : llvm::enumerate(shapes)) {\n    auto found_shape = analysis.GetValueInfo(shape.value());\n    if (!found_shape) return {};\n    shapes_found.push_back(*found_shape);\n    maxRank = std::max(maxRank, found_shape->size());\n  }\n  if (maxRank == 0) {\n    return Value(builder->create<tensor::FromElementsOp>(\n        loc, shapes[0].getType(), SmallVector<Value>()));\n  }\n\n  SmallVector<const ShapeComponentAnalysis::SymbolicExpr*> joined_dimensions(\n      maxRank);\n  SmallVector<std::pair<Value, int64_t>> shape_and_rank_for_dim(maxRank);\n  for (const auto &shape : llvm::enumerate(shapes_found)) {\n    for (const auto &dim : llvm::enumerate(llvm::reverse(shape.value()))) {\n      // 1 dimensions don't contribute to the final result.\n      if (dim.value().isConstant(1)) continue;\n      // If it's not a 1 dimension it will be present in the result. Remember\n      // where it came from.\n      auto index = maxRank - dim.index() - 1;\n      if (!joined_dimensions[index]) {\n        joined_dimensions[index] = &dim.value();\n        shape_and_rank_for_dim[index] =\n            std::make_pair(shapes[shape.index()], shape.value().size());\n        continue;\n      }\n      // Bail if the dimensions are neither equal nor 1.\n      if (*joined_dimensions[index] != dim.value()) return {};\n    }\n  }\n  // If the output is the same as one of the inputs just return that.\n  if (llvm::is_splat(shape_and_rank_for_dim) &&\n      shape_and_rank_for_dim[0].first) {\n    return shape_and_rank_for_dim[0].first;\n  }\n  // Otherwise rematerialize the shape from the pieces we have.\n  SmallVector<Value> elements;\n  for (int i = 0; i != maxRank; ++i) {\n    // 1 dimensions are filtered above, recreate the constant.\n    if (!shape_and_rank_for_dim[i].first) {\n      auto one = builder->getIntegerAttr(\n          shapes[0].getType().cast<RankedTensorType>().getElementType(), 1);\n      elements.push_back(builder->create<ConstantOp>(loc, one));\n      continue;\n    }\n    // Extract from one of the shapes, accounting for the reverse indexing\n    // performed by broadcast.\n    Value index = builder->create<ConstantIndexOp>(\n        loc, i - maxRank + shape_and_rank_for_dim[i].second);\n    elements.push_back(builder->create<tensor::ExtractOp>(\n        loc, shape_and_rank_for_dim[i].first, index));\n  }\n  return Value(builder->create<tensor::FromElementsOp>(loc, elements));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,10 @@\n     if (!found_shape) return {};\n     shapes_found.push_back(*found_shape);\n     maxRank = std::max(maxRank, found_shape->size());\n+  }\n+  if (maxRank == 0) {\n+    return Value(builder->create<tensor::FromElementsOp>(\n+        loc, shapes[0].getType(), SmallVector<Value>()));\n   }\n \n   SmallVector<const ShapeComponentAnalysis::SymbolicExpr*> joined_dimensions(",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  }",
                "  if (maxRank == 0) {",
                "    return Value(builder->create<tensor::FromElementsOp>(",
                "        loc, shapes[0].getType(), SmallVector<Value>()));"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-35173",
        "func_name": "nginx/njs/njs_generate_try_catch",
        "description": "An issue was discovered in Nginx NJS v0.7.5. The JUMP offset for a break instruction was not set to a correct offset during code generation, leading to a segmentation violation.",
        "git_url": "https://github.com/nginx/njs/commit/404553896792b8f5f429dc8852d15784a59d8d3e",
        "commit_title": "Fixed break instruction in a try-catch block.",
        "commit_text": " Previously, JUMP offset for a break instruction inside a try-catch block was not set to a correct offset during code generation when a return instruction was present in inner try-catch block.  The fix is to update the JUMP offset appropriately.  This closes #553 issue on Github.",
        "func_before": "static njs_int_t\nnjs_generate_try_catch(njs_vm_t *vm, njs_generator_t *generator,\n    njs_parser_node_t *node)\n{\n    njs_int_t                ret;\n    njs_index_t              exit_index;\n    njs_vmcode_finally_t     *finally;\n    njs_generator_patch_t    *patch;\n    njs_generator_block_t    *block, *try_block;\n    njs_generator_try_ctx_t  *ctx;\n\n    ctx = generator->context;\n\n    try_block = ctx->try_block;\n    exit_index = try_block->index;\n\n    njs_code_set_jump_offset(generator, njs_vmcode_try_end_t,\n                             ctx->try_offset);\n\n    if (try_block->continuation != NULL || try_block->exit != NULL) {\n        njs_generate_code_finally(generator, finally, ctx->exception_index,\n                                  exit_index, NULL);\n\n        if (try_block->continuation != NULL) {\n            /*\n             * block != NULL is checked\n             * by njs_generate_continue_statement()\n             */\n            block = njs_generate_find_block(vm, generator->block,\n                                            NJS_GENERATOR_LOOP,\n                                            &ctx->try_cont_label);\n\n            patch = njs_generate_make_continuation_patch(vm, block,\n                                                         &ctx->try_cont_label,\n                        njs_code_offset(generator, finally)\n                         + offsetof(njs_vmcode_finally_t, continue_offset));\n            if (njs_slow_path(patch == NULL)) {\n                return NJS_ERROR;\n            }\n        }\n\n        if (try_block->exit != NULL) {\n            block = njs_generate_find_block(vm, generator->block,\n                                            NJS_GENERATOR_ALL,\n                                            &ctx->try_exit_label);\n\n            if (block != NULL) {\n                patch = njs_generate_make_exit_patch(vm, block,\n                                                     &ctx->try_exit_label,\n                            njs_code_offset(generator, finally)\n                            + offsetof(njs_vmcode_finally_t, break_offset));\n                if (njs_slow_path(patch == NULL)) {\n                    return NJS_ERROR;\n                }\n            }\n        }\n    }\n\n    /* TODO: release exception variable index. */\n\n    ret = njs_generate_index_release(vm, generator, ctx->exception_index);\n    if (njs_slow_path(ret != NJS_OK)) {\n        return ret;\n    }\n\n    return njs_generator_stack_pop(vm, generator, ctx);\n}",
        "func": "static njs_int_t\nnjs_generate_try_catch(njs_vm_t *vm, njs_generator_t *generator,\n    njs_parser_node_t *node)\n{\n    njs_int_t                ret;\n    njs_index_t              exit_index;\n    njs_vmcode_finally_t     *finally;\n    njs_generator_patch_t    *patch;\n    njs_generator_block_t    *block, *try_block;\n    njs_generator_try_ctx_t  *ctx;\n\n    ctx = generator->context;\n\n    try_block = ctx->try_block;\n    exit_index = try_block->index;\n\n    njs_code_set_jump_offset(generator, njs_vmcode_try_end_t,\n                             ctx->try_offset);\n\n    if (try_block->continuation != NULL || try_block->exit != NULL) {\n        njs_generate_code_finally(generator, finally, ctx->exception_index,\n                                  exit_index, NULL);\n\n        if (try_block->continuation != NULL) {\n            /*\n             * block != NULL is checked\n             * by njs_generate_continue_statement()\n             */\n            block = njs_generate_find_block(vm, generator->block,\n                                            NJS_GENERATOR_LOOP,\n                                            &ctx->try_cont_label);\n\n            patch = njs_generate_make_continuation_patch(vm, block,\n                                                         &ctx->try_cont_label,\n                        njs_code_offset(generator, finally)\n                         + offsetof(njs_vmcode_finally_t, continue_offset));\n            if (njs_slow_path(patch == NULL)) {\n                return NJS_ERROR;\n            }\n        }\n\n        if (try_block->exit != NULL) {\n            block = njs_generate_find_block(vm, generator->block,\n                                            NJS_GENERATOR_ALL,\n                                            &ctx->try_exit_label);\n\n            /*\n             * block can be NULL when &ctx->try_exit_label is \"@return\"\n             * for outermost try-catch block.\n             */\n\n            if (block != NULL) {\n                patch = njs_generate_make_exit_patch(vm, block,\n                                                     &ctx->try_exit_label,\n                            njs_code_offset(generator, finally)\n                            + offsetof(njs_vmcode_finally_t, break_offset));\n                if (njs_slow_path(patch == NULL)) {\n                    return NJS_ERROR;\n                }\n\n            } else {\n\n                /*\n                 * when block == NULL, we still want to patch the \"finally\"\n                 * instruction break_offset.\n                 */\n\n                block = njs_generate_find_block(vm, generator->block,\n                                                NJS_GENERATOR_ALL,\n                                                &no_label);\n\n                if (block != NULL) {\n                    patch = njs_generate_make_exit_patch(vm, block, &no_label,\n                                njs_code_offset(generator, finally)\n                                + offsetof(njs_vmcode_finally_t, break_offset));\n                    if (njs_slow_path(patch == NULL)) {\n                        return NJS_ERROR;\n                    }\n                }\n            }\n        }\n    }\n\n    /* TODO: release exception variable index. */\n\n    ret = njs_generate_index_release(vm, generator, ctx->exception_index);\n    if (njs_slow_path(ret != NJS_OK)) {\n        return ret;\n    }\n\n    return njs_generator_stack_pop(vm, generator, ctx);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -44,6 +44,11 @@\n                                             NJS_GENERATOR_ALL,\n                                             &ctx->try_exit_label);\n \n+            /*\n+             * block can be NULL when &ctx->try_exit_label is \"@return\"\n+             * for outermost try-catch block.\n+             */\n+\n             if (block != NULL) {\n                 patch = njs_generate_make_exit_patch(vm, block,\n                                                      &ctx->try_exit_label,\n@@ -51,6 +56,26 @@\n                             + offsetof(njs_vmcode_finally_t, break_offset));\n                 if (njs_slow_path(patch == NULL)) {\n                     return NJS_ERROR;\n+                }\n+\n+            } else {\n+\n+                /*\n+                 * when block == NULL, we still want to patch the \"finally\"\n+                 * instruction break_offset.\n+                 */\n+\n+                block = njs_generate_find_block(vm, generator->block,\n+                                                NJS_GENERATOR_ALL,\n+                                                &no_label);\n+\n+                if (block != NULL) {\n+                    patch = njs_generate_make_exit_patch(vm, block, &no_label,\n+                                njs_code_offset(generator, finally)\n+                                + offsetof(njs_vmcode_finally_t, break_offset));\n+                    if (njs_slow_path(patch == NULL)) {\n+                        return NJS_ERROR;\n+                    }\n                 }\n             }\n         }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "            /*",
                "             * block can be NULL when &ctx->try_exit_label is \"@return\"",
                "             * for outermost try-catch block.",
                "             */",
                "",
                "                }",
                "",
                "            } else {",
                "",
                "                /*",
                "                 * when block == NULL, we still want to patch the \"finally\"",
                "                 * instruction break_offset.",
                "                 */",
                "",
                "                block = njs_generate_find_block(vm, generator->block,",
                "                                                NJS_GENERATOR_ALL,",
                "                                                &no_label);",
                "",
                "                if (block != NULL) {",
                "                    patch = njs_generate_make_exit_patch(vm, block, &no_label,",
                "                                njs_code_offset(generator, finally)",
                "                                + offsetof(njs_vmcode_finally_t, break_offset));",
                "                    if (njs_slow_path(patch == NULL)) {",
                "                        return NJS_ERROR;",
                "                    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-35173",
        "func_name": "nginx/njs/njs_generate_try_end",
        "description": "An issue was discovered in Nginx NJS v0.7.5. The JUMP offset for a break instruction was not set to a correct offset during code generation, leading to a segmentation violation.",
        "git_url": "https://github.com/nginx/njs/commit/404553896792b8f5f429dc8852d15784a59d8d3e",
        "commit_title": "Fixed break instruction in a try-catch block.",
        "commit_text": " Previously, JUMP offset for a break instruction inside a try-catch block was not set to a correct offset during code generation when a return instruction was present in inner try-catch block.  The fix is to update the JUMP offset appropriately.  This closes #553 issue on Github.",
        "func_before": "static njs_int_t\nnjs_generate_try_end(njs_vm_t *vm, njs_generator_t *generator,\n    njs_parser_node_t *node)\n{\n    njs_int_t                ret;\n    njs_index_t              exit_index;\n    const njs_str_t          *dest_label;\n    njs_vmcode_finally_t     *finally;\n    njs_generator_patch_t    *patch;\n    njs_generator_block_t    *block, *try_block, *catch_block;\n    njs_generator_try_ctx_t  *ctx;\n\n    ctx = generator->context;\n\n    try_block = ctx->try_block;\n    exit_index = try_block->index;\n    catch_block = ctx->catch_block;\n\n    njs_generate_code_finally(generator, finally, ctx->exception_index,\n                              exit_index, node);\n\n    if (try_block->continuation != NULL\n        || (catch_block && catch_block->continuation != NULL))\n    {\n        dest_label = njs_generate_jump_destination(vm, generator->block,\n                                                   \"try continue\",\n                                                   NJS_GENERATOR_LOOP,\n                                                   &ctx->try_cont_label,\n                                                   &ctx->catch_cont_label);\n        if (njs_slow_path(dest_label == NULL)) {\n            return NJS_ERROR;\n        }\n\n        /*\n         * block != NULL is checked\n         * by njs_generate_continue_statement()\n         */\n        block = njs_generate_find_block(vm, generator->block,\n                                        NJS_GENERATOR_LOOP, dest_label);\n\n        patch = njs_generate_make_continuation_patch(vm, block, dest_label,\n                         njs_code_offset(generator, finally)\n                         + offsetof(njs_vmcode_finally_t, continue_offset));\n        if (njs_slow_path(patch == NULL)) {\n            return NJS_ERROR;\n        }\n    }\n\n    if (try_block->exit != NULL\n        || (catch_block != NULL && catch_block->exit != NULL))\n    {\n        dest_label = njs_generate_jump_destination(vm, generator->block,\n                                                   \"try break/return\",\n                                                   NJS_GENERATOR_ALL\n                                                   | NJS_GENERATOR_TRY,\n                                                   &ctx->try_exit_label,\n                                                   &ctx->catch_exit_label);\n        if (njs_slow_path(dest_label == NULL)) {\n            return NJS_ERROR;\n        }\n\n        /*\n         * block can be NULL for \"return\" instruction in\n         * outermost try-catch block.\n         */\n        block = njs_generate_find_block(vm, generator->block,\n                                        NJS_GENERATOR_ALL\n                                        | NJS_GENERATOR_TRY, dest_label);\n        if (block != NULL) {\n            patch = njs_generate_make_exit_patch(vm, block, dest_label,\n                            njs_code_offset(generator, finally)\n                            + offsetof(njs_vmcode_finally_t, break_offset));\n            if (njs_slow_path(patch == NULL)) {\n                return NJS_ERROR;\n            }\n        }\n    }\n\n    ret = njs_generate_index_release(vm, generator, ctx->exception_index);\n    if (njs_slow_path(ret != NJS_OK)) {\n        return ret;\n    }\n\n    return njs_generator_stack_pop(vm, generator, ctx);\n}",
        "func": "static njs_int_t\nnjs_generate_try_end(njs_vm_t *vm, njs_generator_t *generator,\n    njs_parser_node_t *node)\n{\n    njs_int_t                ret;\n    njs_index_t              exit_index;\n    const njs_str_t          *dest_label;\n    njs_vmcode_finally_t     *finally;\n    njs_generator_patch_t    *patch;\n    njs_generator_block_t    *block, *try_block, *catch_block;\n    njs_generator_try_ctx_t  *ctx;\n\n    ctx = generator->context;\n\n    try_block = ctx->try_block;\n    exit_index = try_block->index;\n    catch_block = ctx->catch_block;\n\n    njs_generate_code_finally(generator, finally, ctx->exception_index,\n                              exit_index, node);\n\n    if (try_block->continuation != NULL\n        || (catch_block && catch_block->continuation != NULL))\n    {\n        dest_label = njs_generate_jump_destination(vm, generator->block,\n                                                   \"try continue\",\n                                                   NJS_GENERATOR_LOOP,\n                                                   &ctx->try_cont_label,\n                                                   &ctx->catch_cont_label);\n        if (njs_slow_path(dest_label == NULL)) {\n            return NJS_ERROR;\n        }\n\n        /*\n         * block != NULL is checked\n         * by njs_generate_continue_statement()\n         */\n        block = njs_generate_find_block(vm, generator->block,\n                                        NJS_GENERATOR_LOOP, dest_label);\n\n        patch = njs_generate_make_continuation_patch(vm, block, dest_label,\n                         njs_code_offset(generator, finally)\n                         + offsetof(njs_vmcode_finally_t, continue_offset));\n        if (njs_slow_path(patch == NULL)) {\n            return NJS_ERROR;\n        }\n    }\n\n    if (try_block->exit != NULL\n        || (catch_block != NULL && catch_block->exit != NULL))\n    {\n        dest_label = njs_generate_jump_destination(vm, generator->block,\n                                                   \"try break/return\",\n                                                   NJS_GENERATOR_ALL\n                                                   | NJS_GENERATOR_TRY,\n                                                   &ctx->try_exit_label,\n                                                   &ctx->catch_exit_label);\n        if (njs_slow_path(dest_label == NULL)) {\n            return NJS_ERROR;\n        }\n\n        /*\n         * block can be NULL for \"return\" instruction in\n         * outermost try-catch block.\n         */\n        block = njs_generate_find_block(vm, generator->block,\n                                        NJS_GENERATOR_ALL, dest_label);\n        if (block != NULL) {\n            patch = njs_generate_make_exit_patch(vm, block, dest_label,\n                            njs_code_offset(generator, finally)\n                            + offsetof(njs_vmcode_finally_t, break_offset));\n            if (njs_slow_path(patch == NULL)) {\n                return NJS_ERROR;\n            }\n\n        } else {\n\n            block = njs_generate_find_block(vm, generator->block,\n                                            NJS_GENERATOR_ALL, &no_label);\n            if (block != NULL) {\n                patch = njs_generate_make_exit_patch(vm, block, &no_label,\n                                njs_code_offset(generator, finally)\n                                + offsetof(njs_vmcode_finally_t, break_offset));\n                if (njs_slow_path(patch == NULL)) {\n                    return NJS_ERROR;\n                }\n            }\n        }\n    }\n\n    ret = njs_generate_index_release(vm, generator, ctx->exception_index);\n    if (njs_slow_path(ret != NJS_OK)) {\n        return ret;\n    }\n\n    return njs_generator_stack_pop(vm, generator, ctx);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -64,14 +64,26 @@\n          * outermost try-catch block.\n          */\n         block = njs_generate_find_block(vm, generator->block,\n-                                        NJS_GENERATOR_ALL\n-                                        | NJS_GENERATOR_TRY, dest_label);\n+                                        NJS_GENERATOR_ALL, dest_label);\n         if (block != NULL) {\n             patch = njs_generate_make_exit_patch(vm, block, dest_label,\n                             njs_code_offset(generator, finally)\n                             + offsetof(njs_vmcode_finally_t, break_offset));\n             if (njs_slow_path(patch == NULL)) {\n                 return NJS_ERROR;\n+            }\n+\n+        } else {\n+\n+            block = njs_generate_find_block(vm, generator->block,\n+                                            NJS_GENERATOR_ALL, &no_label);\n+            if (block != NULL) {\n+                patch = njs_generate_make_exit_patch(vm, block, &no_label,\n+                                njs_code_offset(generator, finally)\n+                                + offsetof(njs_vmcode_finally_t, break_offset));\n+                if (njs_slow_path(patch == NULL)) {\n+                    return NJS_ERROR;\n+                }\n             }\n         }\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "                                        NJS_GENERATOR_ALL",
                "                                        | NJS_GENERATOR_TRY, dest_label);"
            ],
            "added_lines": [
                "                                        NJS_GENERATOR_ALL, dest_label);",
                "            }",
                "",
                "        } else {",
                "",
                "            block = njs_generate_find_block(vm, generator->block,",
                "                                            NJS_GENERATOR_ALL, &no_label);",
                "            if (block != NULL) {",
                "                patch = njs_generate_make_exit_patch(vm, block, &no_label,",
                "                                njs_code_offset(generator, finally)",
                "                                + offsetof(njs_vmcode_finally_t, break_offset));",
                "                if (njs_slow_path(patch == NULL)) {",
                "                    return NJS_ERROR;",
                "                }"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-1737",
        "func_name": "torvalds/linux/raw_cmd_copyin",
        "description": "The raw_cmd_copyin function in drivers/block/floppy.c in the Linux kernel through 3.14.3 does not properly handle error conditions during processing of an FDRAWCMD ioctl call, which allows local users to trigger kfree operations and gain privileges by leveraging write access to a /dev/fd device.",
        "git_url": "https://github.com/torvalds/linux/commit/ef87dbe7614341c2e7bfe8d32fcb7028cc97442c",
        "commit_title": "floppy: ignore kernel-only members in FDRAWCMD ioctl input",
        "commit_text": " Always clear out these floppy_raw_cmd struct members after copying the entire structure from userspace so that the in-kernel version is always valid and never left in an interdeterminate state. ",
        "func_before": "static int raw_cmd_copyin(int cmd, void __user *param,\n\t\t\t\t struct floppy_raw_cmd **rcmd)\n{\n\tstruct floppy_raw_cmd *ptr;\n\tint ret;\n\tint i;\n\n\t*rcmd = NULL;\n\nloop:\n\tptr = kmalloc(sizeof(struct floppy_raw_cmd), GFP_USER);\n\tif (!ptr)\n\t\treturn -ENOMEM;\n\t*rcmd = ptr;\n\tret = copy_from_user(ptr, param, sizeof(*ptr));\n\tif (ret)\n\t\treturn -EFAULT;\n\tptr->next = NULL;\n\tptr->buffer_length = 0;\n\tparam += sizeof(struct floppy_raw_cmd);\n\tif (ptr->cmd_count > 33)\n\t\t\t/* the command may now also take up the space\n\t\t\t * initially intended for the reply & the\n\t\t\t * reply count. Needed for long 82078 commands\n\t\t\t * such as RESTORE, which takes ... 17 command\n\t\t\t * bytes. Murphy's law #137: When you reserve\n\t\t\t * 16 bytes for a structure, you'll one day\n\t\t\t * discover that you really need 17...\n\t\t\t */\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < 16; i++)\n\t\tptr->reply[i] = 0;\n\tptr->resultcode = 0;\n\tptr->kernel_data = NULL;\n\n\tif (ptr->flags & (FD_RAW_READ | FD_RAW_WRITE)) {\n\t\tif (ptr->length <= 0)\n\t\t\treturn -EINVAL;\n\t\tptr->kernel_data = (char *)fd_dma_mem_alloc(ptr->length);\n\t\tfallback_on_nodma_alloc(&ptr->kernel_data, ptr->length);\n\t\tif (!ptr->kernel_data)\n\t\t\treturn -ENOMEM;\n\t\tptr->buffer_length = ptr->length;\n\t}\n\tif (ptr->flags & FD_RAW_WRITE) {\n\t\tret = fd_copyin(ptr->data, ptr->kernel_data, ptr->length);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (ptr->flags & FD_RAW_MORE) {\n\t\trcmd = &(ptr->next);\n\t\tptr->rate &= 0x43;\n\t\tgoto loop;\n\t}\n\n\treturn 0;\n}",
        "func": "static int raw_cmd_copyin(int cmd, void __user *param,\n\t\t\t\t struct floppy_raw_cmd **rcmd)\n{\n\tstruct floppy_raw_cmd *ptr;\n\tint ret;\n\tint i;\n\n\t*rcmd = NULL;\n\nloop:\n\tptr = kmalloc(sizeof(struct floppy_raw_cmd), GFP_USER);\n\tif (!ptr)\n\t\treturn -ENOMEM;\n\t*rcmd = ptr;\n\tret = copy_from_user(ptr, param, sizeof(*ptr));\n\tptr->next = NULL;\n\tptr->buffer_length = 0;\n\tptr->kernel_data = NULL;\n\tif (ret)\n\t\treturn -EFAULT;\n\tparam += sizeof(struct floppy_raw_cmd);\n\tif (ptr->cmd_count > 33)\n\t\t\t/* the command may now also take up the space\n\t\t\t * initially intended for the reply & the\n\t\t\t * reply count. Needed for long 82078 commands\n\t\t\t * such as RESTORE, which takes ... 17 command\n\t\t\t * bytes. Murphy's law #137: When you reserve\n\t\t\t * 16 bytes for a structure, you'll one day\n\t\t\t * discover that you really need 17...\n\t\t\t */\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < 16; i++)\n\t\tptr->reply[i] = 0;\n\tptr->resultcode = 0;\n\n\tif (ptr->flags & (FD_RAW_READ | FD_RAW_WRITE)) {\n\t\tif (ptr->length <= 0)\n\t\t\treturn -EINVAL;\n\t\tptr->kernel_data = (char *)fd_dma_mem_alloc(ptr->length);\n\t\tfallback_on_nodma_alloc(&ptr->kernel_data, ptr->length);\n\t\tif (!ptr->kernel_data)\n\t\t\treturn -ENOMEM;\n\t\tptr->buffer_length = ptr->length;\n\t}\n\tif (ptr->flags & FD_RAW_WRITE) {\n\t\tret = fd_copyin(ptr->data, ptr->kernel_data, ptr->length);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (ptr->flags & FD_RAW_MORE) {\n\t\trcmd = &(ptr->next);\n\t\tptr->rate &= 0x43;\n\t\tgoto loop;\n\t}\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,10 +13,11 @@\n \t\treturn -ENOMEM;\n \t*rcmd = ptr;\n \tret = copy_from_user(ptr, param, sizeof(*ptr));\n+\tptr->next = NULL;\n+\tptr->buffer_length = 0;\n+\tptr->kernel_data = NULL;\n \tif (ret)\n \t\treturn -EFAULT;\n-\tptr->next = NULL;\n-\tptr->buffer_length = 0;\n \tparam += sizeof(struct floppy_raw_cmd);\n \tif (ptr->cmd_count > 33)\n \t\t\t/* the command may now also take up the space\n@@ -32,7 +33,6 @@\n \tfor (i = 0; i < 16; i++)\n \t\tptr->reply[i] = 0;\n \tptr->resultcode = 0;\n-\tptr->kernel_data = NULL;\n \n \tif (ptr->flags & (FD_RAW_READ | FD_RAW_WRITE)) {\n \t\tif (ptr->length <= 0)",
        "diff_line_info": {
            "deleted_lines": [
                "\tptr->next = NULL;",
                "\tptr->buffer_length = 0;",
                "\tptr->kernel_data = NULL;"
            ],
            "added_lines": [
                "\tptr->next = NULL;",
                "\tptr->buffer_length = 0;",
                "\tptr->kernel_data = NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9633",
        "func_name": "GNOME/glib/g_socket_client_tls_handshake",
        "description": "gio/gsocketclient.c in GNOME GLib 2.59.2 does not ensure that a parent GTask remains alive during the execution of a connection-attempting enumeration, which allows remote attackers to cause a denial of service (g_socket_client_connected_callback mishandling and application crash) via a crafted web site, as demonstrated by GNOME Web (aka Epiphany).",
        "git_url": "https://github.com/GNOME/glib/commit/d553d92d6e9f53cbe5a34166fcb919ba652c6a8e",
        "commit_title": "gsocketclient: Fix criticals",
        "commit_text": " This ensures the parent GTask is kept alive as long as an enumeration is running and trying to connect.  Closes #1646 Closes #1649",
        "func_before": "static void\ng_socket_client_tls_handshake (GSocketClientAsyncConnectData *data)\n{\n  GIOStream *tlsconn;\n\n  if (!data->client->priv->tls)\n    {\n      g_socket_client_async_connect_complete (data);\n      return;\n    }\n\n  tlsconn = g_tls_client_connection_new (data->connection,\n\t\t\t\t\t data->connectable,\n\t\t\t\t\t &data->last_error);\n  if (tlsconn)\n    {\n      g_tls_client_connection_set_validation_flags (G_TLS_CLIENT_CONNECTION (tlsconn),\n                                                    data->client->priv->tls_validation_flags);\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_TLS_HANDSHAKING, data->connectable, G_IO_STREAM (tlsconn));\n      g_tls_connection_handshake_async (G_TLS_CONNECTION (tlsconn),\n\t\t\t\t\tG_PRIORITY_DEFAULT,\n\t\t\t\t\tg_task_get_cancellable (data->task),\n\t\t\t\t\tg_socket_client_tls_handshake_callback,\n\t\t\t\t\tdata);\n    }\n  else\n    {\n      enumerator_next_async (data);\n    }\n}",
        "func": "static void\ng_socket_client_tls_handshake (GSocketClientAsyncConnectData *data)\n{\n  GIOStream *tlsconn;\n\n  if (!data->client->priv->tls)\n    {\n      g_socket_client_async_connect_complete (data);\n      return;\n    }\n\n  tlsconn = g_tls_client_connection_new (data->connection,\n\t\t\t\t\t data->connectable,\n\t\t\t\t\t &data->last_error);\n  if (tlsconn)\n    {\n      g_tls_client_connection_set_validation_flags (G_TLS_CLIENT_CONNECTION (tlsconn),\n                                                    data->client->priv->tls_validation_flags);\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_TLS_HANDSHAKING, data->connectable, G_IO_STREAM (tlsconn));\n      g_tls_connection_handshake_async (G_TLS_CONNECTION (tlsconn),\n\t\t\t\t\tG_PRIORITY_DEFAULT,\n\t\t\t\t\tg_task_get_cancellable (data->task),\n\t\t\t\t\tg_socket_client_tls_handshake_callback,\n\t\t\t\t\tdata);\n    }\n  else\n    {\n      enumerator_next_async (data, FALSE);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,6 +25,6 @@\n     }\n   else\n     {\n-      enumerator_next_async (data);\n+      enumerator_next_async (data, FALSE);\n     }\n }",
        "diff_line_info": {
            "deleted_lines": [
                "      enumerator_next_async (data);"
            ],
            "added_lines": [
                "      enumerator_next_async (data, FALSE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9633",
        "func_name": "GNOME/glib/enumerator_next_async",
        "description": "gio/gsocketclient.c in GNOME GLib 2.59.2 does not ensure that a parent GTask remains alive during the execution of a connection-attempting enumeration, which allows remote attackers to cause a denial of service (g_socket_client_connected_callback mishandling and application crash) via a crafted web site, as demonstrated by GNOME Web (aka Epiphany).",
        "git_url": "https://github.com/GNOME/glib/commit/d553d92d6e9f53cbe5a34166fcb919ba652c6a8e",
        "commit_title": "gsocketclient: Fix criticals",
        "commit_text": " This ensures the parent GTask is kept alive as long as an enumeration is running and trying to connect.  Closes #1646 Closes #1649",
        "func_before": "static void\nenumerator_next_async (GSocketClientAsyncConnectData *data)\n{\n  /* We need to cleanup the state */\n  g_clear_object (&data->socket);\n  g_clear_object (&data->proxy_addr);\n  g_clear_object (&data->connection);\n\n  g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_RESOLVING, data->connectable, NULL);\n  g_socket_address_enumerator_next_async (data->enumerator,\n\t\t\t\t\t  g_task_get_cancellable (data->task),\n\t\t\t\t\t  g_socket_client_enumerator_callback,\n\t\t\t\t\t  data);\n}",
        "func": "static void\nenumerator_next_async (GSocketClientAsyncConnectData *data,\n                       gboolean                       add_task_ref)\n{\n  /* We need to cleanup the state */\n  g_clear_object (&data->socket);\n  g_clear_object (&data->proxy_addr);\n  g_clear_object (&data->connection);\n\n  /* Each enumeration takes a ref. This arg just avoids repeated unrefs when\n     an enumeration starts another enumeration */\n  if (add_task_ref)\n    g_object_ref (data->task);\n\n  g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_RESOLVING, data->connectable, NULL);\n  g_socket_address_enumerator_next_async (data->enumerator,\n\t\t\t\t\t  g_task_get_cancellable (data->task),\n\t\t\t\t\t  g_socket_client_enumerator_callback,\n\t\t\t\t\t  data);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,16 @@\n static void\n-enumerator_next_async (GSocketClientAsyncConnectData *data)\n+enumerator_next_async (GSocketClientAsyncConnectData *data,\n+                       gboolean                       add_task_ref)\n {\n   /* We need to cleanup the state */\n   g_clear_object (&data->socket);\n   g_clear_object (&data->proxy_addr);\n   g_clear_object (&data->connection);\n+\n+  /* Each enumeration takes a ref. This arg just avoids repeated unrefs when\n+     an enumeration starts another enumeration */\n+  if (add_task_ref)\n+    g_object_ref (data->task);\n \n   g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_RESOLVING, data->connectable, NULL);\n   g_socket_address_enumerator_next_async (data->enumerator,",
        "diff_line_info": {
            "deleted_lines": [
                "enumerator_next_async (GSocketClientAsyncConnectData *data)"
            ],
            "added_lines": [
                "enumerator_next_async (GSocketClientAsyncConnectData *data,",
                "                       gboolean                       add_task_ref)",
                "",
                "  /* Each enumeration takes a ref. This arg just avoids repeated unrefs when",
                "     an enumeration starts another enumeration */",
                "  if (add_task_ref)",
                "    g_object_ref (data->task);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9633",
        "func_name": "GNOME/glib/g_socket_client_proxy_connect_callback",
        "description": "gio/gsocketclient.c in GNOME GLib 2.59.2 does not ensure that a parent GTask remains alive during the execution of a connection-attempting enumeration, which allows remote attackers to cause a denial of service (g_socket_client_connected_callback mishandling and application crash) via a crafted web site, as demonstrated by GNOME Web (aka Epiphany).",
        "git_url": "https://github.com/GNOME/glib/commit/d553d92d6e9f53cbe5a34166fcb919ba652c6a8e",
        "commit_title": "gsocketclient: Fix criticals",
        "commit_text": " This ensures the parent GTask is kept alive as long as an enumeration is running and trying to connect.  Closes #1646 Closes #1649",
        "func_before": "static void\ng_socket_client_proxy_connect_callback (GObject      *object,\n\t\t\t\t\tGAsyncResult *result,\n\t\t\t\t\tgpointer      user_data)\n{\n  GSocketClientAsyncConnectData *data = user_data;\n\n  g_object_unref (data->connection);\n  data->connection = g_proxy_connect_finish (G_PROXY (object),\n\t\t\t\t\t     result,\n\t\t\t\t\t     &data->last_error);\n  if (data->connection)\n    {\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_PROXY_NEGOTIATED, data->connectable, data->connection);\n    }\n  else\n    {\n      enumerator_next_async (data);\n      return;\n    }\n\n  g_socket_client_tls_handshake (data);\n}",
        "func": "static void\ng_socket_client_proxy_connect_callback (GObject      *object,\n\t\t\t\t\tGAsyncResult *result,\n\t\t\t\t\tgpointer      user_data)\n{\n  GSocketClientAsyncConnectData *data = user_data;\n\n  g_object_unref (data->connection);\n  data->connection = g_proxy_connect_finish (G_PROXY (object),\n\t\t\t\t\t     result,\n\t\t\t\t\t     &data->last_error);\n  if (data->connection)\n    {\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_PROXY_NEGOTIATED, data->connectable, data->connection);\n    }\n  else\n    {\n      enumerator_next_async (data, FALSE);\n      return;\n    }\n\n  g_socket_client_tls_handshake (data);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,7 +15,7 @@\n     }\n   else\n     {\n-      enumerator_next_async (data);\n+      enumerator_next_async (data, FALSE);\n       return;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "      enumerator_next_async (data);"
            ],
            "added_lines": [
                "      enumerator_next_async (data, FALSE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9633",
        "func_name": "GNOME/glib/g_socket_client_connected_callback",
        "description": "gio/gsocketclient.c in GNOME GLib 2.59.2 does not ensure that a parent GTask remains alive during the execution of a connection-attempting enumeration, which allows remote attackers to cause a denial of service (g_socket_client_connected_callback mishandling and application crash) via a crafted web site, as demonstrated by GNOME Web (aka Epiphany).",
        "git_url": "https://github.com/GNOME/glib/commit/d553d92d6e9f53cbe5a34166fcb919ba652c6a8e",
        "commit_title": "gsocketclient: Fix criticals",
        "commit_text": " This ensures the parent GTask is kept alive as long as an enumeration is running and trying to connect.  Closes #1646 Closes #1649",
        "func_before": "static void\ng_socket_client_connected_callback (GObject      *source,\n\t\t\t\t    GAsyncResult *result,\n\t\t\t\t    gpointer      user_data)\n{\n  ConnectionAttempt *attempt = user_data;\n  GSocketClientAsyncConnectData *data = attempt->data;\n  GSList *l;\n  GError *error = NULL;\n  GProxy *proxy;\n  const gchar *protocol;\n\n  /* data is NULL once the task is completed */\n  if (data && g_task_return_error_if_cancelled (data->task))\n    {\n      g_object_unref (data->task);\n      connection_attempt_unref (attempt);\n      return;\n    }\n\n  if (attempt->timeout_source)\n    {\n      g_source_destroy (attempt->timeout_source);\n      g_clear_pointer (&attempt->timeout_source, g_source_unref);\n    }\n\n  if (!g_socket_connection_connect_finish (G_SOCKET_CONNECTION (source),\n\t\t\t\t\t   result, &error))\n    {\n      if (!g_cancellable_is_cancelled (attempt->cancellable))\n        {\n          clarify_connect_error (error, data->connectable, attempt->address);\n          set_last_error (data, error);\n        }\n      else\n        g_clear_error (&error);\n\n      if (data)\n        {\n          connection_attempt_remove (attempt);\n          enumerator_next_async (data);\n        }\n      else\n        connection_attempt_unref (attempt);\n\n      return;\n    }\n\n  data->socket = g_steal_pointer (&attempt->socket);\n  data->connection = g_steal_pointer (&attempt->connection);\n\n  for (l = data->connection_attempts; l; l = g_slist_next (l))\n    {\n      ConnectionAttempt *attempt_entry = l->data;\n      g_cancellable_cancel (attempt_entry->cancellable);\n      attempt_entry->data = NULL;\n      connection_attempt_unref (attempt_entry);\n    }\n  g_slist_free (data->connection_attempts);\n  data->connection_attempts = NULL;\n  connection_attempt_unref (attempt);\n\n  g_socket_connection_set_cached_remote_address ((GSocketConnection*)data->connection, NULL);\n  g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_CONNECTED, data->connectable, data->connection);\n\n  /* wrong, but backward compatible */\n  g_socket_set_blocking (data->socket, TRUE);\n\n  if (!data->proxy_addr)\n    {\n      g_socket_client_tls_handshake (data);\n      return;\n    }\n\n  protocol = g_proxy_address_get_protocol (data->proxy_addr);\n\n  /* The connection should not be anything other than TCP,\n   * but let's put a safety guard in case\n   */\n  if (!G_IS_TCP_CONNECTION (data->connection))\n    {\n      g_critical (\"Trying to proxy over non-TCP connection, this is \"\n          \"most likely a bug in GLib IO library.\");\n\n      g_set_error_literal (&data->last_error,\n          G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED,\n          _(\"Proxying over a non-TCP connection is not supported.\"));\n\n      enumerator_next_async (data);\n    }\n  else if (g_hash_table_contains (data->client->priv->app_proxies, protocol))\n    {\n      /* Simply complete the connection, we don't want to do TLS handshake\n       * as the application proxy handling may need proxy handshake first */\n      g_socket_client_async_connect_complete (data);\n    }\n  else if ((proxy = g_proxy_get_default_for_protocol (protocol)))\n    {\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_PROXY_NEGOTIATING, data->connectable, data->connection);\n      g_proxy_connect_async (proxy,\n                             data->connection,\n                             data->proxy_addr,\n                             g_task_get_cancellable (data->task),\n                             g_socket_client_proxy_connect_callback,\n                             data);\n      g_object_unref (proxy);\n    }\n  else\n    {\n      g_clear_error (&data->last_error);\n\n      g_set_error (&data->last_error, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED,\n          _(\"Proxy protocol %s is not supported.\"),\n          protocol);\n\n      enumerator_next_async (data);\n    }\n}",
        "func": "static void\ng_socket_client_connected_callback (GObject      *source,\n\t\t\t\t    GAsyncResult *result,\n\t\t\t\t    gpointer      user_data)\n{\n  ConnectionAttempt *attempt = user_data;\n  GSocketClientAsyncConnectData *data = attempt->data;\n  GSList *l;\n  GError *error = NULL;\n  GProxy *proxy;\n  const gchar *protocol;\n\n  if (g_cancellable_is_cancelled (attempt->cancellable) || task_completed_or_cancelled (data->task))\n    {\n      g_object_unref (data->task);\n      connection_attempt_unref (attempt);\n      return;\n    }\n\n  if (attempt->timeout_source)\n    {\n      g_source_destroy (attempt->timeout_source);\n      g_clear_pointer (&attempt->timeout_source, g_source_unref);\n    }\n\n  if (!g_socket_connection_connect_finish (G_SOCKET_CONNECTION (source),\n\t\t\t\t\t   result, &error))\n    {\n      if (!g_cancellable_is_cancelled (attempt->cancellable))\n        {\n          clarify_connect_error (error, data->connectable, attempt->address);\n          set_last_error (data, error);\n          connection_attempt_remove (attempt);\n          enumerator_next_async (data, FALSE);\n        }\n      else\n        {\n          g_clear_error (&error);\n          g_object_unref (data->task);\n          connection_attempt_unref (attempt);\n        }\n\n      return;\n    }\n\n  data->socket = g_steal_pointer (&attempt->socket);\n  data->connection = g_steal_pointer (&attempt->connection);\n\n  for (l = data->connection_attempts; l; l = g_slist_next (l))\n    {\n      ConnectionAttempt *attempt_entry = l->data;\n      g_cancellable_cancel (attempt_entry->cancellable);\n      connection_attempt_unref (attempt_entry);\n    }\n  g_slist_free (data->connection_attempts);\n  data->connection_attempts = NULL;\n  connection_attempt_unref (attempt);\n\n  g_socket_connection_set_cached_remote_address ((GSocketConnection*)data->connection, NULL);\n  g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_CONNECTED, data->connectable, data->connection);\n\n  /* wrong, but backward compatible */\n  g_socket_set_blocking (data->socket, TRUE);\n\n  if (!data->proxy_addr)\n    {\n      g_socket_client_tls_handshake (data);\n      return;\n    }\n\n  protocol = g_proxy_address_get_protocol (data->proxy_addr);\n\n  /* The connection should not be anything other than TCP,\n   * but let's put a safety guard in case\n   */\n  if (!G_IS_TCP_CONNECTION (data->connection))\n    {\n      g_critical (\"Trying to proxy over non-TCP connection, this is \"\n          \"most likely a bug in GLib IO library.\");\n\n      g_set_error_literal (&data->last_error,\n          G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED,\n          _(\"Proxying over a non-TCP connection is not supported.\"));\n\n      enumerator_next_async (data, FALSE);\n    }\n  else if (g_hash_table_contains (data->client->priv->app_proxies, protocol))\n    {\n      /* Simply complete the connection, we don't want to do TLS handshake\n       * as the application proxy handling may need proxy handshake first */\n      g_socket_client_async_connect_complete (data);\n    }\n  else if ((proxy = g_proxy_get_default_for_protocol (protocol)))\n    {\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_PROXY_NEGOTIATING, data->connectable, data->connection);\n      g_proxy_connect_async (proxy,\n                             data->connection,\n                             data->proxy_addr,\n                             g_task_get_cancellable (data->task),\n                             g_socket_client_proxy_connect_callback,\n                             data);\n      g_object_unref (proxy);\n    }\n  else\n    {\n      g_clear_error (&data->last_error);\n\n      g_set_error (&data->last_error, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED,\n          _(\"Proxy protocol %s is not supported.\"),\n          protocol);\n\n      enumerator_next_async (data, FALSE);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,8 +10,7 @@\n   GProxy *proxy;\n   const gchar *protocol;\n \n-  /* data is NULL once the task is completed */\n-  if (data && g_task_return_error_if_cancelled (data->task))\n+  if (g_cancellable_is_cancelled (attempt->cancellable) || task_completed_or_cancelled (data->task))\n     {\n       g_object_unref (data->task);\n       connection_attempt_unref (attempt);\n@@ -31,17 +30,15 @@\n         {\n           clarify_connect_error (error, data->connectable, attempt->address);\n           set_last_error (data, error);\n+          connection_attempt_remove (attempt);\n+          enumerator_next_async (data, FALSE);\n         }\n       else\n-        g_clear_error (&error);\n-\n-      if (data)\n         {\n-          connection_attempt_remove (attempt);\n-          enumerator_next_async (data);\n+          g_clear_error (&error);\n+          g_object_unref (data->task);\n+          connection_attempt_unref (attempt);\n         }\n-      else\n-        connection_attempt_unref (attempt);\n \n       return;\n     }\n@@ -53,7 +50,6 @@\n     {\n       ConnectionAttempt *attempt_entry = l->data;\n       g_cancellable_cancel (attempt_entry->cancellable);\n-      attempt_entry->data = NULL;\n       connection_attempt_unref (attempt_entry);\n     }\n   g_slist_free (data->connection_attempts);\n@@ -86,7 +82,7 @@\n           G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED,\n           _(\"Proxying over a non-TCP connection is not supported.\"));\n \n-      enumerator_next_async (data);\n+      enumerator_next_async (data, FALSE);\n     }\n   else if (g_hash_table_contains (data->client->priv->app_proxies, protocol))\n     {\n@@ -113,6 +109,6 @@\n           _(\"Proxy protocol %s is not supported.\"),\n           protocol);\n \n-      enumerator_next_async (data);\n+      enumerator_next_async (data, FALSE);\n     }\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  /* data is NULL once the task is completed */",
                "  if (data && g_task_return_error_if_cancelled (data->task))",
                "        g_clear_error (&error);",
                "",
                "      if (data)",
                "          connection_attempt_remove (attempt);",
                "          enumerator_next_async (data);",
                "      else",
                "        connection_attempt_unref (attempt);",
                "      attempt_entry->data = NULL;",
                "      enumerator_next_async (data);",
                "      enumerator_next_async (data);"
            ],
            "added_lines": [
                "  if (g_cancellable_is_cancelled (attempt->cancellable) || task_completed_or_cancelled (data->task))",
                "          connection_attempt_remove (attempt);",
                "          enumerator_next_async (data, FALSE);",
                "          g_clear_error (&error);",
                "          g_object_unref (data->task);",
                "          connection_attempt_unref (attempt);",
                "      enumerator_next_async (data, FALSE);",
                "      enumerator_next_async (data, FALSE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9633",
        "func_name": "GNOME/glib/on_connection_attempt_timeout",
        "description": "gio/gsocketclient.c in GNOME GLib 2.59.2 does not ensure that a parent GTask remains alive during the execution of a connection-attempting enumeration, which allows remote attackers to cause a denial of service (g_socket_client_connected_callback mishandling and application crash) via a crafted web site, as demonstrated by GNOME Web (aka Epiphany).",
        "git_url": "https://github.com/GNOME/glib/commit/d553d92d6e9f53cbe5a34166fcb919ba652c6a8e",
        "commit_title": "gsocketclient: Fix criticals",
        "commit_text": " This ensures the parent GTask is kept alive as long as an enumeration is running and trying to connect.  Closes #1646 Closes #1649",
        "func_before": "static gboolean\non_connection_attempt_timeout (gpointer data)\n{\n  ConnectionAttempt *attempt = data;\n\n  enumerator_next_async (attempt->data);\n\n  g_clear_pointer (&attempt->timeout_source, g_source_unref);\n  return G_SOURCE_REMOVE;\n}",
        "func": "static gboolean\non_connection_attempt_timeout (gpointer data)\n{\n  ConnectionAttempt *attempt = data;\n\n  enumerator_next_async (attempt->data, TRUE);\n\n  g_clear_pointer (&attempt->timeout_source, g_source_unref);\n  return G_SOURCE_REMOVE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n {\n   ConnectionAttempt *attempt = data;\n \n-  enumerator_next_async (attempt->data);\n+  enumerator_next_async (attempt->data, TRUE);\n \n   g_clear_pointer (&attempt->timeout_source, g_source_unref);\n   return G_SOURCE_REMOVE;",
        "diff_line_info": {
            "deleted_lines": [
                "  enumerator_next_async (attempt->data);"
            ],
            "added_lines": [
                "  enumerator_next_async (attempt->data, TRUE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9633",
        "func_name": "GNOME/glib/g_socket_client_async_connect_data_free",
        "description": "gio/gsocketclient.c in GNOME GLib 2.59.2 does not ensure that a parent GTask remains alive during the execution of a connection-attempting enumeration, which allows remote attackers to cause a denial of service (g_socket_client_connected_callback mishandling and application crash) via a crafted web site, as demonstrated by GNOME Web (aka Epiphany).",
        "git_url": "https://github.com/GNOME/glib/commit/d553d92d6e9f53cbe5a34166fcb919ba652c6a8e",
        "commit_title": "gsocketclient: Fix criticals",
        "commit_text": " This ensures the parent GTask is kept alive as long as an enumeration is running and trying to connect.  Closes #1646 Closes #1649",
        "func_before": "static void\ng_socket_client_async_connect_data_free (GSocketClientAsyncConnectData *data)\n{\n  g_clear_object (&data->connectable);\n  g_clear_object (&data->enumerator);\n  g_clear_object (&data->proxy_addr);\n  g_clear_object (&data->socket);\n  g_clear_object (&data->connection);\n  g_slist_free_full (data->connection_attempts, connection_attempt_unref);\n\n  g_clear_error (&data->last_error);\n\n  g_slice_free (GSocketClientAsyncConnectData, data);\n}",
        "func": "static void\ng_socket_client_async_connect_data_free (GSocketClientAsyncConnectData *data)\n{\n  data->task = NULL;\n  g_clear_object (&data->connectable);\n  g_clear_object (&data->enumerator);\n  g_clear_object (&data->proxy_addr);\n  g_clear_object (&data->socket);\n  g_clear_object (&data->connection);\n  g_slist_free_full (data->connection_attempts, connection_attempt_unref);\n\n  g_clear_error (&data->last_error);\n\n  g_slice_free (GSocketClientAsyncConnectData, data);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n static void\n g_socket_client_async_connect_data_free (GSocketClientAsyncConnectData *data)\n {\n+  data->task = NULL;\n   g_clear_object (&data->connectable);\n   g_clear_object (&data->enumerator);\n   g_clear_object (&data->proxy_addr);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  data->task = NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9633",
        "func_name": "GNOME/glib/g_socket_client_connect_async",
        "description": "gio/gsocketclient.c in GNOME GLib 2.59.2 does not ensure that a parent GTask remains alive during the execution of a connection-attempting enumeration, which allows remote attackers to cause a denial of service (g_socket_client_connected_callback mishandling and application crash) via a crafted web site, as demonstrated by GNOME Web (aka Epiphany).",
        "git_url": "https://github.com/GNOME/glib/commit/d553d92d6e9f53cbe5a34166fcb919ba652c6a8e",
        "commit_title": "gsocketclient: Fix criticals",
        "commit_text": " This ensures the parent GTask is kept alive as long as an enumeration is running and trying to connect.  Closes #1646 Closes #1649",
        "func_before": "void\ng_socket_client_connect_async (GSocketClient       *client,\n\t\t\t       GSocketConnectable  *connectable,\n\t\t\t       GCancellable        *cancellable,\n\t\t\t       GAsyncReadyCallback  callback,\n\t\t\t       gpointer             user_data)\n{\n  GSocketClientAsyncConnectData *data;\n\n  g_return_if_fail (G_IS_SOCKET_CLIENT (client));\n\n  data = g_slice_new0 (GSocketClientAsyncConnectData);\n  data->client = client;\n  data->connectable = g_object_ref (connectable);\n\n  if (can_use_proxy (client))\n    {\n      data->enumerator = g_socket_connectable_proxy_enumerate (connectable);\n      if (client->priv->proxy_resolver &&\n          G_IS_PROXY_ADDRESS_ENUMERATOR (data->enumerator))\n        {\n          g_object_set (G_OBJECT (data->enumerator),\n                        \"proxy-resolver\", client->priv->proxy_resolver,\n                        NULL);\n        }\n    }\n  else\n    data->enumerator = g_socket_connectable_enumerate (connectable);\n\n  data->task = g_task_new (client, cancellable, callback, user_data);\n  g_task_set_source_tag (data->task, g_socket_client_connect_async);\n  g_task_set_task_data (data->task, data, (GDestroyNotify)g_socket_client_async_connect_data_free);\n\n  enumerator_next_async (data);\n}",
        "func": "void\ng_socket_client_connect_async (GSocketClient       *client,\n\t\t\t       GSocketConnectable  *connectable,\n\t\t\t       GCancellable        *cancellable,\n\t\t\t       GAsyncReadyCallback  callback,\n\t\t\t       gpointer             user_data)\n{\n  GSocketClientAsyncConnectData *data;\n\n  g_return_if_fail (G_IS_SOCKET_CLIENT (client));\n\n  data = g_slice_new0 (GSocketClientAsyncConnectData);\n  data->client = client;\n  data->connectable = g_object_ref (connectable);\n\n  if (can_use_proxy (client))\n    {\n      data->enumerator = g_socket_connectable_proxy_enumerate (connectable);\n      if (client->priv->proxy_resolver &&\n          G_IS_PROXY_ADDRESS_ENUMERATOR (data->enumerator))\n        {\n          g_object_set (G_OBJECT (data->enumerator),\n                        \"proxy-resolver\", client->priv->proxy_resolver,\n                        NULL);\n        }\n    }\n  else\n    data->enumerator = g_socket_connectable_enumerate (connectable);\n\n  /* The flow and ownership here isn't quite obvious:\n    - The task starts an async attempt to connect.\n      - Each attempt holds a single ref on task.\n      - Each attempt may create new attempts by timing out (not a failure) so\n        there are multiple attempts happening in parallel.\n      - Upon failure an attempt will start a new attempt that steals its ref\n        until there are no more attempts left and it drops its ref.\n      - Upon success it will cancel all other attempts and continue on\n        to the rest of the connection (tls, proxies, etc) which do not\n        happen in parallel and at the very end drop its ref.\n      - Upon cancellation an attempt drops its ref.\n   */\n\n  data->task = g_task_new (client, cancellable, callback, user_data);\n  g_task_set_source_tag (data->task, g_socket_client_connect_async);\n  g_task_set_task_data (data->task, data, (GDestroyNotify)g_socket_client_async_connect_data_free);\n\n  enumerator_next_async (data, FALSE);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,9 +27,22 @@\n   else\n     data->enumerator = g_socket_connectable_enumerate (connectable);\n \n+  /* The flow and ownership here isn't quite obvious:\n+    - The task starts an async attempt to connect.\n+      - Each attempt holds a single ref on task.\n+      - Each attempt may create new attempts by timing out (not a failure) so\n+        there are multiple attempts happening in parallel.\n+      - Upon failure an attempt will start a new attempt that steals its ref\n+        until there are no more attempts left and it drops its ref.\n+      - Upon success it will cancel all other attempts and continue on\n+        to the rest of the connection (tls, proxies, etc) which do not\n+        happen in parallel and at the very end drop its ref.\n+      - Upon cancellation an attempt drops its ref.\n+   */\n+\n   data->task = g_task_new (client, cancellable, callback, user_data);\n   g_task_set_source_tag (data->task, g_socket_client_connect_async);\n   g_task_set_task_data (data->task, data, (GDestroyNotify)g_socket_client_async_connect_data_free);\n \n-  enumerator_next_async (data);\n+  enumerator_next_async (data, FALSE);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  enumerator_next_async (data);"
            ],
            "added_lines": [
                "  /* The flow and ownership here isn't quite obvious:",
                "    - The task starts an async attempt to connect.",
                "      - Each attempt holds a single ref on task.",
                "      - Each attempt may create new attempts by timing out (not a failure) so",
                "        there are multiple attempts happening in parallel.",
                "      - Upon failure an attempt will start a new attempt that steals its ref",
                "        until there are no more attempts left and it drops its ref.",
                "      - Upon success it will cancel all other attempts and continue on",
                "        to the rest of the connection (tls, proxies, etc) which do not",
                "        happen in parallel and at the very end drop its ref.",
                "      - Upon cancellation an attempt drops its ref.",
                "   */",
                "",
                "  enumerator_next_async (data, FALSE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9633",
        "func_name": "GNOME/glib/g_socket_client_enumerator_callback",
        "description": "gio/gsocketclient.c in GNOME GLib 2.59.2 does not ensure that a parent GTask remains alive during the execution of a connection-attempting enumeration, which allows remote attackers to cause a denial of service (g_socket_client_connected_callback mishandling and application crash) via a crafted web site, as demonstrated by GNOME Web (aka Epiphany).",
        "git_url": "https://github.com/GNOME/glib/commit/d553d92d6e9f53cbe5a34166fcb919ba652c6a8e",
        "commit_title": "gsocketclient: Fix criticals",
        "commit_text": " This ensures the parent GTask is kept alive as long as an enumeration is running and trying to connect.  Closes #1646 Closes #1649",
        "func_before": "static void\ng_socket_client_enumerator_callback (GObject      *object,\n\t\t\t\t     GAsyncResult *result,\n\t\t\t\t     gpointer      user_data)\n{\n  GSocketClientAsyncConnectData *data = user_data;\n  GSocketAddress *address = NULL;\n  GSocket *socket;\n  ConnectionAttempt *attempt;\n  GError *error = NULL;\n\n  if (g_task_return_error_if_cancelled (data->task))\n    {\n      g_object_unref (data->task);\n      return;\n    }\n\n  address = g_socket_address_enumerator_next_finish (data->enumerator,\n\t\t\t\t\t\t     result, &error);\n  if (address == NULL)\n    {\n      if (data->connection_attempts)\n        return;\n\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_COMPLETE, data->connectable, NULL);\n      if (!error)\n\t{\n\t  if (data->last_error)\n\t    {\n\t      error = data->last_error;\n\t      data->last_error = NULL;\n\t    }\n\t  else\n\t    {\n\t      g_set_error_literal (&error, G_IO_ERROR, G_IO_ERROR_FAILED,\n\t\t\t\t   _(\"Unknown error on connect\"));\n\t    }\n\t}\n      g_task_return_error (data->task, error);\n      g_object_unref (data->task);\n      return;\n    }\n\n  g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_RESOLVED,\n\t\t\t      data->connectable, NULL);\n\n  if (G_IS_PROXY_ADDRESS (address) &&\n      data->client->priv->enable_proxy)\n    data->proxy_addr = g_object_ref (G_PROXY_ADDRESS (address));\n\n  g_clear_error (&data->last_error);\n\n  socket = create_socket (data->client, address, &data->last_error);\n  if (socket == NULL)\n    {\n      g_object_unref (address);\n      enumerator_next_async (data);\n      return;\n    }\n\n  attempt = connection_attempt_new ();\n  attempt->data = data;\n  attempt->socket = socket;\n  attempt->address = address;\n  attempt->cancellable = g_cancellable_new ();\n  attempt->connection = (GIOStream *)g_socket_connection_factory_create_connection (socket);\n  attempt->timeout_source = g_timeout_source_new (HAPPY_EYEBALLS_CONNECTION_ATTEMPT_TIMEOUT_MS);\n  g_source_set_callback (attempt->timeout_source, on_connection_attempt_timeout, attempt, NULL);\n  g_source_attach (attempt->timeout_source, g_main_context_get_thread_default ());\n  data->connection_attempts = g_slist_append (data->connection_attempts, attempt);\n\n  if (g_task_get_cancellable (data->task))\n    g_cancellable_connect (g_task_get_cancellable (data->task), G_CALLBACK (on_connection_cancelled),\n                           g_object_ref (attempt->cancellable), g_object_unref);\n\n  g_socket_connection_set_cached_remote_address ((GSocketConnection *)attempt->connection, address);\n  g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_CONNECTING, data->connectable, attempt->connection);\n  g_socket_connection_connect_async (G_SOCKET_CONNECTION (attempt->connection),\n\t\t\t\t     address,\n\t\t\t\t     attempt->cancellable,\n\t\t\t\t     g_socket_client_connected_callback, connection_attempt_ref (attempt));\n}",
        "func": "static void\ng_socket_client_enumerator_callback (GObject      *object,\n\t\t\t\t     GAsyncResult *result,\n\t\t\t\t     gpointer      user_data)\n{\n  GSocketClientAsyncConnectData *data = user_data;\n  GSocketAddress *address = NULL;\n  GSocket *socket;\n  ConnectionAttempt *attempt;\n  GError *error = NULL;\n\n  if (task_completed_or_cancelled (data->task))\n    {\n      g_object_unref (data->task);\n      return;\n    }\n\n  address = g_socket_address_enumerator_next_finish (data->enumerator,\n\t\t\t\t\t\t     result, &error);\n  if (address == NULL)\n    {\n      if (data->connection_attempts)\n        {\n          g_object_unref (data->task);\n          return;\n        }\n\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_COMPLETE, data->connectable, NULL);\n      if (!error)\n\t{\n\t  if (data->last_error)\n\t    {\n\t      error = data->last_error;\n\t      data->last_error = NULL;\n\t    }\n\t  else\n\t    {\n\t      g_set_error_literal (&error, G_IO_ERROR, G_IO_ERROR_FAILED,\n\t\t\t\t   _(\"Unknown error on connect\"));\n\t    }\n\t}\n      g_task_return_error (data->task, error);\n      g_object_unref (data->task);\n      return;\n    }\n\n  g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_RESOLVED,\n\t\t\t      data->connectable, NULL);\n\n  if (G_IS_PROXY_ADDRESS (address) &&\n      data->client->priv->enable_proxy)\n    data->proxy_addr = g_object_ref (G_PROXY_ADDRESS (address));\n\n  g_clear_error (&data->last_error);\n\n  socket = create_socket (data->client, address, &data->last_error);\n  if (socket == NULL)\n    {\n      g_object_unref (address);\n      enumerator_next_async (data, FALSE);\n      return;\n    }\n\n  attempt = connection_attempt_new ();\n  attempt->data = data;\n  attempt->socket = socket;\n  attempt->address = address;\n  attempt->cancellable = g_cancellable_new ();\n  attempt->connection = (GIOStream *)g_socket_connection_factory_create_connection (socket);\n  attempt->timeout_source = g_timeout_source_new (HAPPY_EYEBALLS_CONNECTION_ATTEMPT_TIMEOUT_MS);\n  g_source_set_callback (attempt->timeout_source, on_connection_attempt_timeout, attempt, NULL);\n  g_source_attach (attempt->timeout_source, g_main_context_get_thread_default ());\n  data->connection_attempts = g_slist_append (data->connection_attempts, attempt);\n\n  if (g_task_get_cancellable (data->task))\n    g_cancellable_connect (g_task_get_cancellable (data->task), G_CALLBACK (on_connection_cancelled),\n                           g_object_ref (attempt->cancellable), g_object_unref);\n\n  g_socket_connection_set_cached_remote_address ((GSocketConnection *)attempt->connection, address);\n  g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_CONNECTING, data->connectable, attempt->connection);\n  g_socket_connection_connect_async (G_SOCKET_CONNECTION (attempt->connection),\n\t\t\t\t     address,\n\t\t\t\t     attempt->cancellable,\n\t\t\t\t     g_socket_client_connected_callback, connection_attempt_ref (attempt));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n   ConnectionAttempt *attempt;\n   GError *error = NULL;\n \n-  if (g_task_return_error_if_cancelled (data->task))\n+  if (task_completed_or_cancelled (data->task))\n     {\n       g_object_unref (data->task);\n       return;\n@@ -20,7 +20,10 @@\n   if (address == NULL)\n     {\n       if (data->connection_attempts)\n-        return;\n+        {\n+          g_object_unref (data->task);\n+          return;\n+        }\n \n       g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_COMPLETE, data->connectable, NULL);\n       if (!error)\n@@ -54,7 +57,7 @@\n   if (socket == NULL)\n     {\n       g_object_unref (address);\n-      enumerator_next_async (data);\n+      enumerator_next_async (data, FALSE);\n       return;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "  if (g_task_return_error_if_cancelled (data->task))",
                "        return;",
                "      enumerator_next_async (data);"
            ],
            "added_lines": [
                "  if (task_completed_or_cancelled (data->task))",
                "        {",
                "          g_object_unref (data->task);",
                "          return;",
                "        }",
                "      enumerator_next_async (data, FALSE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9633",
        "func_name": "GNOME/glib/g_socket_client_tls_handshake_callback",
        "description": "gio/gsocketclient.c in GNOME GLib 2.59.2 does not ensure that a parent GTask remains alive during the execution of a connection-attempting enumeration, which allows remote attackers to cause a denial of service (g_socket_client_connected_callback mishandling and application crash) via a crafted web site, as demonstrated by GNOME Web (aka Epiphany).",
        "git_url": "https://github.com/GNOME/glib/commit/d553d92d6e9f53cbe5a34166fcb919ba652c6a8e",
        "commit_title": "gsocketclient: Fix criticals",
        "commit_text": " This ensures the parent GTask is kept alive as long as an enumeration is running and trying to connect.  Closes #1646 Closes #1649",
        "func_before": "static void\ng_socket_client_tls_handshake_callback (GObject      *object,\n\t\t\t\t\tGAsyncResult *result,\n\t\t\t\t\tgpointer      user_data)\n{\n  GSocketClientAsyncConnectData *data = user_data;\n\n  if (g_tls_connection_handshake_finish (G_TLS_CONNECTION (object),\n\t\t\t\t\t result,\n\t\t\t\t\t &data->last_error))\n    {\n      g_object_unref (data->connection);\n      data->connection = G_IO_STREAM (object);\n\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_TLS_HANDSHAKED, data->connectable, data->connection);\n      g_socket_client_async_connect_complete (data);\n    }\n  else\n    {\n      g_object_unref (object);\n      enumerator_next_async (data);\n    }\n}",
        "func": "static void\ng_socket_client_tls_handshake_callback (GObject      *object,\n\t\t\t\t\tGAsyncResult *result,\n\t\t\t\t\tgpointer      user_data)\n{\n  GSocketClientAsyncConnectData *data = user_data;\n\n  if (g_tls_connection_handshake_finish (G_TLS_CONNECTION (object),\n\t\t\t\t\t result,\n\t\t\t\t\t &data->last_error))\n    {\n      g_object_unref (data->connection);\n      data->connection = G_IO_STREAM (object);\n\n      g_socket_client_emit_event (data->client, G_SOCKET_CLIENT_TLS_HANDSHAKED, data->connectable, data->connection);\n      g_socket_client_async_connect_complete (data);\n    }\n  else\n    {\n      g_object_unref (object);\n      enumerator_next_async (data, FALSE);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,6 +18,6 @@\n   else\n     {\n       g_object_unref (object);\n-      enumerator_next_async (data);\n+      enumerator_next_async (data, FALSE);\n     }\n }",
        "diff_line_info": {
            "deleted_lines": [
                "      enumerator_next_async (data);"
            ],
            "added_lines": [
                "      enumerator_next_async (data, FALSE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11459",
        "func_name": "GNOME/evince/tiff_document_render",
        "description": "The tiff_document_render() and tiff_document_get_thumbnail() functions in the TIFF document backend in GNOME Evince through 3.32.0 did not handle errors from TIFFReadRGBAImageOriented(), leading to uninitialized memory use when processing certain TIFF image files.",
        "git_url": "https://github.com/GNOME/evince/commit/3e38d5ad724a042eebadcba8c2d57b0f48b7a8c7",
        "commit_title": "tiff: Handle failure from TIFFReadRGBAImageOriented",
        "commit_text": " The TIFFReadRGBAImageOriented function returns zero if it was unable to read the image. Return NULL in this case instead of displaying uninitialized memory.  Fixes #1129",
        "func_before": "static cairo_surface_t *\ntiff_document_render (EvDocument      *document,\n\t\t      EvRenderContext *rc)\n{\n\tTiffDocument *tiff_document = TIFF_DOCUMENT (document);\n\tint width, height;\n\tint scaled_width, scaled_height;\n\tfloat x_res, y_res;\n\tgint rowstride, bytes;\n\tguchar *pixels = NULL;\n\tguchar *p;\n\tint orientation;\n\tcairo_surface_t *surface;\n\tcairo_surface_t *rotated_surface;\n\tstatic const cairo_user_data_key_t key;\n\t\n\tg_return_val_if_fail (TIFF_IS_DOCUMENT (document), NULL);\n\tg_return_val_if_fail (tiff_document->tiff != NULL, NULL);\n  \n\tpush_handlers ();\n\tif (TIFFSetDirectory (tiff_document->tiff, rc->page->index) != 1) {\n\t\tpop_handlers ();\n\t\tg_warning(\"Failed to select page %d\", rc->page->index);\n\t\treturn NULL;\n\t}\n\n\tif (!TIFFGetField (tiff_document->tiff, TIFFTAG_IMAGEWIDTH, &width)) {\n\t\tpop_handlers ();\n\t\tg_warning(\"Failed to read image width\");\n\t\treturn NULL;\n\t}\n\n\tif (! TIFFGetField (tiff_document->tiff, TIFFTAG_IMAGELENGTH, &height)) {\n\t\tpop_handlers ();\n\t\tg_warning(\"Failed to read image height\");\n\t\treturn NULL;\n\t}\n\n\tif (! TIFFGetField (tiff_document->tiff, TIFFTAG_ORIENTATION, &orientation)) {\n\t\torientation = ORIENTATION_TOPLEFT;\n\t}\n\n\ttiff_document_get_resolution (tiff_document, &x_res, &y_res);\n\t\n\tpop_handlers ();\n  \n\t/* Sanity check the doc */\n\tif (width <= 0 || height <= 0) {\n\t\tg_warning(\"Invalid width or height.\");\n\t\treturn NULL;\n\t}\n\n\trowstride = cairo_format_stride_for_width (CAIRO_FORMAT_RGB24, width);\n\tif (rowstride / 4 != width) {\n\t\tg_warning(\"Overflow while rendering document.\");\n\t\t/* overflow, or cairo was changed in an unsupported way */\n\t\treturn NULL;                \n\t}\n\t\n\tif (height >= INT_MAX / rowstride) {\n\t\tg_warning(\"Overflow while rendering document.\");\n\t\t/* overflow */\n\t\treturn NULL;\n\t}\n\tbytes = height * rowstride;\n\t\n\tpixels = g_try_malloc (bytes);\n\tif (!pixels) {\n\t\tg_warning(\"Failed to allocate memory for rendering.\");\n\t\treturn NULL;\n\t}\n\t\n\tsurface = cairo_image_surface_create_for_data (pixels,\n\t\t\t\t\t\t       CAIRO_FORMAT_RGB24,\n\t\t\t\t\t\t       width, height,\n\t\t\t\t\t\t       rowstride);\n\tcairo_surface_set_user_data (surface, &key,\n\t\t\t\t     pixels, (cairo_destroy_func_t)g_free);\n\n\tTIFFReadRGBAImageOriented (tiff_document->tiff,\n\t\t\t\t   width, height,\n\t\t\t\t   (uint32 *)pixels,\n\t\t\t\t   orientation, 0);\n\tpop_handlers ();\n\n\t/* Convert the format returned by libtiff to\n\t* what cairo expects\n\t*/\n\tp = pixels;\n\twhile (p < pixels + bytes) {\n\t\tguint32 *pixel = (guint32*)p;\n\t\tguint8 r = TIFFGetR(*pixel);\n\t\tguint8 g = TIFFGetG(*pixel);\n\t\tguint8 b = TIFFGetB(*pixel);\n\t\tguint8 a = TIFFGetA(*pixel);\n\n\t\t*pixel = (a << 24) | (r << 16) | (g << 8) | b;\n\n\t\tp += 4;\n\t}\n\n\tev_render_context_compute_scaled_size (rc, width, height * (x_res / y_res),\n\t\t\t\t\t       &scaled_width, &scaled_height);\n\trotated_surface = ev_document_misc_surface_rotate_and_scale (surface,\n\t\t\t\t\t\t\t\t     scaled_width, scaled_height,\n\t\t\t\t\t\t\t\t     rc->rotation);\n\tcairo_surface_destroy (surface);\n\t\n\treturn rotated_surface;\n}",
        "func": "static cairo_surface_t *\ntiff_document_render (EvDocument      *document,\n\t\t      EvRenderContext *rc)\n{\n\tTiffDocument *tiff_document = TIFF_DOCUMENT (document);\n\tint width, height;\n\tint scaled_width, scaled_height;\n\tfloat x_res, y_res;\n\tgint rowstride, bytes;\n\tguchar *pixels = NULL;\n\tguchar *p;\n\tint orientation;\n\tcairo_surface_t *surface;\n\tcairo_surface_t *rotated_surface;\n\tstatic const cairo_user_data_key_t key;\n\t\n\tg_return_val_if_fail (TIFF_IS_DOCUMENT (document), NULL);\n\tg_return_val_if_fail (tiff_document->tiff != NULL, NULL);\n  \n\tpush_handlers ();\n\tif (TIFFSetDirectory (tiff_document->tiff, rc->page->index) != 1) {\n\t\tpop_handlers ();\n\t\tg_warning(\"Failed to select page %d\", rc->page->index);\n\t\treturn NULL;\n\t}\n\n\tif (!TIFFGetField (tiff_document->tiff, TIFFTAG_IMAGEWIDTH, &width)) {\n\t\tpop_handlers ();\n\t\tg_warning(\"Failed to read image width\");\n\t\treturn NULL;\n\t}\n\n\tif (! TIFFGetField (tiff_document->tiff, TIFFTAG_IMAGELENGTH, &height)) {\n\t\tpop_handlers ();\n\t\tg_warning(\"Failed to read image height\");\n\t\treturn NULL;\n\t}\n\n\tif (! TIFFGetField (tiff_document->tiff, TIFFTAG_ORIENTATION, &orientation)) {\n\t\torientation = ORIENTATION_TOPLEFT;\n\t}\n\n\ttiff_document_get_resolution (tiff_document, &x_res, &y_res);\n\t\n\tpop_handlers ();\n  \n\t/* Sanity check the doc */\n\tif (width <= 0 || height <= 0) {\n\t\tg_warning(\"Invalid width or height.\");\n\t\treturn NULL;\n\t}\n\n\trowstride = cairo_format_stride_for_width (CAIRO_FORMAT_RGB24, width);\n\tif (rowstride / 4 != width) {\n\t\tg_warning(\"Overflow while rendering document.\");\n\t\t/* overflow, or cairo was changed in an unsupported way */\n\t\treturn NULL;                \n\t}\n\t\n\tif (height >= INT_MAX / rowstride) {\n\t\tg_warning(\"Overflow while rendering document.\");\n\t\t/* overflow */\n\t\treturn NULL;\n\t}\n\tbytes = height * rowstride;\n\t\n\tpixels = g_try_malloc (bytes);\n\tif (!pixels) {\n\t\tg_warning(\"Failed to allocate memory for rendering.\");\n\t\treturn NULL;\n\t}\n\n\tif (!TIFFReadRGBAImageOriented (tiff_document->tiff,\n\t\t\t\t\twidth, height,\n\t\t\t\t\t(uint32 *)pixels,\n\t\t\t\t\torientation, 0)) {\n\t\tg_warning (\"Failed to read TIFF image.\");\n\t\tg_free (pixels);\n\t\treturn NULL;\n\t}\n\n\tsurface = cairo_image_surface_create_for_data (pixels,\n\t\t\t\t\t\t       CAIRO_FORMAT_RGB24,\n\t\t\t\t\t\t       width, height,\n\t\t\t\t\t\t       rowstride);\n\tcairo_surface_set_user_data (surface, &key,\n\t\t\t\t     pixels, (cairo_destroy_func_t)g_free);\n\tpop_handlers ();\n\n\t/* Convert the format returned by libtiff to\n\t* what cairo expects\n\t*/\n\tp = pixels;\n\twhile (p < pixels + bytes) {\n\t\tguint32 *pixel = (guint32*)p;\n\t\tguint8 r = TIFFGetR(*pixel);\n\t\tguint8 g = TIFFGetG(*pixel);\n\t\tguint8 b = TIFFGetB(*pixel);\n\t\tguint8 a = TIFFGetA(*pixel);\n\n\t\t*pixel = (a << 24) | (r << 16) | (g << 8) | b;\n\n\t\tp += 4;\n\t}\n\n\tev_render_context_compute_scaled_size (rc, width, height * (x_res / y_res),\n\t\t\t\t\t       &scaled_width, &scaled_height);\n\trotated_surface = ev_document_misc_surface_rotate_and_scale (surface,\n\t\t\t\t\t\t\t\t     scaled_width, scaled_height,\n\t\t\t\t\t\t\t\t     rc->rotation);\n\tcairo_surface_destroy (surface);\n\t\n\treturn rotated_surface;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -69,18 +69,22 @@\n \t\tg_warning(\"Failed to allocate memory for rendering.\");\n \t\treturn NULL;\n \t}\n-\t\n+\n+\tif (!TIFFReadRGBAImageOriented (tiff_document->tiff,\n+\t\t\t\t\twidth, height,\n+\t\t\t\t\t(uint32 *)pixels,\n+\t\t\t\t\torientation, 0)) {\n+\t\tg_warning (\"Failed to read TIFF image.\");\n+\t\tg_free (pixels);\n+\t\treturn NULL;\n+\t}\n+\n \tsurface = cairo_image_surface_create_for_data (pixels,\n \t\t\t\t\t\t       CAIRO_FORMAT_RGB24,\n \t\t\t\t\t\t       width, height,\n \t\t\t\t\t\t       rowstride);\n \tcairo_surface_set_user_data (surface, &key,\n \t\t\t\t     pixels, (cairo_destroy_func_t)g_free);\n-\n-\tTIFFReadRGBAImageOriented (tiff_document->tiff,\n-\t\t\t\t   width, height,\n-\t\t\t\t   (uint32 *)pixels,\n-\t\t\t\t   orientation, 0);\n \tpop_handlers ();\n \n \t/* Convert the format returned by libtiff to",
        "diff_line_info": {
            "deleted_lines": [
                "\t",
                "",
                "\tTIFFReadRGBAImageOriented (tiff_document->tiff,",
                "\t\t\t\t   width, height,",
                "\t\t\t\t   (uint32 *)pixels,",
                "\t\t\t\t   orientation, 0);"
            ],
            "added_lines": [
                "",
                "\tif (!TIFFReadRGBAImageOriented (tiff_document->tiff,",
                "\t\t\t\t\twidth, height,",
                "\t\t\t\t\t(uint32 *)pixels,",
                "\t\t\t\t\torientation, 0)) {",
                "\t\tg_warning (\"Failed to read TIFF image.\");",
                "\t\tg_free (pixels);",
                "\t\treturn NULL;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11459",
        "func_name": "GNOME/evince/tiff_document_get_thumbnail",
        "description": "The tiff_document_render() and tiff_document_get_thumbnail() functions in the TIFF document backend in GNOME Evince through 3.32.0 did not handle errors from TIFFReadRGBAImageOriented(), leading to uninitialized memory use when processing certain TIFF image files.",
        "git_url": "https://github.com/GNOME/evince/commit/3e38d5ad724a042eebadcba8c2d57b0f48b7a8c7",
        "commit_title": "tiff: Handle failure from TIFFReadRGBAImageOriented",
        "commit_text": " The TIFFReadRGBAImageOriented function returns zero if it was unable to read the image. Return NULL in this case instead of displaying uninitialized memory.  Fixes #1129",
        "func_before": "static GdkPixbuf *\ntiff_document_get_thumbnail (EvDocument      *document,\n\t\t\t     EvRenderContext *rc)\n{\n\tTiffDocument *tiff_document = TIFF_DOCUMENT (document);\n\tint width, height;\n\tint scaled_width, scaled_height;\n\tfloat x_res, y_res;\n\tgint rowstride, bytes;\n\tguchar *pixels = NULL;\n\tGdkPixbuf *pixbuf;\n\tGdkPixbuf *scaled_pixbuf;\n\tGdkPixbuf *rotated_pixbuf;\n\t\n\tpush_handlers ();\n\tif (TIFFSetDirectory (tiff_document->tiff, rc->page->index) != 1) {\n\t\tpop_handlers ();\n\t\treturn NULL;\n\t}\n\n\tif (!TIFFGetField (tiff_document->tiff, TIFFTAG_IMAGEWIDTH, &width)) {\n\t\tpop_handlers ();\n\t\treturn NULL;\n\t}\n\n\tif (! TIFFGetField (tiff_document->tiff, TIFFTAG_IMAGELENGTH, &height)) {\n\t\tpop_handlers ();\n\t\treturn NULL;\n\t}\n\n\ttiff_document_get_resolution (tiff_document, &x_res, &y_res);\n\t\n\tpop_handlers ();\n  \n\t/* Sanity check the doc */\n\tif (width <= 0 || height <= 0)\n\t\treturn NULL;                \n\n\tif (width >= INT_MAX / 4)\n\t\t/* overflow */\n\t\treturn NULL;                \n\trowstride = width * 4;\n        \n\tif (height >= INT_MAX / rowstride)\n\t\t/* overflow */\n\t\treturn NULL;                \n\tbytes = height * rowstride;\n\t\n\tpixels = g_try_malloc (bytes);\n\tif (!pixels)\n\t\treturn NULL;\n\t\n\tpixbuf = gdk_pixbuf_new_from_data (pixels, GDK_COLORSPACE_RGB, TRUE, 8, \n\t\t\t\t\t   width, height, rowstride,\n\t\t\t\t\t   (GdkPixbufDestroyNotify) g_free, NULL);\n\tTIFFReadRGBAImageOriented (tiff_document->tiff,\n\t\t\t\t   width, height,\n\t\t\t\t   (uint32 *)pixels,\n\t\t\t\t   ORIENTATION_TOPLEFT, 0);\n\tpop_handlers ();\n\n\tev_render_context_compute_scaled_size (rc, width, height * (x_res / y_res),\n\t\t\t\t\t       &scaled_width, &scaled_height);\n\tscaled_pixbuf = gdk_pixbuf_scale_simple (pixbuf,\n\t\t\t\t\t\t scaled_width, scaled_height,\n\t\t\t\t\t\t GDK_INTERP_BILINEAR);\n\tg_object_unref (pixbuf);\n\t\n\trotated_pixbuf = gdk_pixbuf_rotate_simple (scaled_pixbuf, 360 - rc->rotation);\n\tg_object_unref (scaled_pixbuf);\n\t\n\treturn rotated_pixbuf;\n}",
        "func": "static GdkPixbuf *\ntiff_document_get_thumbnail (EvDocument      *document,\n\t\t\t     EvRenderContext *rc)\n{\n\tTiffDocument *tiff_document = TIFF_DOCUMENT (document);\n\tint width, height;\n\tint scaled_width, scaled_height;\n\tfloat x_res, y_res;\n\tgint rowstride, bytes;\n\tguchar *pixels = NULL;\n\tGdkPixbuf *pixbuf;\n\tGdkPixbuf *scaled_pixbuf;\n\tGdkPixbuf *rotated_pixbuf;\n\t\n\tpush_handlers ();\n\tif (TIFFSetDirectory (tiff_document->tiff, rc->page->index) != 1) {\n\t\tpop_handlers ();\n\t\treturn NULL;\n\t}\n\n\tif (!TIFFGetField (tiff_document->tiff, TIFFTAG_IMAGEWIDTH, &width)) {\n\t\tpop_handlers ();\n\t\treturn NULL;\n\t}\n\n\tif (! TIFFGetField (tiff_document->tiff, TIFFTAG_IMAGELENGTH, &height)) {\n\t\tpop_handlers ();\n\t\treturn NULL;\n\t}\n\n\ttiff_document_get_resolution (tiff_document, &x_res, &y_res);\n\t\n\tpop_handlers ();\n  \n\t/* Sanity check the doc */\n\tif (width <= 0 || height <= 0)\n\t\treturn NULL;                \n\n\tif (width >= INT_MAX / 4)\n\t\t/* overflow */\n\t\treturn NULL;                \n\trowstride = width * 4;\n        \n\tif (height >= INT_MAX / rowstride)\n\t\t/* overflow */\n\t\treturn NULL;                \n\tbytes = height * rowstride;\n\t\n\tpixels = g_try_malloc (bytes);\n\tif (!pixels)\n\t\treturn NULL;\n\t\n\tif (!TIFFReadRGBAImageOriented (tiff_document->tiff,\n\t\t\t\t\twidth, height,\n\t\t\t\t\t(uint32 *)pixels,\n\t\t\t\t\tORIENTATION_TOPLEFT, 0)) {\n\t\tg_free (pixels);\n\t\treturn NULL;\n\t}\n\n\tpixbuf = gdk_pixbuf_new_from_data (pixels, GDK_COLORSPACE_RGB, TRUE, 8, \n\t\t\t\t\t   width, height, rowstride,\n\t\t\t\t\t   (GdkPixbufDestroyNotify) g_free, NULL);\n\tpop_handlers ();\n\n\tev_render_context_compute_scaled_size (rc, width, height * (x_res / y_res),\n\t\t\t\t\t       &scaled_width, &scaled_height);\n\tscaled_pixbuf = gdk_pixbuf_scale_simple (pixbuf,\n\t\t\t\t\t\t scaled_width, scaled_height,\n\t\t\t\t\t\t GDK_INTERP_BILINEAR);\n\tg_object_unref (pixbuf);\n\t\n\trotated_pixbuf = gdk_pixbuf_rotate_simple (scaled_pixbuf, 360 - rc->rotation);\n\tg_object_unref (scaled_pixbuf);\n\t\n\treturn rotated_pixbuf;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -50,13 +50,17 @@\n \tif (!pixels)\n \t\treturn NULL;\n \t\n+\tif (!TIFFReadRGBAImageOriented (tiff_document->tiff,\n+\t\t\t\t\twidth, height,\n+\t\t\t\t\t(uint32 *)pixels,\n+\t\t\t\t\tORIENTATION_TOPLEFT, 0)) {\n+\t\tg_free (pixels);\n+\t\treturn NULL;\n+\t}\n+\n \tpixbuf = gdk_pixbuf_new_from_data (pixels, GDK_COLORSPACE_RGB, TRUE, 8, \n \t\t\t\t\t   width, height, rowstride,\n \t\t\t\t\t   (GdkPixbufDestroyNotify) g_free, NULL);\n-\tTIFFReadRGBAImageOriented (tiff_document->tiff,\n-\t\t\t\t   width, height,\n-\t\t\t\t   (uint32 *)pixels,\n-\t\t\t\t   ORIENTATION_TOPLEFT, 0);\n \tpop_handlers ();\n \n \tev_render_context_compute_scaled_size (rc, width, height * (x_res / y_res),",
        "diff_line_info": {
            "deleted_lines": [
                "\tTIFFReadRGBAImageOriented (tiff_document->tiff,",
                "\t\t\t\t   width, height,",
                "\t\t\t\t   (uint32 *)pixels,",
                "\t\t\t\t   ORIENTATION_TOPLEFT, 0);"
            ],
            "added_lines": [
                "\tif (!TIFFReadRGBAImageOriented (tiff_document->tiff,",
                "\t\t\t\t\twidth, height,",
                "\t\t\t\t\t(uint32 *)pixels,",
                "\t\t\t\t\tORIENTATION_TOPLEFT, 0)) {",
                "\t\tg_free (pixels);",
                "\t\treturn NULL;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-17044",
        "func_name": "xen-project/xen/p2m_pod_demand_populate",
        "description": "An issue was discovered in Xen through 4.9.x allowing HVM guest OS users to cause a denial of service (infinite loop and host OS hang) by leveraging the mishandling of Populate on Demand (PoD) errors.",
        "git_url": "https://github.com/xen-project/xen/commit/a1c6c6768971ea387d7eba0803908ef0928b43ac",
        "commit_title": "x86/pod: prevent infinite loop when shattering large pages",
        "commit_text": " When populating pages, the PoD may need to split large ones using p2m_set_entry and request the caller to retry (see ept_get_entry for instance).  p2m_set_entry may fail to shatter if it is not possible to allocate memory for the new page table. However, the error is not propagated resulting to the callers to retry infinitely the PoD.  Prevent the infinite loop by return false when it is not possible to shatter the large mapping.  This is XSA-246. ",
        "func_before": "bool\np2m_pod_demand_populate(struct p2m_domain *p2m, gfn_t gfn,\n                        unsigned int order)\n{\n    struct domain *d = p2m->domain;\n    struct page_info *p = NULL; /* Compiler warnings */\n    gfn_t gfn_aligned = _gfn((gfn_x(gfn) >> order) << order);\n    mfn_t mfn;\n    unsigned long i;\n\n    ASSERT(gfn_locked_by_me(p2m, gfn));\n    pod_lock(p2m);\n\n    /*\n     * This check is done with the pod lock held.  This will make sure that\n     * even if d->is_dying changes under our feet, p2m_pod_empty_cache()\n     * won't start until we're done.\n     */\n    if ( unlikely(d->is_dying) )\n        goto out_fail;\n\n\n    /*\n     * Because PoD does not have cache list for 1GB pages, it has to remap\n     * 1GB region to 2MB chunks for a retry.\n     */\n    if ( order == PAGE_ORDER_1G )\n    {\n        pod_unlock(p2m);\n        /*\n         * Note that we are supposed to call p2m_set_entry() 512 times to\n         * split 1GB into 512 2MB pages here. But We only do once here because\n         * p2m_set_entry() should automatically shatter the 1GB page into\n         * 512 2MB pages. The rest of 511 calls are unnecessary.\n         *\n         * NOTE: In a fine-grained p2m locking scenario this operation\n         * may need to promote its locking from gfn->1g superpage\n         */\n        p2m_set_entry(p2m, gfn_aligned, INVALID_MFN, PAGE_ORDER_2M,\n                      p2m_populate_on_demand, p2m->default_access);\n        return true;\n    }\n\n    /* Only reclaim if we're in actual need of more cache. */\n    if ( p2m->pod.entry_count > p2m->pod.count )\n        pod_eager_reclaim(p2m);\n\n    /*\n     * Only sweep if we're actually out of memory.  Doing anything else\n     * causes unnecessary time and fragmentation of superpages in the p2m.\n     */\n    if ( p2m->pod.count == 0 )\n        p2m_pod_emergency_sweep(p2m);\n\n    /* If the sweep failed, give up. */\n    if ( p2m->pod.count == 0 )\n        goto out_of_memory;\n\n    /* Keep track of the highest gfn demand-populated by a guest fault */\n    p2m->pod.max_guest = gfn_max(gfn, p2m->pod.max_guest);\n\n    /*\n     * Get a page f/ the cache.  A NULL return value indicates that the\n     * 2-meg range should be marked singleton PoD, and retried.\n     */\n    if ( (p = p2m_pod_cache_get(p2m, order)) == NULL )\n        goto remap_and_retry;\n\n    mfn = page_to_mfn(p);\n\n    BUG_ON((mfn_x(mfn) & ((1UL << order) - 1)) != 0);\n\n    p2m_set_entry(p2m, gfn_aligned, mfn, order, p2m_ram_rw,\n                  p2m->default_access);\n\n    for( i = 0; i < (1UL << order); i++ )\n    {\n        set_gpfn_from_mfn(mfn_x(mfn) + i, gfn_x(gfn_aligned) + i);\n        paging_mark_dirty(d, mfn_add(mfn, i));\n    }\n\n    p2m->pod.entry_count -= (1UL << order);\n    BUG_ON(p2m->pod.entry_count < 0);\n\n    pod_eager_record(p2m, gfn_aligned, order);\n\n    if ( tb_init_done )\n    {\n        struct {\n            u64 gfn, mfn;\n            int d:16,order:16;\n        } t;\n\n        t.gfn = gfn_x(gfn);\n        t.mfn = mfn_x(mfn);\n        t.d = d->domain_id;\n        t.order = order;\n\n        __trace_var(TRC_MEM_POD_POPULATE, 0, sizeof(t), &t);\n    }\n\n    pod_unlock(p2m);\n    return true;\nout_of_memory:\n    pod_unlock(p2m);\n\n    printk(\"%s: Dom%d out of PoD memory! (tot=%\"PRIu32\" ents=%ld dom%d)\\n\",\n           __func__, d->domain_id, d->tot_pages, p2m->pod.entry_count,\n           current->domain->domain_id);\n    domain_crash(d);\n    return false;\nout_fail:\n    pod_unlock(p2m);\n    return false;\nremap_and_retry:\n    BUG_ON(order != PAGE_ORDER_2M);\n    pod_unlock(p2m);\n\n    /* Remap this 2-meg region in singleton chunks */\n    /*\n     * NOTE: In a p2m fine-grained lock scenario this might\n     * need promoting the gfn lock from gfn->2M superpage.\n     */\n    for ( i = 0; i < (1UL << order); i++ )\n        p2m_set_entry(p2m, gfn_add(gfn_aligned, i), INVALID_MFN, PAGE_ORDER_4K,\n                      p2m_populate_on_demand, p2m->default_access);\n    if ( tb_init_done )\n    {\n        struct {\n            u64 gfn;\n            int d:16;\n        } t;\n\n        t.gfn = gfn_x(gfn);\n        t.d = d->domain_id;\n\n        __trace_var(TRC_MEM_POD_SUPERPAGE_SPLINTER, 0, sizeof(t), &t);\n    }\n\n    return true;\n}",
        "func": "bool\np2m_pod_demand_populate(struct p2m_domain *p2m, gfn_t gfn,\n                        unsigned int order)\n{\n    struct domain *d = p2m->domain;\n    struct page_info *p = NULL; /* Compiler warnings */\n    gfn_t gfn_aligned = _gfn((gfn_x(gfn) >> order) << order);\n    mfn_t mfn;\n    unsigned long i;\n\n    ASSERT(gfn_locked_by_me(p2m, gfn));\n    pod_lock(p2m);\n\n    /*\n     * This check is done with the pod lock held.  This will make sure that\n     * even if d->is_dying changes under our feet, p2m_pod_empty_cache()\n     * won't start until we're done.\n     */\n    if ( unlikely(d->is_dying) )\n        goto out_fail;\n\n\n    /*\n     * Because PoD does not have cache list for 1GB pages, it has to remap\n     * 1GB region to 2MB chunks for a retry.\n     */\n    if ( order == PAGE_ORDER_1G )\n    {\n        pod_unlock(p2m);\n        /*\n         * Note that we are supposed to call p2m_set_entry() 512 times to\n         * split 1GB into 512 2MB pages here. But We only do once here because\n         * p2m_set_entry() should automatically shatter the 1GB page into\n         * 512 2MB pages. The rest of 511 calls are unnecessary.\n         *\n         * NOTE: In a fine-grained p2m locking scenario this operation\n         * may need to promote its locking from gfn->1g superpage\n         */\n        return !p2m_set_entry(p2m, gfn_aligned, INVALID_MFN, PAGE_ORDER_2M,\n                              p2m_populate_on_demand, p2m->default_access);\n    }\n\n    /* Only reclaim if we're in actual need of more cache. */\n    if ( p2m->pod.entry_count > p2m->pod.count )\n        pod_eager_reclaim(p2m);\n\n    /*\n     * Only sweep if we're actually out of memory.  Doing anything else\n     * causes unnecessary time and fragmentation of superpages in the p2m.\n     */\n    if ( p2m->pod.count == 0 )\n        p2m_pod_emergency_sweep(p2m);\n\n    /* If the sweep failed, give up. */\n    if ( p2m->pod.count == 0 )\n        goto out_of_memory;\n\n    /* Keep track of the highest gfn demand-populated by a guest fault */\n    p2m->pod.max_guest = gfn_max(gfn, p2m->pod.max_guest);\n\n    /*\n     * Get a page f/ the cache.  A NULL return value indicates that the\n     * 2-meg range should be marked singleton PoD, and retried.\n     */\n    if ( (p = p2m_pod_cache_get(p2m, order)) == NULL )\n        goto remap_and_retry;\n\n    mfn = page_to_mfn(p);\n\n    BUG_ON((mfn_x(mfn) & ((1UL << order) - 1)) != 0);\n\n    if ( p2m_set_entry(p2m, gfn_aligned, mfn, order, p2m_ram_rw,\n                       p2m->default_access) )\n    {\n        p2m_pod_cache_add(p2m, p, order);\n        goto out_fail;\n    }\n\n    for( i = 0; i < (1UL << order); i++ )\n    {\n        set_gpfn_from_mfn(mfn_x(mfn) + i, gfn_x(gfn_aligned) + i);\n        paging_mark_dirty(d, mfn_add(mfn, i));\n    }\n\n    p2m->pod.entry_count -= (1UL << order);\n    BUG_ON(p2m->pod.entry_count < 0);\n\n    pod_eager_record(p2m, gfn_aligned, order);\n\n    if ( tb_init_done )\n    {\n        struct {\n            u64 gfn, mfn;\n            int d:16,order:16;\n        } t;\n\n        t.gfn = gfn_x(gfn);\n        t.mfn = mfn_x(mfn);\n        t.d = d->domain_id;\n        t.order = order;\n\n        __trace_var(TRC_MEM_POD_POPULATE, 0, sizeof(t), &t);\n    }\n\n    pod_unlock(p2m);\n    return true;\nout_of_memory:\n    pod_unlock(p2m);\n\n    printk(\"%s: Dom%d out of PoD memory! (tot=%\"PRIu32\" ents=%ld dom%d)\\n\",\n           __func__, d->domain_id, d->tot_pages, p2m->pod.entry_count,\n           current->domain->domain_id);\n    domain_crash(d);\n    return false;\nout_fail:\n    pod_unlock(p2m);\n    return false;\nremap_and_retry:\n    BUG_ON(order != PAGE_ORDER_2M);\n    pod_unlock(p2m);\n\n    /*\n     * Remap this 2-meg region in singleton chunks. See the comment on the\n     * 1G page splitting path above for why a single call suffices.\n     *\n     * NOTE: In a p2m fine-grained lock scenario this might\n     * need promoting the gfn lock from gfn->2M superpage.\n     */\n    if ( p2m_set_entry(p2m, gfn_aligned, INVALID_MFN, PAGE_ORDER_4K,\n                       p2m_populate_on_demand, p2m->default_access) )\n        return false;\n\n    if ( tb_init_done )\n    {\n        struct {\n            u64 gfn;\n            int d:16;\n        } t;\n\n        t.gfn = gfn_x(gfn);\n        t.d = d->domain_id;\n\n        __trace_var(TRC_MEM_POD_SUPERPAGE_SPLINTER, 0, sizeof(t), &t);\n    }\n\n    return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,9 +36,8 @@\n          * NOTE: In a fine-grained p2m locking scenario this operation\n          * may need to promote its locking from gfn->1g superpage\n          */\n-        p2m_set_entry(p2m, gfn_aligned, INVALID_MFN, PAGE_ORDER_2M,\n-                      p2m_populate_on_demand, p2m->default_access);\n-        return true;\n+        return !p2m_set_entry(p2m, gfn_aligned, INVALID_MFN, PAGE_ORDER_2M,\n+                              p2m_populate_on_demand, p2m->default_access);\n     }\n \n     /* Only reclaim if we're in actual need of more cache. */\n@@ -70,8 +69,12 @@\n \n     BUG_ON((mfn_x(mfn) & ((1UL << order) - 1)) != 0);\n \n-    p2m_set_entry(p2m, gfn_aligned, mfn, order, p2m_ram_rw,\n-                  p2m->default_access);\n+    if ( p2m_set_entry(p2m, gfn_aligned, mfn, order, p2m_ram_rw,\n+                       p2m->default_access) )\n+    {\n+        p2m_pod_cache_add(p2m, p, order);\n+        goto out_fail;\n+    }\n \n     for( i = 0; i < (1UL << order); i++ )\n     {\n@@ -116,14 +119,17 @@\n     BUG_ON(order != PAGE_ORDER_2M);\n     pod_unlock(p2m);\n \n-    /* Remap this 2-meg region in singleton chunks */\n     /*\n+     * Remap this 2-meg region in singleton chunks. See the comment on the\n+     * 1G page splitting path above for why a single call suffices.\n+     *\n      * NOTE: In a p2m fine-grained lock scenario this might\n      * need promoting the gfn lock from gfn->2M superpage.\n      */\n-    for ( i = 0; i < (1UL << order); i++ )\n-        p2m_set_entry(p2m, gfn_add(gfn_aligned, i), INVALID_MFN, PAGE_ORDER_4K,\n-                      p2m_populate_on_demand, p2m->default_access);\n+    if ( p2m_set_entry(p2m, gfn_aligned, INVALID_MFN, PAGE_ORDER_4K,\n+                       p2m_populate_on_demand, p2m->default_access) )\n+        return false;\n+\n     if ( tb_init_done )\n     {\n         struct {",
        "diff_line_info": {
            "deleted_lines": [
                "        p2m_set_entry(p2m, gfn_aligned, INVALID_MFN, PAGE_ORDER_2M,",
                "                      p2m_populate_on_demand, p2m->default_access);",
                "        return true;",
                "    p2m_set_entry(p2m, gfn_aligned, mfn, order, p2m_ram_rw,",
                "                  p2m->default_access);",
                "    /* Remap this 2-meg region in singleton chunks */",
                "    for ( i = 0; i < (1UL << order); i++ )",
                "        p2m_set_entry(p2m, gfn_add(gfn_aligned, i), INVALID_MFN, PAGE_ORDER_4K,",
                "                      p2m_populate_on_demand, p2m->default_access);"
            ],
            "added_lines": [
                "        return !p2m_set_entry(p2m, gfn_aligned, INVALID_MFN, PAGE_ORDER_2M,",
                "                              p2m_populate_on_demand, p2m->default_access);",
                "    if ( p2m_set_entry(p2m, gfn_aligned, mfn, order, p2m_ram_rw,",
                "                       p2m->default_access) )",
                "    {",
                "        p2m_pod_cache_add(p2m, p, order);",
                "        goto out_fail;",
                "    }",
                "     * Remap this 2-meg region in singleton chunks. See the comment on the",
                "     * 1G page splitting path above for why a single call suffices.",
                "     *",
                "    if ( p2m_set_entry(p2m, gfn_aligned, INVALID_MFN, PAGE_ORDER_4K,",
                "                       p2m_populate_on_demand, p2m->default_access) )",
                "        return false;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-17083",
        "func_name": "wireshark/process_netbios_name",
        "description": "In Wireshark 2.4.0 to 2.4.2 and 2.2.0 to 2.2.10, the NetBIOS dissector could crash. This was addressed in epan/dissectors/packet-netbios.c by ensuring that write operations are bounded by the beginning of a buffer.",
        "git_url": "https://github.com/wireshark/wireshark/commit/79768d63d14fbce6bf7fb4d4a1c86be0c5205eb3",
        "commit_title": "NetBIOS: Don't write past the beginning of a buffer.",
        "commit_text": " Make sure process_netbios_name doesn't write past the beginning of its buffer.  Bug: 14249 (cherry picked from commit b59dc97dfef3bcce71cd393f4d2493e7ba1a8f82)",
        "func_before": "int\nprocess_netbios_name(const guchar *name_ptr, char *name_ret, int name_ret_len)\n{\n\tint    i;\n\tint    name_type = *(name_ptr + NETBIOS_NAME_LEN - 1);\n\tguchar name_char;\n\tstatic const char hex_digits[16] = {'0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f'};\n\n\tfor (i = 0; i < NETBIOS_NAME_LEN - 1; i++) {\n\t\tname_char = *name_ptr++;\n\t\tif (name_char >= ' ' && name_char <= '~') {\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = name_char;\n\t\t} else {\n\t\t\t/* It's not printable; show it as <XX>, where\n\t\t\t   XX is the value in hex. */\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = '<';\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = hex_digits[(name_char >> 4)];\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = hex_digits[(name_char & 0x0F)];\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = '>';\n\t\t}\n\t}\n\t*name_ret = '\\0';\n\n\t/* Remove trailing space characters from name. */\n\n\tname_ret--;\n\n\tfor (i = 0; i < NETBIOS_NAME_LEN - 1; i++) {\n\t\tif (*name_ret != ' ') {\n\t\t\t*(name_ret + 1) = 0;\n\t\t\tbreak;\n\t\t}\n\t\tname_ret--;\n\t}\n\n\treturn name_type;\n}",
        "func": "int\nprocess_netbios_name(const guchar *name_ptr, char *name_ret, int name_ret_len)\n{\n\tint    i;\n\tint    name_type = *(name_ptr + NETBIOS_NAME_LEN - 1);\n\tguchar name_char;\n\tchar  *name_ret_orig = name_ret;\n\tstatic const char hex_digits[16] = {'0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f'};\n\n\tfor (i = 0; i < NETBIOS_NAME_LEN - 1; i++) {\n\t\tname_char = *name_ptr++;\n\t\tif (name_char >= ' ' && name_char <= '~') {\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = name_char;\n\t\t} else {\n\t\t\t/* It's not printable; show it as <XX>, where\n\t\t\t   XX is the value in hex. */\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = '<';\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = hex_digits[(name_char >> 4)];\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = hex_digits[(name_char & 0x0F)];\n\t\t\tif (--name_ret_len > 0)\n\t\t\t\t*name_ret++ = '>';\n\t\t}\n\t}\n\t*name_ret = '\\0';\n\n\t/* Remove trailing space characters from name. */\n\n\tname_ret--;\n\n\twhile (name_ret >= name_ret_orig) {\n\t\tif (*name_ret != ' ') {\n\t\t\t*(name_ret + 1) = 0;\n\t\t\tbreak;\n\t\t}\n\t\tname_ret--;\n\t}\n\n\treturn name_type;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,7 @@\n \tint    i;\n \tint    name_type = *(name_ptr + NETBIOS_NAME_LEN - 1);\n \tguchar name_char;\n+\tchar  *name_ret_orig = name_ret;\n \tstatic const char hex_digits[16] = {'0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f'};\n \n \tfor (i = 0; i < NETBIOS_NAME_LEN - 1; i++) {\n@@ -30,7 +31,7 @@\n \n \tname_ret--;\n \n-\tfor (i = 0; i < NETBIOS_NAME_LEN - 1; i++) {\n+\twhile (name_ret >= name_ret_orig) {\n \t\tif (*name_ret != ' ') {\n \t\t\t*(name_ret + 1) = 0;\n \t\t\tbreak;",
        "diff_line_info": {
            "deleted_lines": [
                "\tfor (i = 0; i < NETBIOS_NAME_LEN - 1; i++) {"
            ],
            "added_lines": [
                "\tchar  *name_ret_orig = name_ret;",
                "\twhile (name_ret >= name_ret_orig) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-17084",
        "func_name": "wireshark/dissect_mpa_fpdu",
        "description": "In Wireshark 2.4.0 to 2.4.2 and 2.2.0 to 2.2.10, the IWARP_MPA dissector could crash. This was addressed in epan/dissectors/packet-iwarp-mpa.c by validating a ULPDU length.",
        "git_url": "https://github.com/wireshark/wireshark/commit/8502fe94ef9e431860921507e1a351c5e3f5c634",
        "commit_title": "packet-iwarp-mpa.c:  Stop FPDU dissection if the ULPDU_LENGTH field does NOT contain what is expected",
        "commit_text": " Bug: 14236 (cherry picked from commit f23a6e193f90a02542c85cad07cb073abd6eb678)",
        "func_before": "static guint16\ndissect_mpa_fpdu(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree,\n\t\tmpa_state_t *state, struct tcpinfo *tcpinfo, guint8 endpoint)\n{\n\tproto_item *mpa_item = NULL;\n\tproto_item *mpa_header_item = NULL;\n\n\tproto_tree *mpa_tree = NULL;\n\tproto_tree *mpa_header_tree = NULL;\n\n\tguint8 pad_length;\n\tguint16 ulpdu_length, exp_ulpdu_length;\n\tguint32 offset, total_length;\n\tguint32 num_of_m = 0;\n\n\t/*\n\t * Initialize starting offset for this FPDU. Deals with the case that this\n\t * FPDU may start with a Marker instead of the ULPDU_LENTH header field.\n\t */\n\tif (state->minfo[endpoint].valid\n\t\t\t&& get_first_marker_offset(state, tcpinfo, endpoint) == 0) {\n\t\toffset = MPA_MARKER_LEN;\n\t} else {\n\t\toffset = 0;\n\t}\n\n\t/* get ULPDU length of this FPDU */\n\tulpdu_length = (guint16) tvb_get_ntohs(tvb, offset);\n\n\tmpa_packetlist(pinfo, MPA_FPDU);\n\n\tif (state->minfo[endpoint].valid) {\n\t\tnum_of_m = number_of_markers(state, tcpinfo, endpoint);\n\t}\n\n\n\t\t/*\n\t\t * Stop FPDU dissection if the read ULPDU_LENGTH field does NOT contain\n\t\t * what is expected.\n\t\t * Reasons for getting a wrong ULPDU_LENGTH can be lost packets (because\n\t\t * libpcap was not able to capture every packet) or lost alignment (the\n\t\t * MPA FPDU header does not start right after TCP header).\n\t\t * We consider the above to be an error since we make the assumption\n\t\t * that\texactly one MPA FPDU is contained in one TCP segment and starts\n\t\t * always either with a Marker or the ULPDU_LENGTH header field.\n\t\t */\n\t\tpad_length = fpdu_pad_length(ulpdu_length);\n\t\texp_ulpdu_length = expected_ulpdu_length(state, tcpinfo, endpoint);\n\t\tif (!exp_ulpdu_length || exp_ulpdu_length != (ulpdu_length + pad_length)) {\n\t\t\tproto_tree_add_expert_format(tree, pinfo, &ei_mpa_bad_length, tvb, offset,\n\t\t\t\tMPA_ULPDU_LENGTH_LEN,\n\t\t\t\t\"[ULPDU length [%u] field does not contain the expected length[%u]]\",\n\t\t\t\texp_ulpdu_length, ulpdu_length + pad_length);\n\t\t}\n\n\t\tmpa_item = proto_tree_add_item(tree, proto_iwarp_mpa, tvb, 0,\n\t\t\t\t-1, ENC_NA);\n\t\tmpa_tree = proto_item_add_subtree(mpa_item, ett_mpa);\n\n\t\tmpa_header_item = proto_tree_add_item(mpa_tree, hf_mpa_fpdu,\n\t\t\t\ttvb, offset, -1, ENC_NA);\n\t\tmpa_header_tree = proto_item_add_subtree(mpa_header_item,\n\t\t\t\tett_mpa);\n\n\t\t/* ULPDU Length header field */\n\t\tproto_tree_add_uint(mpa_header_tree,\n\t\t\t\thf_mpa_ulpdu_length, tvb, offset,\n\t\t\t\tMPA_ULPDU_LENGTH_LEN, ulpdu_length);\n\n\t\t/* Markers are present in this FPDU */\n\t\tif (state->minfo[endpoint].valid && num_of_m > 0) {\n\n\t\t\ttotal_length = fpdu_total_length(tcpinfo);\n\n\t\t\tif (pad_length > 0) {\n\t\t\t\tproto_tree_add_item(mpa_header_tree, hf_mpa_pad,\n\t\t\t\t\t\ttvb, pad_offset(tcpinfo,\n\t\t\t\t\t\t\t\ttotal_length,\n\t\t\t\t\t\t\t\tpad_length),\n\t\t\t\t\t\t\t\tpad_length, ENC_NA);\n\t\t\t}\n\n\t\t\tdissect_fpdu_crc(tvb, mpa_header_tree, state,\n\t\t\t\t\ttotal_length-MPA_CRC_LEN, num_of_m * MPA_MARKER_LEN +\n\t\t\t\t\tulpdu_length + pad_length + MPA_ULPDU_LENGTH_LEN);\n\n\t\t\tdissect_fpdu_markers(tvb, mpa_tree, state, tcpinfo, endpoint);\n\n\t\t} else { /* Markers are not present or not enabled */\n\n\t\t\toffset += MPA_ULPDU_LENGTH_LEN + ulpdu_length;\n\n\t\t\tif (pad_length > 0) {\n\t\t\t\tproto_tree_add_item(mpa_header_tree, hf_mpa_pad, tvb, offset,\n\t\t\t\t\t\tpad_length, ENC_NA);\n\t\t\t\toffset += pad_length;\n\t\t\t}\n\n\t\t\tdissect_fpdu_crc(tvb, mpa_header_tree, state, offset,\n\t\t\t\t\tulpdu_length+pad_length+MPA_ULPDU_LENGTH_LEN);\n\t\t}\n\treturn ulpdu_length;\n}",
        "func": "static guint16\ndissect_mpa_fpdu(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree,\n\t\tmpa_state_t *state, struct tcpinfo *tcpinfo, guint8 endpoint)\n{\n\tproto_item *mpa_item = NULL;\n\tproto_item *mpa_header_item = NULL;\n\n\tproto_tree *mpa_tree = NULL;\n\tproto_tree *mpa_header_tree = NULL;\n\n\tguint8 pad_length;\n\tguint16 ulpdu_length, exp_ulpdu_length;\n\tguint32 offset, total_length;\n\tguint32 num_of_m = 0;\n\n\t/*\n\t * Initialize starting offset for this FPDU. Deals with the case that this\n\t * FPDU may start with a Marker instead of the ULPDU_LENTH header field.\n\t */\n\tif (state->minfo[endpoint].valid\n\t\t\t&& get_first_marker_offset(state, tcpinfo, endpoint) == 0) {\n\t\toffset = MPA_MARKER_LEN;\n\t} else {\n\t\toffset = 0;\n\t}\n\n\t/* get ULPDU length of this FPDU */\n\tulpdu_length = (guint16) tvb_get_ntohs(tvb, offset);\n\n\tif (state->minfo[endpoint].valid) {\n\t\tnum_of_m = number_of_markers(state, tcpinfo, endpoint);\n\t}\n\n\n\t\t/*\n\t\t * Stop FPDU dissection if the read ULPDU_LENGTH field does NOT contain\n\t\t * what is expected.\n\t\t * Reasons for getting a wrong ULPDU_LENGTH can be lost packets (because\n\t\t * libpcap was not able to capture every packet) or lost alignment (the\n\t\t * MPA FPDU header does not start right after TCP header).\n\t\t * We consider the above to be an error since we make the assumption\n\t\t * that\texactly one MPA FPDU is contained in one TCP segment and starts\n\t\t * always either with a Marker or the ULPDU_LENGTH header field.\n\t\t */\n\t\tpad_length = fpdu_pad_length(ulpdu_length);\n\t\texp_ulpdu_length = expected_ulpdu_length(state, tcpinfo, endpoint);\n\t\tif (!exp_ulpdu_length || exp_ulpdu_length != (ulpdu_length + pad_length)) {\n\t\t\treturn 0;\n\t\t}\n\n\t\tmpa_packetlist(pinfo, MPA_FPDU);\n\n\t\tmpa_item = proto_tree_add_item(tree, proto_iwarp_mpa, tvb, 0,\n\t\t\t\t-1, ENC_NA);\n\t\tmpa_tree = proto_item_add_subtree(mpa_item, ett_mpa);\n\n\t\tmpa_header_item = proto_tree_add_item(mpa_tree, hf_mpa_fpdu,\n\t\t\t\ttvb, offset, -1, ENC_NA);\n\t\tmpa_header_tree = proto_item_add_subtree(mpa_header_item,\n\t\t\t\tett_mpa);\n\n\t\t/* ULPDU Length header field */\n\t\tproto_tree_add_uint(mpa_header_tree,\n\t\t\t\thf_mpa_ulpdu_length, tvb, offset,\n\t\t\t\tMPA_ULPDU_LENGTH_LEN, ulpdu_length);\n\n\t\t/* Markers are present in this FPDU */\n\t\tif (state->minfo[endpoint].valid && num_of_m > 0) {\n\n\t\t\ttotal_length = fpdu_total_length(tcpinfo);\n\n\t\t\tif (pad_length > 0) {\n\t\t\t\tproto_tree_add_item(mpa_header_tree, hf_mpa_pad,\n\t\t\t\t\t\ttvb, pad_offset(tcpinfo,\n\t\t\t\t\t\t\t\ttotal_length,\n\t\t\t\t\t\t\t\tpad_length),\n\t\t\t\t\t\t\t\tpad_length, ENC_NA);\n\t\t\t}\n\n\t\t\tdissect_fpdu_crc(tvb, mpa_header_tree, state,\n\t\t\t\t\ttotal_length-MPA_CRC_LEN, num_of_m * MPA_MARKER_LEN +\n\t\t\t\t\tulpdu_length + pad_length + MPA_ULPDU_LENGTH_LEN);\n\n\t\t\tdissect_fpdu_markers(tvb, mpa_tree, state, tcpinfo, endpoint);\n\n\t\t} else { /* Markers are not present or not enabled */\n\n\t\t\toffset += MPA_ULPDU_LENGTH_LEN + ulpdu_length;\n\n\t\t\tif (pad_length > 0) {\n\t\t\t\tproto_tree_add_item(mpa_header_tree, hf_mpa_pad, tvb, offset,\n\t\t\t\t\t\tpad_length, ENC_NA);\n\t\t\t\toffset += pad_length;\n\t\t\t}\n\n\t\t\tdissect_fpdu_crc(tvb, mpa_header_tree, state, offset,\n\t\t\t\t\tulpdu_length+pad_length+MPA_ULPDU_LENGTH_LEN);\n\t\t}\n\treturn ulpdu_length;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,8 +27,6 @@\n \t/* get ULPDU length of this FPDU */\n \tulpdu_length = (guint16) tvb_get_ntohs(tvb, offset);\n \n-\tmpa_packetlist(pinfo, MPA_FPDU);\n-\n \tif (state->minfo[endpoint].valid) {\n \t\tnum_of_m = number_of_markers(state, tcpinfo, endpoint);\n \t}\n@@ -47,11 +45,10 @@\n \t\tpad_length = fpdu_pad_length(ulpdu_length);\n \t\texp_ulpdu_length = expected_ulpdu_length(state, tcpinfo, endpoint);\n \t\tif (!exp_ulpdu_length || exp_ulpdu_length != (ulpdu_length + pad_length)) {\n-\t\t\tproto_tree_add_expert_format(tree, pinfo, &ei_mpa_bad_length, tvb, offset,\n-\t\t\t\tMPA_ULPDU_LENGTH_LEN,\n-\t\t\t\t\"[ULPDU length [%u] field does not contain the expected length[%u]]\",\n-\t\t\t\texp_ulpdu_length, ulpdu_length + pad_length);\n+\t\t\treturn 0;\n \t\t}\n+\n+\t\tmpa_packetlist(pinfo, MPA_FPDU);\n \n \t\tmpa_item = proto_tree_add_item(tree, proto_iwarp_mpa, tvb, 0,\n \t\t\t\t-1, ENC_NA);",
        "diff_line_info": {
            "deleted_lines": [
                "\tmpa_packetlist(pinfo, MPA_FPDU);",
                "",
                "\t\t\tproto_tree_add_expert_format(tree, pinfo, &ei_mpa_bad_length, tvb, offset,",
                "\t\t\t\tMPA_ULPDU_LENGTH_LEN,",
                "\t\t\t\t\"[ULPDU length [%u] field does not contain the expected length[%u]]\",",
                "\t\t\t\texp_ulpdu_length, ulpdu_length + pad_length);"
            ],
            "added_lines": [
                "\t\t\treturn 0;",
                "",
                "\t\tmpa_packetlist(pinfo, MPA_FPDU);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-17085",
        "func_name": "wireshark/dissect_cip_safety_data",
        "description": "In Wireshark 2.4.0 to 2.4.2 and 2.2.0 to 2.2.10, the CIP Safety dissector could crash. This was addressed in epan/dissectors/packet-cipsafety.c by validating the packet length.",
        "git_url": "https://github.com/wireshark/wireshark/commit/f5939debe96e3c3953c6020818f1fbb80eb83ce8",
        "commit_title": "CIP Safety: check packet length before dissecting",
        "commit_text": " Otherwise we can call CRC functions with a negative value, leading to a segmentation fault.  Bug: 14250 (cherry picked from commit 041e3e7c27c78308d0d515171f52a39f8260782b)",
        "func_before": "static void\ndissect_cip_safety_data( proto_tree *tree, proto_item *item, tvbuff_t *tvb, int item_length, packet_info *pinfo)\n{\n   int base_length, io_data_size;\n   gboolean multicast = (((pntoh32(pinfo->dst.data)) & 0xf0000000) == 0xe0000000);\n   gboolean server_dir = FALSE;\n   enum enip_connid_type conn_type = ECIDT_UNKNOWN;\n   enum cip_safety_format_type format = CIP_SAFETY_BASE_FORMAT;\n   cip_safety_info_t* safety_info = (cip_safety_info_t*)p_get_proto_data(wmem_file_scope(), pinfo, proto_cipsafety, 0 );\n\n   /* Make entries in Protocol column and Info column on summary display */\n   col_set_str(pinfo->cinfo, COL_PROTOCOL, \"CIP Safety\");\n\n   /* determine the connection type as it affects the fields dissected */\n   if (safety_info != NULL)\n   {\n      conn_type = safety_info->conn_type;\n      format = safety_info->format;\n      server_dir = safety_info->server_dir;\n   }\n\n   /* compute the base packet length to determine what is actual I/O data */\n   base_length = multicast ? 12 : 6;\n\n   if (((conn_type == ECIDT_O2T) && (server_dir == FALSE)) ||\n       ((conn_type == ECIDT_T2O) && (server_dir == TRUE)))\n   {\n      /* consumer data */\n      dissect_ack_byte(tree, tvb, 0, pinfo);\n      proto_tree_add_item(tree, hf_cipsafety_consumer_time_value, tvb, 1, 2, ENC_LITTLE_ENDIAN);\n\n      switch (format)\n      {\n      case CIP_SAFETY_BASE_FORMAT:\n         proto_tree_add_item(tree, hf_cipsafety_ack_byte2, tvb, 3, 1, ENC_LITTLE_ENDIAN);\n         proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, 4, 2, ENC_LITTLE_ENDIAN);\n         break;\n      case CIP_SAFETY_EXTENDED_FORMAT:\n         proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, 3, 1, ENC_LITTLE_ENDIAN);\n         proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, 4, 1, ENC_LITTLE_ENDIAN);\n         proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, 5, 1, ENC_LITTLE_ENDIAN);\n         break;\n      }\n   }\n   else if (((conn_type == ECIDT_O2T) && (server_dir == TRUE)) ||\n            ((conn_type == ECIDT_T2O) && (server_dir == FALSE)))\n   {\n      /* producer data */\n      switch (format)\n      {\n      case CIP_SAFETY_BASE_FORMAT:\n         if (item_length-base_length <= 2)\n         {\n            /* Short Format (1-2 bytes I/O data) */\n            proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, item_length-base_length, ENC_NA);\n            dissect_mode_byte(tree, tvb, item_length-base_length, pinfo);\n\n            proto_tree_add_item(tree, hf_cipsafety_crc_s1,    tvb, item_length-base_length+1, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s2,    tvb, item_length-base_length+2, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_timestamp, tvb, item_length-base_length+3, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s1,    tvb, item_length-base_length+5, 1, ENC_LITTLE_ENDIAN);\n\n            if (multicast)\n            {\n               dissect_mcast_byte(tree, tvb, item_length-6, pinfo);\n               proto_tree_add_item(tree, hf_cipsafety_time_correction, tvb, item_length-5, 2, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_mcast_byte2,     tvb, item_length-3, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s3,          tvb, item_length-2, 2, ENC_LITTLE_ENDIAN);\n            }\n         }\n         else\n         {\n            /* Long Format (3-250 bytes I/O data) */\n            if (item_length%2 == 1)\n            {\n               /* Malformed packet */\n               expert_add_info(pinfo, item, &ei_mal_io);\n               return;\n            }\n\n            io_data_size = multicast ? ((item_length-14)/2) : ((item_length-8)/2);\n\n            proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, io_data_size, ENC_NA);\n            dissect_mode_byte(tree, tvb, io_data_size, pinfo);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, io_data_size+1, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_complement_data, tvb, io_data_size+3, io_data_size, ENC_NA);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, (io_data_size*2)+3, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_timestamp, tvb, (io_data_size*2)+5, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s1, tvb, (io_data_size*2)+7, 1, ENC_LITTLE_ENDIAN);\n\n            if (multicast)\n            {\n               dissect_mcast_byte(tree, tvb, (io_data_size*2)+5, pinfo);\n               proto_tree_add_item(tree, hf_cipsafety_time_correction, tvb, (io_data_size*2)+6, 2, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_mcast_byte2, tvb, (io_data_size*2)+8, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, (io_data_size*2)+9, 2, ENC_LITTLE_ENDIAN);\n            }\n         }\n         break;\n      case CIP_SAFETY_EXTENDED_FORMAT:\n         if (item_length-base_length <= 2)\n         {\n            /* Short Format (1-2 bytes I/O data) */\n            proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, item_length-base_length, ENC_NA);\n            dissect_mode_byte(tree, tvb, item_length-base_length, pinfo);\n\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, item_length-base_length+1, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, item_length-base_length+2, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_timestamp, tvb, item_length-base_length+3, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, item_length-base_length+5, 1, ENC_LITTLE_ENDIAN);\n\n            if (multicast)\n            {\n               dissect_mcast_byte(tree, tvb, item_length-6, pinfo);\n               proto_tree_add_item(tree, hf_cipsafety_time_correction, tvb, item_length-5, 2, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, item_length-3, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, item_length-2, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, item_length-1, 1, ENC_LITTLE_ENDIAN);\n            }\n         }\n         else\n         {\n            /* Long Format (3-250 bytes I/O data) */\n            if (item_length%2 == 1)\n            {\n               /* Malformed packet */\n               expert_add_info(pinfo, item, &ei_mal_io);\n               return;\n            }\n\n            io_data_size = multicast ? ((item_length-14)/2) : ((item_length-8)/2);\n\n            proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, io_data_size, ENC_NA);\n            dissect_mode_byte(tree, tvb, io_data_size, pinfo);\n\n            proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, io_data_size+1, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_complement_data, tvb, io_data_size+3, io_data_size, ENC_NA);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, (io_data_size*2)+3, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, (io_data_size*2)+4, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_timestamp, tvb, (io_data_size*2)+5, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, (io_data_size*2)+7, 1, ENC_LITTLE_ENDIAN);\n\n            if (multicast)\n            {\n               dissect_mcast_byte(tree, tvb, (io_data_size*2)+8, pinfo);\n               proto_tree_add_item(tree, hf_cipsafety_time_correction, tvb, (io_data_size*2)+9, 2, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, (io_data_size*2)+11, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, (io_data_size*2)+12, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, (io_data_size*2)+13, 1, ENC_LITTLE_ENDIAN);\n            }\n         }\n         break;\n      }\n   }\n   else\n   {\n      /* Shouldn't happen, but at least dissect it as data */\n      proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, item_length, ENC_NA);\n   }\n}",
        "func": "static void\ndissect_cip_safety_data( proto_tree *tree, proto_item *item, tvbuff_t *tvb, int item_length, packet_info *pinfo)\n{\n   int base_length, io_data_size;\n   gboolean multicast = (((pntoh32(pinfo->dst.data)) & 0xf0000000) == 0xe0000000);\n   gboolean server_dir = FALSE;\n   enum enip_connid_type conn_type = ECIDT_UNKNOWN;\n   enum cip_safety_format_type format = CIP_SAFETY_BASE_FORMAT;\n   cip_safety_info_t* safety_info = (cip_safety_info_t*)p_get_proto_data(wmem_file_scope(), pinfo, proto_cipsafety, 0 );\n\n   /* Make entries in Protocol column and Info column on summary display */\n   col_set_str(pinfo->cinfo, COL_PROTOCOL, \"CIP Safety\");\n\n   /* determine the connection type as it affects the fields dissected */\n   if (safety_info != NULL)\n   {\n      conn_type = safety_info->conn_type;\n      format = safety_info->format;\n      server_dir = safety_info->server_dir;\n   }\n\n   /* compute the base packet length to determine what is actual I/O data */\n   base_length = multicast ? 12 : 6;\n\n   if (item_length <= base_length) {\n      expert_add_info(pinfo, item, &ei_mal_io);\n      return;\n   }\n\n   if (((conn_type == ECIDT_O2T) && (server_dir == FALSE)) ||\n       ((conn_type == ECIDT_T2O) && (server_dir == TRUE)))\n   {\n      /* consumer data */\n      dissect_ack_byte(tree, tvb, 0, pinfo);\n      proto_tree_add_item(tree, hf_cipsafety_consumer_time_value, tvb, 1, 2, ENC_LITTLE_ENDIAN);\n\n      switch (format)\n      {\n      case CIP_SAFETY_BASE_FORMAT:\n         proto_tree_add_item(tree, hf_cipsafety_ack_byte2, tvb, 3, 1, ENC_LITTLE_ENDIAN);\n         proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, 4, 2, ENC_LITTLE_ENDIAN);\n         break;\n      case CIP_SAFETY_EXTENDED_FORMAT:\n         proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, 3, 1, ENC_LITTLE_ENDIAN);\n         proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, 4, 1, ENC_LITTLE_ENDIAN);\n         proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, 5, 1, ENC_LITTLE_ENDIAN);\n         break;\n      }\n   }\n   else if (((conn_type == ECIDT_O2T) && (server_dir == TRUE)) ||\n            ((conn_type == ECIDT_T2O) && (server_dir == FALSE)))\n   {\n      /* producer data */\n      switch (format)\n      {\n      case CIP_SAFETY_BASE_FORMAT:\n         if (item_length-base_length <= 2)\n         {\n            /* Short Format (1-2 bytes I/O data) */\n            proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, item_length-base_length, ENC_NA);\n            dissect_mode_byte(tree, tvb, item_length-base_length, pinfo);\n\n            proto_tree_add_item(tree, hf_cipsafety_crc_s1,    tvb, item_length-base_length+1, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s2,    tvb, item_length-base_length+2, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_timestamp, tvb, item_length-base_length+3, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s1,    tvb, item_length-base_length+5, 1, ENC_LITTLE_ENDIAN);\n\n            if (multicast)\n            {\n               dissect_mcast_byte(tree, tvb, item_length-6, pinfo);\n               proto_tree_add_item(tree, hf_cipsafety_time_correction, tvb, item_length-5, 2, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_mcast_byte2,     tvb, item_length-3, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s3,          tvb, item_length-2, 2, ENC_LITTLE_ENDIAN);\n            }\n         }\n         else\n         {\n            /* Long Format (3-250 bytes I/O data) */\n            if (item_length%2 == 1)\n            {\n               /* Malformed packet */\n               expert_add_info(pinfo, item, &ei_mal_io);\n               return;\n            }\n\n            io_data_size = multicast ? ((item_length-14)/2) : ((item_length-8)/2);\n\n            proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, io_data_size, ENC_NA);\n            dissect_mode_byte(tree, tvb, io_data_size, pinfo);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, io_data_size+1, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_complement_data, tvb, io_data_size+3, io_data_size, ENC_NA);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, (io_data_size*2)+3, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_timestamp, tvb, (io_data_size*2)+5, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s1, tvb, (io_data_size*2)+7, 1, ENC_LITTLE_ENDIAN);\n\n            if (multicast)\n            {\n               dissect_mcast_byte(tree, tvb, (io_data_size*2)+5, pinfo);\n               proto_tree_add_item(tree, hf_cipsafety_time_correction, tvb, (io_data_size*2)+6, 2, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_mcast_byte2, tvb, (io_data_size*2)+8, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, (io_data_size*2)+9, 2, ENC_LITTLE_ENDIAN);\n            }\n         }\n         break;\n      case CIP_SAFETY_EXTENDED_FORMAT:\n         if (item_length-base_length <= 2)\n         {\n            /* Short Format (1-2 bytes I/O data) */\n            proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, item_length-base_length, ENC_NA);\n            dissect_mode_byte(tree, tvb, item_length-base_length, pinfo);\n\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, item_length-base_length+1, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, item_length-base_length+2, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_timestamp, tvb, item_length-base_length+3, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, item_length-base_length+5, 1, ENC_LITTLE_ENDIAN);\n\n            if (multicast)\n            {\n               dissect_mcast_byte(tree, tvb, item_length-6, pinfo);\n               proto_tree_add_item(tree, hf_cipsafety_time_correction, tvb, item_length-5, 2, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, item_length-3, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, item_length-2, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, item_length-1, 1, ENC_LITTLE_ENDIAN);\n            }\n         }\n         else\n         {\n            /* Long Format (3-250 bytes I/O data) */\n            if (item_length%2 == 1)\n            {\n               /* Malformed packet */\n               expert_add_info(pinfo, item, &ei_mal_io);\n               return;\n            }\n\n            io_data_size = multicast ? ((item_length-14)/2) : ((item_length-8)/2);\n\n            proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, io_data_size, ENC_NA);\n            dissect_mode_byte(tree, tvb, io_data_size, pinfo);\n\n            proto_tree_add_item(tree, hf_cipsafety_crc_s3, tvb, io_data_size+1, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_complement_data, tvb, io_data_size+3, io_data_size, ENC_NA);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, (io_data_size*2)+3, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, (io_data_size*2)+4, 1, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_timestamp, tvb, (io_data_size*2)+5, 2, ENC_LITTLE_ENDIAN);\n            proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, (io_data_size*2)+7, 1, ENC_LITTLE_ENDIAN);\n\n            if (multicast)\n            {\n               dissect_mcast_byte(tree, tvb, (io_data_size*2)+8, pinfo);\n               proto_tree_add_item(tree, hf_cipsafety_time_correction, tvb, (io_data_size*2)+9, 2, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_0, tvb, (io_data_size*2)+11, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_1, tvb, (io_data_size*2)+12, 1, ENC_LITTLE_ENDIAN);\n               proto_tree_add_item(tree, hf_cipsafety_crc_s5_2, tvb, (io_data_size*2)+13, 1, ENC_LITTLE_ENDIAN);\n            }\n         }\n         break;\n      }\n   }\n   else\n   {\n      /* Shouldn't happen, but at least dissect it as data */\n      proto_tree_add_item(tree, hf_cipsafety_data, tvb, 0, item_length, ENC_NA);\n   }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,6 +21,11 @@\n \n    /* compute the base packet length to determine what is actual I/O data */\n    base_length = multicast ? 12 : 6;\n+\n+   if (item_length <= base_length) {\n+      expert_add_info(pinfo, item, &ei_mal_io);\n+      return;\n+   }\n \n    if (((conn_type == ECIDT_O2T) && (server_dir == FALSE)) ||\n        ((conn_type == ECIDT_T2O) && (server_dir == TRUE)))",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "   if (item_length <= base_length) {",
                "      expert_add_info(pinfo, item, &ei_mal_io);",
                "      return;",
                "   }"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-32846",
        "func_name": "moby/hyperkit/pci_vtsock_proc_tx",
        "description": "HyperKit is a toolkit for embedding hypervisor capabilities in an application. In versions 0.20210107, function `pci_vtsock_proc_tx` in `virtio-sock` can lead to to uninitialized memory use. In this situation, there is a check for the return value to be less or equal to `VTSOCK_MAXSEGS`, but that check is not sufficient because the function can return `-1` if it finds an error it cannot recover from. Moreover, the negative return value will be used by `iovec_pull` in a while condition that can further lead to more corruption because the function is not designed to handle a negative `iov_len`. This issue may lead to a guest crashing the host causing a denial of service and, under certain circumstance, memory corruption. This issue is fixed in commit af5eba2360a7351c08dfd9767d9be863a50ebaba.",
        "git_url": "https://github.com/moby/hyperkit/commit/af5eba2360a7351c08dfd9767d9be863a50ebaba",
        "commit_title": "Fix virtio-sock pci_vtsock_proc_tx uninitialized memory use (GHSL-2021-057)",
        "commit_text": "",
        "func_before": "static void pci_vtsock_proc_tx(struct pci_vtsock_softc *sc,\n\t\t\t       struct vqueue_info *vq)\n{\n\tstruct pci_vtsock_sock *sock;\n\tstruct iovec iov_array[VTSOCK_MAXSEGS], *iov = iov_array;\n\tuint16_t idx, flags[VTSOCK_MAXSEGS];\n\tstruct virtio_sock_hdr hdr;\n\tint iovec_len;\n\tsize_t pulled;\n\n\tiovec_len = vq_getchain(vq, &idx, iov, VTSOCK_MAXSEGS, flags);\n\tassert(iovec_len <= VTSOCK_MAXSEGS);\n\n\tDPRINTF((\"TX: chain with %d buffers at idx %\"PRIx16\"\\n\",\n\t\t iovec_len, idx));\n\tdprint_chain(iov, iovec_len, \"TX\");\n\t//assert(iov[0].iov_len >= sizeof(*hdr));\n\t//hdr = iov[0].iov_base;\n\n\tpulled = iovec_pull(&iov, &iovec_len, &hdr, sizeof(hdr));\n\tassert(pulled == sizeof(hdr));\n\n\tdprint_header(&hdr, 1, \"TX\");\n\n\tdprint_iovec(iov, iovec_len, \"TX\");\n\n\tif (hdr.src_cid != sc->vssc_cfg.guest_cid ||\n\t    hdr.dst_cid != VMADDR_CID_HOST ||\n\t    hdr.type != VIRTIO_VSOCK_TYPE_STREAM) {\n\t\tDPRINTF((\"TX: Bad src/dst address/type\\n\"));\n\t\tsend_response_nosock(sc, VIRTIO_VSOCK_OP_RST,\n\t\t\t\t     hdr.type,\n\t\t\t\t     (struct vsock_addr) {\n\t\t\t\t\t     .cid = hdr.dst_cid,\n\t\t\t\t\t     .port =hdr.dst_port\n\t\t\t\t     },\n\t\t\t\t     (struct vsock_addr) {\n\t\t\t\t\t     .cid = hdr.src_cid,\n\t\t\t\t\t     .port =hdr.src_port\n\t\t\t\t     });\n\t\tvq_relchain(vq, idx, 0);\n\t\treturn;\n\t}\n\n\tsock = lookup_sock(sc, VIRTIO_VSOCK_TYPE_STREAM,\n\t\t\t   (struct vsock_addr) {\n\t\t\t\t   .cid = hdr.dst_cid,\n\t\t\t\t\t   .port =hdr.dst_port\n\t\t\t   },\n\t\t\t   (struct vsock_addr) {\n\t\t\t\t   .cid = hdr.src_cid,\n\t\t\t\t\t   .port =hdr.src_port\n\t\t\t   });\n\n\tif (sock) {\n\t\tsock->peer_buf_alloc = hdr.buf_alloc;\n\t\tsock->peer_fwd_cnt = hdr.fwd_cnt;\n\t}\n\n\tswitch (hdr.op) {\n\tcase VIRTIO_VSOCK_OP_INVALID:\n\t\tPPRINTF((\"TX: => INVALID\\n\"));\n\t\tgoto do_rst;\n\n\tcase VIRTIO_VSOCK_OP_REQUEST:\n\t\t/* Attempt to (re)connect existing sock? Naughty! */\n\t\t/* Or is it -- what are the semantics? */\n\t\tif (sock) {\n\t\t\tPPRINTF((\"TX: Attempt to reconnect sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\n\t\tif (hdr.dst_cid == sc->vssc_cfg.guest_cid) {\n\t\t\tPPRINTF((\"TX: Attempt to connect back to guest\\n!\"));\n\t\t\tgoto do_rst;\n\t\t}\n\n\t\tsock = connect_sock(sc,\n\t\t\t\t    (struct vsock_addr){\n\t\t\t\t\t    .cid = hdr.dst_cid, .port = hdr.dst_port\n\t\t\t\t    },\n\t\t\t\t    (struct vsock_addr){\n\t\t\t\t\t    .cid = hdr.src_cid, .port = hdr.src_port\n\t\t\t\t    }, hdr.buf_alloc, hdr.fwd_cnt);\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: Failed to open sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\n\t\tsend_response_sock(sc, VIRTIO_VSOCK_OP_RESPONSE, 0, sock);\n\t\tvq_relchain(vq, idx, 0);\n\t\t/* No rx kick required, send_response_sock did one */\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_RESPONSE:\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: RESPONSE to non-existent sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTING) {\n\t\t\tPPRINTF((\"TX: RESPONSE to non-connecting sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tPPRINTF((\"TX: SOCK connected (%d) \"PRIaddr\" <=> \"PRIaddr\"\\n\",\n\t\t\t sock->fd, FMTADDR(sock->local_addr), FMTADDR(sock->peer_addr)));\n\t\tsock->state = SOCK_CONNECTED;\n\t\tvq_relchain(vq, idx, 0);\n\t\tkick_rx(sc, \"new outgoing sock\");\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_RST:\n\t\t/* No response */\n\t\tif (!sock)\n\t\t\tPPRINTF((\"TX: RST to non-existent sock\\n\"));\n\t\tclose_sock(sc, sock, \"TX\");\n\t\tvq_relchain(vq, idx, 0);\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_SHUTDOWN:\n\t\tif (!sock) {\n\t\t\tDPRINTF((\"TX: SHUTDOWN to non-existent sock \"PRIcid\".\"PRIport\"\\n\",\n\t\t\t\t hdr.dst_cid, hdr.dst_port));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTED) {\n\t\t\tPPRINTF((\"TX: SHUTDOWN to non-connected sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (hdr.flags & ~VIRTIO_VSOCK_FLAG_SHUTDOWN_ALL) {\n\t\t\tPPRINTF((\"TX: SHUTDOWN with reserved flags %\"PRIx32\"\\n\",\n\t\t\t\t hdr.flags));\n\t\t\tgoto do_rst; /* ??? */\n\t\t}\n\t\tif (!(hdr.flags & VIRTIO_VSOCK_FLAG_SHUTDOWN_ALL)) {\n\t\t\tPPRINTF((\"TX: SHUTDOWN with no flags %\"PRIx32\"\\n\",\n\t\t\t\t hdr.flags));\n\t\t\tgoto do_rst; /* ??? */\n\t\t}\n\n\t\tshutdown_peer_local_fd(sock, hdr.flags, \"TX\");\n\n\t\t/* If the peer is now SHUTDOWN_ALL then we should send\n\t\t * a RST to the peer to finalise the shutdown.\n\t\t */\n\t\tif (sock->peer_shutdown == VIRTIO_VSOCK_FLAG_SHUTDOWN_ALL)\n\t\t\tgoto do_rst;\n\n\t\tvq_relchain(vq, idx, 0);\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_RW:\n\t{\n\t\tint rc;\n\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: RW with no sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTED) {\n\t\t\tPPRINTF((\"TX: RW to non-connected sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->peer_shutdown & VIRTIO_VSOCK_FLAG_SHUTDOWN_TX) {\n\t\t\tPPRINTF((\"TX: RW to socket with peer_shutdown.TX\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->local_shutdown & VIRTIO_VSOCK_FLAG_SHUTDOWN_RX) {\n\t\t\tPPRINTF((\"TX: RW to socket with local_shutdown.RX\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\trc = handle_write(sc, sock, hdr.len, iov, iovec_len);\n\t\tif (rc < 0) goto do_rst;\n\t\tvq_relchain(vq, idx, 0);\n\t\tif (rc == 1)\n\t\t\tset_credit_update_required(sc, sock);\n\t\tbreak;\n\t}\n\n\tcase VIRTIO_VSOCK_OP_CREDIT_UPDATE:\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: CREDIT_UPDATE to non-existent sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTED) {\n\t\t\tPPRINTF((\"TX: CREDIT_UPDATE to non-connected sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\t/* No response needed, we updated above */\n\t\tvq_relchain(vq, idx, 0);\n\t\t/* But kick rx thread to attempt to send more */\n\t\tsc->rx_kick_pending = true;\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_CREDIT_REQUEST:\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: CREDIT_REQUEST to non-existent sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTED) {\n\t\t\tPPRINTF((\"TX: CREDIT_REQUEST to non-connected sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tvq_relchain(vq, idx, 0);\n\t\tset_credit_update_required(sc, sock);\n\t\tbreak;\n\t}\n\n\tif (sock)\n\t\tput_sock(sock);\n\n\treturn;\n\ndo_rst:\n\tif (sock)\n\t\tsend_response_sock(sc, VIRTIO_VSOCK_OP_RST, 0, sock);\n\telse\n\t\tsend_response_nosock(sc, VIRTIO_VSOCK_OP_RST, hdr.type,\n\t\t\t\t     (struct vsock_addr) {\n\t\t\t\t\t     .cid = hdr.dst_cid,\n\t\t\t\t\t     .port =hdr.dst_port\n\t\t\t\t     },\n\t\t\t\t     (struct vsock_addr) {\n\t\t\t\t\t     .cid = hdr.src_cid,\n\t\t\t\t\t     .port =hdr.src_port\n\t\t\t\t     });\n\tvq_relchain(vq, idx, 0);\n\tclose_sock(sc, sock, \"TX\");\n\tif (sock) put_sock(sock);\n\treturn;\n}",
        "func": "static void pci_vtsock_proc_tx(struct pci_vtsock_softc *sc,\n\t\t\t       struct vqueue_info *vq)\n{\n\tstruct pci_vtsock_sock *sock;\n\tstruct iovec iov_array[VTSOCK_MAXSEGS], *iov = iov_array;\n\tuint16_t idx, flags[VTSOCK_MAXSEGS];\n\tstruct virtio_sock_hdr hdr;\n\tint iovec_len;\n\tsize_t pulled;\n\n\tiovec_len = vq_getchain(vq, &idx, iov, VTSOCK_MAXSEGS, flags);\n\tif (iovec_len < 0) {\n\t\tfprintf(stderr, \"TX: failed to get chain at idx %\"PRIx16\"\\n\", idx);\n\t\treturn;\n\t}\n\n\tassert(iovec_len <= VTSOCK_MAXSEGS);\n\n\tDPRINTF((\"TX: chain with %d buffers at idx %\"PRIx16\"\\n\",\n\t\t iovec_len, idx));\n\tdprint_chain(iov, iovec_len, \"TX\");\n\t//assert(iov[0].iov_len >= sizeof(*hdr));\n\t//hdr = iov[0].iov_base;\n\n\tpulled = iovec_pull(&iov, &iovec_len, &hdr, sizeof(hdr));\n\tassert(pulled == sizeof(hdr));\n\n\tdprint_header(&hdr, 1, \"TX\");\n\n\tdprint_iovec(iov, iovec_len, \"TX\");\n\n\tif (hdr.src_cid != sc->vssc_cfg.guest_cid ||\n\t    hdr.dst_cid != VMADDR_CID_HOST ||\n\t    hdr.type != VIRTIO_VSOCK_TYPE_STREAM) {\n\t\tDPRINTF((\"TX: Bad src/dst address/type\\n\"));\n\t\tsend_response_nosock(sc, VIRTIO_VSOCK_OP_RST,\n\t\t\t\t     hdr.type,\n\t\t\t\t     (struct vsock_addr) {\n\t\t\t\t\t     .cid = hdr.dst_cid,\n\t\t\t\t\t     .port =hdr.dst_port\n\t\t\t\t     },\n\t\t\t\t     (struct vsock_addr) {\n\t\t\t\t\t     .cid = hdr.src_cid,\n\t\t\t\t\t     .port =hdr.src_port\n\t\t\t\t     });\n\t\tvq_relchain(vq, idx, 0);\n\t\treturn;\n\t}\n\n\tsock = lookup_sock(sc, VIRTIO_VSOCK_TYPE_STREAM,\n\t\t\t   (struct vsock_addr) {\n\t\t\t\t   .cid = hdr.dst_cid,\n\t\t\t\t\t   .port =hdr.dst_port\n\t\t\t   },\n\t\t\t   (struct vsock_addr) {\n\t\t\t\t   .cid = hdr.src_cid,\n\t\t\t\t\t   .port =hdr.src_port\n\t\t\t   });\n\n\tif (sock) {\n\t\tsock->peer_buf_alloc = hdr.buf_alloc;\n\t\tsock->peer_fwd_cnt = hdr.fwd_cnt;\n\t}\n\n\tswitch (hdr.op) {\n\tcase VIRTIO_VSOCK_OP_INVALID:\n\t\tPPRINTF((\"TX: => INVALID\\n\"));\n\t\tgoto do_rst;\n\n\tcase VIRTIO_VSOCK_OP_REQUEST:\n\t\t/* Attempt to (re)connect existing sock? Naughty! */\n\t\t/* Or is it -- what are the semantics? */\n\t\tif (sock) {\n\t\t\tPPRINTF((\"TX: Attempt to reconnect sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\n\t\tif (hdr.dst_cid == sc->vssc_cfg.guest_cid) {\n\t\t\tPPRINTF((\"TX: Attempt to connect back to guest\\n!\"));\n\t\t\tgoto do_rst;\n\t\t}\n\n\t\tsock = connect_sock(sc,\n\t\t\t\t    (struct vsock_addr){\n\t\t\t\t\t    .cid = hdr.dst_cid, .port = hdr.dst_port\n\t\t\t\t    },\n\t\t\t\t    (struct vsock_addr){\n\t\t\t\t\t    .cid = hdr.src_cid, .port = hdr.src_port\n\t\t\t\t    }, hdr.buf_alloc, hdr.fwd_cnt);\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: Failed to open sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\n\t\tsend_response_sock(sc, VIRTIO_VSOCK_OP_RESPONSE, 0, sock);\n\t\tvq_relchain(vq, idx, 0);\n\t\t/* No rx kick required, send_response_sock did one */\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_RESPONSE:\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: RESPONSE to non-existent sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTING) {\n\t\t\tPPRINTF((\"TX: RESPONSE to non-connecting sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tPPRINTF((\"TX: SOCK connected (%d) \"PRIaddr\" <=> \"PRIaddr\"\\n\",\n\t\t\t sock->fd, FMTADDR(sock->local_addr), FMTADDR(sock->peer_addr)));\n\t\tsock->state = SOCK_CONNECTED;\n\t\tvq_relchain(vq, idx, 0);\n\t\tkick_rx(sc, \"new outgoing sock\");\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_RST:\n\t\t/* No response */\n\t\tif (!sock)\n\t\t\tPPRINTF((\"TX: RST to non-existent sock\\n\"));\n\t\tclose_sock(sc, sock, \"TX\");\n\t\tvq_relchain(vq, idx, 0);\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_SHUTDOWN:\n\t\tif (!sock) {\n\t\t\tDPRINTF((\"TX: SHUTDOWN to non-existent sock \"PRIcid\".\"PRIport\"\\n\",\n\t\t\t\t hdr.dst_cid, hdr.dst_port));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTED) {\n\t\t\tPPRINTF((\"TX: SHUTDOWN to non-connected sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (hdr.flags & ~VIRTIO_VSOCK_FLAG_SHUTDOWN_ALL) {\n\t\t\tPPRINTF((\"TX: SHUTDOWN with reserved flags %\"PRIx32\"\\n\",\n\t\t\t\t hdr.flags));\n\t\t\tgoto do_rst; /* ??? */\n\t\t}\n\t\tif (!(hdr.flags & VIRTIO_VSOCK_FLAG_SHUTDOWN_ALL)) {\n\t\t\tPPRINTF((\"TX: SHUTDOWN with no flags %\"PRIx32\"\\n\",\n\t\t\t\t hdr.flags));\n\t\t\tgoto do_rst; /* ??? */\n\t\t}\n\n\t\tshutdown_peer_local_fd(sock, hdr.flags, \"TX\");\n\n\t\t/* If the peer is now SHUTDOWN_ALL then we should send\n\t\t * a RST to the peer to finalise the shutdown.\n\t\t */\n\t\tif (sock->peer_shutdown == VIRTIO_VSOCK_FLAG_SHUTDOWN_ALL)\n\t\t\tgoto do_rst;\n\n\t\tvq_relchain(vq, idx, 0);\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_RW:\n\t{\n\t\tint rc;\n\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: RW with no sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTED) {\n\t\t\tPPRINTF((\"TX: RW to non-connected sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->peer_shutdown & VIRTIO_VSOCK_FLAG_SHUTDOWN_TX) {\n\t\t\tPPRINTF((\"TX: RW to socket with peer_shutdown.TX\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->local_shutdown & VIRTIO_VSOCK_FLAG_SHUTDOWN_RX) {\n\t\t\tPPRINTF((\"TX: RW to socket with local_shutdown.RX\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\trc = handle_write(sc, sock, hdr.len, iov, iovec_len);\n\t\tif (rc < 0) goto do_rst;\n\t\tvq_relchain(vq, idx, 0);\n\t\tif (rc == 1)\n\t\t\tset_credit_update_required(sc, sock);\n\t\tbreak;\n\t}\n\n\tcase VIRTIO_VSOCK_OP_CREDIT_UPDATE:\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: CREDIT_UPDATE to non-existent sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTED) {\n\t\t\tPPRINTF((\"TX: CREDIT_UPDATE to non-connected sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\t/* No response needed, we updated above */\n\t\tvq_relchain(vq, idx, 0);\n\t\t/* But kick rx thread to attempt to send more */\n\t\tsc->rx_kick_pending = true;\n\t\tbreak;\n\n\tcase VIRTIO_VSOCK_OP_CREDIT_REQUEST:\n\t\tif (!sock) {\n\t\t\tPPRINTF((\"TX: CREDIT_REQUEST to non-existent sock\\n\"));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tif (sock->state != SOCK_CONNECTED) {\n\t\t\tPPRINTF((\"TX: CREDIT_REQUEST to non-connected sock (state %d)\\n\",\n\t\t\t\t sock->state));\n\t\t\tgoto do_rst;\n\t\t}\n\t\tvq_relchain(vq, idx, 0);\n\t\tset_credit_update_required(sc, sock);\n\t\tbreak;\n\t}\n\n\tif (sock)\n\t\tput_sock(sock);\n\n\treturn;\n\ndo_rst:\n\tif (sock)\n\t\tsend_response_sock(sc, VIRTIO_VSOCK_OP_RST, 0, sock);\n\telse\n\t\tsend_response_nosock(sc, VIRTIO_VSOCK_OP_RST, hdr.type,\n\t\t\t\t     (struct vsock_addr) {\n\t\t\t\t\t     .cid = hdr.dst_cid,\n\t\t\t\t\t     .port =hdr.dst_port\n\t\t\t\t     },\n\t\t\t\t     (struct vsock_addr) {\n\t\t\t\t\t     .cid = hdr.src_cid,\n\t\t\t\t\t     .port =hdr.src_port\n\t\t\t\t     });\n\tvq_relchain(vq, idx, 0);\n\tclose_sock(sc, sock, \"TX\");\n\tif (sock) put_sock(sock);\n\treturn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,11 @@\n \tsize_t pulled;\n \n \tiovec_len = vq_getchain(vq, &idx, iov, VTSOCK_MAXSEGS, flags);\n+\tif (iovec_len < 0) {\n+\t\tfprintf(stderr, \"TX: failed to get chain at idx %\"PRIx16\"\\n\", idx);\n+\t\treturn;\n+\t}\n+\n \tassert(iovec_len <= VTSOCK_MAXSEGS);\n \n \tDPRINTF((\"TX: chain with %d buffers at idx %\"PRIx16\"\\n\",",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (iovec_len < 0) {",
                "\t\tfprintf(stderr, \"TX: failed to get chain at idx %\"PRIx16\"\\n\", idx);",
                "\t\treturn;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15695",
        "func_name": "CendioOssman/tigervnc/PixelFormat::is888",
        "description": "TigerVNC version prior to 1.10.1 is vulnerable to stack buffer overflow, which could be triggered from CMsgReader::readSetCursor. This vulnerability occurs due to insufficient sanitization of PixelFormat. Since remote attacker can choose offset from start of the buffer to start writing his values, exploitation of this vulnerability could potentially result into remote code execution. This attack appear to be exploitable via network connectivity.",
        "git_url": "https://github.com/CendioOssman/tigervnc/commit/05e28490873a861379c943bf616614b78b558b89",
        "commit_title": "Handle pixel formats with odd shift values",
        "commit_text": " Our fast paths assume that each channel fits in to a separate byte. That means the shift needs to be a multiple of 8. Start actually checking this so that a client cannot trip us up and possibly cause incorrect code exection.  Issue found by Pavel Cheremushkin from Kaspersky Lab.",
        "func_before": "bool PixelFormat::is888(void) const\n{\n  if (!trueColour)\n    return false;\n  if (bpp != 32)\n    return false;\n  if (depth != 24)\n    return false;\n  if (redMax != 255)\n    return false;\n  if (greenMax != 255)\n    return false;\n  if (blueMax != 255)\n    return false;\n\n  return true;\n}",
        "func": "bool PixelFormat::is888(void) const\n{\n  if (!trueColour)\n    return false;\n  if (bpp != 32)\n    return false;\n  if (depth != 24)\n    return false;\n  if (redMax != 255)\n    return false;\n  if (greenMax != 255)\n    return false;\n  if (blueMax != 255)\n    return false;\n  if ((redShift & 0x7) != 0)\n    return false;\n  if ((greenShift & 0x7) != 0)\n    return false;\n  if ((blueShift & 0x7) != 0)\n    return false;\n\n  return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,12 @@\n     return false;\n   if (blueMax != 255)\n     return false;\n+  if ((redShift & 0x7) != 0)\n+    return false;\n+  if ((greenShift & 0x7) != 0)\n+    return false;\n+  if ((blueShift & 0x7) != 0)\n+    return false;\n \n   return true;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if ((redShift & 0x7) != 0)",
                "    return false;",
                "  if ((greenShift & 0x7) != 0)",
                "    return false;",
                "  if ((blueShift & 0x7) != 0)",
                "    return false;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-18690",
        "func_name": "torvalds/linux/xfs_attr_shortform_addname",
        "description": "In the Linux kernel before 4.17, a local attacker able to set attributes on an xfs filesystem could make this filesystem non-operational until the next mount by triggering an unchecked error condition during an xfs attribute change, because xfs_attr_shortform_addname in fs/xfs/libxfs/xfs_attr.c mishandles ATTR_REPLACE operations with conversion of an attr from short to long form.",
        "git_url": "https://github.com/torvalds/linux/commit/7b38460dc8e4eafba06c78f8e37099d3b34d473c",
        "commit_title": "xfs: don't fail when converting shortform attr to long form during ATTR_REPLACE",
        "commit_text": " Kanda Motohiro reported that expanding a tiny xattr into a large xattr fails on XFS because we remove the tiny xattr from a shortform fork and then try to re-add it after converting the fork to extents format having not removed the ATTR_REPLACE flag.  This fails because the attr is no longer present, causing a fs shutdown.  This is derived from the patch in his bug report, but we really shouldn't ignore a nonzero retval from the remove call.  Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=199119",
        "func_before": "STATIC int\nxfs_attr_shortform_addname(xfs_da_args_t *args)\n{\n\tint newsize, forkoff, retval;\n\n\ttrace_xfs_attr_sf_addname(args);\n\n\tretval = xfs_attr_shortform_lookup(args);\n\tif ((args->flags & ATTR_REPLACE) && (retval == -ENOATTR)) {\n\t\treturn retval;\n\t} else if (retval == -EEXIST) {\n\t\tif (args->flags & ATTR_CREATE)\n\t\t\treturn retval;\n\t\tretval = xfs_attr_shortform_remove(args);\n\t\tASSERT(retval == 0);\n\t}\n\n\tif (args->namelen >= XFS_ATTR_SF_ENTSIZE_MAX ||\n\t    args->valuelen >= XFS_ATTR_SF_ENTSIZE_MAX)\n\t\treturn -ENOSPC;\n\n\tnewsize = XFS_ATTR_SF_TOTSIZE(args->dp);\n\tnewsize += XFS_ATTR_SF_ENTSIZE_BYNAME(args->namelen, args->valuelen);\n\n\tforkoff = xfs_attr_shortform_bytesfit(args->dp, newsize);\n\tif (!forkoff)\n\t\treturn -ENOSPC;\n\n\txfs_attr_shortform_add(args, forkoff);\n\treturn 0;\n}",
        "func": "STATIC int\nxfs_attr_shortform_addname(xfs_da_args_t *args)\n{\n\tint newsize, forkoff, retval;\n\n\ttrace_xfs_attr_sf_addname(args);\n\n\tretval = xfs_attr_shortform_lookup(args);\n\tif ((args->flags & ATTR_REPLACE) && (retval == -ENOATTR)) {\n\t\treturn retval;\n\t} else if (retval == -EEXIST) {\n\t\tif (args->flags & ATTR_CREATE)\n\t\t\treturn retval;\n\t\tretval = xfs_attr_shortform_remove(args);\n\t\tif (retval)\n\t\t\treturn retval;\n\t\t/*\n\t\t * Since we have removed the old attr, clear ATTR_REPLACE so\n\t\t * that the leaf format add routine won't trip over the attr\n\t\t * not being around.\n\t\t */\n\t\targs->flags &= ~ATTR_REPLACE;\n\t}\n\n\tif (args->namelen >= XFS_ATTR_SF_ENTSIZE_MAX ||\n\t    args->valuelen >= XFS_ATTR_SF_ENTSIZE_MAX)\n\t\treturn -ENOSPC;\n\n\tnewsize = XFS_ATTR_SF_TOTSIZE(args->dp);\n\tnewsize += XFS_ATTR_SF_ENTSIZE_BYNAME(args->namelen, args->valuelen);\n\n\tforkoff = xfs_attr_shortform_bytesfit(args->dp, newsize);\n\tif (!forkoff)\n\t\treturn -ENOSPC;\n\n\txfs_attr_shortform_add(args, forkoff);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,14 @@\n \t\tif (args->flags & ATTR_CREATE)\n \t\t\treturn retval;\n \t\tretval = xfs_attr_shortform_remove(args);\n-\t\tASSERT(retval == 0);\n+\t\tif (retval)\n+\t\t\treturn retval;\n+\t\t/*\n+\t\t * Since we have removed the old attr, clear ATTR_REPLACE so\n+\t\t * that the leaf format add routine won't trip over the attr\n+\t\t * not being around.\n+\t\t */\n+\t\targs->flags &= ~ATTR_REPLACE;\n \t}\n \n \tif (args->namelen >= XFS_ATTR_SF_ENTSIZE_MAX ||",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tASSERT(retval == 0);"
            ],
            "added_lines": [
                "\t\tif (retval)",
                "\t\t\treturn retval;",
                "\t\t/*",
                "\t\t * Since we have removed the old attr, clear ATTR_REPLACE so",
                "\t\t * that the leaf format add routine won't trip over the attr",
                "\t\t * not being around.",
                "\t\t */",
                "\t\targs->flags &= ~ATTR_REPLACE;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5678",
        "func_name": "openssl/DH_check_pub_key",
        "description": "Issue summary: Generating excessively long X9.42 DH keys or checking\nexcessively long X9.42 DH keys or parameters may be very slow.\n\nImpact summary: Applications that use the functions DH_generate_key() to\ngenerate an X9.42 DH key may experience long delays.  Likewise, applications\nthat use DH_check_pub_key(), DH_check_pub_key_ex() or EVP_PKEY_public_check()\nto check an X9.42 DH key or X9.42 DH parameters may experience long delays.\nWhere the key or parameters that are being checked have been obtained from\nan untrusted source this may lead to a Denial of Service.\n\nWhile DH_check() performs all the necessary checks (as of CVE-2023-3817),\nDH_check_pub_key() doesn't make any of these checks, and is therefore\nvulnerable for excessively large P and Q parameters.\n\nLikewise, while DH_generate_key() performs a check for an excessively large\nP, it doesn't check for an excessively large Q.\n\nAn application that calls DH_generate_key() or DH_check_pub_key() and\nsupplies a key or parameters obtained from an untrusted source could be\nvulnerable to a Denial of Service attack.\n\nDH_generate_key() and DH_check_pub_key() are also called by a number of\nother OpenSSL functions.  An application calling any of those other\nfunctions may similarly be affected.  The other functions affected by this\nare DH_check_pub_key_ex(), EVP_PKEY_public_check(), and EVP_PKEY_generate().\n\nAlso vulnerable are the OpenSSL pkey command line application when using the\n\"-pubcheck\" option, as well as the OpenSSL genpkey command line application.\n\nThe OpenSSL SSL/TLS implementation is not affected by this issue.\n\nThe OpenSSL 3.0 and 3.1 FIPS providers are not affected by this issue.\n\n",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=db925ae2e65d0d925adef429afc37f75bd1c2017",
        "commit_title": "",
        "commit_text": "Make DH_check_pub_key() and DH_generate_key() safer yet  We already check for an excessively large P in DH_generate_key(), but not in DH_check_pub_key(), and none of them check for an excessively large Q.  This change adds all the missing excessive size checks of P and Q.  It's to be noted that behaviours surrounding excessively sized P and Q differ.  DH_check() raises an error on the excessively sized P, but only sets a flag for the excessively sized Q.  This behaviour is mimicked in DH_check_pub_key().  (Merged from https://github.com/openssl/openssl/pull/22518)  (cherry picked from commit ddeb4b6c6d527e54ce9a99cba785c0f7776e54b6) ",
        "func_before": "int DH_check_pub_key(const DH *dh, const BIGNUM *pub_key, int *ret)\n{\n    return ossl_ffc_validate_public_key(&dh->params, pub_key, ret);\n}",
        "func": "int DH_check_pub_key(const DH *dh, const BIGNUM *pub_key, int *ret)\n{\n    /* Don't do any checks at all with an excessively large modulus */\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_CHECK_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        *ret = DH_MODULUS_TOO_LARGE | DH_CHECK_PUBKEY_INVALID;\n        return 0;\n    }\n\n    if (dh->params.q != NULL && BN_ucmp(dh->params.p, dh->params.q) < 0) {\n        *ret |= DH_CHECK_INVALID_Q_VALUE | DH_CHECK_PUBKEY_INVALID;\n        return 1;\n    }\n\n    return ossl_ffc_validate_public_key(&dh->params, pub_key, ret);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,16 @@\n int DH_check_pub_key(const DH *dh, const BIGNUM *pub_key, int *ret)\n {\n+    /* Don't do any checks at all with an excessively large modulus */\n+    if (BN_num_bits(dh->params.p) > OPENSSL_DH_CHECK_MAX_MODULUS_BITS) {\n+        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n+        *ret = DH_MODULUS_TOO_LARGE | DH_CHECK_PUBKEY_INVALID;\n+        return 0;\n+    }\n+\n+    if (dh->params.q != NULL && BN_ucmp(dh->params.p, dh->params.q) < 0) {\n+        *ret |= DH_CHECK_INVALID_Q_VALUE | DH_CHECK_PUBKEY_INVALID;\n+        return 1;\n+    }\n+\n     return ossl_ffc_validate_public_key(&dh->params, pub_key, ret);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    /* Don't do any checks at all with an excessively large modulus */",
                "    if (BN_num_bits(dh->params.p) > OPENSSL_DH_CHECK_MAX_MODULUS_BITS) {",
                "        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);",
                "        *ret = DH_MODULUS_TOO_LARGE | DH_CHECK_PUBKEY_INVALID;",
                "        return 0;",
                "    }",
                "",
                "    if (dh->params.q != NULL && BN_ucmp(dh->params.p, dh->params.q) < 0) {",
                "        *ret |= DH_CHECK_INVALID_Q_VALUE | DH_CHECK_PUBKEY_INVALID;",
                "        return 1;",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5678",
        "func_name": "openssl/ossl_dh_compute_key",
        "description": "Issue summary: Generating excessively long X9.42 DH keys or checking\nexcessively long X9.42 DH keys or parameters may be very slow.\n\nImpact summary: Applications that use the functions DH_generate_key() to\ngenerate an X9.42 DH key may experience long delays.  Likewise, applications\nthat use DH_check_pub_key(), DH_check_pub_key_ex() or EVP_PKEY_public_check()\nto check an X9.42 DH key or X9.42 DH parameters may experience long delays.\nWhere the key or parameters that are being checked have been obtained from\nan untrusted source this may lead to a Denial of Service.\n\nWhile DH_check() performs all the necessary checks (as of CVE-2023-3817),\nDH_check_pub_key() doesn't make any of these checks, and is therefore\nvulnerable for excessively large P and Q parameters.\n\nLikewise, while DH_generate_key() performs a check for an excessively large\nP, it doesn't check for an excessively large Q.\n\nAn application that calls DH_generate_key() or DH_check_pub_key() and\nsupplies a key or parameters obtained from an untrusted source could be\nvulnerable to a Denial of Service attack.\n\nDH_generate_key() and DH_check_pub_key() are also called by a number of\nother OpenSSL functions.  An application calling any of those other\nfunctions may similarly be affected.  The other functions affected by this\nare DH_check_pub_key_ex(), EVP_PKEY_public_check(), and EVP_PKEY_generate().\n\nAlso vulnerable are the OpenSSL pkey command line application when using the\n\"-pubcheck\" option, as well as the OpenSSL genpkey command line application.\n\nThe OpenSSL SSL/TLS implementation is not affected by this issue.\n\nThe OpenSSL 3.0 and 3.1 FIPS providers are not affected by this issue.\n\n",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=db925ae2e65d0d925adef429afc37f75bd1c2017",
        "commit_title": "",
        "commit_text": "Make DH_check_pub_key() and DH_generate_key() safer yet  We already check for an excessively large P in DH_generate_key(), but not in DH_check_pub_key(), and none of them check for an excessively large Q.  This change adds all the missing excessive size checks of P and Q.  It's to be noted that behaviours surrounding excessively sized P and Q differ.  DH_check() raises an error on the excessively sized P, but only sets a flag for the excessively sized Q.  This behaviour is mimicked in DH_check_pub_key().  (Merged from https://github.com/openssl/openssl/pull/22518)  (cherry picked from commit ddeb4b6c6d527e54ce9a99cba785c0f7776e54b6) ",
        "func_before": "int ossl_dh_compute_key(unsigned char *key, const BIGNUM *pub_key, DH *dh)\n{\n    BN_CTX *ctx = NULL;\n    BN_MONT_CTX *mont = NULL;\n    BIGNUM *z = NULL, *pminus1;\n    int ret = -1;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        goto err;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n    BN_CTX_start(ctx);\n    pminus1 = BN_CTX_get(ctx);\n    z = BN_CTX_get(ctx);\n    if (z == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        ERR_raise(ERR_LIB_DH, DH_R_NO_PRIVATE_VALUE);\n        goto err;\n    }\n\n    if (dh->flags & DH_FLAG_CACHE_MONT_P) {\n        mont = BN_MONT_CTX_set_locked(&dh->method_mont_p,\n                                      dh->lock, dh->params.p, ctx);\n        BN_set_flags(dh->priv_key, BN_FLG_CONSTTIME);\n        if (!mont)\n            goto err;\n    }\n\n    /* (Step 1) Z = pub_key^priv_key mod p */\n    if (!dh->meth->bn_mod_exp(dh, z, pub_key, dh->priv_key, dh->params.p, ctx,\n                              mont)) {\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n        goto err;\n    }\n\n    /* (Step 2) Error if z <= 1 or z = p - 1 */\n    if (BN_copy(pminus1, dh->params.p) == NULL\n        || !BN_sub_word(pminus1, 1)\n        || BN_cmp(z, BN_value_one()) <= 0\n        || BN_cmp(z, pminus1) == 0) {\n        ERR_raise(ERR_LIB_DH, DH_R_INVALID_SECRET);\n        goto err;\n    }\n\n    /* return the padded key, i.e. same number of bytes as the modulus */\n    ret = BN_bn2binpad(z, key, BN_num_bytes(dh->params.p));\n err:\n    BN_clear(z); /* (Step 2) destroy intermediate values */\n    BN_CTX_end(ctx);\n    BN_CTX_free(ctx);\n    return ret;\n}",
        "func": "int ossl_dh_compute_key(unsigned char *key, const BIGNUM *pub_key, DH *dh)\n{\n    BN_CTX *ctx = NULL;\n    BN_MONT_CTX *mont = NULL;\n    BIGNUM *z = NULL, *pminus1;\n    int ret = -1;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        goto err;\n    }\n\n    if (dh->params.q != NULL\n        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);\n        goto err;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n    BN_CTX_start(ctx);\n    pminus1 = BN_CTX_get(ctx);\n    z = BN_CTX_get(ctx);\n    if (z == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        ERR_raise(ERR_LIB_DH, DH_R_NO_PRIVATE_VALUE);\n        goto err;\n    }\n\n    if (dh->flags & DH_FLAG_CACHE_MONT_P) {\n        mont = BN_MONT_CTX_set_locked(&dh->method_mont_p,\n                                      dh->lock, dh->params.p, ctx);\n        BN_set_flags(dh->priv_key, BN_FLG_CONSTTIME);\n        if (!mont)\n            goto err;\n    }\n\n    /* (Step 1) Z = pub_key^priv_key mod p */\n    if (!dh->meth->bn_mod_exp(dh, z, pub_key, dh->priv_key, dh->params.p, ctx,\n                              mont)) {\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n        goto err;\n    }\n\n    /* (Step 2) Error if z <= 1 or z = p - 1 */\n    if (BN_copy(pminus1, dh->params.p) == NULL\n        || !BN_sub_word(pminus1, 1)\n        || BN_cmp(z, BN_value_one()) <= 0\n        || BN_cmp(z, pminus1) == 0) {\n        ERR_raise(ERR_LIB_DH, DH_R_INVALID_SECRET);\n        goto err;\n    }\n\n    /* return the padded key, i.e. same number of bytes as the modulus */\n    ret = BN_bn2binpad(z, key, BN_num_bytes(dh->params.p));\n err:\n    BN_clear(z); /* (Step 2) destroy intermediate values */\n    BN_CTX_end(ctx);\n    BN_CTX_free(ctx);\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,12 @@\n \n     if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n         ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n+        goto err;\n+    }\n+\n+    if (dh->params.q != NULL\n+        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {\n+        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);\n         goto err;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        goto err;",
                "    }",
                "",
                "    if (dh->params.q != NULL",
                "        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {",
                "        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5678",
        "func_name": "openssl/generate_key",
        "description": "Issue summary: Generating excessively long X9.42 DH keys or checking\nexcessively long X9.42 DH keys or parameters may be very slow.\n\nImpact summary: Applications that use the functions DH_generate_key() to\ngenerate an X9.42 DH key may experience long delays.  Likewise, applications\nthat use DH_check_pub_key(), DH_check_pub_key_ex() or EVP_PKEY_public_check()\nto check an X9.42 DH key or X9.42 DH parameters may experience long delays.\nWhere the key or parameters that are being checked have been obtained from\nan untrusted source this may lead to a Denial of Service.\n\nWhile DH_check() performs all the necessary checks (as of CVE-2023-3817),\nDH_check_pub_key() doesn't make any of these checks, and is therefore\nvulnerable for excessively large P and Q parameters.\n\nLikewise, while DH_generate_key() performs a check for an excessively large\nP, it doesn't check for an excessively large Q.\n\nAn application that calls DH_generate_key() or DH_check_pub_key() and\nsupplies a key or parameters obtained from an untrusted source could be\nvulnerable to a Denial of Service attack.\n\nDH_generate_key() and DH_check_pub_key() are also called by a number of\nother OpenSSL functions.  An application calling any of those other\nfunctions may similarly be affected.  The other functions affected by this\nare DH_check_pub_key_ex(), EVP_PKEY_public_check(), and EVP_PKEY_generate().\n\nAlso vulnerable are the OpenSSL pkey command line application when using the\n\"-pubcheck\" option, as well as the OpenSSL genpkey command line application.\n\nThe OpenSSL SSL/TLS implementation is not affected by this issue.\n\nThe OpenSSL 3.0 and 3.1 FIPS providers are not affected by this issue.\n\n",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=db925ae2e65d0d925adef429afc37f75bd1c2017",
        "commit_title": "",
        "commit_text": "Make DH_check_pub_key() and DH_generate_key() safer yet  We already check for an excessively large P in DH_generate_key(), but not in DH_check_pub_key(), and none of them check for an excessively large Q.  This change adds all the missing excessive size checks of P and Q.  It's to be noted that behaviours surrounding excessively sized P and Q differ.  DH_check() raises an error on the excessively sized P, but only sets a flag for the excessively sized Q.  This behaviour is mimicked in DH_check_pub_key().  (Merged from https://github.com/openssl/openssl/pull/22518)  (cherry picked from commit ddeb4b6c6d527e54ce9a99cba785c0f7776e54b6) ",
        "func_before": "static int generate_key(DH *dh)\n{\n    int ok = 0;\n    int generate_new_key = 0;\n#ifndef FIPS_MODULE\n    unsigned l;\n#endif\n    BN_CTX *ctx = NULL;\n    BIGNUM *pub_key = NULL, *priv_key = NULL;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        return 0;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        priv_key = BN_secure_new();\n        if (priv_key == NULL)\n            goto err;\n        generate_new_key = 1;\n    } else {\n        priv_key = dh->priv_key;\n    }\n\n    if (dh->pub_key == NULL) {\n        pub_key = BN_new();\n        if (pub_key == NULL)\n            goto err;\n    } else {\n        pub_key = dh->pub_key;\n    }\n    if (generate_new_key) {\n        /* Is it an approved safe prime ?*/\n        if (DH_get_nid(dh) != NID_undef) {\n            int max_strength =\n                    ossl_ifc_ffc_compute_security_bits(BN_num_bits(dh->params.p));\n\n            if (dh->params.q == NULL\n                || dh->length > BN_num_bits(dh->params.q))\n                goto err;\n            /* dh->length = maximum bit length of generated private key */\n            if (!ossl_ffc_generate_private_key(ctx, &dh->params, dh->length,\n                                               max_strength, priv_key))\n                goto err;\n        } else {\n#ifdef FIPS_MODULE\n            if (dh->params.q == NULL)\n                goto err;\n#else\n            if (dh->params.q == NULL) {\n                /* secret exponent length, must satisfy 2^(l-1) <= p */\n                if (dh->length != 0\n                    && dh->length >= BN_num_bits(dh->params.p))\n                    goto err;\n                l = dh->length ? dh->length : BN_num_bits(dh->params.p) - 1;\n                if (!BN_priv_rand_ex(priv_key, l, BN_RAND_TOP_ONE,\n                                     BN_RAND_BOTTOM_ANY, 0, ctx))\n                    goto err;\n                /*\n                 * We handle just one known case where g is a quadratic non-residue:\n                 * for g = 2: p % 8 == 3\n                 */\n                if (BN_is_word(dh->params.g, DH_GENERATOR_2)\n                    && !BN_is_bit_set(dh->params.p, 2)) {\n                    /* clear bit 0, since it won't be a secret anyway */\n                    if (!BN_clear_bit(priv_key, 0))\n                        goto err;\n                }\n            } else\n#endif\n            {\n                /* Do a partial check for invalid p, q, g */\n                if (!ossl_ffc_params_simple_validate(dh->libctx, &dh->params,\n                                                     FFC_PARAM_TYPE_DH, NULL))\n                    goto err;\n                /*\n                 * For FFC FIPS 186-4 keygen\n                 * security strength s = 112,\n                 * Max Private key size N = len(q)\n                 */\n                if (!ossl_ffc_generate_private_key(ctx, &dh->params,\n                                                   BN_num_bits(dh->params.q),\n                                                   MIN_STRENGTH,\n                                                   priv_key))\n                    goto err;\n            }\n        }\n    }\n\n    if (!ossl_dh_generate_public_key(ctx, dh, priv_key, pub_key))\n        goto err;\n\n    dh->pub_key = pub_key;\n    dh->priv_key = priv_key;\n    dh->dirty_cnt++;\n    ok = 1;\n err:\n    if (ok != 1)\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n\n    if (pub_key != dh->pub_key)\n        BN_free(pub_key);\n    if (priv_key != dh->priv_key)\n        BN_free(priv_key);\n    BN_CTX_free(ctx);\n    return ok;\n}",
        "func": "static int generate_key(DH *dh)\n{\n    int ok = 0;\n    int generate_new_key = 0;\n#ifndef FIPS_MODULE\n    unsigned l;\n#endif\n    BN_CTX *ctx = NULL;\n    BIGNUM *pub_key = NULL, *priv_key = NULL;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        return 0;\n    }\n\n    if (dh->params.q != NULL\n        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);\n        return 0;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        priv_key = BN_secure_new();\n        if (priv_key == NULL)\n            goto err;\n        generate_new_key = 1;\n    } else {\n        priv_key = dh->priv_key;\n    }\n\n    if (dh->pub_key == NULL) {\n        pub_key = BN_new();\n        if (pub_key == NULL)\n            goto err;\n    } else {\n        pub_key = dh->pub_key;\n    }\n    if (generate_new_key) {\n        /* Is it an approved safe prime ?*/\n        if (DH_get_nid(dh) != NID_undef) {\n            int max_strength =\n                    ossl_ifc_ffc_compute_security_bits(BN_num_bits(dh->params.p));\n\n            if (dh->params.q == NULL\n                || dh->length > BN_num_bits(dh->params.q))\n                goto err;\n            /* dh->length = maximum bit length of generated private key */\n            if (!ossl_ffc_generate_private_key(ctx, &dh->params, dh->length,\n                                               max_strength, priv_key))\n                goto err;\n        } else {\n#ifdef FIPS_MODULE\n            if (dh->params.q == NULL)\n                goto err;\n#else\n            if (dh->params.q == NULL) {\n                /* secret exponent length, must satisfy 2^(l-1) <= p */\n                if (dh->length != 0\n                    && dh->length >= BN_num_bits(dh->params.p))\n                    goto err;\n                l = dh->length ? dh->length : BN_num_bits(dh->params.p) - 1;\n                if (!BN_priv_rand_ex(priv_key, l, BN_RAND_TOP_ONE,\n                                     BN_RAND_BOTTOM_ANY, 0, ctx))\n                    goto err;\n                /*\n                 * We handle just one known case where g is a quadratic non-residue:\n                 * for g = 2: p % 8 == 3\n                 */\n                if (BN_is_word(dh->params.g, DH_GENERATOR_2)\n                    && !BN_is_bit_set(dh->params.p, 2)) {\n                    /* clear bit 0, since it won't be a secret anyway */\n                    if (!BN_clear_bit(priv_key, 0))\n                        goto err;\n                }\n            } else\n#endif\n            {\n                /* Do a partial check for invalid p, q, g */\n                if (!ossl_ffc_params_simple_validate(dh->libctx, &dh->params,\n                                                     FFC_PARAM_TYPE_DH, NULL))\n                    goto err;\n                /*\n                 * For FFC FIPS 186-4 keygen\n                 * security strength s = 112,\n                 * Max Private key size N = len(q)\n                 */\n                if (!ossl_ffc_generate_private_key(ctx, &dh->params,\n                                                   BN_num_bits(dh->params.q),\n                                                   MIN_STRENGTH,\n                                                   priv_key))\n                    goto err;\n            }\n        }\n    }\n\n    if (!ossl_dh_generate_public_key(ctx, dh, priv_key, pub_key))\n        goto err;\n\n    dh->pub_key = pub_key;\n    dh->priv_key = priv_key;\n    dh->dirty_cnt++;\n    ok = 1;\n err:\n    if (ok != 1)\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n\n    if (pub_key != dh->pub_key)\n        BN_free(pub_key);\n    if (priv_key != dh->priv_key)\n        BN_free(priv_key);\n    BN_CTX_free(ctx);\n    return ok;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,12 @@\n \n     if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n         ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n+        return 0;\n+    }\n+\n+    if (dh->params.q != NULL\n+        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {\n+        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);\n         return 0;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        return 0;",
                "    }",
                "",
                "    if (dh->params.q != NULL",
                "        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {",
                "        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-29531",
        "func_name": "tensorflow/Compute",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. An attacker can trigger a `CHECK` fail in PNG encoding by providing an empty input tensor as the pixel data. This is because the implementation(https://github.com/tensorflow/tensorflow/blob/e312e0791ce486a80c9d23110841525c6f7c3289/tensorflow/core/kernels/image/encode_png_op.cc#L57-L60) only validates that the total number of pixels in the image does not overflow. Thus, an attacker can send an empty matrix for encoding. However, if the tensor is empty, then the associated buffer is `nullptr`. Hence, when calling `png::WriteImageToBuffer`(https://github.com/tensorflow/tensorflow/blob/e312e0791ce486a80c9d23110841525c6f7c3289/tensorflow/core/kernels/image/encode_png_op.cc#L79-L93), the first argument (i.e., `image.flat<T>().data()`) is `NULL`. This then triggers the `CHECK_NOTNULL` in the first line of `png::WriteImageToBuffer`(https://github.com/tensorflow/tensorflow/blob/e312e0791ce486a80c9d23110841525c6f7c3289/tensorflow/core/lib/png/png_io.cc#L345-L349). Since `image` is null, this results in `abort` being called after printing the stacktrace. Effectively, this allows an attacker to mount a denial of service attack. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/26eb323554ffccd173e8a79a8c05c15b685ae4d1",
        "commit_title": "Fix null CHECK issue with `tf.raw_ops.EncodePng`.",
        "commit_text": " PiperOrigin-RevId: 369717714",
        "func_before": "void Compute(OpKernelContext* context) override {\n    const Tensor& image = context->input(0);\n    OP_REQUIRES(context, image.dims() == 3,\n                errors::InvalidArgument(\"image must be 3-dimensional\",\n                                        image.shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        FastBoundsCheck(image.NumElements(), std::numeric_limits<int32>::max()),\n        errors::InvalidArgument(\"image cannot have >= int32 max elements\"));\n    const int32 height = static_cast<int32>(image.dim_size(0));\n    const int32 width = static_cast<int32>(image.dim_size(1));\n    const int32 channels = static_cast<int32>(image.dim_size(2));\n\n    // In some cases, we pass width*channels*2 to png.\n    const int32 max_row_width = std::numeric_limits<int32>::max() / 2;\n\n    OP_REQUIRES(context, FastBoundsCheck(width * channels, max_row_width),\n                errors::InvalidArgument(\"image too wide to encode\"));\n\n    OP_REQUIRES(context, channels >= 1 && channels <= 4,\n                errors::InvalidArgument(\n                    \"image must have 1, 2, 3, or 4 channels, got \", channels));\n\n    // Encode image to png string\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({}), &output));\n    if (desired_channel_bits_ == 8) {\n      OP_REQUIRES(context,\n                  png::WriteImageToBuffer(\n                      image.flat<uint8>().data(), width, height,\n                      width * channels, channels, desired_channel_bits_,\n                      compression_, &output->scalar<tstring>()(), nullptr),\n                  errors::Internal(\"PNG encoding failed\"));\n    } else {\n      OP_REQUIRES(context,\n                  png::WriteImageToBuffer(\n                      image.flat<uint16>().data(), width, height,\n                      width * channels * 2, channels, desired_channel_bits_,\n                      compression_, &output->scalar<tstring>()(), nullptr),\n                  errors::Internal(\"PNG encoding failed\"));\n    }\n  }",
        "func": "void Compute(OpKernelContext* context) override {\n    const Tensor& image = context->input(0);\n    OP_REQUIRES(context, image.dims() == 3,\n                errors::InvalidArgument(\"image must be 3-dimensional\",\n                                        image.shape().DebugString()));\n    OP_REQUIRES(context, image.NumElements() > 0,\n                errors::Internal(\"Invalid image provided.\"));\n    OP_REQUIRES(\n        context,\n        FastBoundsCheck(image.NumElements(), std::numeric_limits<int32>::max()),\n        errors::InvalidArgument(\"image cannot have >= int32 max elements\"));\n    const int32 height = static_cast<int32>(image.dim_size(0));\n    const int32 width = static_cast<int32>(image.dim_size(1));\n    const int32 channels = static_cast<int32>(image.dim_size(2));\n\n    // In some cases, we pass width*channels*2 to png.\n    const int32 max_row_width = std::numeric_limits<int32>::max() / 2;\n\n    OP_REQUIRES(context, FastBoundsCheck(width * channels, max_row_width),\n                errors::InvalidArgument(\"image too wide to encode\"));\n\n    OP_REQUIRES(context, channels >= 1 && channels <= 4,\n                errors::InvalidArgument(\n                    \"image must have 1, 2, 3, or 4 channels, got \", channels));\n\n    // Encode image to png string\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({}), &output));\n    if (desired_channel_bits_ == 8) {\n      OP_REQUIRES(context,\n                  png::WriteImageToBuffer(\n                      image.flat<uint8>().data(), width, height,\n                      width * channels, channels, desired_channel_bits_,\n                      compression_, &output->scalar<tstring>()(), nullptr),\n                  errors::Internal(\"PNG encoding failed\"));\n    } else {\n      OP_REQUIRES(context,\n                  png::WriteImageToBuffer(\n                      image.flat<uint16>().data(), width, height,\n                      width * channels * 2, channels, desired_channel_bits_,\n                      compression_, &output->scalar<tstring>()(), nullptr),\n                  errors::Internal(\"PNG encoding failed\"));\n    }\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,8 @@\n     OP_REQUIRES(context, image.dims() == 3,\n                 errors::InvalidArgument(\"image must be 3-dimensional\",\n                                         image.shape().DebugString()));\n+    OP_REQUIRES(context, image.NumElements() > 0,\n+                errors::Internal(\"Invalid image provided.\"));\n     OP_REQUIRES(\n         context,\n         FastBoundsCheck(image.NumElements(), std::numeric_limits<int32>::max()),",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    OP_REQUIRES(context, image.NumElements() > 0,",
                "                errors::Internal(\"Invalid image provided.\"));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-29533",
        "func_name": "tensorflow/Compute",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. An attacker can trigger a denial of service via a `CHECK` failure by passing an empty image to `tf.raw_ops.DrawBoundingBoxes`. This is because the implementation(https://github.com/tensorflow/tensorflow/blob/ea34a18dc3f5c8d80a40ccca1404f343b5d55f91/tensorflow/core/kernels/image/draw_bounding_box_op.cc#L148-L165) uses `CHECK_*` assertions instead of `OP_REQUIRES` to validate user controlled inputs. Whereas `OP_REQUIRES` allows returning an error condition back to the user, the `CHECK_*` macros result in a crash if the condition is false, similar to `assert`. In this case, `height` is 0 from the `images` input. This results in `max_box_row_clamp` being negative and the assertion being falsified, followed by aborting program execution. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/b432a38fe0e1b4b904a6c222cbce794c39703e87",
        "commit_title": "Fix overflow CHECK issue with `tf.raw_ops.DrawBoundingBoxes`.",
        "commit_text": " PiperOrigin-RevId: 369753591",
        "func_before": "void Compute(OpKernelContext* context) override {\n    const Tensor& images = context->input(0);\n    const Tensor& boxes = context->input(1);\n    const int64 depth = images.dim_size(3);\n\n    OP_REQUIRES(context, images.dims() == 4,\n                errors::InvalidArgument(\"The rank of the images should be 4\"));\n    OP_REQUIRES(\n        context, boxes.dims() == 3,\n        errors::InvalidArgument(\"The rank of the boxes tensor should be 3\"));\n    OP_REQUIRES(context, images.dim_size(0) == boxes.dim_size(0),\n                errors::InvalidArgument(\"The batch sizes should be the same\"));\n\n    OP_REQUIRES(\n        context, depth == 4 || depth == 1 || depth == 3,\n        errors::InvalidArgument(\"Channel depth should be either 1 (GRY), \"\n                                \"3 (RGB), or 4 (RGBA)\"));\n\n    const int64 batch_size = images.dim_size(0);\n    const int64 height = images.dim_size(1);\n    const int64 width = images.dim_size(2);\n    std::vector<std::vector<float>> color_table;\n    if (context->num_inputs() == 3) {\n      const Tensor& colors_tensor = context->input(2);\n      OP_REQUIRES(context, colors_tensor.shape().dims() == 2,\n                  errors::InvalidArgument(\"colors must be a 2-D matrix\",\n                                          colors_tensor.shape().DebugString()));\n      OP_REQUIRES(context, colors_tensor.shape().dim_size(1) >= depth,\n                  errors::InvalidArgument(\"colors must have equal or more \",\n                                          \"channels than the image provided: \",\n                                          colors_tensor.shape().DebugString()));\n      if (colors_tensor.NumElements() != 0) {\n        color_table.clear();\n\n        auto colors = colors_tensor.matrix<float>();\n        for (int64 i = 0; i < colors.dimension(0); i++) {\n          std::vector<float> color_value(4);\n          for (int64 j = 0; j < 4; j++) {\n            color_value[j] = colors(i, j);\n          }\n          color_table.emplace_back(color_value);\n        }\n      }\n    }\n    if (color_table.empty()) {\n      color_table = DefaultColorTable(depth);\n    }\n    Tensor* output;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            0, TensorShape({batch_size, height, width, depth}), &output));\n\n    output->tensor<T, 4>() = images.tensor<T, 4>();\n    auto canvas = output->tensor<T, 4>();\n\n    for (int64 b = 0; b < batch_size; ++b) {\n      const int64 num_boxes = boxes.dim_size(1);\n      const auto tboxes = boxes.tensor<T, 3>();\n      for (int64 bb = 0; bb < num_boxes; ++bb) {\n        int64 color_index = bb % color_table.size();\n        const int64 min_box_row =\n            static_cast<float>(tboxes(b, bb, 0)) * (height - 1);\n        const int64 min_box_row_clamp = std::max<int64>(min_box_row, int64{0});\n        const int64 max_box_row =\n            static_cast<float>(tboxes(b, bb, 2)) * (height - 1);\n        const int64 max_box_row_clamp =\n            std::min<int64>(max_box_row, height - 1);\n        const int64 min_box_col =\n            static_cast<float>(tboxes(b, bb, 1)) * (width - 1);\n        const int64 min_box_col_clamp = std::max<int64>(min_box_col, int64{0});\n        const int64 max_box_col =\n            static_cast<float>(tboxes(b, bb, 3)) * (width - 1);\n        const int64 max_box_col_clamp = std::min<int64>(max_box_col, width - 1);\n\n        if (min_box_row > max_box_row || min_box_col > max_box_col) {\n          LOG(WARNING) << \"Bounding box (\" << min_box_row << \",\" << min_box_col\n                       << \",\" << max_box_row << \",\" << max_box_col\n                       << \") is inverted and will not be drawn.\";\n          continue;\n        }\n        if (min_box_row >= height || max_box_row < 0 || min_box_col >= width ||\n            max_box_col < 0) {\n          LOG(WARNING) << \"Bounding box (\" << min_box_row << \",\" << min_box_col\n                       << \",\" << max_box_row << \",\" << max_box_col\n                       << \") is completely outside the image\"\n                       << \" and will not be drawn.\";\n          continue;\n        }\n\n        // At this point, {min,max}_box_{row,col}_clamp are inside the\n        // image.\n        CHECK_GE(min_box_row_clamp, 0);\n        CHECK_GE(max_box_row_clamp, 0);\n        CHECK_LT(min_box_row_clamp, height);\n        CHECK_LT(max_box_row_clamp, height);\n        CHECK_GE(min_box_col_clamp, 0);\n        CHECK_GE(max_box_col_clamp, 0);\n        CHECK_LT(min_box_col_clamp, width);\n        CHECK_LT(max_box_col_clamp, width);\n\n        // At this point, the min_box_row and min_box_col are either\n        // in the image or above/left of it, and max_box_row and\n        // max_box_col are either in the image or below/right or it.\n        CHECK_LT(min_box_row, height);\n        CHECK_GE(max_box_row, 0);\n        CHECK_LT(min_box_col, width);\n        CHECK_GE(max_box_col, 0);\n\n        // Draw top line.\n        if (min_box_row >= 0) {\n          for (int64 j = min_box_col_clamp; j <= max_box_col_clamp; ++j)\n            for (int64 c = 0; c < depth; c++) {\n              canvas(b, min_box_row, j, c) =\n                  static_cast<T>(color_table[color_index][c]);\n            }\n        }\n        // Draw bottom line.\n        if (max_box_row < height) {\n          for (int64 j = min_box_col_clamp; j <= max_box_col_clamp; ++j)\n            for (int64 c = 0; c < depth; c++) {\n              canvas(b, max_box_row, j, c) =\n                  static_cast<T>(color_table[color_index][c]);\n            }\n        }\n        // Draw left line.\n        if (min_box_col >= 0) {\n          for (int64 i = min_box_row_clamp; i <= max_box_row_clamp; ++i)\n            for (int64 c = 0; c < depth; c++) {\n              canvas(b, i, min_box_col, c) =\n                  static_cast<T>(color_table[color_index][c]);\n            }\n        }\n        // Draw right line.\n        if (max_box_col < width) {\n          for (int64 i = min_box_row_clamp; i <= max_box_row_clamp; ++i)\n            for (int64 c = 0; c < depth; c++) {\n              canvas(b, i, max_box_col, c) =\n                  static_cast<T>(color_table[color_index][c]);\n            }\n        }\n      }\n    }\n  }",
        "func": "void Compute(OpKernelContext* context) override {\n    const Tensor& images = context->input(0);\n    const Tensor& boxes = context->input(1);\n    const int64 depth = images.dim_size(3);\n\n    OP_REQUIRES(context, images.dims() == 4,\n                errors::InvalidArgument(\"The rank of the images should be 4\"));\n    OP_REQUIRES(\n        context, boxes.dims() == 3,\n        errors::InvalidArgument(\"The rank of the boxes tensor should be 3\"));\n    OP_REQUIRES(context, images.dim_size(0) == boxes.dim_size(0),\n                errors::InvalidArgument(\"The batch sizes should be the same\"));\n\n    OP_REQUIRES(\n        context, depth == 4 || depth == 1 || depth == 3,\n        errors::InvalidArgument(\"Channel depth should be either 1 (GRY), \"\n                                \"3 (RGB), or 4 (RGBA)\"));\n\n    const int64 batch_size = images.dim_size(0);\n    const int64 height = images.dim_size(1);\n    const int64 width = images.dim_size(2);\n    std::vector<std::vector<float>> color_table;\n    if (context->num_inputs() == 3) {\n      const Tensor& colors_tensor = context->input(2);\n      OP_REQUIRES(context, colors_tensor.shape().dims() == 2,\n                  errors::InvalidArgument(\"colors must be a 2-D matrix\",\n                                          colors_tensor.shape().DebugString()));\n      OP_REQUIRES(context, colors_tensor.shape().dim_size(1) >= depth,\n                  errors::InvalidArgument(\"colors must have equal or more \",\n                                          \"channels than the image provided: \",\n                                          colors_tensor.shape().DebugString()));\n      if (colors_tensor.NumElements() != 0) {\n        color_table.clear();\n\n        auto colors = colors_tensor.matrix<float>();\n        for (int64 i = 0; i < colors.dimension(0); i++) {\n          std::vector<float> color_value(4);\n          for (int64 j = 0; j < 4; j++) {\n            color_value[j] = colors(i, j);\n          }\n          color_table.emplace_back(color_value);\n        }\n      }\n    }\n    if (color_table.empty()) {\n      color_table = DefaultColorTable(depth);\n    }\n    Tensor* output;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            0, TensorShape({batch_size, height, width, depth}), &output));\n\n    output->tensor<T, 4>() = images.tensor<T, 4>();\n    auto canvas = output->tensor<T, 4>();\n\n    for (int64 b = 0; b < batch_size; ++b) {\n      const int64 num_boxes = boxes.dim_size(1);\n      const auto tboxes = boxes.tensor<T, 3>();\n      for (int64 bb = 0; bb < num_boxes; ++bb) {\n        int64 color_index = bb % color_table.size();\n        const int64 min_box_row =\n            static_cast<float>(tboxes(b, bb, 0)) * (height - 1);\n        const int64 min_box_row_clamp = std::max<int64>(min_box_row, int64{0});\n        const int64 max_box_row =\n            static_cast<float>(tboxes(b, bb, 2)) * (height - 1);\n        const int64 max_box_row_clamp =\n            std::min<int64>(max_box_row, height - 1);\n        const int64 min_box_col =\n            static_cast<float>(tboxes(b, bb, 1)) * (width - 1);\n        const int64 min_box_col_clamp = std::max<int64>(min_box_col, int64{0});\n        const int64 max_box_col =\n            static_cast<float>(tboxes(b, bb, 3)) * (width - 1);\n        const int64 max_box_col_clamp = std::min<int64>(max_box_col, width - 1);\n\n        if (min_box_row > max_box_row || min_box_col > max_box_col) {\n          LOG(WARNING) << \"Bounding box (\" << min_box_row << \",\" << min_box_col\n                       << \",\" << max_box_row << \",\" << max_box_col\n                       << \") is inverted and will not be drawn.\";\n          continue;\n        }\n        if (min_box_row >= height || max_box_row < 0 || min_box_col >= width ||\n            max_box_col < 0) {\n          LOG(WARNING) << \"Bounding box (\" << min_box_row << \",\" << min_box_col\n                       << \",\" << max_box_row << \",\" << max_box_col\n                       << \") is completely outside the image\"\n                       << \" and will not be drawn.\";\n          continue;\n        }\n\n        // At this point, {min,max}_box_{row,col}_clamp are inside the\n        // image.\n        OP_REQUIRES(\n            context, min_box_row_clamp >= 0,\n            errors::InvalidArgument(\"Min box row clamp is less than 0.\"));\n        OP_REQUIRES(\n            context, max_box_row_clamp >= 0,\n            errors::InvalidArgument(\"Max box row clamp is less than 0.\"));\n        OP_REQUIRES(context, min_box_row_clamp <= height,\n                    errors::InvalidArgument(\n                        \"Min box row clamp is greater than height.\"));\n        OP_REQUIRES(context, max_box_row_clamp <= height,\n                    errors::InvalidArgument(\n                        \"Max box row clamp is greater than height.\"));\n\n        OP_REQUIRES(\n            context, min_box_col_clamp >= 0,\n            errors::InvalidArgument(\"Min box col clamp is less than 0.\"));\n        OP_REQUIRES(\n            context, max_box_col_clamp >= 0,\n            errors::InvalidArgument(\"Max box col clamp is less than 0.\"));\n        OP_REQUIRES(context, min_box_col_clamp <= width,\n                    errors::InvalidArgument(\n                        \"Min box col clamp is greater than width.\"));\n        OP_REQUIRES(context, max_box_col_clamp <= width,\n                    errors::InvalidArgument(\n                        \"Max box col clamp is greater than width.\"));\n\n        // At this point, the min_box_row and min_box_col are either\n        // in the image or above/left of it, and max_box_row and\n        // max_box_col are either in the image or below/right or it.\n\n        OP_REQUIRES(\n            context, min_box_row <= height,\n            errors::InvalidArgument(\"Min box row is greater than height.\"));\n        OP_REQUIRES(context, max_box_row >= 0,\n                    errors::InvalidArgument(\"Max box row is less than 0.\"));\n        OP_REQUIRES(\n            context, min_box_col <= width,\n            errors::InvalidArgument(\"Min box col is greater than width.\"));\n        OP_REQUIRES(context, max_box_col >= 0,\n                    errors::InvalidArgument(\"Max box col is less than 0.\"));\n\n        // Draw top line.\n        if (min_box_row >= 0) {\n          for (int64 j = min_box_col_clamp; j <= max_box_col_clamp; ++j)\n            for (int64 c = 0; c < depth; c++) {\n              canvas(b, min_box_row, j, c) =\n                  static_cast<T>(color_table[color_index][c]);\n            }\n        }\n        // Draw bottom line.\n        if (max_box_row < height) {\n          for (int64 j = min_box_col_clamp; j <= max_box_col_clamp; ++j)\n            for (int64 c = 0; c < depth; c++) {\n              canvas(b, max_box_row, j, c) =\n                  static_cast<T>(color_table[color_index][c]);\n            }\n        }\n        // Draw left line.\n        if (min_box_col >= 0) {\n          for (int64 i = min_box_row_clamp; i <= max_box_row_clamp; ++i)\n            for (int64 c = 0; c < depth; c++) {\n              canvas(b, i, min_box_col, c) =\n                  static_cast<T>(color_table[color_index][c]);\n            }\n        }\n        // Draw right line.\n        if (max_box_col < width) {\n          for (int64 i = min_box_row_clamp; i <= max_box_row_clamp; ++i)\n            for (int64 c = 0; c < depth; c++) {\n              canvas(b, i, max_box_col, c) =\n                  static_cast<T>(color_table[color_index][c]);\n            }\n        }\n      }\n    }\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -90,22 +90,46 @@\n \n         // At this point, {min,max}_box_{row,col}_clamp are inside the\n         // image.\n-        CHECK_GE(min_box_row_clamp, 0);\n-        CHECK_GE(max_box_row_clamp, 0);\n-        CHECK_LT(min_box_row_clamp, height);\n-        CHECK_LT(max_box_row_clamp, height);\n-        CHECK_GE(min_box_col_clamp, 0);\n-        CHECK_GE(max_box_col_clamp, 0);\n-        CHECK_LT(min_box_col_clamp, width);\n-        CHECK_LT(max_box_col_clamp, width);\n+        OP_REQUIRES(\n+            context, min_box_row_clamp >= 0,\n+            errors::InvalidArgument(\"Min box row clamp is less than 0.\"));\n+        OP_REQUIRES(\n+            context, max_box_row_clamp >= 0,\n+            errors::InvalidArgument(\"Max box row clamp is less than 0.\"));\n+        OP_REQUIRES(context, min_box_row_clamp <= height,\n+                    errors::InvalidArgument(\n+                        \"Min box row clamp is greater than height.\"));\n+        OP_REQUIRES(context, max_box_row_clamp <= height,\n+                    errors::InvalidArgument(\n+                        \"Max box row clamp is greater than height.\"));\n+\n+        OP_REQUIRES(\n+            context, min_box_col_clamp >= 0,\n+            errors::InvalidArgument(\"Min box col clamp is less than 0.\"));\n+        OP_REQUIRES(\n+            context, max_box_col_clamp >= 0,\n+            errors::InvalidArgument(\"Max box col clamp is less than 0.\"));\n+        OP_REQUIRES(context, min_box_col_clamp <= width,\n+                    errors::InvalidArgument(\n+                        \"Min box col clamp is greater than width.\"));\n+        OP_REQUIRES(context, max_box_col_clamp <= width,\n+                    errors::InvalidArgument(\n+                        \"Max box col clamp is greater than width.\"));\n \n         // At this point, the min_box_row and min_box_col are either\n         // in the image or above/left of it, and max_box_row and\n         // max_box_col are either in the image or below/right or it.\n-        CHECK_LT(min_box_row, height);\n-        CHECK_GE(max_box_row, 0);\n-        CHECK_LT(min_box_col, width);\n-        CHECK_GE(max_box_col, 0);\n+\n+        OP_REQUIRES(\n+            context, min_box_row <= height,\n+            errors::InvalidArgument(\"Min box row is greater than height.\"));\n+        OP_REQUIRES(context, max_box_row >= 0,\n+                    errors::InvalidArgument(\"Max box row is less than 0.\"));\n+        OP_REQUIRES(\n+            context, min_box_col <= width,\n+            errors::InvalidArgument(\"Min box col is greater than width.\"));\n+        OP_REQUIRES(context, max_box_col >= 0,\n+                    errors::InvalidArgument(\"Max box col is less than 0.\"));\n \n         // Draw top line.\n         if (min_box_row >= 0) {",
        "diff_line_info": {
            "deleted_lines": [
                "        CHECK_GE(min_box_row_clamp, 0);",
                "        CHECK_GE(max_box_row_clamp, 0);",
                "        CHECK_LT(min_box_row_clamp, height);",
                "        CHECK_LT(max_box_row_clamp, height);",
                "        CHECK_GE(min_box_col_clamp, 0);",
                "        CHECK_GE(max_box_col_clamp, 0);",
                "        CHECK_LT(min_box_col_clamp, width);",
                "        CHECK_LT(max_box_col_clamp, width);",
                "        CHECK_LT(min_box_row, height);",
                "        CHECK_GE(max_box_row, 0);",
                "        CHECK_LT(min_box_col, width);",
                "        CHECK_GE(max_box_col, 0);"
            ],
            "added_lines": [
                "        OP_REQUIRES(",
                "            context, min_box_row_clamp >= 0,",
                "            errors::InvalidArgument(\"Min box row clamp is less than 0.\"));",
                "        OP_REQUIRES(",
                "            context, max_box_row_clamp >= 0,",
                "            errors::InvalidArgument(\"Max box row clamp is less than 0.\"));",
                "        OP_REQUIRES(context, min_box_row_clamp <= height,",
                "                    errors::InvalidArgument(",
                "                        \"Min box row clamp is greater than height.\"));",
                "        OP_REQUIRES(context, max_box_row_clamp <= height,",
                "                    errors::InvalidArgument(",
                "                        \"Max box row clamp is greater than height.\"));",
                "",
                "        OP_REQUIRES(",
                "            context, min_box_col_clamp >= 0,",
                "            errors::InvalidArgument(\"Min box col clamp is less than 0.\"));",
                "        OP_REQUIRES(",
                "            context, max_box_col_clamp >= 0,",
                "            errors::InvalidArgument(\"Max box col clamp is less than 0.\"));",
                "        OP_REQUIRES(context, min_box_col_clamp <= width,",
                "                    errors::InvalidArgument(",
                "                        \"Min box col clamp is greater than width.\"));",
                "        OP_REQUIRES(context, max_box_col_clamp <= width,",
                "                    errors::InvalidArgument(",
                "                        \"Max box col clamp is greater than width.\"));",
                "",
                "        OP_REQUIRES(",
                "            context, min_box_row <= height,",
                "            errors::InvalidArgument(\"Min box row is greater than height.\"));",
                "        OP_REQUIRES(context, max_box_row >= 0,",
                "                    errors::InvalidArgument(\"Max box row is less than 0.\"));",
                "        OP_REQUIRES(",
                "            context, min_box_col <= width,",
                "            errors::InvalidArgument(\"Min box col is greater than width.\"));",
                "        OP_REQUIRES(context, max_box_col >= 0,",
                "                    errors::InvalidArgument(\"Max box col is less than 0.\"));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-29607",
        "func_name": "tensorflow/Compute",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. Incomplete validation in `SparseAdd` results in allowing attackers to exploit undefined behavior (dereferencing null pointers) as well as write outside of bounds of heap allocated data. The implementation(https://github.com/tensorflow/tensorflow/blob/656e7673b14acd7835dc778867f84916c6d1cac2/tensorflow/core/kernels/sparse_sparse_binary_op_shared.cc) has a large set of validation for the two sparse tensor inputs (6 tensors in total), but does not validate that the tensors are not empty or that the second dimension of `*_indices` matches the size of corresponding `*_shape`. This allows attackers to send tensor triples that represent invalid sparse tensors to abuse code assumptions that are not protected by validation. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ba6822bd7b7324ba201a28b2f278c29a98edbef2",
        "commit_title": "Fix OOB issue with `tf.raw_ops.SparseSparseMinimum`.",
        "commit_text": " PiperOrigin-RevId: 371005787",
        "func_before": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b_indices_t,\n        *b_values_t, *b_shape_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_indices\", &b_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape_t));\n\n    // Validations.\n    OP_REQUIRES(\n        ctx,\n        TensorShapeUtils::IsMatrix(a_indices_t->shape()) &&\n            TensorShapeUtils::IsMatrix(b_indices_t->shape()),\n        errors::InvalidArgument(\"Inputs a_indices and b_indices should be \"\n                                \"matrices but received shapes: \",\n                                a_indices_t->shape().DebugString(), \", \",\n                                b_indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_values_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_values_t->shape()),\n                errors::InvalidArgument(\n                    \"Inputs a_values and b_values should be vectors \"\n                    \"but received shapes: \",\n                    a_values_t->shape().DebugString(), \" and \",\n                    b_values_t->shape().DebugString()));\n\n    const int64 a_nnz = a_indices_t->dim_size(0);\n    const int64 b_nnz = b_indices_t->dim_size(0);\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    OP_REQUIRES(\n        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n        errors::InvalidArgument(\"Expected \", a_nnz, \" and \", b_nnz,\n                                \" non-empty input values, got \",\n                                a_values.size(), \" and \", b_values.size()));\n\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_shape_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape_t->shape().DebugString(), \" and \",\n                    b_shape_t->shape().DebugString()));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n    const auto a_shape = a_shape_t->flat<int64>();\n    const auto b_shape = b_shape_t->flat<int64>();\n    for (int i = 0; i < a_shape_t->NumElements(); ++i) {\n      OP_REQUIRES(ctx, a_shape(i) == b_shape(i),\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    const int num_dims = a_indices_t->dim_size(1);\n    const auto a_indices_mat = a_indices_t->matrix<int64>();\n    const auto b_indices_mat = b_indices_t->matrix<int64>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,\n                                b_values, b_nnz, num_dims, &a_augmented_values,\n                                &b_augmented_values, &entries_to_copy);\n\n    // Allocates and fills output tensors.\n    const int64 sum_nnz = a_augmented_values.size();\n    Tensor *output_indices_t, *output_values_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({sum_nnz, num_dims}),\n                                        &output_indices_t));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(1, TensorShape({sum_nnz}), &output_values_t));\n    auto output_indices_mat = output_indices_t->matrix<int64>();\n\n    for (int64 i = 0; i < sum_nnz; ++i) {\n      const bool from_a = entries_to_copy[i].first;\n      const int64 idx = entries_to_copy[i].second;\n      output_indices_mat.chip<0>(i) =\n          from_a ? a_indices_mat.chip<0>(idx) : b_indices_mat.chip<0>(idx);\n    }\n\n    // Performs the functor operation using Eigen.\n    //\n    // Note that the two stack-allocated std::vector's may not be aligned. Using\n    // allocate_temp() would've given us aligned storage, but we do not know\n    // their sizes in advance, so we couldn't use allocate_temp() anyway.\n    //\n    // TODO(zongheng): measure if it's worthwhile to somehow force alignment.\n    using UnalignedTensorMap =\n        Eigen::TensorMap<Eigen::Tensor<const T, 1, Eigen::RowMajor>,\n                         Eigen::Unaligned>;\n    auto a_augmented_values_t =\n        UnalignedTensorMap(a_augmented_values.data(), sum_nnz);\n    auto b_augmented_values_t =\n        UnalignedTensorMap(b_augmented_values.data(), sum_nnz);\n    output_values_t->flat<T>().device(ctx->eigen_device<Device>()) =\n        a_augmented_values_t.binaryExpr(b_augmented_values_t,\n                                        typename Functor::func());\n  }",
        "func": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b_indices_t,\n        *b_values_t, *b_shape_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_indices\", &b_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape_t));\n\n    // Validations.\n    OP_REQUIRES(\n        ctx,\n        TensorShapeUtils::IsMatrix(a_indices_t->shape()) &&\n            TensorShapeUtils::IsMatrix(b_indices_t->shape()),\n        errors::InvalidArgument(\"Inputs a_indices and b_indices should be \"\n                                \"matrices but received shapes: \",\n                                a_indices_t->shape().DebugString(), \", \",\n                                b_indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_values_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_values_t->shape()),\n                errors::InvalidArgument(\n                    \"Inputs a_values and b_values should be vectors \"\n                    \"but received shapes: \",\n                    a_values_t->shape().DebugString(), \" and \",\n                    b_values_t->shape().DebugString()));\n\n    const int64 a_nnz = a_indices_t->dim_size(0);\n    const int64 b_nnz = b_indices_t->dim_size(0);\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    OP_REQUIRES(\n        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n        errors::InvalidArgument(\"Expected \", a_nnz, \" and \", b_nnz,\n                                \" non-empty input values, got \",\n                                a_values.size(), \" and \", b_values.size()));\n\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_shape_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape_t->shape().DebugString(), \" and \",\n                    b_shape_t->shape().DebugString()));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n    const auto a_shape = a_shape_t->flat<int64>();\n    const auto b_shape = b_shape_t->flat<int64>();\n    for (int i = 0; i < a_shape_t->NumElements(); ++i) {\n      OP_REQUIRES(ctx, a_shape(i) == b_shape(i),\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    OP_REQUIRES(\n        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),\n        errors::InvalidArgument(\n            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),\n            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));\n    const int num_dims = a_indices_t->dim_size(1);\n    const auto a_indices_mat = a_indices_t->matrix<int64>();\n    const auto b_indices_mat = b_indices_t->matrix<int64>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,\n                                b_values, b_nnz, num_dims, &a_augmented_values,\n                                &b_augmented_values, &entries_to_copy);\n\n    // Allocates and fills output tensors.\n    const int64 sum_nnz = a_augmented_values.size();\n    Tensor *output_indices_t, *output_values_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({sum_nnz, num_dims}),\n                                        &output_indices_t));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(1, TensorShape({sum_nnz}), &output_values_t));\n    auto output_indices_mat = output_indices_t->matrix<int64>();\n\n    for (int64 i = 0; i < sum_nnz; ++i) {\n      const bool from_a = entries_to_copy[i].first;\n      const int64 idx = entries_to_copy[i].second;\n      output_indices_mat.chip<0>(i) =\n          from_a ? a_indices_mat.chip<0>(idx) : b_indices_mat.chip<0>(idx);\n    }\n\n    // Performs the functor operation using Eigen.\n    //\n    // Note that the two stack-allocated std::vector's may not be aligned. Using\n    // allocate_temp() would've given us aligned storage, but we do not know\n    // their sizes in advance, so we couldn't use allocate_temp() anyway.\n    //\n    // TODO(zongheng): measure if it's worthwhile to somehow force alignment.\n    using UnalignedTensorMap =\n        Eigen::TensorMap<Eigen::Tensor<const T, 1, Eigen::RowMajor>,\n                         Eigen::Unaligned>;\n    auto a_augmented_values_t =\n        UnalignedTensorMap(a_augmented_values.data(), sum_nnz);\n    auto b_augmented_values_t =\n        UnalignedTensorMap(b_augmented_values.data(), sum_nnz);\n    output_values_t->flat<T>().device(ctx->eigen_device<Device>()) =\n        a_augmented_values_t.binaryExpr(b_augmented_values_t,\n                                        typename Functor::func());\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -58,6 +58,11 @@\n                                           \" for dimension \", i));\n     }\n \n+    OP_REQUIRES(\n+        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),\n+        errors::InvalidArgument(\n+            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),\n+            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));\n     const int num_dims = a_indices_t->dim_size(1);\n     const auto a_indices_mat = a_indices_t->matrix<int64>();\n     const auto b_indices_mat = b_indices_t->matrix<int64>();",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    OP_REQUIRES(",
                "        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),",
                "        errors::InvalidArgument(",
                "            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),",
                "            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-29607",
        "func_name": "tensorflow/Compute",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. Incomplete validation in `SparseAdd` results in allowing attackers to exploit undefined behavior (dereferencing null pointers) as well as write outside of bounds of heap allocated data. The implementation(https://github.com/tensorflow/tensorflow/blob/656e7673b14acd7835dc778867f84916c6d1cac2/tensorflow/core/kernels/sparse_sparse_binary_op_shared.cc) has a large set of validation for the two sparse tensor inputs (6 tensors in total), but does not validate that the tensors are not empty or that the second dimension of `*_indices` matches the size of corresponding `*_shape`. This allows attackers to send tensor triples that represent invalid sparse tensors to abuse code assumptions that are not protected by validation. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/f6fde895ef9c77d848061c0517f19d0ec2682f3a",
        "commit_title": "Validate that a and b are proper sparse tensors",
        "commit_text": " PiperOrigin-RevId: 373274848",
        "func_before": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b_indices_t,\n        *b_values_t, *b_shape_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_indices\", &b_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape_t));\n\n    // Validations.\n    OP_REQUIRES(\n        ctx,\n        TensorShapeUtils::IsMatrix(a_indices_t->shape()) &&\n            TensorShapeUtils::IsMatrix(b_indices_t->shape()),\n        errors::InvalidArgument(\"Inputs a_indices and b_indices should be \"\n                                \"matrices but received shapes: \",\n                                a_indices_t->shape().DebugString(), \", \",\n                                b_indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_values_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_values_t->shape()),\n                errors::InvalidArgument(\n                    \"Inputs a_values and b_values should be vectors \"\n                    \"but received shapes: \",\n                    a_values_t->shape().DebugString(), \" and \",\n                    b_values_t->shape().DebugString()));\n\n    const int64 a_nnz = a_indices_t->dim_size(0);\n    const int64 b_nnz = b_indices_t->dim_size(0);\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    OP_REQUIRES(\n        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n        errors::InvalidArgument(\"Expected \", a_nnz, \" and \", b_nnz,\n                                \" non-empty input values, got \",\n                                a_values.size(), \" and \", b_values.size()));\n\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_shape_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape_t->shape().DebugString(), \" and \",\n                    b_shape_t->shape().DebugString()));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n    const auto a_shape = a_shape_t->flat<int64>();\n    const auto b_shape = b_shape_t->flat<int64>();\n    for (int i = 0; i < a_shape_t->NumElements(); ++i) {\n      OP_REQUIRES(ctx, a_shape(i) == b_shape(i),\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    OP_REQUIRES(\n        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),\n        errors::InvalidArgument(\n            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),\n            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));\n    const int num_dims = a_indices_t->dim_size(1);\n    const auto a_indices_mat = a_indices_t->matrix<int64>();\n    const auto b_indices_mat = b_indices_t->matrix<int64>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,\n                                b_values, b_nnz, num_dims, &a_augmented_values,\n                                &b_augmented_values, &entries_to_copy);\n\n    // Allocates and fills output tensors.\n    const int64 sum_nnz = a_augmented_values.size();\n    Tensor *output_indices_t, *output_values_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({sum_nnz, num_dims}),\n                                        &output_indices_t));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(1, TensorShape({sum_nnz}), &output_values_t));\n    auto output_indices_mat = output_indices_t->matrix<int64>();\n\n    for (int64 i = 0; i < sum_nnz; ++i) {\n      const bool from_a = entries_to_copy[i].first;\n      const int64 idx = entries_to_copy[i].second;\n      output_indices_mat.chip<0>(i) =\n          from_a ? a_indices_mat.chip<0>(idx) : b_indices_mat.chip<0>(idx);\n    }\n\n    // Performs the functor operation using Eigen.\n    //\n    // Note that the two stack-allocated std::vector's may not be aligned. Using\n    // allocate_temp() would've given us aligned storage, but we do not know\n    // their sizes in advance, so we couldn't use allocate_temp() anyway.\n    //\n    // TODO(zongheng): measure if it's worthwhile to somehow force alignment.\n    using UnalignedTensorMap =\n        Eigen::TensorMap<Eigen::Tensor<const T, 1, Eigen::RowMajor>,\n                         Eigen::Unaligned>;\n    auto a_augmented_values_t =\n        UnalignedTensorMap(a_augmented_values.data(), sum_nnz);\n    auto b_augmented_values_t =\n        UnalignedTensorMap(b_augmented_values.data(), sum_nnz);\n    output_values_t->flat<T>().device(ctx->eigen_device<Device>()) =\n        a_augmented_values_t.binaryExpr(b_augmented_values_t,\n                                        typename Functor::func());\n  }",
        "func": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b_indices_t,\n        *b_values_t, *b_shape_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_indices\", &b_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape_t));\n\n    // Validations.\n    OP_REQUIRES(\n        ctx,\n        TensorShapeUtils::IsMatrix(a_indices_t->shape()) &&\n            TensorShapeUtils::IsMatrix(b_indices_t->shape()),\n        errors::InvalidArgument(\"Inputs a_indices and b_indices should be \"\n                                \"matrices but received shapes: \",\n                                a_indices_t->shape().DebugString(), \", \",\n                                b_indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_values_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_values_t->shape()),\n                errors::InvalidArgument(\n                    \"Inputs a_values and b_values should be vectors \"\n                    \"but received shapes: \",\n                    a_values_t->shape().DebugString(), \" and \",\n                    b_values_t->shape().DebugString()));\n\n    const int64 a_nnz = a_indices_t->dim_size(0);\n    const int64 b_nnz = b_indices_t->dim_size(0);\n\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    OP_REQUIRES(\n        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n        errors::InvalidArgument(\"Expected \", a_nnz, \" and \", b_nnz,\n                                \" non-empty input values, got \",\n                                a_values.size(), \" and \", b_values.size()));\n\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_shape_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape_t->shape().DebugString(), \" and \",\n                    b_shape_t->shape().DebugString()));\n    const int num_dims = a_indices_t->dim_size(1);\n    OP_REQUIRES(\n        ctx, a_shape_t->NumElements() == num_dims,\n        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\n                                \"a_shape must match, got \",\n                                num_dims, \" and \", a_shape_t->NumElements()));\n    OP_REQUIRES(ctx, num_dims > 0,\n                errors::InvalidArgument(\"Tensors must not be empty\"));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n    const auto a_shape = a_shape_t->flat<int64>();\n    const auto b_shape = b_shape_t->flat<int64>();\n    for (int i = 0; i < a_shape_t->NumElements(); ++i) {\n      OP_REQUIRES(ctx, a_shape(i) == b_shape(i),\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    const auto a_indices_mat = a_indices_t->matrix<int64>();\n    const auto b_indices_mat = b_indices_t->matrix<int64>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,\n                                b_values, b_nnz, num_dims, &a_augmented_values,\n                                &b_augmented_values, &entries_to_copy);\n\n    // Allocates and fills output tensors.\n    const int64 sum_nnz = a_augmented_values.size();\n    Tensor *output_indices_t, *output_values_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({sum_nnz, num_dims}),\n                                        &output_indices_t));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(1, TensorShape({sum_nnz}), &output_values_t));\n    auto output_indices_mat = output_indices_t->matrix<int64>();\n\n    for (int64 i = 0; i < sum_nnz; ++i) {\n      const bool from_a = entries_to_copy[i].first;\n      const int64 idx = entries_to_copy[i].second;\n      output_indices_mat.chip<0>(i) =\n          from_a ? a_indices_mat.chip<0>(idx) : b_indices_mat.chip<0>(idx);\n    }\n\n    // Performs the functor operation using Eigen.\n    //\n    // Note that the two stack-allocated std::vector's may not be aligned. Using\n    // allocate_temp() would've given us aligned storage, but we do not know\n    // their sizes in advance, so we couldn't use allocate_temp() anyway.\n    //\n    // TODO(zongheng): measure if it's worthwhile to somehow force alignment.\n    using UnalignedTensorMap =\n        Eigen::TensorMap<Eigen::Tensor<const T, 1, Eigen::RowMajor>,\n                         Eigen::Unaligned>;\n    auto a_augmented_values_t =\n        UnalignedTensorMap(a_augmented_values.data(), sum_nnz);\n    auto b_augmented_values_t =\n        UnalignedTensorMap(b_augmented_values.data(), sum_nnz);\n    output_values_t->flat<T>().device(ctx->eigen_device<Device>()) =\n        a_augmented_values_t.binaryExpr(b_augmented_values_t,\n                                        typename Functor::func());\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,6 +28,7 @@\n \n     const int64 a_nnz = a_indices_t->dim_size(0);\n     const int64 b_nnz = b_indices_t->dim_size(0);\n+\n     const auto a_values = a_values_t->vec<T>();\n     const auto b_values = b_values_t->vec<T>();\n \n@@ -44,6 +45,14 @@\n                     \"Input shapes should be a vector but received shapes \",\n                     a_shape_t->shape().DebugString(), \" and \",\n                     b_shape_t->shape().DebugString()));\n+    const int num_dims = a_indices_t->dim_size(1);\n+    OP_REQUIRES(\n+        ctx, a_shape_t->NumElements() == num_dims,\n+        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\n+                                \"a_shape must match, got \",\n+                                num_dims, \" and \", a_shape_t->NumElements()));\n+    OP_REQUIRES(ctx, num_dims > 0,\n+                errors::InvalidArgument(\"Tensors must not be empty\"));\n     OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                 errors::InvalidArgument(\n                     \"Operands do not have the same ranks; got shapes: \",\n@@ -58,12 +67,6 @@\n                                           \" for dimension \", i));\n     }\n \n-    OP_REQUIRES(\n-        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),\n-        errors::InvalidArgument(\n-            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),\n-            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));\n-    const int num_dims = a_indices_t->dim_size(1);\n     const auto a_indices_mat = a_indices_t->matrix<int64>();\n     const auto b_indices_mat = b_indices_t->matrix<int64>();\n     std::vector<T> a_augmented_values, b_augmented_values;",
        "diff_line_info": {
            "deleted_lines": [
                "    OP_REQUIRES(",
                "        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),",
                "        errors::InvalidArgument(",
                "            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),",
                "            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));",
                "    const int num_dims = a_indices_t->dim_size(1);"
            ],
            "added_lines": [
                "",
                "    const int num_dims = a_indices_t->dim_size(1);",
                "    OP_REQUIRES(",
                "        ctx, a_shape_t->NumElements() == num_dims,",
                "        errors::InvalidArgument(\"Second dimension of a_indices and length of \"",
                "                                \"a_shape must match, got \",",
                "                                num_dims, \" and \", a_shape_t->NumElements()));",
                "    OP_REQUIRES(ctx, num_dims > 0,",
                "                errors::InvalidArgument(\"Tensors must not be empty\"));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-26197",
        "func_name": "jerryscript-project/jerryscript/main_print_unhandled_exception",
        "description": "An issue was discovered in JerryScript 2.4.0. There is a SEGV in main_print_unhandled_exception in main-utils.c file.",
        "git_url": "https://github.com/jerryscript-project/jerryscript/commit/39b9e47d503eac26fc1649396d0bd13aa46a2ec5",
        "commit_title": "Fix unavailable source file handling in main_print_unhandled_exception",
        "commit_text": " This patch fixes #4403.  JerryScript-DCO-1.0-Signed-off-by: Robert Fancsik frobert@inf.u-szeged.hu",
        "func_before": "void\nmain_print_unhandled_exception (jerry_value_t error_value) /**< error value */\n{\n  assert (jerry_value_is_error (error_value));\n  error_value = jerry_get_value_from_error (error_value, true);\n\n  jerry_char_t err_str_buf[256];\n\n  jerry_value_t err_str_val = jerry_value_to_string (error_value);\n  jerry_size_t err_str_size = jerry_get_utf8_string_size (err_str_val);\n\n  if (err_str_size >= 256)\n  {\n    const char msg[] = \"[Error message too long]\";\n    err_str_size = sizeof (msg) / sizeof (char) - 1;\n    memcpy (err_str_buf, msg, err_str_size + 1);\n  }\n  else\n  {\n    jerry_size_t string_end = jerry_string_to_utf8_char_buffer (err_str_val, err_str_buf, err_str_size);\n    assert (string_end == err_str_size);\n    err_str_buf[string_end] = 0;\n\n    if (jerry_is_feature_enabled (JERRY_FEATURE_ERROR_MESSAGES)\n        && jerry_get_error_type (error_value) == JERRY_ERROR_SYNTAX)\n    {\n      jerry_char_t *string_end_p = err_str_buf + string_end;\n      unsigned int err_line = 0;\n      unsigned int err_col = 0;\n      char *path_str_p = NULL;\n      char *path_str_end_p = NULL;\n\n      /* 1. parse column and line information */\n      for (jerry_char_t *current_p = err_str_buf; current_p < string_end_p; current_p++)\n      {\n        if (*current_p == '[')\n        {\n          current_p++;\n\n          if (*current_p == '<')\n          {\n            break;\n          }\n\n          path_str_p = (char *) current_p;\n          while (current_p < string_end_p && *current_p != ':')\n          {\n            current_p++;\n          }\n\n          path_str_end_p = (char *) current_p++;\n\n          err_line = (unsigned int) strtol ((char *) current_p, (char **) &current_p, 10);\n\n          current_p++;\n\n          err_col = (unsigned int) strtol ((char *) current_p, NULL, 10);\n          break;\n        }\n      } /* for */\n\n      if (err_line != 0 && err_col > 0 && err_col < SYNTAX_ERROR_MAX_LINE_LENGTH)\n      {\n        uint32_t curr_line = 1;\n        uint32_t pos = 0;\n\n        /* Temporarily modify the error message, so we can use the path. */\n        *path_str_end_p = '\\0';\n\n        size_t source_size;\n        uint8_t *source_p = jerry_port_read_source (path_str_p, &source_size);\n\n        /* Revert the error message. */\n        *path_str_end_p = ':';\n\n        /* 2. seek and print */\n        while (pos < source_size && curr_line < err_line)\n        {\n          if (source_p[pos] == '\\n')\n          {\n            curr_line++;\n          }\n\n          pos++;\n        }\n\n        /* Print character if:\n         * - The max line length is not reached.\n         * - The current position is valid (it is not the end of the source).\n         * - The current character is not a newline.\n         **/\n        for (uint32_t char_count = 0;\n             (char_count < SYNTAX_ERROR_MAX_LINE_LENGTH) && (pos < source_size) && (source_p[pos] != '\\n');\n             char_count++, pos++)\n        {\n          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"%c\", source_p[pos]);\n        }\n        jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"\\n\");\n\n        jerry_port_release_source (source_p);\n\n        while (--err_col)\n        {\n          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"~\");\n        }\n\n        jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"^\\n\\n\");\n      }\n    }\n  }\n\n  jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"%s\\n\", err_str_buf);\n  jerry_release_value (err_str_val);\n\n  if (jerry_value_is_object (error_value))\n  {\n    jerry_value_t stack_str = jerry_create_string ((const jerry_char_t *) \"stack\");\n    jerry_value_t backtrace_val = jerry_get_property (error_value, stack_str);\n    jerry_release_value (stack_str);\n\n    if (jerry_value_is_array (backtrace_val))\n    {\n      uint32_t length = jerry_get_array_length (backtrace_val);\n\n      /* This length should be enough. */\n      if (length > 32)\n      {\n        length = 32;\n      }\n\n      for (uint32_t i = 0; i < length; i++)\n      {\n        jerry_value_t item_val = jerry_get_property_by_index (backtrace_val, i);\n\n        if (jerry_value_is_string (item_val))\n        {\n          jerry_size_t str_size = jerry_get_utf8_string_size (item_val);\n\n          if (str_size >= 256)\n          {\n            printf (\"%6u: [Backtrace string too long]\\n\", i);\n          }\n          else\n          {\n            jerry_size_t string_end = jerry_string_to_utf8_char_buffer (item_val, err_str_buf, str_size);\n            assert (string_end == str_size);\n            err_str_buf[string_end] = 0;\n\n            printf (\"%6u: %s\\n\", i, err_str_buf);\n          }\n        }\n\n        jerry_release_value (item_val);\n      }\n    }\n\n    jerry_release_value (backtrace_val);\n  }\n\n  jerry_release_value (error_value);\n}",
        "func": "void\nmain_print_unhandled_exception (jerry_value_t error_value) /**< error value */\n{\n  assert (jerry_value_is_error (error_value));\n  error_value = jerry_get_value_from_error (error_value, true);\n\n  jerry_char_t err_str_buf[256];\n\n  jerry_value_t err_str_val = jerry_value_to_string (error_value);\n  jerry_size_t err_str_size = jerry_get_utf8_string_size (err_str_val);\n\n  if (err_str_size >= 256)\n  {\n    const char msg[] = \"[Error message too long]\";\n    err_str_size = sizeof (msg) / sizeof (char) - 1;\n    memcpy (err_str_buf, msg, err_str_size + 1);\n  }\n  else\n  {\n    jerry_size_t string_end = jerry_string_to_utf8_char_buffer (err_str_val, err_str_buf, err_str_size);\n    assert (string_end == err_str_size);\n    err_str_buf[string_end] = 0;\n\n    if (jerry_is_feature_enabled (JERRY_FEATURE_ERROR_MESSAGES)\n        && jerry_get_error_type (error_value) == JERRY_ERROR_SYNTAX)\n    {\n      jerry_char_t *string_end_p = err_str_buf + string_end;\n      unsigned int err_line = 0;\n      unsigned int err_col = 0;\n      char *path_str_p = NULL;\n      char *path_str_end_p = NULL;\n\n      /* 1. parse column and line information */\n      for (jerry_char_t *current_p = err_str_buf; current_p < string_end_p; current_p++)\n      {\n        if (*current_p == '[')\n        {\n          current_p++;\n\n          if (*current_p == '<')\n          {\n            break;\n          }\n\n          path_str_p = (char *) current_p;\n          while (current_p < string_end_p && *current_p != ':')\n          {\n            current_p++;\n          }\n\n          path_str_end_p = (char *) current_p++;\n\n          err_line = (unsigned int) strtol ((char *) current_p, (char **) &current_p, 10);\n\n          current_p++;\n\n          err_col = (unsigned int) strtol ((char *) current_p, NULL, 10);\n          break;\n        }\n      } /* for */\n\n      if (err_line != 0 && err_col > 0 && err_col < SYNTAX_ERROR_MAX_LINE_LENGTH)\n      {\n        /* Temporarily modify the error message, so we can use the path. */\n        *path_str_end_p = '\\0';\n\n        size_t source_size;\n        uint8_t *source_p = jerry_port_read_source (path_str_p, &source_size);\n\n        /* Revert the error message. */\n        *path_str_end_p = ':';\n\n        if (source_p != NULL)\n        {\n          uint32_t curr_line = 1;\n          uint32_t pos = 0;\n\n          /* 2. seek and print */\n          while (pos < source_size && curr_line < err_line)\n          {\n            if (source_p[pos] == '\\n')\n            {\n              curr_line++;\n            }\n\n            pos++;\n          }\n\n          /* Print character if:\n          * - The max line length is not reached.\n          * - The current position is valid (it is not the end of the source).\n          * - The current character is not a newline.\n          **/\n          for (uint32_t char_count = 0;\n              (char_count < SYNTAX_ERROR_MAX_LINE_LENGTH) && (pos < source_size) && (source_p[pos] != '\\n');\n              char_count++, pos++)\n          {\n            jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"%c\", source_p[pos]);\n          }\n          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"\\n\");\n\n          jerry_port_release_source (source_p);\n\n          while (--err_col)\n          {\n            jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"~\");\n          }\n\n          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"^\\n\\n\");\n        }\n      }\n    }\n  }\n\n  jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"%s\\n\", err_str_buf);\n  jerry_release_value (err_str_val);\n\n  if (jerry_value_is_object (error_value))\n  {\n    jerry_value_t stack_str = jerry_create_string ((const jerry_char_t *) \"stack\");\n    jerry_value_t backtrace_val = jerry_get_property (error_value, stack_str);\n    jerry_release_value (stack_str);\n\n    if (jerry_value_is_array (backtrace_val))\n    {\n      uint32_t length = jerry_get_array_length (backtrace_val);\n\n      /* This length should be enough. */\n      if (length > 32)\n      {\n        length = 32;\n      }\n\n      for (uint32_t i = 0; i < length; i++)\n      {\n        jerry_value_t item_val = jerry_get_property_by_index (backtrace_val, i);\n\n        if (jerry_value_is_string (item_val))\n        {\n          jerry_size_t str_size = jerry_get_utf8_string_size (item_val);\n\n          if (str_size >= 256)\n          {\n            printf (\"%6u: [Backtrace string too long]\\n\", i);\n          }\n          else\n          {\n            jerry_size_t string_end = jerry_string_to_utf8_char_buffer (item_val, err_str_buf, str_size);\n            assert (string_end == str_size);\n            err_str_buf[string_end] = 0;\n\n            printf (\"%6u: %s\\n\", i, err_str_buf);\n          }\n        }\n\n        jerry_release_value (item_val);\n      }\n    }\n\n    jerry_release_value (backtrace_val);\n  }\n\n  jerry_release_value (error_value);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -61,9 +61,6 @@\n \n       if (err_line != 0 && err_col > 0 && err_col < SYNTAX_ERROR_MAX_LINE_LENGTH)\n       {\n-        uint32_t curr_line = 1;\n-        uint32_t pos = 0;\n-\n         /* Temporarily modify the error message, so we can use the path. */\n         *path_str_end_p = '\\0';\n \n@@ -73,38 +70,44 @@\n         /* Revert the error message. */\n         *path_str_end_p = ':';\n \n-        /* 2. seek and print */\n-        while (pos < source_size && curr_line < err_line)\n+        if (source_p != NULL)\n         {\n-          if (source_p[pos] == '\\n')\n+          uint32_t curr_line = 1;\n+          uint32_t pos = 0;\n+\n+          /* 2. seek and print */\n+          while (pos < source_size && curr_line < err_line)\n           {\n-            curr_line++;\n+            if (source_p[pos] == '\\n')\n+            {\n+              curr_line++;\n+            }\n+\n+            pos++;\n           }\n \n-          pos++;\n+          /* Print character if:\n+          * - The max line length is not reached.\n+          * - The current position is valid (it is not the end of the source).\n+          * - The current character is not a newline.\n+          **/\n+          for (uint32_t char_count = 0;\n+              (char_count < SYNTAX_ERROR_MAX_LINE_LENGTH) && (pos < source_size) && (source_p[pos] != '\\n');\n+              char_count++, pos++)\n+          {\n+            jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"%c\", source_p[pos]);\n+          }\n+          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"\\n\");\n+\n+          jerry_port_release_source (source_p);\n+\n+          while (--err_col)\n+          {\n+            jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"~\");\n+          }\n+\n+          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"^\\n\\n\");\n         }\n-\n-        /* Print character if:\n-         * - The max line length is not reached.\n-         * - The current position is valid (it is not the end of the source).\n-         * - The current character is not a newline.\n-         **/\n-        for (uint32_t char_count = 0;\n-             (char_count < SYNTAX_ERROR_MAX_LINE_LENGTH) && (pos < source_size) && (source_p[pos] != '\\n');\n-             char_count++, pos++)\n-        {\n-          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"%c\", source_p[pos]);\n-        }\n-        jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"\\n\");\n-\n-        jerry_port_release_source (source_p);\n-\n-        while (--err_col)\n-        {\n-          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"~\");\n-        }\n-\n-        jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"^\\n\\n\");\n       }\n     }\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "        uint32_t curr_line = 1;",
                "        uint32_t pos = 0;",
                "",
                "        /* 2. seek and print */",
                "        while (pos < source_size && curr_line < err_line)",
                "          if (source_p[pos] == '\\n')",
                "            curr_line++;",
                "          pos++;",
                "",
                "        /* Print character if:",
                "         * - The max line length is not reached.",
                "         * - The current position is valid (it is not the end of the source).",
                "         * - The current character is not a newline.",
                "         **/",
                "        for (uint32_t char_count = 0;",
                "             (char_count < SYNTAX_ERROR_MAX_LINE_LENGTH) && (pos < source_size) && (source_p[pos] != '\\n');",
                "             char_count++, pos++)",
                "        {",
                "          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"%c\", source_p[pos]);",
                "        }",
                "        jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"\\n\");",
                "",
                "        jerry_port_release_source (source_p);",
                "",
                "        while (--err_col)",
                "        {",
                "          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"~\");",
                "        }",
                "",
                "        jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"^\\n\\n\");"
            ],
            "added_lines": [
                "        if (source_p != NULL)",
                "          uint32_t curr_line = 1;",
                "          uint32_t pos = 0;",
                "",
                "          /* 2. seek and print */",
                "          while (pos < source_size && curr_line < err_line)",
                "            if (source_p[pos] == '\\n')",
                "            {",
                "              curr_line++;",
                "            }",
                "",
                "            pos++;",
                "          /* Print character if:",
                "          * - The max line length is not reached.",
                "          * - The current position is valid (it is not the end of the source).",
                "          * - The current character is not a newline.",
                "          **/",
                "          for (uint32_t char_count = 0;",
                "              (char_count < SYNTAX_ERROR_MAX_LINE_LENGTH) && (pos < source_size) && (source_p[pos] != '\\n');",
                "              char_count++, pos++)",
                "          {",
                "            jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"%c\", source_p[pos]);",
                "          }",
                "          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"\\n\");",
                "",
                "          jerry_port_release_source (source_p);",
                "",
                "          while (--err_col)",
                "          {",
                "            jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"~\");",
                "          }",
                "",
                "          jerry_port_log (JERRY_LOG_LEVEL_ERROR, \"^\\n\\n\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19646",
        "func_name": "sqlite/sqlite3CreateColumnExpr",
        "description": "pragma.c in SQLite through 3.30.1 mishandles NOT NULL in an integrity_check PRAGMA command in certain cases of generated columns.",
        "git_url": "https://github.com/sqlite/sqlite/commit/926f796e8feec15f3836aa0a060ed906f8ae04d3",
        "commit_title": "Ensure that the SrcList_item.colUsed field is set correctly (set to have a",
        "commit_text": "1 for all columns of the table) when a generated column appears in the USING clause of a join.  FossilOrigin-Name: 1923efb283e8840fa7436eb20b9d2174ef7cace1690d3b97b572a0db2048b8e3",
        "func_before": "Expr *sqlite3CreateColumnExpr(sqlite3 *db, SrcList *pSrc, int iSrc, int iCol){\n  Expr *p = sqlite3ExprAlloc(db, TK_COLUMN, 0, 0);\n  if( p ){\n    struct SrcList_item *pItem = &pSrc->a[iSrc];\n    p->y.pTab = pItem->pTab;\n    p->iTable = pItem->iCursor;\n    if( p->y.pTab->iPKey==iCol ){\n      p->iColumn = -1;\n    }else{\n      p->iColumn = (ynVar)iCol;\n      testcase( iCol==BMS );\n      testcase( iCol==BMS-1 );\n      pItem->colUsed |= ((Bitmask)1)<<(iCol>=BMS ? BMS-1 : iCol);\n    }\n  }\n  return p;\n}",
        "func": "Expr *sqlite3CreateColumnExpr(sqlite3 *db, SrcList *pSrc, int iSrc, int iCol){\n  Expr *p = sqlite3ExprAlloc(db, TK_COLUMN, 0, 0);\n  if( p ){\n    struct SrcList_item *pItem = &pSrc->a[iSrc];\n    Table *pTab = p->y.pTab = pItem->pTab;\n    p->iTable = pItem->iCursor;\n    if( p->y.pTab->iPKey==iCol ){\n      p->iColumn = -1;\n    }else{\n      p->iColumn = (ynVar)iCol;\n      if( pTab->tabFlags & TF_HasGenerated ){\n        Column *pColumn = pTab->aCol + iCol;\n        if( pColumn->colFlags & COLFLAG_GENERATED ){\n          testcase( pTab->nCol==63 );\n          testcase( pTab->nCol==64 );\n          if( pTab->nCol>=64 ){\n            pItem->colUsed = ALLBITS;\n          }else{\n            pItem->colUsed = MASKBIT(pTab->nCol)-1;\n          }\n        }\n      }else{\n        testcase( iCol==BMS );\n        testcase( iCol==BMS-1 );\n        pItem->colUsed |= ((Bitmask)1)<<(iCol>=BMS ? BMS-1 : iCol);\n      }\n    }\n  }\n  return p;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,15 +2,28 @@\n   Expr *p = sqlite3ExprAlloc(db, TK_COLUMN, 0, 0);\n   if( p ){\n     struct SrcList_item *pItem = &pSrc->a[iSrc];\n-    p->y.pTab = pItem->pTab;\n+    Table *pTab = p->y.pTab = pItem->pTab;\n     p->iTable = pItem->iCursor;\n     if( p->y.pTab->iPKey==iCol ){\n       p->iColumn = -1;\n     }else{\n       p->iColumn = (ynVar)iCol;\n-      testcase( iCol==BMS );\n-      testcase( iCol==BMS-1 );\n-      pItem->colUsed |= ((Bitmask)1)<<(iCol>=BMS ? BMS-1 : iCol);\n+      if( pTab->tabFlags & TF_HasGenerated ){\n+        Column *pColumn = pTab->aCol + iCol;\n+        if( pColumn->colFlags & COLFLAG_GENERATED ){\n+          testcase( pTab->nCol==63 );\n+          testcase( pTab->nCol==64 );\n+          if( pTab->nCol>=64 ){\n+            pItem->colUsed = ALLBITS;\n+          }else{\n+            pItem->colUsed = MASKBIT(pTab->nCol)-1;\n+          }\n+        }\n+      }else{\n+        testcase( iCol==BMS );\n+        testcase( iCol==BMS-1 );\n+        pItem->colUsed |= ((Bitmask)1)<<(iCol>=BMS ? BMS-1 : iCol);\n+      }\n     }\n   }\n   return p;",
        "diff_line_info": {
            "deleted_lines": [
                "    p->y.pTab = pItem->pTab;",
                "      testcase( iCol==BMS );",
                "      testcase( iCol==BMS-1 );",
                "      pItem->colUsed |= ((Bitmask)1)<<(iCol>=BMS ? BMS-1 : iCol);"
            ],
            "added_lines": [
                "    Table *pTab = p->y.pTab = pItem->pTab;",
                "      if( pTab->tabFlags & TF_HasGenerated ){",
                "        Column *pColumn = pTab->aCol + iCol;",
                "        if( pColumn->colFlags & COLFLAG_GENERATED ){",
                "          testcase( pTab->nCol==63 );",
                "          testcase( pTab->nCol==64 );",
                "          if( pTab->nCol>=64 ){",
                "            pItem->colUsed = ALLBITS;",
                "          }else{",
                "            pItem->colUsed = MASKBIT(pTab->nCol)-1;",
                "          }",
                "        }",
                "      }else{",
                "        testcase( iCol==BMS );",
                "        testcase( iCol==BMS-1 );",
                "        pItem->colUsed |= ((Bitmask)1)<<(iCol>=BMS ? BMS-1 : iCol);",
                "      }"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-13142",
        "func_name": "ImageMagick/RegisterPNGImage",
        "description": "In ImageMagick before 6.9.9-0 and 7.x before 7.0.6-1, a crafted PNG file could trigger a crash because there was an insufficient check for short files.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/aa84944b405acebbeefe871d0f64969b9e9f31ac",
        "commit_title": "...",
        "commit_text": "",
        "func_before": "ModuleExport size_t RegisterPNGImage(void)\n{\n  char\n    version[MaxTextExtent];\n\n  MagickInfo\n    *entry;\n\n  static const char\n    *PNGNote=\n    {\n      \"See http://www.libpng.org/ for details about the PNG format.\"\n    },\n\n    *JNGNote=\n    {\n      \"See http://www.libpng.org/pub/mng/ for details about the JNG\\n\"\n      \"format.\"\n    },\n\n    *MNGNote=\n    {\n      \"See http://www.libpng.org/pub/mng/ for details about the MNG\\n\"\n      \"format.\"\n    };\n\n  *version='\\0';\n\n#if defined(PNG_LIBPNG_VER_STRING)\n  (void) ConcatenateMagickString(version,\"libpng \",MaxTextExtent);\n  (void) ConcatenateMagickString(version,PNG_LIBPNG_VER_STRING,MaxTextExtent);\n\n  if (LocaleCompare(PNG_LIBPNG_VER_STRING,png_get_header_ver(NULL)) != 0)\n    {\n      (void) ConcatenateMagickString(version,\",\",MaxTextExtent);\n      (void) ConcatenateMagickString(version,png_get_libpng_ver(NULL),\n            MaxTextExtent);\n    }\n#endif\n\n  entry=SetMagickInfo(\"MNG\");\n  entry->seekable_stream=MagickTrue;  /* To do: eliminate this. */\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadMNGImage;\n  entry->encoder=(EncodeImageHandler *) WriteMNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsMNG;\n  entry->description=ConstantString(\"Multiple-image Network Graphics\");\n\n  if (*version != '\\0')\n    entry->version=ConstantString(version);\n  entry->mime_type=ConstantString(\"video/x-mng\");\n  entry->module=ConstantString(\"PNG\");\n  entry->note=ConstantString(MNGNote);\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"Portable Network Graphics\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n\n  if (*version != '\\0')\n    entry->version=ConstantString(version);\n\n  entry->note=ConstantString(PNGNote);\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG8\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\n            \"8-bit indexed with optional binary transparency\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG24\");\n  *version='\\0';\n\n#if defined(ZLIB_VERSION)\n  (void) ConcatenateMagickString(version,\"zlib \",MaxTextExtent);\n  (void) ConcatenateMagickString(version,ZLIB_VERSION,MaxTextExtent);\n\n  if (LocaleCompare(ZLIB_VERSION,zlib_version) != 0)\n    {\n      (void) ConcatenateMagickString(version,\",\",MaxTextExtent);\n      (void) ConcatenateMagickString(version,zlib_version,MaxTextExtent);\n    }\n#endif\n\n  if (*version != '\\0')\n    entry->version=ConstantString(version);\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"opaque or binary transparent 24-bit RGB\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG32\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"opaque or transparent 32-bit RGBA\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG48\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"opaque or binary transparent 48-bit RGB\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG64\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"opaque or transparent 64-bit RGBA\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG00\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\n     \"PNG inheriting bit-depth, color-type from original if possible\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"JNG\");\n\n#if defined(JNG_SUPPORTED)\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadJNGImage;\n  entry->encoder=(EncodeImageHandler *) WriteJNGImage;\n#endif\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsJNG;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"JPEG Network Graphics\");\n  entry->mime_type=ConstantString(\"image/x-jng\");\n  entry->module=ConstantString(\"PNG\");\n  entry->note=ConstantString(JNGNote);\n  (void) RegisterMagickInfo(entry);\n\n#ifdef IMPNG_SETJMP_NOT_THREAD_SAFE\n  ping_semaphore=AllocateSemaphoreInfo();\n#endif\n\n  return(MagickImageCoderSignature);\n}",
        "func": "ModuleExport size_t RegisterPNGImage(void)\n{\n  char\n    version[MaxTextExtent];\n\n  MagickInfo\n    *entry;\n\n  static const char\n    *PNGNote=\n    {\n      \"See http://www.libpng.org/ for details about the PNG format.\"\n    },\n\n    *JNGNote=\n    {\n      \"See http://www.libpng.org/pub/mng/ for details about the JNG\\n\"\n      \"format.\"\n    },\n\n    *MNGNote=\n    {\n      \"See http://www.libpng.org/pub/mng/ for details about the MNG\\n\"\n      \"format.\"\n    };\n\n  *version='\\0';\n\n#if defined(PNG_LIBPNG_VER_STRING)\n  (void) ConcatenateMagickString(version,\"libpng \",MaxTextExtent);\n  (void) ConcatenateMagickString(version,PNG_LIBPNG_VER_STRING,MaxTextExtent);\n\n  if (LocaleCompare(PNG_LIBPNG_VER_STRING,png_get_header_ver(NULL)) != 0)\n    {\n      (void) ConcatenateMagickString(version,\",\",MaxTextExtent);\n      (void) ConcatenateMagickString(version,png_get_libpng_ver(NULL),\n            MaxTextExtent);\n    }\n#endif\n\n  entry=SetMagickInfo(\"MNG\");\n  entry->seekable_stream=MagickTrue;\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadMNGImage;\n  entry->encoder=(EncodeImageHandler *) WriteMNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsMNG;\n  entry->description=ConstantString(\"Multiple-image Network Graphics\");\n\n  if (*version != '\\0')\n    entry->version=ConstantString(version);\n  entry->mime_type=ConstantString(\"video/x-mng\");\n  entry->module=ConstantString(\"PNG\");\n  entry->note=ConstantString(MNGNote);\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->seekable_stream=MagickTrue;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"Portable Network Graphics\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n\n  if (*version != '\\0')\n    entry->version=ConstantString(version);\n\n  entry->note=ConstantString(PNGNote);\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG8\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->seekable_stream=MagickTrue;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\n            \"8-bit indexed with optional binary transparency\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG24\");\n  *version='\\0';\n\n#if defined(ZLIB_VERSION)\n  (void) ConcatenateMagickString(version,\"zlib \",MaxTextExtent);\n  (void) ConcatenateMagickString(version,ZLIB_VERSION,MaxTextExtent);\n\n  if (LocaleCompare(ZLIB_VERSION,zlib_version) != 0)\n    {\n      (void) ConcatenateMagickString(version,\",\",MaxTextExtent);\n      (void) ConcatenateMagickString(version,zlib_version,MaxTextExtent);\n    }\n#endif\n\n  if (*version != '\\0')\n    entry->version=ConstantString(version);\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->seekable_stream=MagickTrue;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"opaque or binary transparent 24-bit RGB\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG32\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->seekable_stream=MagickTrue;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"opaque or transparent 32-bit RGBA\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG48\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->seekable_stream=MagickTrue;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"opaque or binary transparent 48-bit RGB\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG64\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->seekable_stream=MagickTrue;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"opaque or transparent 64-bit RGBA\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"PNG00\");\n\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadPNGImage;\n  entry->encoder=(EncodeImageHandler *) WritePNGImage;\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsPNG;\n  entry->seekable_stream=MagickTrue;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\n     \"PNG inheriting bit-depth, color-type from original if possible\");\n  entry->mime_type=ConstantString(\"image/png\");\n  entry->module=ConstantString(\"PNG\");\n  (void) RegisterMagickInfo(entry);\n\n  entry=SetMagickInfo(\"JNG\");\n\n#if defined(JNG_SUPPORTED)\n#if defined(MAGICKCORE_PNG_DELEGATE)\n  entry->decoder=(DecodeImageHandler *) ReadJNGImage;\n  entry->encoder=(EncodeImageHandler *) WriteJNGImage;\n#endif\n#endif\n\n  entry->magick=(IsImageFormatHandler *) IsJNG;\n  entry->seekable_stream=MagickTrue;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"JPEG Network Graphics\");\n  entry->mime_type=ConstantString(\"image/x-jng\");\n  entry->module=ConstantString(\"PNG\");\n  entry->note=ConstantString(JNGNote);\n  (void) RegisterMagickInfo(entry);\n\n#ifdef IMPNG_SETJMP_NOT_THREAD_SAFE\n  ping_semaphore=AllocateSemaphoreInfo();\n#endif\n\n  return(MagickImageCoderSignature);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -39,7 +39,7 @@\n #endif\n \n   entry=SetMagickInfo(\"MNG\");\n-  entry->seekable_stream=MagickTrue;  /* To do: eliminate this. */\n+  entry->seekable_stream=MagickTrue;\n \n #if defined(MAGICKCORE_PNG_DELEGATE)\n   entry->decoder=(DecodeImageHandler *) ReadMNGImage;\n@@ -64,6 +64,7 @@\n #endif\n \n   entry->magick=(IsImageFormatHandler *) IsPNG;\n+  entry->seekable_stream=MagickTrue;\n   entry->adjoin=MagickFalse;\n   entry->description=ConstantString(\"Portable Network Graphics\");\n   entry->mime_type=ConstantString(\"image/png\");\n@@ -83,6 +84,7 @@\n #endif\n \n   entry->magick=(IsImageFormatHandler *) IsPNG;\n+  entry->seekable_stream=MagickTrue;\n   entry->adjoin=MagickFalse;\n   entry->description=ConstantString(\n             \"8-bit indexed with optional binary transparency\");\n@@ -113,6 +115,7 @@\n #endif\n \n   entry->magick=(IsImageFormatHandler *) IsPNG;\n+  entry->seekable_stream=MagickTrue;\n   entry->adjoin=MagickFalse;\n   entry->description=ConstantString(\"opaque or binary transparent 24-bit RGB\");\n   entry->mime_type=ConstantString(\"image/png\");\n@@ -127,6 +130,7 @@\n #endif\n \n   entry->magick=(IsImageFormatHandler *) IsPNG;\n+  entry->seekable_stream=MagickTrue;\n   entry->adjoin=MagickFalse;\n   entry->description=ConstantString(\"opaque or transparent 32-bit RGBA\");\n   entry->mime_type=ConstantString(\"image/png\");\n@@ -141,6 +145,7 @@\n #endif\n \n   entry->magick=(IsImageFormatHandler *) IsPNG;\n+  entry->seekable_stream=MagickTrue;\n   entry->adjoin=MagickFalse;\n   entry->description=ConstantString(\"opaque or binary transparent 48-bit RGB\");\n   entry->mime_type=ConstantString(\"image/png\");\n@@ -155,6 +160,7 @@\n #endif\n \n   entry->magick=(IsImageFormatHandler *) IsPNG;\n+  entry->seekable_stream=MagickTrue;\n   entry->adjoin=MagickFalse;\n   entry->description=ConstantString(\"opaque or transparent 64-bit RGBA\");\n   entry->mime_type=ConstantString(\"image/png\");\n@@ -169,6 +175,7 @@\n #endif\n \n   entry->magick=(IsImageFormatHandler *) IsPNG;\n+  entry->seekable_stream=MagickTrue;\n   entry->adjoin=MagickFalse;\n   entry->description=ConstantString(\n      \"PNG inheriting bit-depth, color-type from original if possible\");\n@@ -186,6 +193,7 @@\n #endif\n \n   entry->magick=(IsImageFormatHandler *) IsJNG;\n+  entry->seekable_stream=MagickTrue;\n   entry->adjoin=MagickFalse;\n   entry->description=ConstantString(\"JPEG Network Graphics\");\n   entry->mime_type=ConstantString(\"image/x-jng\");",
        "diff_line_info": {
            "deleted_lines": [
                "  entry->seekable_stream=MagickTrue;  /* To do: eliminate this. */"
            ],
            "added_lines": [
                "  entry->seekable_stream=MagickTrue;",
                "  entry->seekable_stream=MagickTrue;",
                "  entry->seekable_stream=MagickTrue;",
                "  entry->seekable_stream=MagickTrue;",
                "  entry->seekable_stream=MagickTrue;",
                "  entry->seekable_stream=MagickTrue;",
                "  entry->seekable_stream=MagickTrue;",
                "  entry->seekable_stream=MagickTrue;",
                "  entry->seekable_stream=MagickTrue;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-13142",
        "func_name": "ImageMagick/ReadPNGImage",
        "description": "In ImageMagick before 6.9.9-0 and 7.x before 7.0.6-1, a crafted PNG file could trigger a crash because there was an insufficient check for short files.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/aa84944b405acebbeefe871d0f64969b9e9f31ac",
        "commit_title": "...",
        "commit_text": "",
        "func_before": "static Image *ReadPNGImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    logging,\n    status;\n\n  MngInfo\n    *mng_info;\n\n  char\n    magic_number[MaxTextExtent];\n\n  ssize_t\n    count;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging=LogMagickEvent(CoderEvent,GetMagickModule(),\"Enter ReadPNGImage()\");\n  image=AcquireImage(image_info);\n  mng_info=(MngInfo *) NULL;\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n\n  if (status == MagickFalse)\n    ThrowReaderException(FileOpenError,\"UnableToOpenFile\");\n\n  /*\n    Verify PNG signature.\n  */\n  count=ReadBlob(image,8,(unsigned char *) magic_number);\n\n  if (count < 8 || memcmp(magic_number,\"\\211PNG\\r\\n\\032\\n\",8) != 0)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  /*\n    Allocate a MngInfo structure.\n  */\n  mng_info=(MngInfo *) AcquireMagickMemory(sizeof(MngInfo));\n\n  if (mng_info == (MngInfo *) NULL)\n    ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n  /*\n    Initialize members of the MngInfo structure.\n  */\n  (void) ResetMagickMemory(mng_info,0,sizeof(MngInfo));\n  mng_info->image=image;\n\n  image=ReadOnePNGImage(mng_info,image_info,exception);\n  mng_info=MngInfoFreeStruct(mng_info);\n\n  if (image == (Image *) NULL)\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"exit ReadPNGImage() with error\");\n\n      return((Image *) NULL);\n    }\n\n  (void) CloseBlob(image);\n\n  if ((image->columns == 0) || (image->rows == 0))\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"exit ReadPNGImage() with error.\");\n\n      ThrowReaderException(CorruptImageError,\"CorruptImage\");\n    }\n\n  if ((IssRGBColorspace(image->colorspace) != MagickFalse) &&\n      ((image->gamma < .45) || (image->gamma > .46)) &&\n           !(image->chromaticity.red_primary.x>0.6399f &&\n           image->chromaticity.red_primary.x<0.6401f &&\n           image->chromaticity.red_primary.y>0.3299f &&\n           image->chromaticity.red_primary.y<0.3301f &&\n           image->chromaticity.green_primary.x>0.2999f &&\n           image->chromaticity.green_primary.x<0.3001f &&\n           image->chromaticity.green_primary.y>0.5999f &&\n           image->chromaticity.green_primary.y<0.6001f &&\n           image->chromaticity.blue_primary.x>0.1499f &&\n           image->chromaticity.blue_primary.x<0.1501f &&\n           image->chromaticity.blue_primary.y>0.0599f &&\n           image->chromaticity.blue_primary.y<0.0601f &&\n           image->chromaticity.white_point.x>0.3126f &&\n           image->chromaticity.white_point.x<0.3128f &&\n           image->chromaticity.white_point.y>0.3289f &&\n           image->chromaticity.white_point.y<0.3291f))\n    SetImageColorspace(image,RGBColorspace);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  page.w: %.20g, page.h: %.20g,page.x: %.20g, page.y: %.20g.\",\n            (double) image->page.width,(double) image->page.height,\n            (double) image->page.x,(double) image->page.y);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"exit ReadPNGImage()\");\n\n  return(image);\n}",
        "func": "static Image *ReadPNGImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    logging,\n    status;\n\n  MngInfo\n    *mng_info;\n\n  char\n    magic_number[MaxTextExtent];\n\n  ssize_t\n    count;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging=LogMagickEvent(CoderEvent,GetMagickModule(),\"Enter ReadPNGImage()\");\n  image=AcquireImage(image_info);\n  mng_info=(MngInfo *) NULL;\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n\n  if (status == MagickFalse)\n    ThrowReaderException(FileOpenError,\"UnableToOpenFile\");\n\n  /*\n    Verify PNG signature.\n  */\n  count=ReadBlob(image,8,(unsigned char *) magic_number);\n\n  if (count < 8 || memcmp(magic_number,\"\\211PNG\\r\\n\\032\\n\",8) != 0)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  /*\n     Verify that file size large enough to contain a PNG datastream.\n  */\n  if (GetBlobSize(image) < 61)\n    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n\n  /*\n    Allocate a MngInfo structure.\n  */\n  mng_info=(MngInfo *) AcquireMagickMemory(sizeof(MngInfo));\n\n  if (mng_info == (MngInfo *) NULL)\n    ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n  /*\n    Initialize members of the MngInfo structure.\n  */\n  (void) ResetMagickMemory(mng_info,0,sizeof(MngInfo));\n  mng_info->image=image;\n\n  image=ReadOnePNGImage(mng_info,image_info,exception);\n  mng_info=MngInfoFreeStruct(mng_info);\n\n  if (image == (Image *) NULL)\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"exit ReadPNGImage() with error\");\n\n      return((Image *) NULL);\n    }\n\n  (void) CloseBlob(image);\n\n  if ((image->columns == 0) || (image->rows == 0))\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"exit ReadPNGImage() with error.\");\n\n      ThrowReaderException(CorruptImageError,\"CorruptImage\");\n    }\n\n  if ((IssRGBColorspace(image->colorspace) != MagickFalse) &&\n      ((image->gamma < .45) || (image->gamma > .46)) &&\n           !(image->chromaticity.red_primary.x>0.6399f &&\n           image->chromaticity.red_primary.x<0.6401f &&\n           image->chromaticity.red_primary.y>0.3299f &&\n           image->chromaticity.red_primary.y<0.3301f &&\n           image->chromaticity.green_primary.x>0.2999f &&\n           image->chromaticity.green_primary.x<0.3001f &&\n           image->chromaticity.green_primary.y>0.5999f &&\n           image->chromaticity.green_primary.y<0.6001f &&\n           image->chromaticity.blue_primary.x>0.1499f &&\n           image->chromaticity.blue_primary.x<0.1501f &&\n           image->chromaticity.blue_primary.y>0.0599f &&\n           image->chromaticity.blue_primary.y<0.0601f &&\n           image->chromaticity.white_point.x>0.3126f &&\n           image->chromaticity.white_point.x<0.3128f &&\n           image->chromaticity.white_point.y>0.3289f &&\n           image->chromaticity.white_point.y<0.3291f))\n    SetImageColorspace(image,RGBColorspace);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  page.w: %.20g, page.h: %.20g,page.x: %.20g, page.y: %.20g.\",\n            (double) image->page.width,(double) image->page.height,\n            (double) image->page.x,(double) image->page.y);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"exit ReadPNGImage()\");\n\n  return(image);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -43,6 +43,12 @@\n \n   if (count < 8 || memcmp(magic_number,\"\\211PNG\\r\\n\\032\\n\",8) != 0)\n     ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n+\n+  /*\n+     Verify that file size large enough to contain a PNG datastream.\n+  */\n+  if (GetBlobSize(image) < 61)\n+    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n \n   /*\n     Allocate a MngInfo structure.",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  /*",
                "     Verify that file size large enough to contain a PNG datastream.",
                "  */",
                "  if (GetBlobSize(image) < 61)",
                "    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-13142",
        "func_name": "ImageMagick/ReadJNGImage",
        "description": "In ImageMagick before 6.9.9-0 and 7.x before 7.0.6-1, a crafted PNG file could trigger a crash because there was an insufficient check for short files.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/aa84944b405acebbeefe871d0f64969b9e9f31ac",
        "commit_title": "...",
        "commit_text": "",
        "func_before": "static Image *ReadJNGImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    logging,\n    status;\n\n  MngInfo\n    *mng_info;\n\n  char\n    magic_number[MaxTextExtent];\n\n  size_t\n    count;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging=LogMagickEvent(CoderEvent,GetMagickModule(),\"Enter ReadJNGImage()\");\n  image=AcquireImage(image_info);\n  mng_info=(MngInfo *) NULL;\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n\n  if (status == MagickFalse)\n    return((Image *) NULL);\n\n  if (LocaleCompare(image_info->magick,\"JNG\") != 0)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  /* Verify JNG signature.  */\n\n  count=(size_t) ReadBlob(image,8,(unsigned char *) magic_number);\n\n  if (count < 8 || memcmp(magic_number,\"\\213JNG\\r\\n\\032\\n\",8) != 0)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  /* Allocate a MngInfo structure.  */\n\n  mng_info=(MngInfo *) AcquireMagickMemory(sizeof(*mng_info));\n\n  if (mng_info == (MngInfo *) NULL)\n    ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n  /* Initialize members of the MngInfo structure.  */\n\n  (void) ResetMagickMemory(mng_info,0,sizeof(MngInfo));\n\n  mng_info->image=image;\n  image=ReadOneJNGImage(mng_info,image_info,exception);\n  mng_info=MngInfoFreeStruct(mng_info);\n\n  if (image == (Image *) NULL)\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"exit ReadJNGImage() with error\");\n\n      return((Image *) NULL);\n    }\n  (void) CloseBlob(image);\n\n  if (image->columns == 0 || image->rows == 0)\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"exit ReadJNGImage() with error\");\n\n      ThrowReaderException(CorruptImageError,\"CorruptImage\");\n    }\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"exit ReadJNGImage()\");\n\n  return(image);\n}",
        "func": "static Image *ReadJNGImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    logging,\n    status;\n\n  MngInfo\n    *mng_info;\n\n  char\n    magic_number[MaxTextExtent];\n\n  size_t\n    count;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging=LogMagickEvent(CoderEvent,GetMagickModule(),\"Enter ReadJNGImage()\");\n  image=AcquireImage(image_info);\n  mng_info=(MngInfo *) NULL;\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n\n  if (status == MagickFalse)\n    return((Image *) NULL);\n\n  if (LocaleCompare(image_info->magick,\"JNG\") != 0)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  /* Verify JNG signature.  */\n\n  count=(size_t) ReadBlob(image,8,(unsigned char *) magic_number);\n\n  if (count < 8 || memcmp(magic_number,\"\\213JNG\\r\\n\\032\\n\",8) != 0)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  /*\n     Verify that file size large enough to contain a JNG datastream.\n  */\n  if (GetBlobSize(image) < 147)\n    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n\n  /* Allocate a MngInfo structure.  */\n\n  mng_info=(MngInfo *) AcquireMagickMemory(sizeof(*mng_info));\n\n  if (mng_info == (MngInfo *) NULL)\n    ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n  /* Initialize members of the MngInfo structure.  */\n\n  (void) ResetMagickMemory(mng_info,0,sizeof(MngInfo));\n\n  mng_info->image=image;\n  image=ReadOneJNGImage(mng_info,image_info,exception);\n  mng_info=MngInfoFreeStruct(mng_info);\n\n  if (image == (Image *) NULL)\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"exit ReadJNGImage() with error\");\n\n      return((Image *) NULL);\n    }\n  (void) CloseBlob(image);\n\n  if (image->columns == 0 || image->rows == 0)\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"exit ReadJNGImage() with error\");\n\n      ThrowReaderException(CorruptImageError,\"CorruptImage\");\n    }\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"exit ReadJNGImage()\");\n\n  return(image);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -42,6 +42,12 @@\n   if (count < 8 || memcmp(magic_number,\"\\213JNG\\r\\n\\032\\n\",8) != 0)\n     ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n \n+  /*\n+     Verify that file size large enough to contain a JNG datastream.\n+  */\n+  if (GetBlobSize(image) < 147)\n+    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n+\n   /* Allocate a MngInfo structure.  */\n \n   mng_info=(MngInfo *) AcquireMagickMemory(sizeof(*mng_info));",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  /*",
                "     Verify that file size large enough to contain a JNG datastream.",
                "  */",
                "  if (GetBlobSize(image) < 147)",
                "    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-49286",
        "func_name": "squid-cache/squid/ipcCreate",
        "description": "Squid is a caching proxy for the Web supporting HTTP, HTTPS, FTP, and more. Due to an Incorrect Check of Function Return Value bug Squid is vulnerable to a Denial of Service attack against its Helper process management. This bug is fixed by Squid version 6.5. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
        "git_url": "https://github.com/squid-cache/squid/commit/6014c6648a2a54a4ecb7f952ea1163e0798f9264",
        "commit_title": "Exit without asserting when helper process startup fails (#1543)",
        "commit_text": " ... to dup() after fork() and before execvp().  Assertions are for handling program logic errors. Helper initialization code already handled system call errors correctly (i.e. by exiting the newly created helper process with an error), except for a couple of assert()s that could be triggered by dup(2) failures.  This bug was discovered and detailed by Joshua Rogers at https://megamansec.github.io/Squid-Security-Audit/ipc-assert.html where it was filed as 'Assertion in Squid \"Helper\" Process Creator'.",
        "func_before": "pid_t\nipcCreate(int type, const char *prog, const char *const args[], const char *name, Ip::Address &local_addr, int *rfd, int *wfd, void **hIpc)\n{\n    pid_t pid;\n    Ip::Address ChS;\n    Ip::Address PaS;\n    struct addrinfo *AI = nullptr;\n    int crfd = -1;\n    int prfd = -1;\n    int cwfd = -1;\n    int pwfd = -1;\n    int fd;\n    int t1, t2, t3;\n    int x;\n    int xerrno;\n\n#if USE_POLL && _SQUID_OSF_\n    assert(type != IPC_FIFO);\n#endif\n\n    if (rfd)\n        *rfd = -1;\n\n    if (wfd)\n        *wfd = -1;\n\n    if (hIpc)\n        *hIpc = nullptr;\n\n// NP: no wrapping around d and c usage since we *want* code expansion\n#define IPC_CHECK_FAIL(f,d,c) \\\n    if ((f) < 0) { \\\n        debugs(54, DBG_CRITICAL, \"ERROR: Failed to create helper \" d \" FD: \" << c); \\\n        return ipcCloseAllFD(prfd, pwfd, crfd, cwfd); \\\n    } else void(0)\n\n    if (type == IPC_TCP_SOCKET) {\n        crfd = cwfd = comm_open_listener(SOCK_STREAM,\n                                         0,\n                                         local_addr,\n                                         COMM_NOCLOEXEC,\n                                         name);\n        prfd = pwfd = comm_open(SOCK_STREAM,\n                                0,          /* protocol */\n                                local_addr,\n                                0,          /* blocking */\n                                name);\n        IPC_CHECK_FAIL(crfd, \"child read\", \"TCP \" << local_addr);\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"TCP \" << local_addr);\n    } else if (type == IPC_UDP_SOCKET) {\n        crfd = cwfd = comm_open(SOCK_DGRAM,\n                                0,\n                                local_addr,\n                                COMM_NOCLOEXEC,\n                                name);\n        prfd = pwfd = comm_open(SOCK_DGRAM,\n                                0,\n                                local_addr,\n                                0,\n                                name);\n        IPC_CHECK_FAIL(crfd, \"child read\", \"UDP\" << local_addr);\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"UDP\" << local_addr);\n    } else if (type == IPC_FIFO) {\n        int p2c[2];\n        int c2p[2];\n\n        if (pipe(p2c) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: pipe: \" << xstrerr(xerrno));\n            return -1; // maybe ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n        fd_open(prfd = p2c[0], FD_PIPE, \"IPC FIFO Parent Read\");\n        fd_open(cwfd = p2c[1], FD_PIPE, \"IPC FIFO Child Write\");\n\n        if (pipe(c2p) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: pipe: \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n        fd_open(crfd = c2p[0], FD_PIPE, \"IPC FIFO Child Read\");\n        fd_open(pwfd = c2p[1], FD_PIPE, \"IPC FIFO Parent Write\");\n\n        IPC_CHECK_FAIL(crfd, \"child read\", \"FIFO pipe\");\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"FIFO pipe\");\n\n#if HAVE_SOCKETPAIR && defined(AF_UNIX)\n\n    } else if (type == IPC_UNIX_STREAM) {\n        int fds[2];\n        int buflen = 32768;\n\n        if (socketpair(AF_UNIX, SOCK_STREAM, 0, fds) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: socketpair: \" << xstrerr(xerrno));\n            return -1;\n        }\n\n        errno = 0;\n        if (setsockopt(fds[0], SOL_SOCKET, SO_SNDBUF, (void *) &buflen, sizeof(buflen)) == -1)  {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ERROR: setsockopt failed: \" << xstrerr(xerrno));\n            errno = 0;\n        }\n        if (setsockopt(fds[0], SOL_SOCKET, SO_RCVBUF, (void *) &buflen, sizeof(buflen)) == -1) {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ERROR: setsockopt failed: \" << xstrerr(xerrno));\n            errno = 0;\n        }\n        if (setsockopt(fds[1], SOL_SOCKET, SO_SNDBUF, (void *) &buflen, sizeof(buflen)) == -1) {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ERROR: setsockopt failed: \" << xstrerr(xerrno));\n            errno = 0;\n        }\n        if (setsockopt(fds[1], SOL_SOCKET, SO_RCVBUF, (void *) &buflen, sizeof(buflen)) == -1) {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ERROR: setsockopt failed: \" << xstrerr(xerrno));\n            errno = 0;\n        }\n        fd_open(prfd = pwfd = fds[0], FD_PIPE, \"IPC UNIX STREAM Parent\");\n        fd_open(crfd = cwfd = fds[1], FD_PIPE, \"IPC UNIX STREAM Parent\");\n        IPC_CHECK_FAIL(crfd, \"child read\", \"UDS socket\");\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"UDS socket\");\n\n    } else if (type == IPC_UNIX_DGRAM) {\n        int fds[2];\n\n        if (socketpair(AF_UNIX, SOCK_DGRAM, 0, fds) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: socketpair: \" << xstrerr(xerrno));\n            return -1;\n        }\n\n        fd_open(prfd = pwfd = fds[0], FD_PIPE, \"IPC UNIX DGRAM Parent\");\n        fd_open(crfd = cwfd = fds[1], FD_PIPE, \"IPC UNIX DGRAM Parent\");\n\n        IPC_CHECK_FAIL(crfd, \"child read\", \"UDS datagram\");\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"UDS datagram\");\n#endif\n\n    } else {\n        assert(IPC_NONE);\n    }\n\n    debugs(54, 3, \"ipcCreate: prfd FD \" << prfd);\n    debugs(54, 3, \"ipcCreate: pwfd FD \" << pwfd);\n    debugs(54, 3, \"ipcCreate: crfd FD \" << crfd);\n    debugs(54, 3, \"ipcCreate: cwfd FD \" << cwfd);\n\n    if (type == IPC_TCP_SOCKET || type == IPC_UDP_SOCKET) {\n        Ip::Address::InitAddr(AI);\n\n        if (getsockname(pwfd, AI->ai_addr, &AI->ai_addrlen) < 0) {\n            xerrno = errno;\n            Ip::Address::FreeAddr(AI);\n            debugs(54, DBG_CRITICAL, \"ipcCreate: getsockname: \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        PaS = *AI;\n\n        debugs(54, 3, \"ipcCreate: FD \" << pwfd << \" sockaddr \" << PaS);\n\n        Ip::Address::FreeAddr(AI);\n\n        Ip::Address::InitAddr(AI);\n\n        if (getsockname(crfd, AI->ai_addr, &AI->ai_addrlen) < 0) {\n            xerrno = errno;\n            Ip::Address::FreeAddr(AI);\n            debugs(54, DBG_CRITICAL, \"ipcCreate: getsockname: \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        ChS = *AI;\n\n        Ip::Address::FreeAddr(AI);\n\n        debugs(54, 3, \"ipcCreate: FD \" << crfd << \" sockaddr \" << ChS );\n\n    }\n\n    if (type == IPC_TCP_SOCKET) {\n        if (listen(crfd, 1) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ipcCreate: listen FD \" << crfd << \": \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        debugs(54, 3, \"ipcCreate: FD \" << crfd << \" listening...\");\n    }\n\n    /* flush or else we get dup data if unbuffered_logs is set */\n    logsFlush();\n\n    if ((pid = fork()) < 0) {\n        xerrno = errno;\n        debugs(54, DBG_IMPORTANT, \"ipcCreate: fork: \" << xstrerr(xerrno));\n        return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n    }\n\n    if (pid > 0) {      /* parent */\n        /* close shared socket with child */\n        comm_close(crfd);\n\n        if (cwfd != crfd)\n            comm_close(cwfd);\n\n        cwfd = crfd = -1;\n\n        if (type == IPC_TCP_SOCKET || type == IPC_UDP_SOCKET) {\n            if (comm_connect_addr(pwfd, ChS) == Comm::COMM_ERROR)\n                return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        if (type == IPC_UDP_SOCKET)\n            x = comm_udp_recv(prfd, hello_buf, sizeof(hello_buf)-1, 0);\n        else\n            x = read(prfd, hello_buf, sizeof(hello_buf)-1);\n        xerrno = errno;\n        if (x >= 0)\n            hello_buf[x] = '\\0';\n\n        if (x < 0) {\n            debugs(54, DBG_CRITICAL, \"ERROR: ipcCreate: PARENT: hello read test failed\");\n            debugs(54, DBG_CRITICAL, \"--> read: \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        } else if (strcmp(hello_buf, hello_string)) {\n            debugs(54, DBG_CRITICAL, \"ERROR: ipcCreate: PARENT: hello read test failed\");\n            debugs(54, DBG_CRITICAL, \"--> read returned \" << x);\n            debugs(54, DBG_CRITICAL, \"--> got '\" << rfc1738_escape(hello_buf) << \"'\");\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        commUnsetFdTimeout(prfd);\n        commSetNonBlocking(prfd);\n        commSetNonBlocking(pwfd);\n\n        if (rfd)\n            *rfd = prfd;\n\n        if (wfd)\n            *wfd = pwfd;\n\n        fd_table[prfd].flags.ipc = 1;\n\n        fd_table[pwfd].flags.ipc = 1;\n\n        if (Config.sleep_after_fork)\n            std::this_thread::sleep_for(std::chrono::microseconds(Config.sleep_after_fork));\n\n        return pid;\n    }\n\n    /* child */\n    TheProcessKind = pkHelper;\n    no_suid();          /* give up extra privileges */\n\n    /* close shared socket with parent */\n    close(prfd);\n\n    if (pwfd != prfd)\n        close(pwfd);\n\n    pwfd = prfd = -1;\n\n    if (type == IPC_TCP_SOCKET) {\n        debugs(54, 3, \"ipcCreate: calling accept on FD \" << crfd);\n\n        if ((fd = accept(crfd, nullptr, nullptr)) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: FD \" << crfd << \" accept: \" << xstrerr(xerrno));\n            _exit(1);\n        }\n\n        debugs(54, 3, \"ipcCreate: CHILD accepted new FD \" << fd);\n        close(crfd);\n        cwfd = crfd = fd;\n    } else if (type == IPC_UDP_SOCKET) {\n        if (comm_connect_addr(crfd, PaS) == Comm::COMM_ERROR)\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n    }\n\n    if (type == IPC_UDP_SOCKET) {\n        x = comm_udp_send(cwfd, hello_string, strlen(hello_string) + 1, 0);\n\n        if (x < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"sendto FD \" << cwfd << \": \" << xstrerr(xerrno));\n            debugs(54, DBG_CRITICAL, \"ERROR: ipcCreate: CHILD: hello write test failed\");\n            _exit(1);\n        }\n    } else {\n        if (write(cwfd, hello_string, strlen(hello_string) + 1) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"write FD \" << cwfd << \": \" << xstrerr(xerrno));\n            debugs(54, DBG_CRITICAL, \"ERROR: ipcCreate: CHILD: hello write test failed\");\n            _exit(1);\n        }\n    }\n\n    PutEnvironment();\n    /*\n     * This double-dup stuff avoids problems when one of\n     *  crfd, cwfd, or debug_log are in the rage 0-2.\n     */\n\n    do {\n        /* First make sure 0-2 is occupied by something. Gets cleaned up later */\n        x = dup(crfd);\n        assert(x > -1);\n    } while (x < 3 && x > -1);\n\n    close(x);\n\n    t1 = dup(crfd);\n\n    t2 = dup(cwfd);\n\n    t3 = dup(fileno(debug_log));\n\n    assert(t1 > 2 && t2 > 2 && t3 > 2);\n\n    close(crfd);\n\n    close(cwfd);\n\n    close(fileno(debug_log));\n\n    dup2(t1, 0);\n\n    dup2(t2, 1);\n\n    dup2(t3, 2);\n\n    close(t1);\n\n    close(t2);\n\n    close(t3);\n\n    /* Make sure all other filedescriptors are closed */\n    for (x = 3; x < SQUID_MAXFD; ++x)\n        close(x);\n\n#if HAVE_SETSID\n    if (opt_no_daemon)\n        setsid();\n#endif\n\n    execvp(prog, (char *const *) args);\n    xerrno = errno;\n\n    ResyncDebugLog(fdopen(2, \"a+\"));\n\n    debugs(54, DBG_CRITICAL, \"ipcCreate: \" << prog << \": \" << xstrerr(xerrno));\n\n    _exit(1);\n\n    return 0;\n}",
        "func": "pid_t\nipcCreate(int type, const char *prog, const char *const args[], const char *name, Ip::Address &local_addr, int *rfd, int *wfd, void **hIpc)\n{\n    pid_t pid;\n    Ip::Address ChS;\n    Ip::Address PaS;\n    struct addrinfo *AI = nullptr;\n    int crfd = -1;\n    int prfd = -1;\n    int cwfd = -1;\n    int pwfd = -1;\n    int fd;\n    int t1, t2, t3;\n    int x;\n    int xerrno;\n\n#if USE_POLL && _SQUID_OSF_\n    assert(type != IPC_FIFO);\n#endif\n\n    if (rfd)\n        *rfd = -1;\n\n    if (wfd)\n        *wfd = -1;\n\n    if (hIpc)\n        *hIpc = nullptr;\n\n// NP: no wrapping around d and c usage since we *want* code expansion\n#define IPC_CHECK_FAIL(f,d,c) \\\n    if ((f) < 0) { \\\n        debugs(54, DBG_CRITICAL, \"ERROR: Failed to create helper \" d \" FD: \" << c); \\\n        return ipcCloseAllFD(prfd, pwfd, crfd, cwfd); \\\n    } else void(0)\n\n    if (type == IPC_TCP_SOCKET) {\n        crfd = cwfd = comm_open_listener(SOCK_STREAM,\n                                         0,\n                                         local_addr,\n                                         COMM_NOCLOEXEC,\n                                         name);\n        prfd = pwfd = comm_open(SOCK_STREAM,\n                                0,          /* protocol */\n                                local_addr,\n                                0,          /* blocking */\n                                name);\n        IPC_CHECK_FAIL(crfd, \"child read\", \"TCP \" << local_addr);\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"TCP \" << local_addr);\n    } else if (type == IPC_UDP_SOCKET) {\n        crfd = cwfd = comm_open(SOCK_DGRAM,\n                                0,\n                                local_addr,\n                                COMM_NOCLOEXEC,\n                                name);\n        prfd = pwfd = comm_open(SOCK_DGRAM,\n                                0,\n                                local_addr,\n                                0,\n                                name);\n        IPC_CHECK_FAIL(crfd, \"child read\", \"UDP\" << local_addr);\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"UDP\" << local_addr);\n    } else if (type == IPC_FIFO) {\n        int p2c[2];\n        int c2p[2];\n\n        if (pipe(p2c) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: pipe: \" << xstrerr(xerrno));\n            return -1; // maybe ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n        fd_open(prfd = p2c[0], FD_PIPE, \"IPC FIFO Parent Read\");\n        fd_open(cwfd = p2c[1], FD_PIPE, \"IPC FIFO Child Write\");\n\n        if (pipe(c2p) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: pipe: \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n        fd_open(crfd = c2p[0], FD_PIPE, \"IPC FIFO Child Read\");\n        fd_open(pwfd = c2p[1], FD_PIPE, \"IPC FIFO Parent Write\");\n\n        IPC_CHECK_FAIL(crfd, \"child read\", \"FIFO pipe\");\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"FIFO pipe\");\n\n#if HAVE_SOCKETPAIR && defined(AF_UNIX)\n\n    } else if (type == IPC_UNIX_STREAM) {\n        int fds[2];\n        int buflen = 32768;\n\n        if (socketpair(AF_UNIX, SOCK_STREAM, 0, fds) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: socketpair: \" << xstrerr(xerrno));\n            return -1;\n        }\n\n        errno = 0;\n        if (setsockopt(fds[0], SOL_SOCKET, SO_SNDBUF, (void *) &buflen, sizeof(buflen)) == -1)  {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ERROR: setsockopt failed: \" << xstrerr(xerrno));\n            errno = 0;\n        }\n        if (setsockopt(fds[0], SOL_SOCKET, SO_RCVBUF, (void *) &buflen, sizeof(buflen)) == -1) {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ERROR: setsockopt failed: \" << xstrerr(xerrno));\n            errno = 0;\n        }\n        if (setsockopt(fds[1], SOL_SOCKET, SO_SNDBUF, (void *) &buflen, sizeof(buflen)) == -1) {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ERROR: setsockopt failed: \" << xstrerr(xerrno));\n            errno = 0;\n        }\n        if (setsockopt(fds[1], SOL_SOCKET, SO_RCVBUF, (void *) &buflen, sizeof(buflen)) == -1) {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ERROR: setsockopt failed: \" << xstrerr(xerrno));\n            errno = 0;\n        }\n        fd_open(prfd = pwfd = fds[0], FD_PIPE, \"IPC UNIX STREAM Parent\");\n        fd_open(crfd = cwfd = fds[1], FD_PIPE, \"IPC UNIX STREAM Parent\");\n        IPC_CHECK_FAIL(crfd, \"child read\", \"UDS socket\");\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"UDS socket\");\n\n    } else if (type == IPC_UNIX_DGRAM) {\n        int fds[2];\n\n        if (socketpair(AF_UNIX, SOCK_DGRAM, 0, fds) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: socketpair: \" << xstrerr(xerrno));\n            return -1;\n        }\n\n        fd_open(prfd = pwfd = fds[0], FD_PIPE, \"IPC UNIX DGRAM Parent\");\n        fd_open(crfd = cwfd = fds[1], FD_PIPE, \"IPC UNIX DGRAM Parent\");\n\n        IPC_CHECK_FAIL(crfd, \"child read\", \"UDS datagram\");\n        IPC_CHECK_FAIL(prfd, \"parent read\", \"UDS datagram\");\n#endif\n\n    } else {\n        assert(IPC_NONE);\n    }\n\n    debugs(54, 3, \"ipcCreate: prfd FD \" << prfd);\n    debugs(54, 3, \"ipcCreate: pwfd FD \" << pwfd);\n    debugs(54, 3, \"ipcCreate: crfd FD \" << crfd);\n    debugs(54, 3, \"ipcCreate: cwfd FD \" << cwfd);\n\n    if (type == IPC_TCP_SOCKET || type == IPC_UDP_SOCKET) {\n        Ip::Address::InitAddr(AI);\n\n        if (getsockname(pwfd, AI->ai_addr, &AI->ai_addrlen) < 0) {\n            xerrno = errno;\n            Ip::Address::FreeAddr(AI);\n            debugs(54, DBG_CRITICAL, \"ipcCreate: getsockname: \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        PaS = *AI;\n\n        debugs(54, 3, \"ipcCreate: FD \" << pwfd << \" sockaddr \" << PaS);\n\n        Ip::Address::FreeAddr(AI);\n\n        Ip::Address::InitAddr(AI);\n\n        if (getsockname(crfd, AI->ai_addr, &AI->ai_addrlen) < 0) {\n            xerrno = errno;\n            Ip::Address::FreeAddr(AI);\n            debugs(54, DBG_CRITICAL, \"ipcCreate: getsockname: \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        ChS = *AI;\n\n        Ip::Address::FreeAddr(AI);\n\n        debugs(54, 3, \"ipcCreate: FD \" << crfd << \" sockaddr \" << ChS );\n\n    }\n\n    if (type == IPC_TCP_SOCKET) {\n        if (listen(crfd, 1) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_IMPORTANT, \"ipcCreate: listen FD \" << crfd << \": \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        debugs(54, 3, \"ipcCreate: FD \" << crfd << \" listening...\");\n    }\n\n    /* flush or else we get dup data if unbuffered_logs is set */\n    logsFlush();\n\n    if ((pid = fork()) < 0) {\n        xerrno = errno;\n        debugs(54, DBG_IMPORTANT, \"ipcCreate: fork: \" << xstrerr(xerrno));\n        return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n    }\n\n    if (pid > 0) {      /* parent */\n        /* close shared socket with child */\n        comm_close(crfd);\n\n        if (cwfd != crfd)\n            comm_close(cwfd);\n\n        cwfd = crfd = -1;\n\n        if (type == IPC_TCP_SOCKET || type == IPC_UDP_SOCKET) {\n            if (comm_connect_addr(pwfd, ChS) == Comm::COMM_ERROR)\n                return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        if (type == IPC_UDP_SOCKET)\n            x = comm_udp_recv(prfd, hello_buf, sizeof(hello_buf)-1, 0);\n        else\n            x = read(prfd, hello_buf, sizeof(hello_buf)-1);\n        xerrno = errno;\n        if (x >= 0)\n            hello_buf[x] = '\\0';\n\n        if (x < 0) {\n            debugs(54, DBG_CRITICAL, \"ERROR: ipcCreate: PARENT: hello read test failed\");\n            debugs(54, DBG_CRITICAL, \"--> read: \" << xstrerr(xerrno));\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        } else if (strcmp(hello_buf, hello_string)) {\n            debugs(54, DBG_CRITICAL, \"ERROR: ipcCreate: PARENT: hello read test failed\");\n            debugs(54, DBG_CRITICAL, \"--> read returned \" << x);\n            debugs(54, DBG_CRITICAL, \"--> got '\" << rfc1738_escape(hello_buf) << \"'\");\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n        }\n\n        commUnsetFdTimeout(prfd);\n        commSetNonBlocking(prfd);\n        commSetNonBlocking(pwfd);\n\n        if (rfd)\n            *rfd = prfd;\n\n        if (wfd)\n            *wfd = pwfd;\n\n        fd_table[prfd].flags.ipc = 1;\n\n        fd_table[pwfd].flags.ipc = 1;\n\n        if (Config.sleep_after_fork)\n            std::this_thread::sleep_for(std::chrono::microseconds(Config.sleep_after_fork));\n\n        return pid;\n    }\n\n    /* child */\n    TheProcessKind = pkHelper;\n    no_suid();          /* give up extra privileges */\n\n    /* close shared socket with parent */\n    close(prfd);\n\n    if (pwfd != prfd)\n        close(pwfd);\n\n    pwfd = prfd = -1;\n\n    if (type == IPC_TCP_SOCKET) {\n        debugs(54, 3, \"ipcCreate: calling accept on FD \" << crfd);\n\n        if ((fd = accept(crfd, nullptr, nullptr)) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"ipcCreate: FD \" << crfd << \" accept: \" << xstrerr(xerrno));\n            _exit(1);\n        }\n\n        debugs(54, 3, \"ipcCreate: CHILD accepted new FD \" << fd);\n        close(crfd);\n        cwfd = crfd = fd;\n    } else if (type == IPC_UDP_SOCKET) {\n        if (comm_connect_addr(crfd, PaS) == Comm::COMM_ERROR)\n            return ipcCloseAllFD(prfd, pwfd, crfd, cwfd);\n    }\n\n    if (type == IPC_UDP_SOCKET) {\n        x = comm_udp_send(cwfd, hello_string, strlen(hello_string) + 1, 0);\n\n        if (x < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"sendto FD \" << cwfd << \": \" << xstrerr(xerrno));\n            debugs(54, DBG_CRITICAL, \"ERROR: ipcCreate: CHILD: hello write test failed\");\n            _exit(1);\n        }\n    } else {\n        if (write(cwfd, hello_string, strlen(hello_string) + 1) < 0) {\n            xerrno = errno;\n            debugs(54, DBG_CRITICAL, \"write FD \" << cwfd << \": \" << xstrerr(xerrno));\n            debugs(54, DBG_CRITICAL, \"ERROR: ipcCreate: CHILD: hello write test failed\");\n            _exit(1);\n        }\n    }\n\n    PutEnvironment();\n\n    // A dup(2) wrapper that reports and exits the process on errors. The\n    // exiting logic is only suitable for this child process context.\n    const auto dupOrExit = [prog,name](const int oldFd) {\n        const auto newFd = dup(oldFd);\n        if (newFd < 0) {\n            const auto savedErrno = errno;\n            debugs(54, DBG_CRITICAL, \"ERROR: Helper process initialization failure: \" << name <<\n                   Debug::Extra << \"helper (CHILD) PID: \" << getpid() <<\n                   Debug::Extra << \"helper program name: \" << prog <<\n                   Debug::Extra << \"dup(2) system call error for FD \" << oldFd << \": \" << xstrerr(savedErrno));\n            _exit(EXIT_FAILURE);\n        }\n        return newFd;\n    };\n\n    /*\n     * This double-dup stuff avoids problems when one of\n     *  crfd, cwfd, or debug_log are in the rage 0-2.\n     */\n\n    do {\n        /* First make sure 0-2 is occupied by something. Gets cleaned up later */\n        x = dupOrExit(crfd);\n    } while (x < 3);\n\n    close(x);\n\n    t1 = dupOrExit(crfd);\n\n    t2 = dupOrExit(cwfd);\n\n    t3 = dupOrExit(fileno(debug_log));\n\n    assert(t1 > 2 && t2 > 2 && t3 > 2);\n\n    close(crfd);\n\n    close(cwfd);\n\n    close(fileno(debug_log));\n\n    dup2(t1, 0);\n\n    dup2(t2, 1);\n\n    dup2(t3, 2);\n\n    close(t1);\n\n    close(t2);\n\n    close(t3);\n\n    /* Make sure all other filedescriptors are closed */\n    for (x = 3; x < SQUID_MAXFD; ++x)\n        close(x);\n\n#if HAVE_SETSID\n    if (opt_no_daemon)\n        setsid();\n#endif\n\n    execvp(prog, (char *const *) args);\n    xerrno = errno;\n\n    ResyncDebugLog(fdopen(2, \"a+\"));\n\n    debugs(54, DBG_CRITICAL, \"ipcCreate: \" << prog << \": \" << xstrerr(xerrno));\n\n    _exit(1);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -299,6 +299,22 @@\n     }\n \n     PutEnvironment();\n+\n+    // A dup(2) wrapper that reports and exits the process on errors. The\n+    // exiting logic is only suitable for this child process context.\n+    const auto dupOrExit = [prog,name](const int oldFd) {\n+        const auto newFd = dup(oldFd);\n+        if (newFd < 0) {\n+            const auto savedErrno = errno;\n+            debugs(54, DBG_CRITICAL, \"ERROR: Helper process initialization failure: \" << name <<\n+                   Debug::Extra << \"helper (CHILD) PID: \" << getpid() <<\n+                   Debug::Extra << \"helper program name: \" << prog <<\n+                   Debug::Extra << \"dup(2) system call error for FD \" << oldFd << \": \" << xstrerr(savedErrno));\n+            _exit(EXIT_FAILURE);\n+        }\n+        return newFd;\n+    };\n+\n     /*\n      * This double-dup stuff avoids problems when one of\n      *  crfd, cwfd, or debug_log are in the rage 0-2.\n@@ -306,17 +322,16 @@\n \n     do {\n         /* First make sure 0-2 is occupied by something. Gets cleaned up later */\n-        x = dup(crfd);\n-        assert(x > -1);\n-    } while (x < 3 && x > -1);\n+        x = dupOrExit(crfd);\n+    } while (x < 3);\n \n     close(x);\n \n-    t1 = dup(crfd);\n-\n-    t2 = dup(cwfd);\n-\n-    t3 = dup(fileno(debug_log));\n+    t1 = dupOrExit(crfd);\n+\n+    t2 = dupOrExit(cwfd);\n+\n+    t3 = dupOrExit(fileno(debug_log));\n \n     assert(t1 > 2 && t2 > 2 && t3 > 2);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "        x = dup(crfd);",
                "        assert(x > -1);",
                "    } while (x < 3 && x > -1);",
                "    t1 = dup(crfd);",
                "",
                "    t2 = dup(cwfd);",
                "",
                "    t3 = dup(fileno(debug_log));"
            ],
            "added_lines": [
                "",
                "    // A dup(2) wrapper that reports and exits the process on errors. The",
                "    // exiting logic is only suitable for this child process context.",
                "    const auto dupOrExit = [prog,name](const int oldFd) {",
                "        const auto newFd = dup(oldFd);",
                "        if (newFd < 0) {",
                "            const auto savedErrno = errno;",
                "            debugs(54, DBG_CRITICAL, \"ERROR: Helper process initialization failure: \" << name <<",
                "                   Debug::Extra << \"helper (CHILD) PID: \" << getpid() <<",
                "                   Debug::Extra << \"helper program name: \" << prog <<",
                "                   Debug::Extra << \"dup(2) system call error for FD \" << oldFd << \": \" << xstrerr(savedErrno));",
                "            _exit(EXIT_FAILURE);",
                "        }",
                "        return newFd;",
                "    };",
                "",
                "        x = dupOrExit(crfd);",
                "    } while (x < 3);",
                "    t1 = dupOrExit(crfd);",
                "",
                "    t2 = dupOrExit(cwfd);",
                "",
                "    t3 = dupOrExit(fileno(debug_log));"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-52429",
        "func_name": "torvalds/linux/copy_params",
        "description": "dm_table_create in drivers/md/dm-table.c in the Linux kernel through 6.7.4 can attempt to (in alloc_targets) allocate more than INT_MAX bytes, and crash, because of a missing check for struct dm_ioctl.target_count.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=bd504bcfec41a503b32054da5472904b404341a4",
        "commit_title": "The kvmalloc function fails with a warning if the size is larger than",
        "commit_text": "INT_MAX. The warning was triggered by a syscall testing robot.  In order to avoid the warning, this commit limits the number of targets to 1048576 and the size of the parameter area to 1073741824.  ",
        "func_before": "static int copy_params(struct dm_ioctl __user *user, struct dm_ioctl *param_kernel,\n\t\t       int ioctl_flags, struct dm_ioctl **param, int *param_flags)\n{\n\tstruct dm_ioctl *dmi;\n\tint secure_data;\n\tconst size_t minimum_data_size = offsetof(struct dm_ioctl, data);\n\n\t/* check_version() already copied version from userspace, avoid TOCTOU */\n\tif (copy_from_user((char *)param_kernel + sizeof(param_kernel->version),\n\t\t\t   (char __user *)user + sizeof(param_kernel->version),\n\t\t\t   minimum_data_size - sizeof(param_kernel->version)))\n\t\treturn -EFAULT;\n\n\tif (param_kernel->data_size < minimum_data_size) {\n\t\tDMERR(\"Invalid data size in the ioctl structure: %u\",\n\t\t      param_kernel->data_size);\n\t\treturn -EINVAL;\n\t}\n\n\tsecure_data = param_kernel->flags & DM_SECURE_DATA_FLAG;\n\n\t*param_flags = secure_data ? DM_WIPE_BUFFER : 0;\n\n\tif (ioctl_flags & IOCTL_FLAGS_NO_PARAMS) {\n\t\tdmi = param_kernel;\n\t\tdmi->data_size = minimum_data_size;\n\t\tgoto data_copied;\n\t}\n\n\t/*\n\t * Use __GFP_HIGH to avoid low memory issues when a device is\n\t * suspended and the ioctl is needed to resume it.\n\t * Use kmalloc() rather than vmalloc() when we can.\n\t */\n\tdmi = NULL;\n\tdmi = kvmalloc(param_kernel->data_size, GFP_NOIO | __GFP_HIGH);\n\n\tif (!dmi) {\n\t\tif (secure_data && clear_user(user, param_kernel->data_size))\n\t\t\treturn -EFAULT;\n\t\treturn -ENOMEM;\n\t}\n\n\t*param_flags |= DM_PARAMS_MALLOC;\n\n\t/* Copy from param_kernel (which was already copied from user) */\n\tmemcpy(dmi, param_kernel, minimum_data_size);\n\n\tif (copy_from_user(&dmi->data, (char __user *)user + minimum_data_size,\n\t\t\t   param_kernel->data_size - minimum_data_size))\n\t\tgoto bad;\ndata_copied:\n\t/* Wipe the user buffer so we do not return it to userspace */\n\tif (secure_data && clear_user(user, param_kernel->data_size))\n\t\tgoto bad;\n\n\t*param = dmi;\n\treturn 0;\n\nbad:\n\tfree_params(dmi, param_kernel->data_size, *param_flags);\n\n\treturn -EFAULT;\n}",
        "func": "static int copy_params(struct dm_ioctl __user *user, struct dm_ioctl *param_kernel,\n\t\t       int ioctl_flags, struct dm_ioctl **param, int *param_flags)\n{\n\tstruct dm_ioctl *dmi;\n\tint secure_data;\n\tconst size_t minimum_data_size = offsetof(struct dm_ioctl, data);\n\n\t/* check_version() already copied version from userspace, avoid TOCTOU */\n\tif (copy_from_user((char *)param_kernel + sizeof(param_kernel->version),\n\t\t\t   (char __user *)user + sizeof(param_kernel->version),\n\t\t\t   minimum_data_size - sizeof(param_kernel->version)))\n\t\treturn -EFAULT;\n\n\tif (unlikely(param_kernel->data_size < minimum_data_size) ||\n\t    unlikely(param_kernel->data_size > DM_MAX_TARGETS * DM_MAX_TARGET_PARAMS)) {\n\t\tDMERR(\"Invalid data size in the ioctl structure: %u\",\n\t\t      param_kernel->data_size);\n\t\treturn -EINVAL;\n\t}\n\n\tsecure_data = param_kernel->flags & DM_SECURE_DATA_FLAG;\n\n\t*param_flags = secure_data ? DM_WIPE_BUFFER : 0;\n\n\tif (ioctl_flags & IOCTL_FLAGS_NO_PARAMS) {\n\t\tdmi = param_kernel;\n\t\tdmi->data_size = minimum_data_size;\n\t\tgoto data_copied;\n\t}\n\n\t/*\n\t * Use __GFP_HIGH to avoid low memory issues when a device is\n\t * suspended and the ioctl is needed to resume it.\n\t * Use kmalloc() rather than vmalloc() when we can.\n\t */\n\tdmi = NULL;\n\tdmi = kvmalloc(param_kernel->data_size, GFP_NOIO | __GFP_HIGH);\n\n\tif (!dmi) {\n\t\tif (secure_data && clear_user(user, param_kernel->data_size))\n\t\t\treturn -EFAULT;\n\t\treturn -ENOMEM;\n\t}\n\n\t*param_flags |= DM_PARAMS_MALLOC;\n\n\t/* Copy from param_kernel (which was already copied from user) */\n\tmemcpy(dmi, param_kernel, minimum_data_size);\n\n\tif (copy_from_user(&dmi->data, (char __user *)user + minimum_data_size,\n\t\t\t   param_kernel->data_size - minimum_data_size))\n\t\tgoto bad;\ndata_copied:\n\t/* Wipe the user buffer so we do not return it to userspace */\n\tif (secure_data && clear_user(user, param_kernel->data_size))\n\t\tgoto bad;\n\n\t*param = dmi;\n\treturn 0;\n\nbad:\n\tfree_params(dmi, param_kernel->data_size, *param_flags);\n\n\treturn -EFAULT;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,8 @@\n \t\t\t   minimum_data_size - sizeof(param_kernel->version)))\n \t\treturn -EFAULT;\n \n-\tif (param_kernel->data_size < minimum_data_size) {\n+\tif (unlikely(param_kernel->data_size < minimum_data_size) ||\n+\t    unlikely(param_kernel->data_size > DM_MAX_TARGETS * DM_MAX_TARGET_PARAMS)) {\n \t\tDMERR(\"Invalid data size in the ioctl structure: %u\",\n \t\t      param_kernel->data_size);\n \t\treturn -EINVAL;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (param_kernel->data_size < minimum_data_size) {"
            ],
            "added_lines": [
                "\tif (unlikely(param_kernel->data_size < minimum_data_size) ||",
                "\t    unlikely(param_kernel->data_size > DM_MAX_TARGETS * DM_MAX_TARGET_PARAMS)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-52429",
        "func_name": "torvalds/linux/dm_table_create",
        "description": "dm_table_create in drivers/md/dm-table.c in the Linux kernel through 6.7.4 can attempt to (in alloc_targets) allocate more than INT_MAX bytes, and crash, because of a missing check for struct dm_ioctl.target_count.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=bd504bcfec41a503b32054da5472904b404341a4",
        "commit_title": "The kvmalloc function fails with a warning if the size is larger than",
        "commit_text": "INT_MAX. The warning was triggered by a syscall testing robot.  In order to avoid the warning, this commit limits the number of targets to 1048576 and the size of the parameter area to 1073741824.  ",
        "func_before": "int dm_table_create(struct dm_table **result, blk_mode_t mode,\n\t\t    unsigned int num_targets, struct mapped_device *md)\n{\n\tstruct dm_table *t = kzalloc(sizeof(*t), GFP_KERNEL);\n\n\tif (!t)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&t->devices);\n\tinit_rwsem(&t->devices_lock);\n\n\tif (!num_targets)\n\t\tnum_targets = KEYS_PER_NODE;\n\n\tnum_targets = dm_round_up(num_targets, KEYS_PER_NODE);\n\n\tif (!num_targets) {\n\t\tkfree(t);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (alloc_targets(t, num_targets)) {\n\t\tkfree(t);\n\t\treturn -ENOMEM;\n\t}\n\n\tt->type = DM_TYPE_NONE;\n\tt->mode = mode;\n\tt->md = md;\n\t*result = t;\n\treturn 0;\n}",
        "func": "int dm_table_create(struct dm_table **result, blk_mode_t mode,\n\t\t    unsigned int num_targets, struct mapped_device *md)\n{\n\tstruct dm_table *t;\n\n\tif (num_targets > DM_MAX_TARGETS)\n\t\treturn -EOVERFLOW;\n\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\n\tif (!t)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&t->devices);\n\tinit_rwsem(&t->devices_lock);\n\n\tif (!num_targets)\n\t\tnum_targets = KEYS_PER_NODE;\n\n\tnum_targets = dm_round_up(num_targets, KEYS_PER_NODE);\n\n\tif (!num_targets) {\n\t\tkfree(t);\n\t\treturn -EOVERFLOW;\n\t}\n\n\tif (alloc_targets(t, num_targets)) {\n\t\tkfree(t);\n\t\treturn -ENOMEM;\n\t}\n\n\tt->type = DM_TYPE_NONE;\n\tt->mode = mode;\n\tt->md = md;\n\t*result = t;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,12 @@\n int dm_table_create(struct dm_table **result, blk_mode_t mode,\n \t\t    unsigned int num_targets, struct mapped_device *md)\n {\n-\tstruct dm_table *t = kzalloc(sizeof(*t), GFP_KERNEL);\n+\tstruct dm_table *t;\n+\n+\tif (num_targets > DM_MAX_TARGETS)\n+\t\treturn -EOVERFLOW;\n+\n+\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n \n \tif (!t)\n \t\treturn -ENOMEM;\n@@ -16,7 +21,7 @@\n \n \tif (!num_targets) {\n \t\tkfree(t);\n-\t\treturn -ENOMEM;\n+\t\treturn -EOVERFLOW;\n \t}\n \n \tif (alloc_targets(t, num_targets)) {",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct dm_table *t = kzalloc(sizeof(*t), GFP_KERNEL);",
                "\t\treturn -ENOMEM;"
            ],
            "added_lines": [
                "\tstruct dm_table *t;",
                "",
                "\tif (num_targets > DM_MAX_TARGETS)",
                "\t\treturn -EOVERFLOW;",
                "",
                "\tt = kzalloc(sizeof(*t), GFP_KERNEL);",
                "\t\treturn -EOVERFLOW;"
            ]
        }
    },
    {
        "cve_id": "CVE-2024-25739",
        "func_name": "torvalds/linux/ubi_read_volume_table",
        "description": "create_empty_lvol in drivers/mtd/ubi/vtbl.c in the Linux kernel through 6.7.4 can attempt to allocate zero bytes, and crash, because of a missing check for ubi->leb_size.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=68a24aba7c593eafa8fd00f2f76407b9b32b47a9",
        "commit_title": "If the LEB size is smaller than a volume table record we cannot",
        "commit_text": "have volumes. In this case abort attaching.  Cc: Chenyuan Yang <cy54@illinois.edu> Cc: stable@vger.kernel.org Closes: https://lore.kernel.org/linux-mtd/1433EB7A-FC89-47D6-8F47-23BE41B263B3@illinois.edu/ ",
        "func_before": "int ubi_read_volume_table(struct ubi_device *ubi, struct ubi_attach_info *ai)\n{\n\tint err;\n\tstruct ubi_ainf_volume *av;\n\n\tempty_vtbl_record.crc = cpu_to_be32(0xf116c36b);\n\n\t/*\n\t * The number of supported volumes is limited by the eraseblock size\n\t * and by the UBI_MAX_VOLUMES constant.\n\t */\n\tubi->vtbl_slots = ubi->leb_size / UBI_VTBL_RECORD_SIZE;\n\tif (ubi->vtbl_slots > UBI_MAX_VOLUMES)\n\t\tubi->vtbl_slots = UBI_MAX_VOLUMES;\n\n\tubi->vtbl_size = ubi->vtbl_slots * UBI_VTBL_RECORD_SIZE;\n\tubi->vtbl_size = ALIGN(ubi->vtbl_size, ubi->min_io_size);\n\n\tav = ubi_find_av(ai, UBI_LAYOUT_VOLUME_ID);\n\tif (!av) {\n\t\t/*\n\t\t * No logical eraseblocks belonging to the layout volume were\n\t\t * found. This could mean that the flash is just empty. In\n\t\t * this case we create empty layout volume.\n\t\t *\n\t\t * But if flash is not empty this must be a corruption or the\n\t\t * MTD device just contains garbage.\n\t\t */\n\t\tif (ai->is_empty) {\n\t\t\tubi->vtbl = create_empty_lvol(ubi, ai);\n\t\t\tif (IS_ERR(ubi->vtbl))\n\t\t\t\treturn PTR_ERR(ubi->vtbl);\n\t\t} else {\n\t\t\tubi_err(ubi, \"the layout volume was not found\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tif (av->leb_count > UBI_LAYOUT_VOLUME_EBS) {\n\t\t\t/* This must not happen with proper UBI images */\n\t\t\tubi_err(ubi, \"too many LEBs (%d) in layout volume\",\n\t\t\t\tav->leb_count);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tubi->vtbl = process_lvol(ubi, ai, av);\n\t\tif (IS_ERR(ubi->vtbl))\n\t\t\treturn PTR_ERR(ubi->vtbl);\n\t}\n\n\tubi->avail_pebs = ubi->good_peb_count - ubi->corr_peb_count;\n\n\t/*\n\t * The layout volume is OK, initialize the corresponding in-RAM data\n\t * structures.\n\t */\n\terr = init_volumes(ubi, ai, ubi->vtbl);\n\tif (err)\n\t\tgoto out_free;\n\n\t/*\n\t * Make sure that the attaching information is consistent to the\n\t * information stored in the volume table.\n\t */\n\terr = check_attaching_info(ubi, ai);\n\tif (err)\n\t\tgoto out_free;\n\n\treturn 0;\n\nout_free:\n\tvfree(ubi->vtbl);\n\tubi_free_all_volumes(ubi);\n\treturn err;\n}",
        "func": "int ubi_read_volume_table(struct ubi_device *ubi, struct ubi_attach_info *ai)\n{\n\tint err;\n\tstruct ubi_ainf_volume *av;\n\n\tempty_vtbl_record.crc = cpu_to_be32(0xf116c36b);\n\n\t/*\n\t * The number of supported volumes is limited by the eraseblock size\n\t * and by the UBI_MAX_VOLUMES constant.\n\t */\n\n\tif (ubi->leb_size < UBI_VTBL_RECORD_SIZE) {\n\t\tubi_err(ubi, \"LEB size too small for a volume record\");\n\t\treturn -EINVAL;\n\t}\n\n\tubi->vtbl_slots = ubi->leb_size / UBI_VTBL_RECORD_SIZE;\n\tif (ubi->vtbl_slots > UBI_MAX_VOLUMES)\n\t\tubi->vtbl_slots = UBI_MAX_VOLUMES;\n\n\tubi->vtbl_size = ubi->vtbl_slots * UBI_VTBL_RECORD_SIZE;\n\tubi->vtbl_size = ALIGN(ubi->vtbl_size, ubi->min_io_size);\n\n\tav = ubi_find_av(ai, UBI_LAYOUT_VOLUME_ID);\n\tif (!av) {\n\t\t/*\n\t\t * No logical eraseblocks belonging to the layout volume were\n\t\t * found. This could mean that the flash is just empty. In\n\t\t * this case we create empty layout volume.\n\t\t *\n\t\t * But if flash is not empty this must be a corruption or the\n\t\t * MTD device just contains garbage.\n\t\t */\n\t\tif (ai->is_empty) {\n\t\t\tubi->vtbl = create_empty_lvol(ubi, ai);\n\t\t\tif (IS_ERR(ubi->vtbl))\n\t\t\t\treturn PTR_ERR(ubi->vtbl);\n\t\t} else {\n\t\t\tubi_err(ubi, \"the layout volume was not found\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tif (av->leb_count > UBI_LAYOUT_VOLUME_EBS) {\n\t\t\t/* This must not happen with proper UBI images */\n\t\t\tubi_err(ubi, \"too many LEBs (%d) in layout volume\",\n\t\t\t\tav->leb_count);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tubi->vtbl = process_lvol(ubi, ai, av);\n\t\tif (IS_ERR(ubi->vtbl))\n\t\t\treturn PTR_ERR(ubi->vtbl);\n\t}\n\n\tubi->avail_pebs = ubi->good_peb_count - ubi->corr_peb_count;\n\n\t/*\n\t * The layout volume is OK, initialize the corresponding in-RAM data\n\t * structures.\n\t */\n\terr = init_volumes(ubi, ai, ubi->vtbl);\n\tif (err)\n\t\tgoto out_free;\n\n\t/*\n\t * Make sure that the attaching information is consistent to the\n\t * information stored in the volume table.\n\t */\n\terr = check_attaching_info(ubi, ai);\n\tif (err)\n\t\tgoto out_free;\n\n\treturn 0;\n\nout_free:\n\tvfree(ubi->vtbl);\n\tubi_free_all_volumes(ubi);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,12 @@\n \t * The number of supported volumes is limited by the eraseblock size\n \t * and by the UBI_MAX_VOLUMES constant.\n \t */\n+\n+\tif (ubi->leb_size < UBI_VTBL_RECORD_SIZE) {\n+\t\tubi_err(ubi, \"LEB size too small for a volume record\");\n+\t\treturn -EINVAL;\n+\t}\n+\n \tubi->vtbl_slots = ubi->leb_size / UBI_VTBL_RECORD_SIZE;\n \tif (ubi->vtbl_slots > UBI_MAX_VOLUMES)\n \t\tubi->vtbl_slots = UBI_MAX_VOLUMES;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (ubi->leb_size < UBI_VTBL_RECORD_SIZE) {",
                "\t\tubi_err(ubi, \"LEB size too small for a volume record\");",
                "\t\treturn -EINVAL;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35848",
        "func_name": "virtualsquare/picotcp/pico_ipv4_process_in",
        "description": "VirtualSquare picoTCP (aka PicoTCP-NG) through 2.1 lacks certain size calculations before attempting to set a value of an mss structure member.",
        "git_url": "https://github.com/virtualsquare/picotcp/commit/4b9a16764f2b12b611de9c34a50b4713d10ca401",
        "commit_title": "More checks for correct header sizes",
        "commit_text": "",
        "func_before": "static int pico_ipv4_process_in(struct pico_stack *S, struct pico_protocol *self, struct pico_frame *f)\n{\n    uint8_t option_len = 0;\n    int ret = 0;\n    struct pico_ipv4_hdr *hdr = (struct pico_ipv4_hdr *) f->net_hdr;\n    uint16_t max_allowed = (uint16_t) ((int)f->buffer_len - (f->net_hdr - f->buffer) - (int)PICO_SIZE_IP4HDR);\n\n    if (!hdr)\n        return -1;\n\n    (void)self;\n\n    /* NAT needs transport header information */\n    if (((hdr->vhl) & 0x0F) > 5) {\n        option_len =  (uint8_t)(4 * (((hdr->vhl) & 0x0F) - 5));\n    }\n\n    f->transport_hdr = ((uint8_t *)f->net_hdr) + PICO_SIZE_IP4HDR + option_len;\n    f->transport_len = (uint16_t)(short_be(hdr->len) - PICO_SIZE_IP4HDR - option_len);\n    f->net_len = (uint16_t)(PICO_SIZE_IP4HDR + option_len);\n#if defined(PICO_SUPPORT_IPV4FRAG) || defined(PICO_SUPPORT_IPV6FRAG)\n    f->frag = short_be(hdr->frag);\n#endif\n\n    if (f->transport_len > max_allowed) {\n        pico_frame_discard(f);\n        return 0; /* Packet is discarded due to unfeasible length */\n    }\n\n#ifdef PICO_SUPPORT_IPFILTER\n    if (ipfilter(f)) {\n        /*pico_frame is discarded as result of the filtering*/\n        return 0;\n    }\n\n#endif\n    /* ret == 1 indicates to continue the function */\n    ret = pico_ipv4_crc_check(f);\n    if (ret < 1)\n        return ret;\n\n    /* Validate source IP address. Discard quietly if invalid */\n    if (!pico_ipv4_is_valid_src(S, hdr->src.addr, f->dev)) {\n        pico_frame_discard(f);\n        return 0;\n    }\n\n#if defined(PICO_SUPPORT_IPV4FRAG) || defined(PICO_SUPPORT_IPV6FRAG)\n    if (f->frag & PICO_IPV4_EVIL) {\n        (void)pico_icmp4_param_problem(S, f, 0);\n        pico_frame_discard(f); /* RFC 3514 */\n        return 0;\n    }\n#endif\n\n    if ((hdr->vhl & 0x0f) < 5) {\n        /* RFC 791: IHL minimum value is 5 */\n        (void)pico_icmp4_param_problem(S, f, 0);\n        pico_frame_discard(f);\n        return 0;\n    }\n\n#if defined(PICO_SUPPORT_IPV4FRAG) || defined(PICO_SUPPORT_IPV6FRAG)\n    if (f->frag & (PICO_IPV4_MOREFRAG | PICO_IPV4_FRAG_MASK))\n    {\n#ifdef PICO_SUPPORT_IPV4FRAG\n        pico_ipv4_process_frag(hdr, f, hdr->proto);\n        /* Frame can be discarded, frag will handle its own copy */\n#endif\n        /* We do not support fragmentation, discard quietly */\n        pico_frame_discard(f);\n        return 0;\n    }\n#endif\n\n#ifdef PICO_SUPPORT_RAWSOCKETS\n    pico_socket_ipv4_process_in(f);\n#endif\n\n    if (pico_ipv4_process_bcast_in(S, f) > 0)\n        return 0;\n\n    if (pico_ipv4_process_mcast_in(f) > 0)\n        return 0;\n\n    if (pico_ipv4_process_local_unicast_in(S, f) > 0)\n        return 0;\n\n    pico_ipv4_process_finally_try_forward(S, f);\n\n    return 0;\n}",
        "func": "static int pico_ipv4_process_in(struct pico_stack *S, struct pico_protocol *self, struct pico_frame *f)\n{\n    uint8_t option_len = 0;\n    int ret = 0;\n    struct pico_ipv4_hdr *hdr = (struct pico_ipv4_hdr *) f->net_hdr;\n    uint16_t max_allowed = (uint16_t) ((int)f->buffer_len - (f->net_hdr - f->buffer) - (int)PICO_SIZE_IP4HDR);\n\n    if (!hdr)\n        return -1;\n\n    (void)self;\n\n    /* NAT needs transport header information */\n    if (((hdr->vhl) & 0x0F) > 5) {\n        option_len =  (uint8_t)(4 * (((hdr->vhl) & 0x0F) - 5));\n    }\n\n    f->transport_hdr = ((uint8_t *)f->net_hdr) + PICO_SIZE_IP4HDR + option_len;\n    f->transport_len = (uint16_t)(short_be(hdr->len) - PICO_SIZE_IP4HDR - option_len);\n    f->net_len = (uint16_t)(PICO_SIZE_IP4HDR + option_len);\n\n    if ((f->net_hdr + f->net_len) > (f->buffer + f->buffer_len)) {\n        pico_frame_discard(f);\n        return 0;\n    }\n#if defined(PICO_SUPPORT_IPV4FRAG) || defined(PICO_SUPPORT_IPV6FRAG)\n    f->frag = short_be(hdr->frag);\n#endif\n\n    if (f->transport_len > max_allowed) {\n        pico_frame_discard(f);\n        return 0; /* Packet is discarded due to unfeasible length */\n    }\n\n#ifdef PICO_SUPPORT_IPFILTER\n    if (ipfilter(f)) {\n        /*pico_frame is discarded as result of the filtering*/\n        return 0;\n    }\n\n#endif\n    /* ret == 1 indicates to continue the function */\n    ret = pico_ipv4_crc_check(f);\n    if (ret < 1)\n        return ret;\n\n    /* Validate source IP address. Discard quietly if invalid */\n    if (!pico_ipv4_is_valid_src(S, hdr->src.addr, f->dev)) {\n        pico_frame_discard(f);\n        return 0;\n    }\n\n#if defined(PICO_SUPPORT_IPV4FRAG) || defined(PICO_SUPPORT_IPV6FRAG)\n    if (f->frag & PICO_IPV4_EVIL) {\n        (void)pico_icmp4_param_problem(S, f, 0);\n        pico_frame_discard(f); /* RFC 3514 */\n        return 0;\n    }\n#endif\n\n    if ((hdr->vhl & 0x0f) < 5) {\n        /* RFC 791: IHL minimum value is 5 */\n        (void)pico_icmp4_param_problem(S, f, 0);\n        pico_frame_discard(f);\n        return 0;\n    }\n\n#if defined(PICO_SUPPORT_IPV4FRAG) || defined(PICO_SUPPORT_IPV6FRAG)\n    if (f->frag & (PICO_IPV4_MOREFRAG | PICO_IPV4_FRAG_MASK))\n    {\n#ifdef PICO_SUPPORT_IPV4FRAG\n        pico_ipv4_process_frag(hdr, f, hdr->proto);\n        /* Frame can be discarded, frag will handle its own copy */\n#endif\n        /* We do not support fragmentation, discard quietly */\n        pico_frame_discard(f);\n        return 0;\n    }\n#endif\n\n#ifdef PICO_SUPPORT_RAWSOCKETS\n    pico_socket_ipv4_process_in(f);\n#endif\n\n    if (pico_ipv4_process_bcast_in(S, f) > 0)\n        return 0;\n\n    if (pico_ipv4_process_mcast_in(f) > 0)\n        return 0;\n\n    if (pico_ipv4_process_local_unicast_in(S, f) > 0)\n        return 0;\n\n    pico_ipv4_process_finally_try_forward(S, f);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,6 +18,11 @@\n     f->transport_hdr = ((uint8_t *)f->net_hdr) + PICO_SIZE_IP4HDR + option_len;\n     f->transport_len = (uint16_t)(short_be(hdr->len) - PICO_SIZE_IP4HDR - option_len);\n     f->net_len = (uint16_t)(PICO_SIZE_IP4HDR + option_len);\n+\n+    if ((f->net_hdr + f->net_len) > (f->buffer + f->buffer_len)) {\n+        pico_frame_discard(f);\n+        return 0;\n+    }\n #if defined(PICO_SUPPORT_IPV4FRAG) || defined(PICO_SUPPORT_IPV6FRAG)\n     f->frag = short_be(hdr->frag);\n #endif",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    if ((f->net_hdr + f->net_len) > (f->buffer + f->buffer_len)) {",
                "        pico_frame_discard(f);",
                "        return 0;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35848",
        "func_name": "virtualsquare/picotcp/tcp_parse_options",
        "description": "VirtualSquare picoTCP (aka PicoTCP-NG) through 2.1 lacks certain size calculations before attempting to set a value of an mss structure member.",
        "git_url": "https://github.com/virtualsquare/picotcp/commit/4b9a16764f2b12b611de9c34a50b4713d10ca401",
        "commit_title": "More checks for correct header sizes",
        "commit_text": "",
        "func_before": "static int tcp_parse_options(struct pico_frame *f)\n{\n    struct pico_socket_tcp *t = (struct pico_socket_tcp *)f->sock;\n    uint8_t *opt = f->transport_hdr + PICO_SIZE_TCPHDR;\n    uint32_t i = 0;\n    f->timestamp = 0;\n    while (i < (f->transport_len - PICO_SIZE_TCPHDR)) {\n        uint8_t type =  opt[i++];\n        uint8_t len;\n        if(i < (f->transport_len - PICO_SIZE_TCPHDR) && (type > 1))\n            len =  opt[i++];\n        else\n            len = 1;\n\n        if (f->payload && ((opt + i) > f->payload))\n            break;\n\n        if (len == 0) {\n            return -1;\n        }\n\n        tcp_dbg_options(\"Received option '%d', len = %d \\n\", type, len);\n        switch (type) {\n        case PICO_TCP_OPTION_NOOP:\n        case PICO_TCP_OPTION_END:\n            break;\n        case PICO_TCP_OPTION_WS:\n            tcp_parse_option_ws(t, len, opt, &i);\n            break;\n        case PICO_TCP_OPTION_SACK_OK:\n            tcp_parse_option_sack_ok(t, f, len, &i);\n            break;\n        case PICO_TCP_OPTION_MSS:\n            tcp_parse_option_mss(t, len, opt, &i);\n            break;\n        case PICO_TCP_OPTION_TIMESTAMP:\n            tcp_parse_option_timestamp(t, f, len, opt, &i);\n            break;\n\n        case PICO_TCP_OPTION_SACK:\n            tcp_rcv_sack(t, opt + i, len - 2);\n            i = i + len - 2;\n            break;\n        default:\n            tcp_dbg_options(\"TCP: received unsupported option %u\\n\", type);\n            i = i + len - 2;\n        }\n    }\n    return 0;\n}",
        "func": "static int tcp_parse_options(struct pico_frame *f)\n{\n    struct pico_socket_tcp *t = (struct pico_socket_tcp *)f->sock;\n    uint8_t *opt = f->transport_hdr + PICO_SIZE_TCPHDR;\n    uint32_t i = 0;\n    f->timestamp = 0;\n\n    if (f->buffer + f->buffer_len > f->transport_hdr + f->transport_len)\n        return -1;\n\n    while (i < (f->transport_len - PICO_SIZE_TCPHDR)) {\n        uint8_t type =  opt[i++];\n        uint8_t len;\n        if(i < (f->transport_len - PICO_SIZE_TCPHDR) && (type > 1))\n            len =  opt[i++];\n        else\n            len = 1;\n\n        if (f->payload && ((opt + i) > f->payload))\n            break;\n\n        if (len == 0) {\n            return -1;\n        }\n\n        tcp_dbg_options(\"Received option '%d', len = %d \\n\", type, len);\n        switch (type) {\n        case PICO_TCP_OPTION_NOOP:\n        case PICO_TCP_OPTION_END:\n            break;\n        case PICO_TCP_OPTION_WS:\n            tcp_parse_option_ws(t, len, opt, &i);\n            break;\n        case PICO_TCP_OPTION_SACK_OK:\n            tcp_parse_option_sack_ok(t, f, len, &i);\n            break;\n        case PICO_TCP_OPTION_MSS:\n            tcp_parse_option_mss(t, len, opt, &i);\n            break;\n        case PICO_TCP_OPTION_TIMESTAMP:\n            tcp_parse_option_timestamp(t, f, len, opt, &i);\n            break;\n\n        case PICO_TCP_OPTION_SACK:\n            tcp_rcv_sack(t, opt + i, len - 2);\n            i = i + len - 2;\n            break;\n        default:\n            tcp_dbg_options(\"TCP: received unsupported option %u\\n\", type);\n            i = i + len - 2;\n        }\n    }\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,10 @@\n     uint8_t *opt = f->transport_hdr + PICO_SIZE_TCPHDR;\n     uint32_t i = 0;\n     f->timestamp = 0;\n+\n+    if (f->buffer + f->buffer_len > f->transport_hdr + f->transport_len)\n+        return -1;\n+\n     while (i < (f->transport_len - PICO_SIZE_TCPHDR)) {\n         uint8_t type =  opt[i++];\n         uint8_t len;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    if (f->buffer + f->buffer_len > f->transport_hdr + f->transport_len)",
                "        return -1;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-35848",
        "func_name": "virtualsquare/picotcp/tcp_parse_option_mss",
        "description": "VirtualSquare picoTCP (aka PicoTCP-NG) through 2.1 lacks certain size calculations before attempting to set a value of an mss structure member.",
        "git_url": "https://github.com/virtualsquare/picotcp/commit/4b9a16764f2b12b611de9c34a50b4713d10ca401",
        "commit_title": "More checks for correct header sizes",
        "commit_text": "",
        "func_before": "static inline void tcp_parse_option_mss(struct pico_socket_tcp *t, uint8_t len, uint8_t *opt, uint32_t *idx)\n{\n    uint16_t mss;\n    if (tcpopt_len_check(idx, len, PICO_TCPOPTLEN_MSS) < 0)\n        return;\n\n    t->mss_ok = 1;\n    mss = short_from(opt + *idx);\n    *idx += (uint32_t)sizeof(uint16_t);\n    if (t->mss > short_be(mss))\n        t->mss = short_be(mss);\n}",
        "func": "static inline void tcp_parse_option_mss(struct pico_socket_tcp *t, uint8_t len, uint8_t *opt, uint32_t *idx)\n{\n    uint16_t mss;\n    if (tcpopt_len_check(idx, len, PICO_TCPOPTLEN_MSS) < 0)\n        return;\n\n    if ((*idx + PICO_TCPOPTLEN_MSS) > len)\n        return;\n\n    t->mss_ok = 1;\n    mss = short_from(opt + *idx);\n    *idx += (uint32_t)sizeof(uint16_t);\n    if (t->mss > short_be(mss))\n        t->mss = short_be(mss);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,9 @@\n {\n     uint16_t mss;\n     if (tcpopt_len_check(idx, len, PICO_TCPOPTLEN_MSS) < 0)\n+        return;\n+\n+    if ((*idx + PICO_TCPOPTLEN_MSS) > len)\n         return;\n \n     t->mss_ok = 1;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        return;",
                "",
                "    if ((*idx + PICO_TCPOPTLEN_MSS) > len)"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  typename TTypes<T>::ConstFlat means,\n                  typename TTypes<T>::ConstFlat stddevs,\n                  typename TTypes<T>::ConstFlat minvals,\n                  typename TTypes<T>::ConstFlat maxvals,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<T>::Flat output) {\n    // The randn rejection sampling is used when the mean and at least this many\n    // standard deviations are inside the bounds.\n    // The uniform proposal samplers become less efficient as the bounds are\n    // further from the mean, the reverse is true for the randn sampler.\n    // This number was chosen by empirical benchmarking. If modified, the\n    // benchmarks in parameterized_truncated_normal_op_test should also be\n    // changed.\n    const T kStdDevsInsideBoundsToUseRandnSampler = T(1.3);\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    auto do_work = [samples_per_batch, num_elements, &ctx, &means, &stddevs,\n                    &minvals, &maxvals, &gen, &output,\n                    kStdDevsInsideBoundsToUseRandnSampler](int start_batch,\n                                                           int limit_batch) {\n      // Capturing \"gen\" by-value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"gen\" by reference and explicitly do a copy assignment here.\n      random::PhiloxRandom gen_copy = gen;\n      // Skip takes units of 128 bytes.  +3 is so rounding doesn't lead to\n      // us using the same state in different batches.\n      // The sample from each iteration uses 2 random numbers.\n      gen_copy.Skip(start_batch * 2 * kMaxIterations * (samples_per_batch + 3) /\n                    4);\n      using Uniform = random::UniformDistribution<random::PhiloxRandom, T>;\n      Uniform dist;\n      using Normal = random::NormalDistribution<random::PhiloxRandom, T>;\n      Normal normal_dist;\n\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, 4> z;\n      Eigen::array<T, 4> g;\n\n      for (int64 b = start_batch; b < limit_batch; ++b) {\n        // We are passed a flat array for each of the parameter tensors.\n        // The input is either a scalar broadcasted to all batches or a vector\n        // with length num_batches, but the scalar becomes an array of length 1.\n        T mean = means((means.dimension(0) == 1) ? 0 : b);\n        T stddev = stddevs((stddevs.dimension(0) == 1) ? 0 : b);\n        T minval = minvals((minvals.dimension(0) == 1) ? 0 : b);\n        T maxval = maxvals((maxvals.dimension(0) == 1) ? 0 : b);\n\n        // The last batch can be short, if we adjusted num_batches and\n        // samples_per_batch.\n        const int64 limit_sample =\n            std::min((b + 1) * samples_per_batch, num_elements);\n        int64 sample = b * samples_per_batch;\n\n        // On GPU, this check will just fill samples with NAN if it fails.\n        OP_REQUIRES(ctx,\n                    stddev > T(0) && minval < maxval &&\n                        (Eigen::numext::isfinite(minval) ||\n                         Eigen::numext::isfinite(maxval)),\n                    errors::InvalidArgument(\"Invalid parameters\"));\n\n        int num_iterations = 0;\n\n        // If possible, make one-sided bound be the lower bound, or make both\n        // bounds positive. Otherwise, the bounds are on either side of the\n        // mean.\n        if ((Eigen::numext::isinf(minval) && minval < T(0)) || maxval < mean) {\n          // Reverse all calculations. normMin and normMax will be flipped.\n          std::swap(minval, maxval);\n          stddev = -stddev;\n        }\n\n        // Calculate normalized samples, then convert them.\n        const T normMin = (minval - mean) / stddev;\n        const T normMax = (maxval - mean) / stddev;\n\n        // Determine the method to use.\n        const T sqrtFactor = Eigen::numext::sqrt((normMin * normMin) + T(4));\n        const T cutoff =\n            T(2) *\n            Eigen::numext::exp(T(0.5) +\n                               (normMin * (normMin - sqrtFactor)) / T(4)) /\n            (normMin + sqrtFactor);\n        const T diff = normMax - normMin;\n\n        if (((normMin < -kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMax >= T(0.))) ||\n            ((normMax > kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMin <= T(0.)))) {\n          // If the bounds are a least 3 standard deviations from the mean\n          // on at least one side then we rejection sample by sampling\n          // from the normal distribution and rejecting samples outside\n          // the bounds.\n          // Under this condition the acceptance rate per iteration should\n          // always be ~ 50%. This sampler is more efficient (and more\n          // numerically stable when one or both bounds is far from the mean).\n\n          while (sample < limit_sample) {\n            const auto randn_sample = normal_dist(&gen_copy);\n            const int size = randn_sample.size();\n\n            for (int i = 0; i < size; i++) {\n              if ((randn_sample[i] >= normMin) &&\n                  (randn_sample[i] <= normMax)) {\n                output(sample) = randn_sample[i] * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n                if (num_iterations > kMaxIterations) {\n                  // This should never occur because this sampler should\n                  // (by the selection criteria above) be used if at least 3\n                  // standard deviations of one side of the distribution\n                  // is within the limits (so acceptance probability per\n                  // iterations >~ 1/2 per iteration).\n                  LOG(ERROR) << \"TruncatedNormal randn rejection sampler \"\n                             << \"exceeded maximum iterations for \"\n                             << \"normMin=\" << normMin << \" normMax=\" << normMax\n                             << \" kMaxIterations=\" << kMaxIterations;\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal randn rejection sampler failed to accept\"\n                      \" a sample.\"));\n                  return;\n                }\n              }\n            }\n          }\n        } else if (diff < cutoff) {\n          // Sample from a uniform distribution on [normMin, normMax].\n\n          const T plusFactor = (normMin < T(0)) ? T(0) : normMin * normMin;\n\n          while (sample < limit_sample) {\n            const auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            // NOTE(ringwalt): These loops seem to only generate packed AVX\n            // instructions for float32.\n            for (int i = 0; i < size; i++) {\n              z[i] = rand[i] * diff + normMin;\n            }\n            for (int i = 0; i < size; i++) {\n              g[i] = (plusFactor - z[i] * z[i]) / T(2.0);\n            }\n\n            const auto u = dist(&gen_copy);\n            for (int i = 0; i < size; i++) {\n              auto accept = u[i] <= Eigen::numext::exp(g[i]);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                // Accept the sample z.\n                // If we run out of iterations, just use the current uniform\n                // sample, but emit a warning.\n                // TODO(jjhunt) For small entropies (relative to the bounds),\n                // this sampler is poor and may take many iterations since\n                // the proposal distribution is the uniform distribution\n                // U(lower_bound, upper_bound).\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal uniform rejection sampler \"\n                             << \"exceeded max iterations. Sample may contain \"\n                             << \"outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal uniform rejection sampler failed to \"\n                      \" accept a sample.\"));\n                  return;\n                }\n                output(sample) = z[i] * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        } else {\n          // Sample from an exponential distribution with alpha maximizing\n          // acceptance probability, offset by normMin from the origin.\n          // Accept only if less than normMax.\n          const T alpha =\n              (normMin + Eigen::numext::sqrt((normMin * normMin) + T(4))) /\n              T(2);\n          while (sample < limit_sample) {\n            auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            int i = 0;\n            while (i < size) {\n              const T z = -Eigen::numext::log(rand[i]) / alpha + normMin;\n              i++;\n              const T x = normMin < alpha ? alpha - z : normMin - alpha;\n              const T g = Eigen::numext::exp(-x * x / T(2.0));\n              const T u = rand[i];\n              i++;\n              auto accept = (u <= g && z < normMax);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal exponential distribution \"\n                             << \"rejection sampler exceeds max iterations. \"\n                             << \"Sample may contain outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal exponential distribution rejection\"\n                      \" sampler failed to accept a sample.\"));\n                  return;\n                }\n                output(sample) = z * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        }\n      }\n    };\n    // The cost of the initial calculations for the batch.\n    const int64 batchInitCost =\n        // normMin, normMax\n        (Eigen::TensorOpCost::AddCost<T>() +\n         Eigen::TensorOpCost::MulCost<T>()) *\n            2\n        // sqrtFactor\n        + Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_sqrt_op<T>>::Cost\n        // cutoff\n        + Eigen::TensorOpCost::MulCost<T>() * 4 +\n        Eigen::internal::functor_traits<Eigen::internal::scalar_exp_op<T>>::Cost\n        // diff\n        + Eigen::TensorOpCost::AddCost<T>();\n    const int64 uniformSampleCost =\n        random::PhiloxRandom::kElementCost +\n        random::UniformDistribution<random::PhiloxRandom, T>::kElementCost;\n    // The cost of a single uniform sampling round.\n    const int64 uniformRejectionSamplingCost =\n        uniformSampleCost + Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() * 2 +\n        Eigen::TensorOpCost::AddCost<T>() + uniformSampleCost +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_exp_op<T>>::Cost +\n        Eigen::TensorOpCost::MulCost<T>() + Eigen::TensorOpCost::AddCost<T>();\n    // Estimate the cost for an entire batch.\n    // Assume we use uniform sampling, and accept the 2nd sample on average.\n    const int64 batchCost =\n        batchInitCost + uniformRejectionSamplingCost * 2 * samples_per_batch;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_batches,\n          batchCost, do_work);\n  }",
        "func": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  typename TTypes<T>::ConstFlat means,\n                  typename TTypes<T>::ConstFlat stddevs,\n                  typename TTypes<T>::ConstFlat minvals,\n                  typename TTypes<T>::ConstFlat maxvals,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<T>::Flat output) {\n    // The randn rejection sampling is used when the mean and at least this many\n    // standard deviations are inside the bounds.\n    // The uniform proposal samplers become less efficient as the bounds are\n    // further from the mean, the reverse is true for the randn sampler.\n    // This number was chosen by empirical benchmarking. If modified, the\n    // benchmarks in parameterized_truncated_normal_op_test should also be\n    // changed.\n    const T kStdDevsInsideBoundsToUseRandnSampler = T(1.3);\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    auto do_work = [samples_per_batch, num_elements, &ctx, &means, &stddevs,\n                    &minvals, &maxvals, &gen, &output,\n                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_batch,\n                                                           int64 limit_batch) {\n      // Capturing \"gen\" by-value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"gen\" by reference and explicitly do a copy assignment here.\n      random::PhiloxRandom gen_copy = gen;\n      // Skip takes units of 128 bytes.  +3 is so rounding doesn't lead to\n      // us using the same state in different batches.\n      // The sample from each iteration uses 2 random numbers.\n      gen_copy.Skip(start_batch * 2 * kMaxIterations * (samples_per_batch + 3) /\n                    4);\n      using Uniform = random::UniformDistribution<random::PhiloxRandom, T>;\n      Uniform dist;\n      using Normal = random::NormalDistribution<random::PhiloxRandom, T>;\n      Normal normal_dist;\n\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, 4> z;\n      Eigen::array<T, 4> g;\n\n      for (int64 b = start_batch; b < limit_batch; ++b) {\n        // We are passed a flat array for each of the parameter tensors.\n        // The input is either a scalar broadcasted to all batches or a vector\n        // with length num_batches, but the scalar becomes an array of length 1.\n        T mean = means((means.dimension(0) == 1) ? 0 : b);\n        T stddev = stddevs((stddevs.dimension(0) == 1) ? 0 : b);\n        T minval = minvals((minvals.dimension(0) == 1) ? 0 : b);\n        T maxval = maxvals((maxvals.dimension(0) == 1) ? 0 : b);\n\n        // The last batch can be short, if we adjusted num_batches and\n        // samples_per_batch.\n        const int64 limit_sample =\n            std::min((b + 1) * samples_per_batch, num_elements);\n        int64 sample = b * samples_per_batch;\n\n        // On GPU, this check will just fill samples with NAN if it fails.\n        OP_REQUIRES(ctx,\n                    stddev > T(0) && minval < maxval &&\n                        (Eigen::numext::isfinite(minval) ||\n                         Eigen::numext::isfinite(maxval)),\n                    errors::InvalidArgument(\"Invalid parameters\"));\n\n        int num_iterations = 0;\n\n        // If possible, make one-sided bound be the lower bound, or make both\n        // bounds positive. Otherwise, the bounds are on either side of the\n        // mean.\n        if ((Eigen::numext::isinf(minval) && minval < T(0)) || maxval < mean) {\n          // Reverse all calculations. normMin and normMax will be flipped.\n          std::swap(minval, maxval);\n          stddev = -stddev;\n        }\n\n        // Calculate normalized samples, then convert them.\n        const T normMin = (minval - mean) / stddev;\n        const T normMax = (maxval - mean) / stddev;\n\n        // Determine the method to use.\n        const T sqrtFactor = Eigen::numext::sqrt((normMin * normMin) + T(4));\n        const T cutoff =\n            T(2) *\n            Eigen::numext::exp(T(0.5) +\n                               (normMin * (normMin - sqrtFactor)) / T(4)) /\n            (normMin + sqrtFactor);\n        const T diff = normMax - normMin;\n\n        if (((normMin < -kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMax >= T(0.))) ||\n            ((normMax > kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMin <= T(0.)))) {\n          // If the bounds are a least 3 standard deviations from the mean\n          // on at least one side then we rejection sample by sampling\n          // from the normal distribution and rejecting samples outside\n          // the bounds.\n          // Under this condition the acceptance rate per iteration should\n          // always be ~ 50%. This sampler is more efficient (and more\n          // numerically stable when one or both bounds is far from the mean).\n\n          while (sample < limit_sample) {\n            const auto randn_sample = normal_dist(&gen_copy);\n            const int size = randn_sample.size();\n\n            for (int i = 0; i < size; i++) {\n              if ((randn_sample[i] >= normMin) &&\n                  (randn_sample[i] <= normMax)) {\n                output(sample) = randn_sample[i] * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n                if (num_iterations > kMaxIterations) {\n                  // This should never occur because this sampler should\n                  // (by the selection criteria above) be used if at least 3\n                  // standard deviations of one side of the distribution\n                  // is within the limits (so acceptance probability per\n                  // iterations >~ 1/2 per iteration).\n                  LOG(ERROR) << \"TruncatedNormal randn rejection sampler \"\n                             << \"exceeded maximum iterations for \"\n                             << \"normMin=\" << normMin << \" normMax=\" << normMax\n                             << \" kMaxIterations=\" << kMaxIterations;\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal randn rejection sampler failed to accept\"\n                      \" a sample.\"));\n                  return;\n                }\n              }\n            }\n          }\n        } else if (diff < cutoff) {\n          // Sample from a uniform distribution on [normMin, normMax].\n\n          const T plusFactor = (normMin < T(0)) ? T(0) : normMin * normMin;\n\n          while (sample < limit_sample) {\n            const auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            // NOTE(ringwalt): These loops seem to only generate packed AVX\n            // instructions for float32.\n            for (int i = 0; i < size; i++) {\n              z[i] = rand[i] * diff + normMin;\n            }\n            for (int i = 0; i < size; i++) {\n              g[i] = (plusFactor - z[i] * z[i]) / T(2.0);\n            }\n\n            const auto u = dist(&gen_copy);\n            for (int i = 0; i < size; i++) {\n              auto accept = u[i] <= Eigen::numext::exp(g[i]);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                // Accept the sample z.\n                // If we run out of iterations, just use the current uniform\n                // sample, but emit a warning.\n                // TODO(jjhunt) For small entropies (relative to the bounds),\n                // this sampler is poor and may take many iterations since\n                // the proposal distribution is the uniform distribution\n                // U(lower_bound, upper_bound).\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal uniform rejection sampler \"\n                             << \"exceeded max iterations. Sample may contain \"\n                             << \"outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal uniform rejection sampler failed to \"\n                      \" accept a sample.\"));\n                  return;\n                }\n                output(sample) = z[i] * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        } else {\n          // Sample from an exponential distribution with alpha maximizing\n          // acceptance probability, offset by normMin from the origin.\n          // Accept only if less than normMax.\n          const T alpha =\n              (normMin + Eigen::numext::sqrt((normMin * normMin) + T(4))) /\n              T(2);\n          while (sample < limit_sample) {\n            auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            int i = 0;\n            while (i < size) {\n              const T z = -Eigen::numext::log(rand[i]) / alpha + normMin;\n              i++;\n              const T x = normMin < alpha ? alpha - z : normMin - alpha;\n              const T g = Eigen::numext::exp(-x * x / T(2.0));\n              const T u = rand[i];\n              i++;\n              auto accept = (u <= g && z < normMax);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal exponential distribution \"\n                             << \"rejection sampler exceeds max iterations. \"\n                             << \"Sample may contain outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal exponential distribution rejection\"\n                      \" sampler failed to accept a sample.\"));\n                  return;\n                }\n                output(sample) = z * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        }\n      }\n    };\n    // The cost of the initial calculations for the batch.\n    const int64 batchInitCost =\n        // normMin, normMax\n        (Eigen::TensorOpCost::AddCost<T>() +\n         Eigen::TensorOpCost::MulCost<T>()) *\n            2\n        // sqrtFactor\n        + Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_sqrt_op<T>>::Cost\n        // cutoff\n        + Eigen::TensorOpCost::MulCost<T>() * 4 +\n        Eigen::internal::functor_traits<Eigen::internal::scalar_exp_op<T>>::Cost\n        // diff\n        + Eigen::TensorOpCost::AddCost<T>();\n    const int64 uniformSampleCost =\n        random::PhiloxRandom::kElementCost +\n        random::UniformDistribution<random::PhiloxRandom, T>::kElementCost;\n    // The cost of a single uniform sampling round.\n    const int64 uniformRejectionSamplingCost =\n        uniformSampleCost + Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() * 2 +\n        Eigen::TensorOpCost::AddCost<T>() + uniformSampleCost +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_exp_op<T>>::Cost +\n        Eigen::TensorOpCost::MulCost<T>() + Eigen::TensorOpCost::AddCost<T>();\n    // Estimate the cost for an entire batch.\n    // Assume we use uniform sampling, and accept the 2nd sample on average.\n    const int64 batchCost =\n        batchInitCost + uniformRejectionSamplingCost * 2 * samples_per_batch;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_batches,\n          batchCost, do_work);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,8 +18,8 @@\n \n     auto do_work = [samples_per_batch, num_elements, &ctx, &means, &stddevs,\n                     &minvals, &maxvals, &gen, &output,\n-                    kStdDevsInsideBoundsToUseRandnSampler](int start_batch,\n-                                                           int limit_batch) {\n+                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_batch,\n+                                                           int64 limit_batch) {\n       // Capturing \"gen\" by-value would only make a copy for the _shared_\n       // lambda.  Since we want to let each worker have its own copy, we pass\n       // \"gen\" by reference and explicitly do a copy assignment here.",
        "diff_line_info": {
            "deleted_lines": [
                "                    kStdDevsInsideBoundsToUseRandnSampler](int start_batch,",
                "                                                           int limit_batch) {"
            ],
            "added_lines": [
                "                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_batch,",
                "                                                           int64 limit_batch) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  const BCastList<4>& bcast,\n                  typename TTypes<T>::ConstFlat means,\n                  typename TTypes<T>::ConstFlat stddevs,\n                  typename TTypes<T>::ConstFlat minvals,\n                  typename TTypes<T>::ConstFlat maxvals,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<T>::Flat output) {\n    // The randn rejection sampling is used when the mean and at least this many\n    // standard deviations are inside the bounds.\n    // The uniform proposal samplers become less efficient as the bounds are\n    // further from the mean, the reverse is true for the randn sampler.\n    // This number was chosen by empirical benchmarking. If modified, the\n    // benchmarks in parameterized_truncated_normal_op_test should also be\n    // changed.\n    const T kStdDevsInsideBoundsToUseRandnSampler = T(1.3);\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    auto do_work = [num_batches, samples_per_batch, &ctx, &bcast, &means,\n                    &stddevs, &minvals, &maxvals, &gen, &output,\n                    kStdDevsInsideBoundsToUseRandnSampler](int start_output,\n                                                           int limit_output) {\n      // Capturing \"gen\" by-value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"gen\" by reference and explicitly do a copy assignment here.\n      random::PhiloxRandom gen_copy = gen;\n      using Uniform = random::UniformDistribution<random::PhiloxRandom, T>;\n      Uniform dist;\n      using Normal = random::NormalDistribution<random::PhiloxRandom, T>;\n      Normal normal_dist;\n      // Skip takes units of 128 bits. The Uniform::kResultElementCount - 1\n      // is so rounding doesn't lead to\n      // us using the same state in different workloads.\n      // The sample from each iteration uses 2 random numbers.\n      gen_copy.Skip((start_output * 2 * kMaxIterations +\n                     Uniform::kResultElementCount - 1) /\n                    Uniform::kResultElementCount);\n\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, Uniform::kResultElementCount> z;\n      Eigen::array<T, Uniform::kResultElementCount> g;\n\n      const bool should_bcast = bcast.IsBroadcastingRequired();\n      const auto& means_batch_indices = bcast.batch_indices(0);\n      const auto& stddevs_batch_indices = bcast.batch_indices(1);\n      const auto& minvals_batch_indices = bcast.batch_indices(2);\n      const auto& maxvals_batch_indices = bcast.batch_indices(3);\n      auto output_flat = output.data();\n\n      // We partition work across batches and then across samples\n      // per batch member, to avoid extra work.\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           // output_idx is incremented with the inner loops below.\n      ) {\n        int64 batch_idx = output_idx / samples_per_batch;\n        // The output layout is [samples_per_batch, num_batches]. Thus\n        // the output address is sample_idx * num_batches + batch_idx.\n        // Below, code will index at output_batch_offset[sample_idx *\n        // num_batches] matching this.\n        T* const output_batch_offset = output_flat + batch_idx;\n        // Generate batch counts from BCast, as it has the right indices to loop\n        // over.\n        T mean, stddev, minval, maxval;\n        if (should_bcast) {\n          mean = means(means_batch_indices[batch_idx]);\n          stddev = stddevs(stddevs_batch_indices[batch_idx]);\n          minval = minvals(minvals_batch_indices[batch_idx]);\n          maxval = maxvals(maxvals_batch_indices[batch_idx]);\n        } else {\n          mean = means(batch_idx);\n          stddev = stddevs(batch_idx);\n          minval = minvals(batch_idx);\n          maxval = maxvals(batch_idx);\n        }\n\n        // On GPU, this check will just fill samples with NAN if it fails.\n        OP_REQUIRES(ctx,\n                    stddev > T(0) && minval < maxval &&\n                        (Eigen::numext::isfinite(minval) ||\n                         Eigen::numext::isfinite(maxval)),\n                    errors::InvalidArgument(\"Invalid parameters\"));\n\n        int num_iterations = 0;\n\n        // If possible, make one-sided bound be the lower bound, or make both\n        // bounds positive. Otherwise, the bounds are on either side of the\n        // mean.\n        if ((Eigen::numext::isinf(minval) && minval < T(0)) || maxval < mean) {\n          // Reverse all calculations. normMin and normMax will be flipped.\n          std::swap(minval, maxval);\n          stddev = -stddev;\n        }\n\n        // Calculate normalized samples, then convert them.\n        const T normMin = (minval - mean) / stddev;\n        const T normMax = (maxval - mean) / stddev;\n\n        // Determine the method to use.\n        const T sqrtFactor = Eigen::numext::sqrt((normMin * normMin) + T(4));\n        const T cutoff =\n            T(2) *\n            Eigen::numext::exp(T(0.5) +\n                               (normMin * (normMin - sqrtFactor)) / T(4)) /\n            (normMin + sqrtFactor);\n        const T diff = normMax - normMin;\n\n        if (((normMin < -kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMax >= T(0.))) ||\n            ((normMax > kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMin <= T(0.)))) {\n          // If the bounds are a least 3 standard deviations from the mean\n          // on at least one side then we rejection sample by sampling\n          // from the normal distribution and rejecting samples outside\n          // the bounds.\n          // Under this condition the acceptance rate per iteration should\n          // always be ~ 50%. This sampler is more efficient (and more\n          // numerically stable when one or both bounds is far from the mean).\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            const auto randn_sample = normal_dist(&gen_copy);\n            const int size = randn_sample.size();\n            for (int i = 0; i < size; ++i) {\n              if ((randn_sample[i] >= normMin) &&\n                  (randn_sample[i] <= normMax)) {\n                output_batch_offset[sample_idx * num_batches] =\n                    randn_sample[i] * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                ++num_iterations;\n                if (num_iterations > kMaxIterations) {\n                  // This should never occur because this sampler should\n                  // (by the selection criteria above) be used if at least 3\n                  // standard deviations of one side of the distribution\n                  // is within the limits (so acceptance probability per\n                  // iterations >~ 1/2 per iteration).\n                  LOG(ERROR) << \"TruncatedNormal randn rejection sampler \"\n                             << \"exceeded maximum iterations for \"\n                             << \"normMin=\" << normMin << \" normMax=\" << normMax\n                             << \" kMaxIterations=\" << kMaxIterations;\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal randn rejection sampler failed to accept\"\n                      \" a sample.\"));\n                  return;\n                }\n              }\n            }\n          }\n        } else if (diff < cutoff) {\n          // Sample from a uniform distribution on [normMin, normMax].\n\n          const T plusFactor = (normMin < T(0)) ? T(0) : normMin * normMin;\n\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            const auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            // NOTE(ringwalt): These loops seem to only generate packed AVX\n            // instructions for float32.\n            for (int i = 0; i < size; i++) {\n              z[i] = rand[i] * diff + normMin;\n              g[i] = (plusFactor - z[i] * z[i]) / T(2.0);\n            }\n\n            const auto u = dist(&gen_copy);\n            for (int i = 0; i < size; i++) {\n              auto accept = u[i] <= Eigen::numext::exp(g[i]);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                // Accept the sample z.\n                // If we run out of iterations, just use the current uniform\n                // sample, but emit a warning.\n                // TODO(jjhunt) For small entropies (relative to the bounds),\n                // this sampler is poor and may take many iterations since\n                // the proposal distribution is the uniform distribution\n                // U(lower_bound, upper_bound).\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal uniform rejection sampler \"\n                             << \"exceeded max iterations. Sample may contain \"\n                             << \"outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal uniform rejection sampler failed to \"\n                      \" accept a sample.\"));\n                  return;\n                }\n                output_batch_offset[sample_idx * num_batches] =\n                    z[i] * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        } else {\n          // Sample from an exponential distribution with alpha maximizing\n          // acceptance probability, offset by normMin from the origin.\n          // Accept only if less than normMax.\n          const T alpha =\n              (normMin + Eigen::numext::sqrt((normMin * normMin) + T(4))) /\n              T(2);\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            int i = 0;\n            while (i < size) {\n              const T z = -Eigen::numext::log(rand[i]) / alpha + normMin;\n              i++;\n              const T x = normMin < alpha ? alpha - z : normMin - alpha;\n              const T g = Eigen::numext::exp(-x * x / T(2.0));\n              const T u = rand[i];\n              i++;\n              auto accept = (u <= g && z < normMax);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal exponential distribution \"\n                             << \"rejection sampler exceeds max iterations. \"\n                             << \"Sample may contain outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal exponential distribution rejection\"\n                      \" sampler failed to accept a sample.\"));\n                  return;\n                }\n                output_batch_offset[sample_idx * num_batches] =\n                    z * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        }\n      }\n    };\n    // The cost of the initial calculations for the batch.\n    const int64 batchInitCost =\n        // normMin, normMax\n        (Eigen::TensorOpCost::AddCost<T>() +\n         Eigen::TensorOpCost::MulCost<T>()) *\n            2\n        // sqrtFactor\n        + Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_sqrt_op<T>>::Cost\n        // cutoff\n        + Eigen::TensorOpCost::MulCost<T>() * 4 +\n        Eigen::internal::functor_traits<Eigen::internal::scalar_exp_op<T>>::Cost\n        // diff\n        + Eigen::TensorOpCost::AddCost<T>();\n    const int64 uniformSampleCost =\n        random::PhiloxRandom::kElementCost +\n        random::UniformDistribution<random::PhiloxRandom, T>::kElementCost;\n    // The cost of a single uniform sampling round.\n    const int64 uniformRejectionSamplingCost =\n        uniformSampleCost + Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() * 2 +\n        Eigen::TensorOpCost::AddCost<T>() + uniformSampleCost +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_exp_op<T>>::Cost +\n        Eigen::TensorOpCost::MulCost<T>() + Eigen::TensorOpCost::AddCost<T>();\n    // Estimate the cost for an entire batch.\n    // Assume we use uniform sampling, and accept the 2nd sample on average.\n    const int64 batchCost = batchInitCost + uniformRejectionSamplingCost * 2;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_elements,\n          batchCost, do_work);\n  }",
        "func": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  const BCastList<4>& bcast,\n                  typename TTypes<T>::ConstFlat means,\n                  typename TTypes<T>::ConstFlat stddevs,\n                  typename TTypes<T>::ConstFlat minvals,\n                  typename TTypes<T>::ConstFlat maxvals,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<T>::Flat output) {\n    // The randn rejection sampling is used when the mean and at least this many\n    // standard deviations are inside the bounds.\n    // The uniform proposal samplers become less efficient as the bounds are\n    // further from the mean, the reverse is true for the randn sampler.\n    // This number was chosen by empirical benchmarking. If modified, the\n    // benchmarks in parameterized_truncated_normal_op_test should also be\n    // changed.\n    const T kStdDevsInsideBoundsToUseRandnSampler = T(1.3);\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    auto do_work = [num_batches, samples_per_batch, &ctx, &bcast, &means,\n                    &stddevs, &minvals, &maxvals, &gen, &output,\n                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_output,\n                                                           int64 limit_output) {\n      // Capturing \"gen\" by-value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"gen\" by reference and explicitly do a copy assignment here.\n      random::PhiloxRandom gen_copy = gen;\n      using Uniform = random::UniformDistribution<random::PhiloxRandom, T>;\n      Uniform dist;\n      using Normal = random::NormalDistribution<random::PhiloxRandom, T>;\n      Normal normal_dist;\n      // Skip takes units of 128 bits. The Uniform::kResultElementCount - 1\n      // is so rounding doesn't lead to\n      // us using the same state in different workloads.\n      // The sample from each iteration uses 2 random numbers.\n      gen_copy.Skip((start_output * 2 * kMaxIterations +\n                     Uniform::kResultElementCount - 1) /\n                    Uniform::kResultElementCount);\n\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, Uniform::kResultElementCount> z;\n      Eigen::array<T, Uniform::kResultElementCount> g;\n\n      const bool should_bcast = bcast.IsBroadcastingRequired();\n      const auto& means_batch_indices = bcast.batch_indices(0);\n      const auto& stddevs_batch_indices = bcast.batch_indices(1);\n      const auto& minvals_batch_indices = bcast.batch_indices(2);\n      const auto& maxvals_batch_indices = bcast.batch_indices(3);\n      auto output_flat = output.data();\n\n      // We partition work across batches and then across samples\n      // per batch member, to avoid extra work.\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           // output_idx is incremented with the inner loops below.\n      ) {\n        int64 batch_idx = output_idx / samples_per_batch;\n        // The output layout is [samples_per_batch, num_batches]. Thus\n        // the output address is sample_idx * num_batches + batch_idx.\n        // Below, code will index at output_batch_offset[sample_idx *\n        // num_batches] matching this.\n        T* const output_batch_offset = output_flat + batch_idx;\n        // Generate batch counts from BCast, as it has the right indices to loop\n        // over.\n        T mean, stddev, minval, maxval;\n        if (should_bcast) {\n          mean = means(means_batch_indices[batch_idx]);\n          stddev = stddevs(stddevs_batch_indices[batch_idx]);\n          minval = minvals(minvals_batch_indices[batch_idx]);\n          maxval = maxvals(maxvals_batch_indices[batch_idx]);\n        } else {\n          mean = means(batch_idx);\n          stddev = stddevs(batch_idx);\n          minval = minvals(batch_idx);\n          maxval = maxvals(batch_idx);\n        }\n\n        // On GPU, this check will just fill samples with NAN if it fails.\n        OP_REQUIRES(ctx,\n                    stddev > T(0) && minval < maxval &&\n                        (Eigen::numext::isfinite(minval) ||\n                         Eigen::numext::isfinite(maxval)),\n                    errors::InvalidArgument(\"Invalid parameters\"));\n\n        int num_iterations = 0;\n\n        // If possible, make one-sided bound be the lower bound, or make both\n        // bounds positive. Otherwise, the bounds are on either side of the\n        // mean.\n        if ((Eigen::numext::isinf(minval) && minval < T(0)) || maxval < mean) {\n          // Reverse all calculations. normMin and normMax will be flipped.\n          std::swap(minval, maxval);\n          stddev = -stddev;\n        }\n\n        // Calculate normalized samples, then convert them.\n        const T normMin = (minval - mean) / stddev;\n        const T normMax = (maxval - mean) / stddev;\n\n        // Determine the method to use.\n        const T sqrtFactor = Eigen::numext::sqrt((normMin * normMin) + T(4));\n        const T cutoff =\n            T(2) *\n            Eigen::numext::exp(T(0.5) +\n                               (normMin * (normMin - sqrtFactor)) / T(4)) /\n            (normMin + sqrtFactor);\n        const T diff = normMax - normMin;\n\n        if (((normMin < -kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMax >= T(0.))) ||\n            ((normMax > kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMin <= T(0.)))) {\n          // If the bounds are a least 3 standard deviations from the mean\n          // on at least one side then we rejection sample by sampling\n          // from the normal distribution and rejecting samples outside\n          // the bounds.\n          // Under this condition the acceptance rate per iteration should\n          // always be ~ 50%. This sampler is more efficient (and more\n          // numerically stable when one or both bounds is far from the mean).\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            const auto randn_sample = normal_dist(&gen_copy);\n            const int size = randn_sample.size();\n            for (int i = 0; i < size; ++i) {\n              if ((randn_sample[i] >= normMin) &&\n                  (randn_sample[i] <= normMax)) {\n                output_batch_offset[sample_idx * num_batches] =\n                    randn_sample[i] * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                ++num_iterations;\n                if (num_iterations > kMaxIterations) {\n                  // This should never occur because this sampler should\n                  // (by the selection criteria above) be used if at least 3\n                  // standard deviations of one side of the distribution\n                  // is within the limits (so acceptance probability per\n                  // iterations >~ 1/2 per iteration).\n                  LOG(ERROR) << \"TruncatedNormal randn rejection sampler \"\n                             << \"exceeded maximum iterations for \"\n                             << \"normMin=\" << normMin << \" normMax=\" << normMax\n                             << \" kMaxIterations=\" << kMaxIterations;\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal randn rejection sampler failed to accept\"\n                      \" a sample.\"));\n                  return;\n                }\n              }\n            }\n          }\n        } else if (diff < cutoff) {\n          // Sample from a uniform distribution on [normMin, normMax].\n\n          const T plusFactor = (normMin < T(0)) ? T(0) : normMin * normMin;\n\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            const auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            // NOTE(ringwalt): These loops seem to only generate packed AVX\n            // instructions for float32.\n            for (int i = 0; i < size; i++) {\n              z[i] = rand[i] * diff + normMin;\n              g[i] = (plusFactor - z[i] * z[i]) / T(2.0);\n            }\n\n            const auto u = dist(&gen_copy);\n            for (int i = 0; i < size; i++) {\n              auto accept = u[i] <= Eigen::numext::exp(g[i]);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                // Accept the sample z.\n                // If we run out of iterations, just use the current uniform\n                // sample, but emit a warning.\n                // TODO(jjhunt) For small entropies (relative to the bounds),\n                // this sampler is poor and may take many iterations since\n                // the proposal distribution is the uniform distribution\n                // U(lower_bound, upper_bound).\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal uniform rejection sampler \"\n                             << \"exceeded max iterations. Sample may contain \"\n                             << \"outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal uniform rejection sampler failed to \"\n                      \" accept a sample.\"));\n                  return;\n                }\n                output_batch_offset[sample_idx * num_batches] =\n                    z[i] * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        } else {\n          // Sample from an exponential distribution with alpha maximizing\n          // acceptance probability, offset by normMin from the origin.\n          // Accept only if less than normMax.\n          const T alpha =\n              (normMin + Eigen::numext::sqrt((normMin * normMin) + T(4))) /\n              T(2);\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            int i = 0;\n            while (i < size) {\n              const T z = -Eigen::numext::log(rand[i]) / alpha + normMin;\n              i++;\n              const T x = normMin < alpha ? alpha - z : normMin - alpha;\n              const T g = Eigen::numext::exp(-x * x / T(2.0));\n              const T u = rand[i];\n              i++;\n              auto accept = (u <= g && z < normMax);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal exponential distribution \"\n                             << \"rejection sampler exceeds max iterations. \"\n                             << \"Sample may contain outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal exponential distribution rejection\"\n                      \" sampler failed to accept a sample.\"));\n                  return;\n                }\n                output_batch_offset[sample_idx * num_batches] =\n                    z * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        }\n      }\n    };\n    // The cost of the initial calculations for the batch.\n    const int64 batchInitCost =\n        // normMin, normMax\n        (Eigen::TensorOpCost::AddCost<T>() +\n         Eigen::TensorOpCost::MulCost<T>()) *\n            2\n        // sqrtFactor\n        + Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_sqrt_op<T>>::Cost\n        // cutoff\n        + Eigen::TensorOpCost::MulCost<T>() * 4 +\n        Eigen::internal::functor_traits<Eigen::internal::scalar_exp_op<T>>::Cost\n        // diff\n        + Eigen::TensorOpCost::AddCost<T>();\n    const int64 uniformSampleCost =\n        random::PhiloxRandom::kElementCost +\n        random::UniformDistribution<random::PhiloxRandom, T>::kElementCost;\n    // The cost of a single uniform sampling round.\n    const int64 uniformRejectionSamplingCost =\n        uniformSampleCost + Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() * 2 +\n        Eigen::TensorOpCost::AddCost<T>() + uniformSampleCost +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_exp_op<T>>::Cost +\n        Eigen::TensorOpCost::MulCost<T>() + Eigen::TensorOpCost::AddCost<T>();\n    // Estimate the cost for an entire batch.\n    // Assume we use uniform sampling, and accept the 2nd sample on average.\n    const int64 batchCost = batchInitCost + uniformRejectionSamplingCost * 2;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_elements,\n          batchCost, do_work);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,8 +19,8 @@\n \n     auto do_work = [num_batches, samples_per_batch, &ctx, &bcast, &means,\n                     &stddevs, &minvals, &maxvals, &gen, &output,\n-                    kStdDevsInsideBoundsToUseRandnSampler](int start_output,\n-                                                           int limit_output) {\n+                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_output,\n+                                                           int64 limit_output) {\n       // Capturing \"gen\" by-value would only make a copy for the _shared_\n       // lambda.  Since we want to let each worker have its own copy, we pass\n       // \"gen\" by reference and explicitly do a copy assignment here.",
        "diff_line_info": {
            "deleted_lines": [
                "                    kStdDevsInsideBoundsToUseRandnSampler](int start_output,",
                "                                                           int limit_output) {"
            ],
            "added_lines": [
                "                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_output,",
                "                                                           int64 limit_output) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  const BCast& bcast, typename TTypes<T>::ConstFlat counts,\n                  typename TTypes<T>::ConstFlat probs,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<U>::Flat output) {\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    // The output layout is [B1, ... Bk, H1, ... Hm]. We have [B1, ... Bk] for\n    // the sample shape and [H1, ... Hm] for the batch shape of the samples.\n    // We have B1 * ... * Bk samples per batch member we need.\n    auto DoWork = [num_batches, samples_per_batch, &bcast, &counts, &probs,\n                   &gen, &output](int start_output, int limit_output) {\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, 4> z;\n      Eigen::array<T, 4> g;\n      const bool should_bcast = bcast.IsBroadcastingRequired();\n      const auto& counts_batch_indices = bcast.x_batch_indices();\n      const auto& probs_batch_indices = bcast.y_batch_indices();\n      auto output_flat = output.data();\n\n      // We partition work across batches (count, prob) and then across samples\n      // per batch member, to avoid extra work.\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           // output_idx is incremented with the inner loops below.\n      ) {\n        int64 batch_idx = output_idx / samples_per_batch;\n        U* const output_batch_offset = output_flat + batch_idx;\n        // Generate batch counts from BCast, as it has the right indices to loop\n        // over.\n        T count, prob;\n        if (should_bcast) {\n          count = counts(counts_batch_indices[batch_idx]);\n          prob = probs(probs_batch_indices[batch_idx]);\n        } else {\n          count = counts(batch_idx);\n          prob = probs(batch_idx);\n        }\n\n        // Calculate normalized samples, then convert them.\n        // Determine the method to use.\n        double dcount = static_cast<double>(count);\n        if (dcount <= 0.0 || prob <= T(0.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(0.0);\n          }\n        } else if (prob >= T(1.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] =\n                static_cast<U>(dcount);\n          }\n        } else if (prob <= T(0.5)) {\n          double dp = static_cast<double>(prob);\n          if (count * prob >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(btrs(dcount, dp, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(binomial_inversion(dcount, dp, &gen_copy));\n            }\n          }\n        } else if (prob > T(0.5)) {\n          T q = T(1) - prob;\n          double dcount = static_cast<double>(count);\n          double dq = static_cast<double>(q);\n          if (count * q >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(dcount - btrs(dcount, dq, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] = static_cast<U>(\n                  dcount - binomial_inversion(dcount, dq, &gen_copy));\n            }\n          }\n        } else {  // prob is NaN\n          // TODO(srvasude): What should happen if prob is NaN but the output\n          // type is an integer (which doesn't have a sentinel for NaN)?  Fail\n          // the whole batch sample?  Return a specialized sentinel like -1?\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(NAN);\n          }\n        }\n      }\n    };\n\n    // This will depend on count * p (or count * q).\n    // For n * p < 10, on average, O(n * p) calls to uniform are\n    // needed, with that\n    // many multiplies. ~10 uniform calls on average with ~200 cost op calls.\n    //\n    // Very roughly, for rate >= 10, the four calls to log\n    // occur for ~72 percent of samples.\n    // 4 x 100 (64-bit cycles per log) * 0.72 = ~288\n    // Additionally, there are ~10 other ops (+, *, /, ...) at 3-6 cycles each:\n    // 40 * .72  = ~25.\n    //\n    // Finally, there are several other ops that are done every loop along with\n    // 2 uniform generations along with 5 other ops at 3-6 cycles each.\n    // ~15 / .89 = ~16\n    //\n    // In total this (rate >= 10) should be ~329 + 2 * Uniform::kElementCost.\n    // We assume that half the tensor has rate < 10, so on average 6\n    // uniform's\n    // will be needed. We will upper bound the other op cost by the one for\n    // rate > 10.\n    static const int kElementCost = 329 + 6 * Uniform::kElementCost +\n                                    6 * random::PhiloxRandom::kElementCost;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_elements,\n          kElementCost, DoWork);\n  }",
        "func": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  const BCast& bcast, typename TTypes<T>::ConstFlat counts,\n                  typename TTypes<T>::ConstFlat probs,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<U>::Flat output) {\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    // The output layout is [B1, ... Bk, H1, ... Hm]. We have [B1, ... Bk] for\n    // the sample shape and [H1, ... Hm] for the batch shape of the samples.\n    // We have B1 * ... * Bk samples per batch member we need.\n    auto DoWork = [num_batches, samples_per_batch, &bcast, &counts, &probs,\n                   &gen, &output](int64 start_output, int64 limit_output) {\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, 4> z;\n      Eigen::array<T, 4> g;\n      const bool should_bcast = bcast.IsBroadcastingRequired();\n      const auto& counts_batch_indices = bcast.x_batch_indices();\n      const auto& probs_batch_indices = bcast.y_batch_indices();\n      auto output_flat = output.data();\n\n      // We partition work across batches (count, prob) and then across samples\n      // per batch member, to avoid extra work.\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           // output_idx is incremented with the inner loops below.\n      ) {\n        int64 batch_idx = output_idx / samples_per_batch;\n        U* const output_batch_offset = output_flat + batch_idx;\n        // Generate batch counts from BCast, as it has the right indices to loop\n        // over.\n        T count, prob;\n        if (should_bcast) {\n          count = counts(counts_batch_indices[batch_idx]);\n          prob = probs(probs_batch_indices[batch_idx]);\n        } else {\n          count = counts(batch_idx);\n          prob = probs(batch_idx);\n        }\n\n        // Calculate normalized samples, then convert them.\n        // Determine the method to use.\n        double dcount = static_cast<double>(count);\n        if (dcount <= 0.0 || prob <= T(0.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(0.0);\n          }\n        } else if (prob >= T(1.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] =\n                static_cast<U>(dcount);\n          }\n        } else if (prob <= T(0.5)) {\n          double dp = static_cast<double>(prob);\n          if (count * prob >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(btrs(dcount, dp, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(binomial_inversion(dcount, dp, &gen_copy));\n            }\n          }\n        } else if (prob > T(0.5)) {\n          T q = T(1) - prob;\n          double dcount = static_cast<double>(count);\n          double dq = static_cast<double>(q);\n          if (count * q >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(dcount - btrs(dcount, dq, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] = static_cast<U>(\n                  dcount - binomial_inversion(dcount, dq, &gen_copy));\n            }\n          }\n        } else {  // prob is NaN\n          // TODO(srvasude): What should happen if prob is NaN but the output\n          // type is an integer (which doesn't have a sentinel for NaN)?  Fail\n          // the whole batch sample?  Return a specialized sentinel like -1?\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(NAN);\n          }\n        }\n      }\n    };\n\n    // This will depend on count * p (or count * q).\n    // For n * p < 10, on average, O(n * p) calls to uniform are\n    // needed, with that\n    // many multiplies. ~10 uniform calls on average with ~200 cost op calls.\n    //\n    // Very roughly, for rate >= 10, the four calls to log\n    // occur for ~72 percent of samples.\n    // 4 x 100 (64-bit cycles per log) * 0.72 = ~288\n    // Additionally, there are ~10 other ops (+, *, /, ...) at 3-6 cycles each:\n    // 40 * .72  = ~25.\n    //\n    // Finally, there are several other ops that are done every loop along with\n    // 2 uniform generations along with 5 other ops at 3-6 cycles each.\n    // ~15 / .89 = ~16\n    //\n    // In total this (rate >= 10) should be ~329 + 2 * Uniform::kElementCost.\n    // We assume that half the tensor has rate < 10, so on average 6\n    // uniform's\n    // will be needed. We will upper bound the other op cost by the one for\n    // rate > 10.\n    static const int kElementCost = 329 + 6 * Uniform::kElementCost +\n                                    6 * random::PhiloxRandom::kElementCost;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_elements,\n          kElementCost, DoWork);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,7 +10,7 @@\n     // the sample shape and [H1, ... Hm] for the batch shape of the samples.\n     // We have B1 * ... * Bk samples per batch member we need.\n     auto DoWork = [num_batches, samples_per_batch, &bcast, &counts, &probs,\n-                   &gen, &output](int start_output, int limit_output) {\n+                   &gen, &output](int64 start_output, int64 limit_output) {\n       // Vectorized intermediate calculations for uniform rejection sampling.\n       // We always generate at most 4 samples.\n       Eigen::array<T, 4> z;",
        "diff_line_info": {
            "deleted_lines": [
                "                   &gen, &output](int start_output, int limit_output) {"
            ],
            "added_lines": [
                "                   &gen, &output](int64 start_output, int64 limit_output) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "bool operator()(const OpKernelContext* context,\n                  typename TTypes<float, 4>::ConstTensor grads,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  typename TTypes<T, 4>::Tensor grads_image,\n                  const string& method_name) {\n    const int batch_size = grads_image.dimension(0);\n    const int image_height = grads_image.dimension(1);\n    const int image_width = grads_image.dimension(2);\n\n    const int num_boxes = grads.dimension(0);\n    const int crop_height = grads.dimension(1);\n    const int crop_width = grads.dimension(2);\n    const int depth = grads.dimension(3);\n\n    grads_image.setZero();\n\n    auto CropAndResizeBackImgPerBox = [&](int start_box, int limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            continue;\n          }\n          const int top_y_index = floorf(in_y);\n          const int bottom_y_index = ceilf(in_y);\n          const float y_lerp = in_y - top_y_index;\n\n          for (int x = 0; x < crop_width; ++x) {\n            const float in_x = (crop_width > 1)\n                                   ? x1 * (image_width - 1) + x * width_scale\n                                   : 0.5 * (x1 + x2) * (image_width - 1);\n            if (in_x < 0 || in_x > image_width - 1) {\n              continue;\n            }\n\n            if (method_name == \"bilinear\") {\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float dtop = (1 - y_lerp) * grads(b, y, x, d);\n                grads_image(b_in, top_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dtop);\n                grads_image(b_in, top_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dtop);\n                const float dbottom = y_lerp * grads(b, y, x, d);\n                grads_image(b_in, bottom_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dbottom);\n                grads_image(b_in, bottom_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dbottom);\n              }\n            } else {  // method_name == \"nearest\"\n              for (int d = 0; d < depth; ++d) {\n                int closest_x_index = roundf(in_x);\n                int closest_y_index = roundf(in_y);\n                grads_image(b_in, closest_y_index, closest_x_index, d) +=\n                    static_cast<T>(grads(b, y, x, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    // Including calculation cost in the depth loop and pixel loop.\n    const double cost_per_pixel =\n        (method_name == \"bilinear\"\n             ? depth * (Eigen::TensorOpCost::AddCost<float>() * 7 +\n                        Eigen::TensorOpCost::MulCost<float>() * 6 +\n                        Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n                   Eigen::TensorOpCost::AddCost<float>() * 4\n             : depth * (Eigen::TensorOpCost::AddCost<float>() +\n                        Eigen::TensorOpCost::CastCost<T, float>()) +\n                   Eigen::TensorOpCost::AddCost<float>() * 3);\n\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizeBackImgPerBox);\n\n    return true;\n  }",
        "func": "bool operator()(const OpKernelContext* context,\n                  typename TTypes<float, 4>::ConstTensor grads,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  typename TTypes<T, 4>::Tensor grads_image,\n                  const string& method_name) {\n    const int batch_size = grads_image.dimension(0);\n    const int image_height = grads_image.dimension(1);\n    const int image_width = grads_image.dimension(2);\n\n    const int num_boxes = grads.dimension(0);\n    const int crop_height = grads.dimension(1);\n    const int crop_width = grads.dimension(2);\n    const int depth = grads.dimension(3);\n\n    grads_image.setZero();\n\n    auto CropAndResizeBackImgPerBox = [&](int64 start_box, int64 limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            continue;\n          }\n          const int top_y_index = floorf(in_y);\n          const int bottom_y_index = ceilf(in_y);\n          const float y_lerp = in_y - top_y_index;\n\n          for (int x = 0; x < crop_width; ++x) {\n            const float in_x = (crop_width > 1)\n                                   ? x1 * (image_width - 1) + x * width_scale\n                                   : 0.5 * (x1 + x2) * (image_width - 1);\n            if (in_x < 0 || in_x > image_width - 1) {\n              continue;\n            }\n\n            if (method_name == \"bilinear\") {\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float dtop = (1 - y_lerp) * grads(b, y, x, d);\n                grads_image(b_in, top_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dtop);\n                grads_image(b_in, top_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dtop);\n                const float dbottom = y_lerp * grads(b, y, x, d);\n                grads_image(b_in, bottom_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dbottom);\n                grads_image(b_in, bottom_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dbottom);\n              }\n            } else {  // method_name == \"nearest\"\n              for (int d = 0; d < depth; ++d) {\n                int closest_x_index = roundf(in_x);\n                int closest_y_index = roundf(in_y);\n                grads_image(b_in, closest_y_index, closest_x_index, d) +=\n                    static_cast<T>(grads(b, y, x, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    // Including calculation cost in the depth loop and pixel loop.\n    const double cost_per_pixel =\n        (method_name == \"bilinear\"\n             ? depth * (Eigen::TensorOpCost::AddCost<float>() * 7 +\n                        Eigen::TensorOpCost::MulCost<float>() * 6 +\n                        Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n                   Eigen::TensorOpCost::AddCost<float>() * 4\n             : depth * (Eigen::TensorOpCost::AddCost<float>() +\n                        Eigen::TensorOpCost::CastCost<T, float>()) +\n                   Eigen::TensorOpCost::AddCost<float>() * 3);\n\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizeBackImgPerBox);\n\n    return true;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,7 +15,7 @@\n \n     grads_image.setZero();\n \n-    auto CropAndResizeBackImgPerBox = [&](int start_box, int limit_box) {\n+    auto CropAndResizeBackImgPerBox = [&](int64 start_box, int64 limit_box) {\n       for (int b = start_box; b < limit_box; ++b) {\n         const float y1 = boxes(b, 0);\n         const float x1 = boxes(b, 1);",
        "diff_line_info": {
            "deleted_lines": [
                "    auto CropAndResizeBackImgPerBox = [&](int start_box, int limit_box) {"
            ],
            "added_lines": [
                "    auto CropAndResizeBackImgPerBox = [&](int64 start_box, int64 limit_box) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "bool operator()(const OpKernelContext* context,\n                  typename TTypes<T, 4>::ConstTensor image,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  const string& method_name, float extrapolation_value,\n                  typename TTypes<float, 4>::Tensor crops) {\n    const int batch_size = image.dimension(0);\n    const int image_height = image.dimension(1);\n    const int image_width = image.dimension(2);\n\n    const int num_boxes = crops.dimension(0);\n    const int crop_height = crops.dimension(1);\n    const int crop_width = crops.dimension(2);\n    const int depth = crops.dimension(3);\n\n    // Sharding across boxes.\n    auto CropAndResizePerBox = [&](int start_box, int limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            for (int x = 0; x < crop_width; ++x) {\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = extrapolation_value;\n              }\n            }\n            continue;\n          }\n          if (method_name == \"bilinear\") {\n            const int top_y_index = floorf(in_y);\n            const int bottom_y_index = ceilf(in_y);\n            const float y_lerp = in_y - top_y_index;\n\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float top_left(static_cast<float>(\n                    image(b_in, top_y_index, left_x_index, d)));\n                const float top_right(static_cast<float>(\n                    image(b_in, top_y_index, right_x_index, d)));\n                const float bottom_left(static_cast<float>(\n                    image(b_in, bottom_y_index, left_x_index, d)));\n                const float bottom_right(static_cast<float>(\n                    image(b_in, bottom_y_index, right_x_index, d)));\n                const float top = top_left + (top_right - top_left) * x_lerp;\n                const float bottom =\n                    bottom_left + (bottom_right - bottom_left) * x_lerp;\n                crops(b, y, x, d) = top + (bottom - top) * y_lerp;\n              }\n            }\n          } else {  // method == \"nearest\"\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int closest_x_index = roundf(in_x);\n              const int closest_y_index = roundf(in_y);\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = static_cast<float>(\n                    image(b_in, closest_y_index, closest_x_index, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    double cost_per_pixel =\n        depth * (Eigen::TensorOpCost::AddCost<float>() * 6 +\n                 Eigen::TensorOpCost::MulCost<float>() * 3 +\n                 Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n        (Eigen::TensorOpCost::AddCost<float>() * 2 +\n         Eigen::TensorOpCost::AddCost<float>() * 3);\n    if (method_name == \"nearest\") {\n      cost_per_pixel = depth * Eigen::TensorOpCost::CastCost<T, float>() +\n                       Eigen::TensorOpCost::AddCost<float>() * 4 +\n                       Eigen::TensorOpCost::MulCost<float>() * 4;\n    }\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizePerBox);\n\n    return true;\n  }",
        "func": "bool operator()(const OpKernelContext* context,\n                  typename TTypes<T, 4>::ConstTensor image,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  const string& method_name, float extrapolation_value,\n                  typename TTypes<float, 4>::Tensor crops) {\n    const int batch_size = image.dimension(0);\n    const int image_height = image.dimension(1);\n    const int image_width = image.dimension(2);\n\n    const int num_boxes = crops.dimension(0);\n    const int crop_height = crops.dimension(1);\n    const int crop_width = crops.dimension(2);\n    const int depth = crops.dimension(3);\n\n    // Sharding across boxes.\n    auto CropAndResizePerBox = [&](int64 start_box, int64 limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            for (int x = 0; x < crop_width; ++x) {\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = extrapolation_value;\n              }\n            }\n            continue;\n          }\n          if (method_name == \"bilinear\") {\n            const int top_y_index = floorf(in_y);\n            const int bottom_y_index = ceilf(in_y);\n            const float y_lerp = in_y - top_y_index;\n\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float top_left(static_cast<float>(\n                    image(b_in, top_y_index, left_x_index, d)));\n                const float top_right(static_cast<float>(\n                    image(b_in, top_y_index, right_x_index, d)));\n                const float bottom_left(static_cast<float>(\n                    image(b_in, bottom_y_index, left_x_index, d)));\n                const float bottom_right(static_cast<float>(\n                    image(b_in, bottom_y_index, right_x_index, d)));\n                const float top = top_left + (top_right - top_left) * x_lerp;\n                const float bottom =\n                    bottom_left + (bottom_right - bottom_left) * x_lerp;\n                crops(b, y, x, d) = top + (bottom - top) * y_lerp;\n              }\n            }\n          } else {  // method == \"nearest\"\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int closest_x_index = roundf(in_x);\n              const int closest_y_index = roundf(in_y);\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = static_cast<float>(\n                    image(b_in, closest_y_index, closest_x_index, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    double cost_per_pixel =\n        depth * (Eigen::TensorOpCost::AddCost<float>() * 6 +\n                 Eigen::TensorOpCost::MulCost<float>() * 3 +\n                 Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n        (Eigen::TensorOpCost::AddCost<float>() * 2 +\n         Eigen::TensorOpCost::AddCost<float>() * 3);\n    if (method_name == \"nearest\") {\n      cost_per_pixel = depth * Eigen::TensorOpCost::CastCost<T, float>() +\n                       Eigen::TensorOpCost::AddCost<float>() * 4 +\n                       Eigen::TensorOpCost::MulCost<float>() * 4;\n    }\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizePerBox);\n\n    return true;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,7 @@\n     const int depth = crops.dimension(3);\n \n     // Sharding across boxes.\n-    auto CropAndResizePerBox = [&](int start_box, int limit_box) {\n+    auto CropAndResizePerBox = [&](int64 start_box, int64 limit_box) {\n       for (int b = start_box; b < limit_box; ++b) {\n         const float y1 = boxes(b, 0);\n         const float x1 = boxes(b, 1);",
        "diff_line_info": {
            "deleted_lines": [
                "    auto CropAndResizePerBox = [&](int start_box, int limit_box) {"
            ],
            "added_lines": [
                "    auto CropAndResizePerBox = [&](int64 start_box, int64 limit_box) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/Compute",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "static EIGEN_ALWAYS_INLINE Status\n  Compute(OpKernelContext* context, bool sorted, int k,\n          const typename TTypes<T, 2>::ConstTensor& input, const int64 num_rows,\n          const int64 num_cols, typename TTypes<T, 2>::Tensor values,\n          typename TTypes<int, 2>::Tensor indices) {\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    // Special case for k == 1.\n    if (k == 1) {\n#ifdef EIGEN_HAS_INDEX_LIST\n      typename Eigen::IndexList<Eigen::type2index<1>> reduce_on_cols;\n      typename Eigen::IndexList<int, Eigen::type2index<1>> rows_by_one;\n      rows_by_one.set(0, num_rows);\n#else\n      Eigen::array<int, 1> reduce_on_cols = {1};\n      Eigen::array<int, 2> rows_by_one = {static_cast<int>(num_rows), 1};\n#endif\n\n      values.device(d) =\n          input.maximum(/*dims=*/reduce_on_cols).eval().reshape(rows_by_one);\n      // Get the indices of the maximum values.\n      for (int r = 0; r < num_rows; ++r) {\n        indices(r, 0) = 0;\n        for (int c = 0; c < num_cols; ++c) {\n          if (values(r, 0) == input(r, c)) {\n            indices(r, 0) = c;\n            break;\n          }\n        }\n        values(r, 0) = input(r, indices(r, 0));\n      }\n\n      return Status::OK();\n    }\n\n    auto SortIndices = [&](int start_batch, int limit_batch) {\n      for (int32 b = start_batch; b < limit_batch; ++b) {\n        const T* input_data = &input(b, 0);\n        const auto stable_comp = [input_data](const int32 a, const int32 b) {\n          if (input_data[b] < input_data[a]) {\n            return true;\n          } else if (input_data[b] > input_data[a]) {\n            return false;\n          } else {\n            return a < b;\n          }\n        };\n        const auto comp = [input_data](const int32 a, const int32 b) {\n          return input_data[b] < input_data[a];\n        };\n        // TODO(ebrevdo): For large k < num_cols, instead of using\n        // TopN, it may be faster to create a temporary vector of\n        // values 0..num_cols - 1 and then use std::partial_sort_copy\n        // of this into indices. Choosing the appropriate minimum k or\n        // ratio of k/num_cols will require some experimentation.\n        if (k == num_cols) {\n          auto* begin = &indices(b, 0);\n          auto* end = &indices(b, k);\n          // Set the initial array of indices 0 ... k - 1.\n          std::iota(begin, end, 0);\n          // We want an in-place sort, but we can cheat because we're sorting\n          // indices that started out sorted.  First, do a std::sort, which\n          // is notably faster than std::stable_sort.\n          std::sort(begin, end, comp);\n          // Then, for runs of adjacent elements that were equal, sort the\n          // indices in those runs in increasing order.\n          for (auto* run_begin = begin; run_begin != end;) {\n            auto* run_end = run_begin + 1;\n            if (run_end == end) break;\n            if (input_data[*run_begin] == input_data[*run_end]) {\n              while (++run_end != end) {\n                if (input_data[*run_begin] != input_data[*run_end]) break;\n              }\n              std::sort(run_begin, run_end);\n            }\n            run_begin = run_end;\n          }\n        } else {\n          // Use the TopN heap object to sort.\n          gtl::TopN<int32, decltype(stable_comp)> filter(k, stable_comp);\n          filter.reserve(num_cols);\n          for (int32 c = 0; c < num_cols; ++c) {\n            filter.push(c);\n          }\n\n          int32 i = 0;\n          if (sorted) {\n            std::unique_ptr<std::vector<int32>> top_k(filter.Extract());\n            for (auto top_k_it = top_k->begin(); top_k_it != top_k->end();\n                 ++top_k_it, ++i) {\n              indices(b, i) = *top_k_it;\n            }\n          } else {\n            for (auto top_k_it = filter.unsorted_begin();\n                 top_k_it != filter.unsorted_end(); ++top_k_it, ++i) {\n              indices(b, i) = *top_k_it;\n            }\n          }\n        }\n        // Now that the indices are sorted, copy the values over in\n        // sorted order.\n        std::transform(&indices(b, 0), &indices(b, k), &values(b, 0),\n                       [b, &input](const int32 loc) { return input(b, loc); });\n      }  // for (int32 b = ...\n    };\n\n    // Guesstimate of cost; 4*N*log(K) where N == num_cols.\n    // If K == N, assume the cost is N*log(K + 1).\n    const double cmp_cost = 3 * Eigen::TensorOpCost::AddCost<int32>() +\n                            Eigen::TensorOpCost::AddCost<T>();\n    const double base_cost =\n        cmp_cost *\n        static_cast<double>(num_cols *\n                            Eigen::numext::log2(static_cast<float>(k + 1)));\n    const double sort_cost = (k == num_cols) ? base_cost : 4 * base_cost;\n    const double copy_cost = 2 * k * Eigen::TensorOpCost::AddCost<T>();\n    const double total_cost = sort_cost + copy_cost;\n    const int64 final_cost = (total_cost >= static_cast<double>(kint64max))\n                                 ? kint64max\n                                 : static_cast<int64>(total_cost);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_rows,\n          final_cost, SortIndices);\n\n    return Status::OK();\n  }",
        "func": "static EIGEN_ALWAYS_INLINE Status\n  Compute(OpKernelContext* context, bool sorted, int k,\n          const typename TTypes<T, 2>::ConstTensor& input, const int64 num_rows,\n          const int64 num_cols, typename TTypes<T, 2>::Tensor values,\n          typename TTypes<int, 2>::Tensor indices) {\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    // Special case for k == 1.\n    if (k == 1) {\n#ifdef EIGEN_HAS_INDEX_LIST\n      typename Eigen::IndexList<Eigen::type2index<1>> reduce_on_cols;\n      typename Eigen::IndexList<int, Eigen::type2index<1>> rows_by_one;\n      rows_by_one.set(0, num_rows);\n#else\n      Eigen::array<int, 1> reduce_on_cols = {1};\n      Eigen::array<int, 2> rows_by_one = {static_cast<int>(num_rows), 1};\n#endif\n\n      values.device(d) =\n          input.maximum(/*dims=*/reduce_on_cols).eval().reshape(rows_by_one);\n      // Get the indices of the maximum values.\n      for (int r = 0; r < num_rows; ++r) {\n        indices(r, 0) = 0;\n        for (int c = 0; c < num_cols; ++c) {\n          if (values(r, 0) == input(r, c)) {\n            indices(r, 0) = c;\n            break;\n          }\n        }\n        values(r, 0) = input(r, indices(r, 0));\n      }\n\n      return Status::OK();\n    }\n\n    auto SortIndices = [&](int64 start_batch, int64 limit_batch) {\n      for (int32 b = start_batch; b < limit_batch; ++b) {\n        const T* input_data = &input(b, 0);\n        const auto stable_comp = [input_data](const int32 a, const int32 b) {\n          if (input_data[b] < input_data[a]) {\n            return true;\n          } else if (input_data[b] > input_data[a]) {\n            return false;\n          } else {\n            return a < b;\n          }\n        };\n        const auto comp = [input_data](const int32 a, const int32 b) {\n          return input_data[b] < input_data[a];\n        };\n        // TODO(ebrevdo): For large k < num_cols, instead of using\n        // TopN, it may be faster to create a temporary vector of\n        // values 0..num_cols - 1 and then use std::partial_sort_copy\n        // of this into indices. Choosing the appropriate minimum k or\n        // ratio of k/num_cols will require some experimentation.\n        if (k == num_cols) {\n          auto* begin = &indices(b, 0);\n          auto* end = &indices(b, k);\n          // Set the initial array of indices 0 ... k - 1.\n          std::iota(begin, end, 0);\n          // We want an in-place sort, but we can cheat because we're sorting\n          // indices that started out sorted.  First, do a std::sort, which\n          // is notably faster than std::stable_sort.\n          std::sort(begin, end, comp);\n          // Then, for runs of adjacent elements that were equal, sort the\n          // indices in those runs in increasing order.\n          for (auto* run_begin = begin; run_begin != end;) {\n            auto* run_end = run_begin + 1;\n            if (run_end == end) break;\n            if (input_data[*run_begin] == input_data[*run_end]) {\n              while (++run_end != end) {\n                if (input_data[*run_begin] != input_data[*run_end]) break;\n              }\n              std::sort(run_begin, run_end);\n            }\n            run_begin = run_end;\n          }\n        } else {\n          // Use the TopN heap object to sort.\n          gtl::TopN<int32, decltype(stable_comp)> filter(k, stable_comp);\n          filter.reserve(num_cols);\n          for (int32 c = 0; c < num_cols; ++c) {\n            filter.push(c);\n          }\n\n          int32 i = 0;\n          if (sorted) {\n            std::unique_ptr<std::vector<int32>> top_k(filter.Extract());\n            for (auto top_k_it = top_k->begin(); top_k_it != top_k->end();\n                 ++top_k_it, ++i) {\n              indices(b, i) = *top_k_it;\n            }\n          } else {\n            for (auto top_k_it = filter.unsorted_begin();\n                 top_k_it != filter.unsorted_end(); ++top_k_it, ++i) {\n              indices(b, i) = *top_k_it;\n            }\n          }\n        }\n        // Now that the indices are sorted, copy the values over in\n        // sorted order.\n        std::transform(&indices(b, 0), &indices(b, k), &values(b, 0),\n                       [b, &input](const int32 loc) { return input(b, loc); });\n      }  // for (int32 b = ...\n    };\n\n    // Guesstimate of cost; 4*N*log(K) where N == num_cols.\n    // If K == N, assume the cost is N*log(K + 1).\n    const double cmp_cost = 3 * Eigen::TensorOpCost::AddCost<int32>() +\n                            Eigen::TensorOpCost::AddCost<T>();\n    const double base_cost =\n        cmp_cost *\n        static_cast<double>(num_cols *\n                            Eigen::numext::log2(static_cast<float>(k + 1)));\n    const double sort_cost = (k == num_cols) ? base_cost : 4 * base_cost;\n    const double copy_cost = 2 * k * Eigen::TensorOpCost::AddCost<T>();\n    const double total_cost = sort_cost + copy_cost;\n    const int64 final_cost = (total_cost >= static_cast<double>(kint64max))\n                                 ? kint64max\n                                 : static_cast<int64>(total_cost);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_rows,\n          final_cost, SortIndices);\n\n    return Status::OK();\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -33,7 +33,7 @@\n       return Status::OK();\n     }\n \n-    auto SortIndices = [&](int start_batch, int limit_batch) {\n+    auto SortIndices = [&](int64 start_batch, int64 limit_batch) {\n       for (int32 b = start_batch; b < limit_batch; ++b) {\n         const T* input_data = &input(b, 0);\n         const auto stable_comp = [input_data](const int32 a, const int32 b) {",
        "diff_line_info": {
            "deleted_lines": [
                "    auto SortIndices = [&](int start_batch, int limit_batch) {"
            ],
            "added_lines": [
                "    auto SortIndices = [&](int64 start_batch, int64 limit_batch) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/Launch",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "static void Launch(OpKernelContext* context, const Tensor& in_x,\n                     const Tensor& in_y, bool adjoint, bool lower,\n                     const MatMulBCast& bcast, Tensor* out) {\n    // Number of banded matrix triangular solves i.e. size of the batch.\n    const int64 batch_size = bcast.output_batch_size();\n    const int64 cost_per_unit =\n        in_x.dim_size(1) * in_x.dim_size(2) * in_y.dim_size(2);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    using Matrix =\n        Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n    using ConstMatrixMap = Eigen::Map<const Matrix>;\n    using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n    // Check diagonal before doing any solves. This is the first row in the\n    // lower case and else is the last row.\n    auto matrix = ConstMatrixMap(in_x.flat<Scalar>().data(), in_x.dim_size(1),\n                                 in_x.dim_size(2));\n    RealScalar min_abs_pivot;\n    if (lower) {\n      min_abs_pivot = matrix.row(0).cwiseAbs().minCoeff();\n    } else {\n      min_abs_pivot = matrix.row(in_x.dim_size(1) - 1).cwiseAbs().minCoeff();\n    }\n    OP_REQUIRES(context, min_abs_pivot > RealScalar(0),\n                errors::InvalidArgument(\"Input matrix is not invertible.\"));\n\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          cost_per_unit,\n          [&in_x, &in_y, adjoint, lower, &bcast, out](int start, int limit) {\n            SequentialBandedTriangularSolveKernel<Scalar>::Run(\n                in_x, in_y, lower, adjoint, bcast, out, start, limit);\n          });\n  }",
        "func": "static void Launch(OpKernelContext* context, const Tensor& in_x,\n                     const Tensor& in_y, bool adjoint, bool lower,\n                     const MatMulBCast& bcast, Tensor* out) {\n    // Number of banded matrix triangular solves i.e. size of the batch.\n    const int64 batch_size = bcast.output_batch_size();\n    const int64 cost_per_unit =\n        in_x.dim_size(1) * in_x.dim_size(2) * in_y.dim_size(2);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    using Matrix =\n        Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n    using ConstMatrixMap = Eigen::Map<const Matrix>;\n    using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n    // Check diagonal before doing any solves. This is the first row in the\n    // lower case and else is the last row.\n    auto matrix = ConstMatrixMap(in_x.flat<Scalar>().data(), in_x.dim_size(1),\n                                 in_x.dim_size(2));\n    RealScalar min_abs_pivot;\n    if (lower) {\n      min_abs_pivot = matrix.row(0).cwiseAbs().minCoeff();\n    } else {\n      min_abs_pivot = matrix.row(in_x.dim_size(1) - 1).cwiseAbs().minCoeff();\n    }\n    OP_REQUIRES(context, min_abs_pivot > RealScalar(0),\n                errors::InvalidArgument(\"Input matrix is not invertible.\"));\n\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          cost_per_unit,\n          [&in_x, &in_y, adjoint, lower, &bcast, out](int64 start,\n                                                      int64 limit) {\n            SequentialBandedTriangularSolveKernel<Scalar>::Run(\n                in_x, in_y, lower, adjoint, bcast, out, start, limit);\n          });\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,7 +26,8 @@\n \n     Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n           cost_per_unit,\n-          [&in_x, &in_y, adjoint, lower, &bcast, out](int start, int limit) {\n+          [&in_x, &in_y, adjoint, lower, &bcast, out](int64 start,\n+                                                      int64 limit) {\n             SequentialBandedTriangularSolveKernel<Scalar>::Run(\n                 in_x, in_y, lower, adjoint, bcast, out, start, limit);\n           });",
        "diff_line_info": {
            "deleted_lines": [
                "          [&in_x, &in_y, adjoint, lower, &bcast, out](int start, int limit) {"
            ],
            "added_lines": [
                "          [&in_x, &in_y, adjoint, lower, &bcast, out](int64 start,",
                "                                                      int64 limit) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* context, const Tensor& input_tensor,\n                  Tensor& output_tensor, int n, bool reverse) {\n    const T* input = input_tensor.flat<T>().data();\n    T* output = output_tensor.flat<T>().data();\n\n    // Assume input_shape is [d1,d2,...dk], and output_shape is [d1,d2...dk-1],\n    // then num_rows = d1*d2...dk-1, last_dim = dk.\n    const int num_rows = output_tensor.NumElements();\n    const int last_dim = input_tensor.dim_size(input_tensor.dims() - 1);\n\n    // Allocate each row to different shard.\n    auto SubNthElement = [&, input, output, last_dim, n](int start, int limit) {\n      // std::nth_element would rearrange the array, so we need a new buffer.\n      std::vector<T> buf(last_dim);\n\n      for (int b = start; b < limit; ++b) {\n        // Copy from one row of elements to buffer\n        const T* input_start = input + b * last_dim;\n        const T* input_end = input + (b + 1) * last_dim;\n        std::copy(input_start, input_end, buf.begin());\n\n        std::nth_element(buf.begin(), buf.begin() + n, buf.end());\n        // The element placed in the nth position is exactly the element that\n        // would occur in this position if the range was fully sorted.\n        output[b] = buf[n];\n      }\n    };\n\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    // The average time complexity of partition-based nth_element (BFPRT) is\n    // O(n), although the worst time complexity could be O(n^2). Here, 20 is a\n    // empirical factor of cost_per_unit.\n    Shard(worker_threads.num_threads, worker_threads.workers, num_rows,\n          20 * last_dim, SubNthElement);\n  }",
        "func": "void operator()(OpKernelContext* context, const Tensor& input_tensor,\n                  Tensor& output_tensor, int n, bool reverse) {\n    const T* input = input_tensor.flat<T>().data();\n    T* output = output_tensor.flat<T>().data();\n\n    // Assume input_shape is [d1,d2,...dk], and output_shape is [d1,d2...dk-1],\n    // then num_rows = d1*d2...dk-1, last_dim = dk.\n    const int num_rows = output_tensor.NumElements();\n    const int last_dim = input_tensor.dim_size(input_tensor.dims() - 1);\n\n    // Allocate each row to different shard.\n    auto SubNthElement = [&, input, output, last_dim, n](int64 start,\n                                                         int64 limit) {\n      // std::nth_element would rearrange the array, so we need a new buffer.\n      std::vector<T> buf(last_dim);\n\n      for (int b = start; b < limit; ++b) {\n        // Copy from one row of elements to buffer\n        const T* input_start = input + b * last_dim;\n        const T* input_end = input + (b + 1) * last_dim;\n        std::copy(input_start, input_end, buf.begin());\n\n        std::nth_element(buf.begin(), buf.begin() + n, buf.end());\n        // The element placed in the nth position is exactly the element that\n        // would occur in this position if the range was fully sorted.\n        output[b] = buf[n];\n      }\n    };\n\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    // The average time complexity of partition-based nth_element (BFPRT) is\n    // O(n), although the worst time complexity could be O(n^2). Here, 20 is a\n    // empirical factor of cost_per_unit.\n    Shard(worker_threads.num_threads, worker_threads.workers, num_rows,\n          20 * last_dim, SubNthElement);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,8 @@\n     const int last_dim = input_tensor.dim_size(input_tensor.dims() - 1);\n \n     // Allocate each row to different shard.\n-    auto SubNthElement = [&, input, output, last_dim, n](int start, int limit) {\n+    auto SubNthElement = [&, input, output, last_dim, n](int64 start,\n+                                                         int64 limit) {\n       // std::nth_element would rearrange the array, so we need a new buffer.\n       std::vector<T> buf(last_dim);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    auto SubNthElement = [&, input, output, last_dim, n](int start, int limit) {"
            ],
            "added_lines": [
                "    auto SubNthElement = [&, input, output, last_dim, n](int64 start,",
                "                                                         int64 limit) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/Compute",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void Compute(OpKernelContext* const context) override {\n    core::RefCountPtr<BoostedTreesEnsembleResource> resource;\n    // Get the resource.\n    OP_REQUIRES_OK(context, LookupResource(context, HandleFromInput(context, 0),\n                                           &resource));\n\n    // Get the inputs.\n    OpInputList bucketized_features_list;\n    OP_REQUIRES_OK(context, context->input_list(\"bucketized_features\",\n                                                &bucketized_features_list));\n    std::vector<tensorflow::TTypes<int32>::ConstMatrix> bucketized_features;\n    bucketized_features.reserve(bucketized_features_list.size());\n    ConvertVectorsToMatrices(bucketized_features_list, bucketized_features);\n    const int batch_size = bucketized_features[0].dimension(0);\n\n    // We need to get the feature ids used for splitting and the logits after\n    // each split. We will use these to calculate the changes in the prediction\n    // (contributions) for an arbitrary activation function (done in Python) and\n    // attribute them to the associated feature ids. We will store these in\n    // a proto below.\n    Tensor* output_debug_info_t = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\"examples_debug_outputs_serialized\",\n                                          {batch_size}, &output_debug_info_t));\n    // Will contain serialized protos, per example.\n    auto output_debug_info = output_debug_info_t->flat<tstring>();\n    const int32 last_tree = resource->num_trees() - 1;\n\n    // For each given example, traverse through all trees keeping track of the\n    // features used to split and the associated logits at each point along the\n    // path. Note: feature_ids has one less value than logits_path because the\n    // first value of each logit path will be the bias.\n    auto do_work = [&resource, &bucketized_features, &output_debug_info,\n                    last_tree](int32 start, int32 end) {\n      for (int32 i = start; i < end; ++i) {\n        // Proto to store debug outputs, per example.\n        boosted_trees::DebugOutput example_debug_info;\n        // Initial bias prediction. E.g., prediction based off training mean.\n        const auto& tree_logits = resource->node_value(0, 0);\n        DCHECK_EQ(tree_logits.size(), 1);\n        float tree_logit = resource->GetTreeWeight(0) * tree_logits[0];\n        example_debug_info.add_logits_path(tree_logit);\n        int32 node_id = 0;\n        int32 tree_id = 0;\n        int32 feature_id;\n        float past_trees_logit = 0;  // Sum of leaf logits from prior trees.\n        // Go through each tree and populate proto.\n        while (tree_id <= last_tree) {\n          if (resource->is_leaf(tree_id, node_id)) {  // Move onto other trees.\n            // Accumulate tree_logits only if the leaf is non-root, but do so\n            // for bias tree.\n            if (tree_id == 0 || node_id > 0) {\n              past_trees_logit += tree_logit;\n            }\n            ++tree_id;\n            node_id = 0;\n          } else {  // Add to proto.\n            // Feature id used to split.\n            feature_id = resource->feature_id(tree_id, node_id);\n            example_debug_info.add_feature_ids(feature_id);\n            // Get logit after split.\n            node_id =\n                resource->next_node(tree_id, node_id, i, bucketized_features);\n            const auto& tree_logits = resource->node_value(tree_id, node_id);\n            DCHECK_EQ(tree_logits.size(), 1);\n            tree_logit = resource->GetTreeWeight(tree_id) * tree_logits[0];\n            // Output logit incorporates sum of leaf logits from prior trees.\n            example_debug_info.add_logits_path(tree_logit + past_trees_logit);\n          }\n        }\n        // Set output as serialized proto containing debug info.\n        string serialized = example_debug_info.SerializeAsString();\n        output_debug_info(i) = serialized;\n      }\n    };\n\n    // 10 is the magic number. The actual number might depend on (the number of\n    // layers in the trees) and (cpu cycles spent on each layer), but this\n    // value would work for many cases. May be tuned later.\n    const int64 cost = (last_tree + 1) * 10;\n    thread::ThreadPool* const worker_threads =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    Shard(worker_threads->NumThreads(), worker_threads, batch_size,\n          /*cost_per_unit=*/cost, do_work);\n  }",
        "func": "void Compute(OpKernelContext* const context) override {\n    core::RefCountPtr<BoostedTreesEnsembleResource> resource;\n    // Get the resource.\n    OP_REQUIRES_OK(context, LookupResource(context, HandleFromInput(context, 0),\n                                           &resource));\n\n    // Get the inputs.\n    OpInputList bucketized_features_list;\n    OP_REQUIRES_OK(context, context->input_list(\"bucketized_features\",\n                                                &bucketized_features_list));\n    std::vector<tensorflow::TTypes<int32>::ConstMatrix> bucketized_features;\n    bucketized_features.reserve(bucketized_features_list.size());\n    ConvertVectorsToMatrices(bucketized_features_list, bucketized_features);\n    const int batch_size = bucketized_features[0].dimension(0);\n\n    // We need to get the feature ids used for splitting and the logits after\n    // each split. We will use these to calculate the changes in the prediction\n    // (contributions) for an arbitrary activation function (done in Python) and\n    // attribute them to the associated feature ids. We will store these in\n    // a proto below.\n    Tensor* output_debug_info_t = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\"examples_debug_outputs_serialized\",\n                                          {batch_size}, &output_debug_info_t));\n    // Will contain serialized protos, per example.\n    auto output_debug_info = output_debug_info_t->flat<tstring>();\n    const int32 last_tree = resource->num_trees() - 1;\n\n    // For each given example, traverse through all trees keeping track of the\n    // features used to split and the associated logits at each point along the\n    // path. Note: feature_ids has one less value than logits_path because the\n    // first value of each logit path will be the bias.\n    auto do_work = [&resource, &bucketized_features, &output_debug_info,\n                    last_tree](int64 start, int64 end) {\n      for (int32 i = start; i < end; ++i) {\n        // Proto to store debug outputs, per example.\n        boosted_trees::DebugOutput example_debug_info;\n        // Initial bias prediction. E.g., prediction based off training mean.\n        const auto& tree_logits = resource->node_value(0, 0);\n        DCHECK_EQ(tree_logits.size(), 1);\n        float tree_logit = resource->GetTreeWeight(0) * tree_logits[0];\n        example_debug_info.add_logits_path(tree_logit);\n        int32 node_id = 0;\n        int32 tree_id = 0;\n        int32 feature_id;\n        float past_trees_logit = 0;  // Sum of leaf logits from prior trees.\n        // Go through each tree and populate proto.\n        while (tree_id <= last_tree) {\n          if (resource->is_leaf(tree_id, node_id)) {  // Move onto other trees.\n            // Accumulate tree_logits only if the leaf is non-root, but do so\n            // for bias tree.\n            if (tree_id == 0 || node_id > 0) {\n              past_trees_logit += tree_logit;\n            }\n            ++tree_id;\n            node_id = 0;\n          } else {  // Add to proto.\n            // Feature id used to split.\n            feature_id = resource->feature_id(tree_id, node_id);\n            example_debug_info.add_feature_ids(feature_id);\n            // Get logit after split.\n            node_id =\n                resource->next_node(tree_id, node_id, i, bucketized_features);\n            const auto& tree_logits = resource->node_value(tree_id, node_id);\n            DCHECK_EQ(tree_logits.size(), 1);\n            tree_logit = resource->GetTreeWeight(tree_id) * tree_logits[0];\n            // Output logit incorporates sum of leaf logits from prior trees.\n            example_debug_info.add_logits_path(tree_logit + past_trees_logit);\n          }\n        }\n        // Set output as serialized proto containing debug info.\n        string serialized = example_debug_info.SerializeAsString();\n        output_debug_info(i) = serialized;\n      }\n    };\n\n    // 10 is the magic number. The actual number might depend on (the number of\n    // layers in the trees) and (cpu cycles spent on each layer), but this\n    // value would work for many cases. May be tuned later.\n    const int64 cost = (last_tree + 1) * 10;\n    thread::ThreadPool* const worker_threads =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    Shard(worker_threads->NumThreads(), worker_threads, batch_size,\n          /*cost_per_unit=*/cost, do_work);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -31,7 +31,7 @@\n     // path. Note: feature_ids has one less value than logits_path because the\n     // first value of each logit path will be the bias.\n     auto do_work = [&resource, &bucketized_features, &output_debug_info,\n-                    last_tree](int32 start, int32 end) {\n+                    last_tree](int64 start, int64 end) {\n       for (int32 i = start; i < end; ++i) {\n         // Proto to store debug outputs, per example.\n         boosted_trees::DebugOutput example_debug_info;",
        "diff_line_info": {
            "deleted_lines": [
                "                    last_tree](int32 start, int32 end) {"
            ],
            "added_lines": [
                "                    last_tree](int64 start, int64 end) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* ctx, const CPUDevice& d, const T* rate_flat,\n                  int num_rate, int num_samples,\n                  const random::PhiloxRandom& rng, U* samples_flat) {\n    // Two different algorithms are employed, depending on the size of\n    // rate.\n    // If rate < 10, we use an algorithm attributed to Knuth:\n    // Seminumerical Algorithms. Art of Computer Programming, Volume 2.\n    //\n    // This algorithm runs in O(rate) time, and will require O(rate)\n    // uniform variates.\n    //\n    // If rate >= 10 we use a transformation-rejection algorithm from\n    // pairs of uniform random variables due to Hormann.\n    // http://www.sciencedirect.com/science/article/pii/0167668793909974\n    //\n    // The algorithm has an acceptance rate of ~89% for the smallest rate\n    // (~10),\n    // and higher accept rates for higher rate, so runtime is\n    // O(NumRate * NumSamples * k) with k ~ 1 / 0.89.\n    //\n    // We partition work first across rates then across\n    // samples-per-rate to\n    // avoid a couple flops which can be done on a per-rate basis.\n\n    typedef random::UniformDistribution<random::PhiloxRandom, CT> Uniform;\n\n    auto DoWork = [num_samples, num_rate, &rng, samples_flat, rate_flat](\n                      int start_output, int limit_output) {\n      // Capturing \"rng\" by value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"rng\" by reference and explicitly do a copy assignment.\n\n      Uniform uniform;\n      typename Uniform::ResultType uniform_result;\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           /* output_idx incremented within inner loop below */) {\n        const int64 rate_idx = output_idx / num_samples;\n\n        // Several calculations can be done on a per-rate basis.\n        const CT rate = CT(rate_flat[rate_idx]);\n\n        auto samples_rate_output = samples_flat + rate_idx;\n\n        if (rate < CT(10)) {\n          // Knuth's algorithm for generating Poisson random variates.\n          // Given a Poisson process, the time between events is exponentially\n          // distributed. If we have a Poisson process with rate lambda, then,\n          // the time between events is distributed Exp(lambda). If X ~\n          // Uniform(0, 1), then Y ~ Exp(lambda), where Y = -log(X) / lambda.\n          // Thus to simulate a Poisson draw, we can draw X_i ~ Exp(lambda),\n          // and N ~ Poisson(lambda), where N is the least number such that\n          // \\sum_i^N X_i > 1.\n          const CT exp_neg_rate = Eigen::numext::exp(-rate);\n\n          // Compute the rest of the samples for the current rate value.\n          for (int64 sample_idx = output_idx % num_samples;\n               sample_idx < num_samples && output_idx < limit_output;\n               sample_idx++, output_idx++) {\n            random::PhiloxRandom gen = rng;\n            gen.Skip(kReservedSamplesPerOutput * output_idx);\n            int16 uniform_remaining = 0;\n\n            CT prod = 1;\n            CT x = 0;\n\n            // Keep trying until we surpass e^(-rate). This will take\n            // expected time proportional to rate.\n            while (true) {\n              UNIFORM(u);\n              prod = prod * u;\n              if (prod <= exp_neg_rate &&\n                  x <= CT(Eigen::NumTraits<U>::highest())) {\n                samples_rate_output[sample_idx * num_rate] = U(x);\n                break;\n              }\n              x += 1;\n            }\n          }\n          continue;\n        }\n        // Transformed rejection due to Hormann.\n        //\n        // Given a CDF F(x), and G(x), a dominating distribution chosen such\n        // that it is close to the inverse CDF F^-1(x), compute the following\n        // steps:\n        //\n        // 1) Generate U and V, two independent random variates. Set U = U - 0.5\n        // (this step isn't strictly necessary, but is done to make some\n        // calculations symmetric and convenient. Henceforth, G is defined on\n        // [-0.5, 0.5]).\n        //\n        // 2) If V <= alpha * F'(G(U)) * G'(U), return floor(G(U)), else return\n        // to step 1. alpha is the acceptance probability of the rejection\n        // algorithm.\n        //\n        // For more details on transformed rejection, see:\n        // http://citeseer.ist.psu.edu/viewdoc/citations;jsessionid=1BEB35946CC807879F55D42512E5490C?doi=10.1.1.48.3054.\n        //\n        // The dominating distribution in this case:\n        //\n        // G(u) = (2 * a / (2 - |u|) + b) * u + c\n\n        using Eigen::numext::log;\n        const CT log_rate = log(rate);\n\n        // Constants used to define the dominating distribution. Names taken\n        // from Hormann's paper. Constants were chosen to define the tightest\n        // G(u) for the inverse Poisson CDF.\n        const CT b = CT(0.931) + CT(2.53) * Eigen::numext::sqrt(rate);\n        const CT a = CT(-0.059) + CT(0.02483) * b;\n\n        // This is the inverse acceptance rate. At a minimum (when rate = 10),\n        // this corresponds to ~75% acceptance. As the rate becomes larger, this\n        // approaches ~89%.\n        const CT inv_alpha = CT(1.1239) + CT(1.1328) / (b - CT(3.4));\n\n        // Compute the rest of the samples for the current rate value.\n        for (int64 sample_idx = output_idx % num_samples;\n             sample_idx < num_samples && output_idx < limit_output;\n             sample_idx++, output_idx++) {\n          random::PhiloxRandom gen = rng;\n          gen.Skip(kReservedSamplesPerOutput * output_idx);\n          int16 uniform_remaining = 0;\n\n          while (true) {\n            UNIFORM(u);\n            u -= CT(0.5);\n            UNIFORM(v);\n\n            CT u_shifted = CT(0.5) - Eigen::numext::abs(u);\n            CT k = Eigen::numext::floor((CT(2) * a / u_shifted + b) * u + rate +\n                                        CT(0.43));\n\n            if (k > CT(Eigen::NumTraits<U>::highest())) {\n              // retry in case of overflow.\n              continue;\n            }\n\n            // When alpha * f(G(U)) * G'(U) is close to 1, it is possible to\n            // find a rectangle (-u_r, u_r) x (0, v_r) under the curve, such\n            // that if v <= v_r and |u| <= u_r, then we can accept.\n            // Here v_r = 0.9227 - 3.6224 / (b - 2) and u_r = 0.43.\n            if (u_shifted >= CT(0.07) &&\n                v <= CT(0.9277) - CT(3.6224) / (b - CT(2))) {\n              samples_rate_output[sample_idx * num_rate] = U(k);\n              break;\n            }\n\n            if (k < 0 || (u_shifted < CT(0.013) && v > u_shifted)) {\n              continue;\n            }\n\n            // The expression below is equivalent to the computation of step 2)\n            // in transformed rejection (v <= alpha * F'(G(u)) * G'(u)).\n            CT s = log(v * inv_alpha / (a / (u_shifted * u_shifted) + b));\n            CT t = -rate + k * log_rate - Eigen::numext::lgamma(k + 1);\n            if (s <= t) {\n              samples_rate_output[sample_idx * num_rate] = U(k);\n              break;\n            }\n          }\n        }\n      }\n    };\n\n    // This will depend on rate.\n    // For rate < 10, on average, O(rate) calls to uniform are\n    // needed, with that\n    // many multiplies. ~10 uniform calls on average with ~25 cost op calls.\n    //\n    // Very roughly, for rate >= 10, the single call to log + call to\n    // lgamma\n    // occur for ~60 percent of samples.\n    // 2 x 100 (64-bit cycles per log) * 0.62 = ~124\n    // Additionally, there are ~10 other ops (+, *, /, ...) at 3-6 cycles each:\n    // 40 * .62  = ~25.\n    //\n    // Finally, there are several other ops that are done every loop along with\n    // 2 uniform generations along with 5 other ops at 3-6 cycles each.\n    // ~15 / .89 = ~16\n    //\n    // In total this should be ~165 + 2 * Uniform::kElementCost.\n    // We assume that half the tensor has rate < 10, so on average 6\n    // uniform's\n    // will be needed. We will upper bound the other op cost by the one for\n    // rate > 10.\n    static const int kElementCost = 165 + 6 * Uniform::kElementCost +\n                                    6 * random::PhiloxRandom::kElementCost;\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers,\n          num_rate * num_samples, kElementCost, DoWork);\n  }",
        "func": "void operator()(OpKernelContext* ctx, const CPUDevice& d, const T* rate_flat,\n                  int num_rate, int num_samples,\n                  const random::PhiloxRandom& rng, U* samples_flat) {\n    // Two different algorithms are employed, depending on the size of\n    // rate.\n    // If rate < 10, we use an algorithm attributed to Knuth:\n    // Seminumerical Algorithms. Art of Computer Programming, Volume 2.\n    //\n    // This algorithm runs in O(rate) time, and will require O(rate)\n    // uniform variates.\n    //\n    // If rate >= 10 we use a transformation-rejection algorithm from\n    // pairs of uniform random variables due to Hormann.\n    // http://www.sciencedirect.com/science/article/pii/0167668793909974\n    //\n    // The algorithm has an acceptance rate of ~89% for the smallest rate\n    // (~10),\n    // and higher accept rates for higher rate, so runtime is\n    // O(NumRate * NumSamples * k) with k ~ 1 / 0.89.\n    //\n    // We partition work first across rates then across\n    // samples-per-rate to\n    // avoid a couple flops which can be done on a per-rate basis.\n\n    typedef random::UniformDistribution<random::PhiloxRandom, CT> Uniform;\n\n    auto DoWork = [num_samples, num_rate, &rng, samples_flat, rate_flat](\n                      int64 start_output, int64 limit_output) {\n      // Capturing \"rng\" by value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"rng\" by reference and explicitly do a copy assignment.\n\n      Uniform uniform;\n      typename Uniform::ResultType uniform_result;\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           /* output_idx incremented within inner loop below */) {\n        const int64 rate_idx = output_idx / num_samples;\n\n        // Several calculations can be done on a per-rate basis.\n        const CT rate = CT(rate_flat[rate_idx]);\n\n        auto samples_rate_output = samples_flat + rate_idx;\n\n        if (rate < CT(10)) {\n          // Knuth's algorithm for generating Poisson random variates.\n          // Given a Poisson process, the time between events is exponentially\n          // distributed. If we have a Poisson process with rate lambda, then,\n          // the time between events is distributed Exp(lambda). If X ~\n          // Uniform(0, 1), then Y ~ Exp(lambda), where Y = -log(X) / lambda.\n          // Thus to simulate a Poisson draw, we can draw X_i ~ Exp(lambda),\n          // and N ~ Poisson(lambda), where N is the least number such that\n          // \\sum_i^N X_i > 1.\n          const CT exp_neg_rate = Eigen::numext::exp(-rate);\n\n          // Compute the rest of the samples for the current rate value.\n          for (int64 sample_idx = output_idx % num_samples;\n               sample_idx < num_samples && output_idx < limit_output;\n               sample_idx++, output_idx++) {\n            random::PhiloxRandom gen = rng;\n            gen.Skip(kReservedSamplesPerOutput * output_idx);\n            int16 uniform_remaining = 0;\n\n            CT prod = 1;\n            CT x = 0;\n\n            // Keep trying until we surpass e^(-rate). This will take\n            // expected time proportional to rate.\n            while (true) {\n              UNIFORM(u);\n              prod = prod * u;\n              if (prod <= exp_neg_rate &&\n                  x <= CT(Eigen::NumTraits<U>::highest())) {\n                samples_rate_output[sample_idx * num_rate] = U(x);\n                break;\n              }\n              x += 1;\n            }\n          }\n          continue;\n        }\n        // Transformed rejection due to Hormann.\n        //\n        // Given a CDF F(x), and G(x), a dominating distribution chosen such\n        // that it is close to the inverse CDF F^-1(x), compute the following\n        // steps:\n        //\n        // 1) Generate U and V, two independent random variates. Set U = U - 0.5\n        // (this step isn't strictly necessary, but is done to make some\n        // calculations symmetric and convenient. Henceforth, G is defined on\n        // [-0.5, 0.5]).\n        //\n        // 2) If V <= alpha * F'(G(U)) * G'(U), return floor(G(U)), else return\n        // to step 1. alpha is the acceptance probability of the rejection\n        // algorithm.\n        //\n        // For more details on transformed rejection, see:\n        // http://citeseer.ist.psu.edu/viewdoc/citations;jsessionid=1BEB35946CC807879F55D42512E5490C?doi=10.1.1.48.3054.\n        //\n        // The dominating distribution in this case:\n        //\n        // G(u) = (2 * a / (2 - |u|) + b) * u + c\n\n        using Eigen::numext::log;\n        const CT log_rate = log(rate);\n\n        // Constants used to define the dominating distribution. Names taken\n        // from Hormann's paper. Constants were chosen to define the tightest\n        // G(u) for the inverse Poisson CDF.\n        const CT b = CT(0.931) + CT(2.53) * Eigen::numext::sqrt(rate);\n        const CT a = CT(-0.059) + CT(0.02483) * b;\n\n        // This is the inverse acceptance rate. At a minimum (when rate = 10),\n        // this corresponds to ~75% acceptance. As the rate becomes larger, this\n        // approaches ~89%.\n        const CT inv_alpha = CT(1.1239) + CT(1.1328) / (b - CT(3.4));\n\n        // Compute the rest of the samples for the current rate value.\n        for (int64 sample_idx = output_idx % num_samples;\n             sample_idx < num_samples && output_idx < limit_output;\n             sample_idx++, output_idx++) {\n          random::PhiloxRandom gen = rng;\n          gen.Skip(kReservedSamplesPerOutput * output_idx);\n          int16 uniform_remaining = 0;\n\n          while (true) {\n            UNIFORM(u);\n            u -= CT(0.5);\n            UNIFORM(v);\n\n            CT u_shifted = CT(0.5) - Eigen::numext::abs(u);\n            CT k = Eigen::numext::floor((CT(2) * a / u_shifted + b) * u + rate +\n                                        CT(0.43));\n\n            if (k > CT(Eigen::NumTraits<U>::highest())) {\n              // retry in case of overflow.\n              continue;\n            }\n\n            // When alpha * f(G(U)) * G'(U) is close to 1, it is possible to\n            // find a rectangle (-u_r, u_r) x (0, v_r) under the curve, such\n            // that if v <= v_r and |u| <= u_r, then we can accept.\n            // Here v_r = 0.9227 - 3.6224 / (b - 2) and u_r = 0.43.\n            if (u_shifted >= CT(0.07) &&\n                v <= CT(0.9277) - CT(3.6224) / (b - CT(2))) {\n              samples_rate_output[sample_idx * num_rate] = U(k);\n              break;\n            }\n\n            if (k < 0 || (u_shifted < CT(0.013) && v > u_shifted)) {\n              continue;\n            }\n\n            // The expression below is equivalent to the computation of step 2)\n            // in transformed rejection (v <= alpha * F'(G(u)) * G'(u)).\n            CT s = log(v * inv_alpha / (a / (u_shifted * u_shifted) + b));\n            CT t = -rate + k * log_rate - Eigen::numext::lgamma(k + 1);\n            if (s <= t) {\n              samples_rate_output[sample_idx * num_rate] = U(k);\n              break;\n            }\n          }\n        }\n      }\n    };\n\n    // This will depend on rate.\n    // For rate < 10, on average, O(rate) calls to uniform are\n    // needed, with that\n    // many multiplies. ~10 uniform calls on average with ~25 cost op calls.\n    //\n    // Very roughly, for rate >= 10, the single call to log + call to\n    // lgamma\n    // occur for ~60 percent of samples.\n    // 2 x 100 (64-bit cycles per log) * 0.62 = ~124\n    // Additionally, there are ~10 other ops (+, *, /, ...) at 3-6 cycles each:\n    // 40 * .62  = ~25.\n    //\n    // Finally, there are several other ops that are done every loop along with\n    // 2 uniform generations along with 5 other ops at 3-6 cycles each.\n    // ~15 / .89 = ~16\n    //\n    // In total this should be ~165 + 2 * Uniform::kElementCost.\n    // We assume that half the tensor has rate < 10, so on average 6\n    // uniform's\n    // will be needed. We will upper bound the other op cost by the one for\n    // rate > 10.\n    static const int kElementCost = 165 + 6 * Uniform::kElementCost +\n                                    6 * random::PhiloxRandom::kElementCost;\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers,\n          num_rate * num_samples, kElementCost, DoWork);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,7 +25,7 @@\n     typedef random::UniformDistribution<random::PhiloxRandom, CT> Uniform;\n \n     auto DoWork = [num_samples, num_rate, &rng, samples_flat, rate_flat](\n-                      int start_output, int limit_output) {\n+                      int64 start_output, int64 limit_output) {\n       // Capturing \"rng\" by value would only make a copy for the _shared_\n       // lambda.  Since we want to let each worker have its own copy, we pass\n       // \"rng\" by reference and explicitly do a copy assignment.",
        "diff_line_info": {
            "deleted_lines": [
                "                      int start_output, int limit_output) {"
            ],
            "added_lines": [
                "                      int64 start_output, int64 limit_output) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-16125",
        "func_name": "GNOME/gdm/look_for_existing_users_sync",
        "description": "gdm3 versions before 3.36.2 or 3.38.2 would start gnome-initial-setup if gdm3 can't contact the accountservice service via dbus in a timely manner; on Ubuntu (and potentially derivatives) this could be be chained with an additional issue that could allow a local user to create a new privileged account.",
        "git_url": "https://github.com/GNOME/gdm/commit/dc8235128c3a1fcd5da8f30ab6839d413d353f28",
        "commit_title": "display: Exit with failure if loading existing users fails",
        "commit_text": " Given not having users may make GDM to launch initial setup, that allows to create new users (potentially with sudo capabilities), it's better to make look_for_existing_users() to return its status and only if it didn't fail continue the gdm execution.  GHSL-2020-202 CVE-2020-16125  Fixes #642",
        "func_before": "static void\nlook_for_existing_users_sync (GdmDisplay *self)\n{\n        GdmDisplayPrivate *priv;\n        GError *error = NULL;\n        GVariant *call_result;\n        GVariant *user_list;\n\n        priv = gdm_display_get_instance_private (self);\n        priv->accountsservice_proxy = g_dbus_proxy_new_sync (priv->connection,\n                                                             0, NULL,\n                                                             \"org.freedesktop.Accounts\",\n                                                             \"/org/freedesktop/Accounts\",\n                                                             \"org.freedesktop.Accounts\",\n                                                             NULL,\n                                                             &error);\n\n        if (!priv->accountsservice_proxy) {\n                g_warning (\"Failed to contact accountsservice: %s\", error->message);\n                goto out;\n        }\n\n        call_result = g_dbus_proxy_call_sync (priv->accountsservice_proxy,\n                                              \"ListCachedUsers\",\n                                              NULL,\n                                              0,\n                                              -1,\n                                              NULL,\n                                              &error);\n\n        if (!call_result) {\n                g_warning (\"Failed to list cached users: %s\", error->message);\n                goto out;\n        }\n\n        g_variant_get (call_result, \"(@ao)\", &user_list);\n        priv->have_existing_user_accounts = g_variant_n_children (user_list) > 0;\n        g_variant_unref (user_list);\n        g_variant_unref (call_result);\nout:\n        g_clear_error (&error);\n}",
        "func": "static gboolean\nlook_for_existing_users_sync (GdmDisplay *self)\n{\n        GdmDisplayPrivate *priv;\n        GError *error = NULL;\n        GVariant *call_result;\n        GVariant *user_list;\n\n        priv = gdm_display_get_instance_private (self);\n        priv->accountsservice_proxy = g_dbus_proxy_new_sync (priv->connection,\n                                                             0, NULL,\n                                                             \"org.freedesktop.Accounts\",\n                                                             \"/org/freedesktop/Accounts\",\n                                                             \"org.freedesktop.Accounts\",\n                                                             NULL,\n                                                             &error);\n\n        if (!priv->accountsservice_proxy) {\n                g_critical (\"Failed to contact accountsservice: %s\", error->message);\n                goto out;\n        }\n\n        call_result = g_dbus_proxy_call_sync (priv->accountsservice_proxy,\n                                              \"ListCachedUsers\",\n                                              NULL,\n                                              0,\n                                              -1,\n                                              NULL,\n                                              &error);\n\n        if (!call_result) {\n                g_critical (\"Failed to list cached users: %s\", error->message);\n                goto out;\n        }\n\n        g_variant_get (call_result, \"(@ao)\", &user_list);\n        priv->have_existing_user_accounts = g_variant_n_children (user_list) > 0;\n        g_variant_unref (user_list);\n        g_variant_unref (call_result);\nout:\n        g_clear_error (&error);\n        return priv->accountsservice_proxy != NULL && call_result != NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-static void\n+static gboolean\n look_for_existing_users_sync (GdmDisplay *self)\n {\n         GdmDisplayPrivate *priv;\n@@ -16,7 +16,7 @@\n                                                              &error);\n \n         if (!priv->accountsservice_proxy) {\n-                g_warning (\"Failed to contact accountsservice: %s\", error->message);\n+                g_critical (\"Failed to contact accountsservice: %s\", error->message);\n                 goto out;\n         }\n \n@@ -29,7 +29,7 @@\n                                               &error);\n \n         if (!call_result) {\n-                g_warning (\"Failed to list cached users: %s\", error->message);\n+                g_critical (\"Failed to list cached users: %s\", error->message);\n                 goto out;\n         }\n \n@@ -39,4 +39,5 @@\n         g_variant_unref (call_result);\n out:\n         g_clear_error (&error);\n+        return priv->accountsservice_proxy != NULL && call_result != NULL;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static void",
                "                g_warning (\"Failed to contact accountsservice: %s\", error->message);",
                "                g_warning (\"Failed to list cached users: %s\", error->message);"
            ],
            "added_lines": [
                "static gboolean",
                "                g_critical (\"Failed to contact accountsservice: %s\", error->message);",
                "                g_critical (\"Failed to list cached users: %s\", error->message);",
                "        return priv->accountsservice_proxy != NULL && call_result != NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-16125",
        "func_name": "GNOME/gdm/gdm_display_prepare",
        "description": "gdm3 versions before 3.36.2 or 3.38.2 would start gnome-initial-setup if gdm3 can't contact the accountservice service via dbus in a timely manner; on Ubuntu (and potentially derivatives) this could be be chained with an additional issue that could allow a local user to create a new privileged account.",
        "git_url": "https://github.com/GNOME/gdm/commit/dc8235128c3a1fcd5da8f30ab6839d413d353f28",
        "commit_title": "display: Exit with failure if loading existing users fails",
        "commit_text": " Given not having users may make GDM to launch initial setup, that allows to create new users (potentially with sudo capabilities), it's better to make look_for_existing_users() to return its status and only if it didn't fail continue the gdm execution.  GHSL-2020-202 CVE-2020-16125  Fixes #642",
        "func_before": "gboolean\ngdm_display_prepare (GdmDisplay *self)\n{\n        GdmDisplayPrivate *priv;\n        gboolean ret;\n\n        g_return_val_if_fail (GDM_IS_DISPLAY (self), FALSE);\n\n        priv = gdm_display_get_instance_private (self);\n\n        g_debug (\"GdmDisplay: Preparing display: %s\", priv->id);\n\n        /* FIXME: we should probably do this in a more global place,\n         * asynchronously\n         */\n        look_for_existing_users_sync (self);\n\n        priv->doing_initial_setup = wants_initial_setup (self);\n\n        g_object_ref (self);\n        ret = GDM_DISPLAY_GET_CLASS (self)->prepare (self);\n        g_object_unref (self);\n\n        return ret;\n}",
        "func": "gboolean\ngdm_display_prepare (GdmDisplay *self)\n{\n        GdmDisplayPrivate *priv;\n        gboolean ret;\n\n        g_return_val_if_fail (GDM_IS_DISPLAY (self), FALSE);\n\n        priv = gdm_display_get_instance_private (self);\n\n        g_debug (\"GdmDisplay: Preparing display: %s\", priv->id);\n\n        /* FIXME: we should probably do this in a more global place,\n         * asynchronously\n         */\n        if (!look_for_existing_users_sync (self)) {\n                exit (EXIT_FAILURE);\n        }\n\n        priv->doing_initial_setup = wants_initial_setup (self);\n\n        g_object_ref (self);\n        ret = GDM_DISPLAY_GET_CLASS (self)->prepare (self);\n        g_object_unref (self);\n\n        return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,9 @@\n         /* FIXME: we should probably do this in a more global place,\n          * asynchronously\n          */\n-        look_for_existing_users_sync (self);\n+        if (!look_for_existing_users_sync (self)) {\n+                exit (EXIT_FAILURE);\n+        }\n \n         priv->doing_initial_setup = wants_initial_setup (self);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "        look_for_existing_users_sync (self);"
            ],
            "added_lines": [
                "        if (!look_for_existing_users_sync (self)) {",
                "                exit (EXIT_FAILURE);",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-21375",
        "func_name": "pjsip/pjproject/pjmedia_sdp_neg_modify_local_offer2",
        "description": "PJSIP is a free and open source multimedia communication library written in C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In PJSIP version 2.10 and earlier, after an initial INVITE has been sent, when two 183 responses are received, with the first one causing negotiation failure, a crash will occur. This results in a denial of service.",
        "git_url": "https://github.com/pjsip/pjproject/commit/97b3d7addbaa720b7ddb0af9bf6f3e443e664365",
        "commit_title": "Merge pull request from GHSA-hvq6-f89p-frvp",
        "commit_text": "",
        "func_before": "PJ_DEF(pj_status_t) pjmedia_sdp_neg_modify_local_offer2(\n                                    pj_pool_t *pool,\n\t\t\t\t    pjmedia_sdp_neg *neg,\n                                    unsigned flags,\n\t\t\t\t    const pjmedia_sdp_session *local)\n{\n    pjmedia_sdp_session *new_offer;\n    pjmedia_sdp_session *old_offer;\n    char media_used[PJMEDIA_MAX_SDP_MEDIA];\n    unsigned oi; /* old offer media index */\n    pj_status_t status;\n\n    /* Check arguments are valid. */\n    PJ_ASSERT_RETURN(pool && neg && local, PJ_EINVAL);\n\n    /* Can only do this in STATE_DONE. */\n    PJ_ASSERT_RETURN(neg->state == PJMEDIA_SDP_NEG_STATE_DONE, \n\t\t     PJMEDIA_SDPNEG_EINSTATE);\n\n    /* Validate the new offer */\n    status = pjmedia_sdp_validate(local);\n    if (status != PJ_SUCCESS)\n\treturn status;\n\n    /* Change state to STATE_LOCAL_OFFER */\n    neg->state = PJMEDIA_SDP_NEG_STATE_LOCAL_OFFER;\n\n    /* Init vars */\n    pj_bzero(media_used, sizeof(media_used));\n    old_offer = neg->active_local_sdp;\n    new_offer = pjmedia_sdp_session_clone(pool, local);\n\n    /* RFC 3264 Section 8: When issuing an offer that modifies the session,\n     * the \"o=\" line of the new SDP MUST be identical to that in the\n     * previous SDP, except that the version in the origin field MUST\n     * increment by one from the previous SDP.\n     */\n    pj_strdup(pool, &new_offer->origin.user, &old_offer->origin.user);\n    new_offer->origin.id = old_offer->origin.id;\n\n    pj_strdup(pool, &new_offer->origin.net_type, &old_offer->origin.net_type);\n    pj_strdup(pool, &new_offer->origin.addr_type,&old_offer->origin.addr_type);\n    pj_strdup(pool, &new_offer->origin.addr, &old_offer->origin.addr);\n\n    if ((flags & PJMEDIA_SDP_NEG_ALLOW_MEDIA_CHANGE) == 0) {\n       /* Generating the new offer, in the case media lines doesn't match the\n        * active SDP (e.g. current/active SDP's have m=audio and m=video lines,\n        * and the new offer only has m=audio line), the negotiator will fix \n        * the new offer by reordering and adding the missing media line with \n        * port number set to zero.\n        */\n        for (oi = 0; oi < old_offer->media_count; ++oi) {\n\t    pjmedia_sdp_media *om;\n\t    pjmedia_sdp_media *nm;\n\t    unsigned ni; /* new offer media index */\n\t    pj_bool_t found = PJ_FALSE;\n\n\t    om = old_offer->media[oi];\n\t    for (ni = oi; ni < new_offer->media_count; ++ni) {\n\t        nm = new_offer->media[ni];\n\t        if (pj_strcmp(&nm->desc.media, &om->desc.media) == 0) {\n\t\t    if (ni != oi) {\n\t\t        /* The same media found but the position unmatched to\n                         * the old offer, so let's put this media in the right\n                         * place, and keep the order of the rest.\n\t\t         */\n\t\t        pj_array_insert(\n                            new_offer->media,\t\t /* array    */\n\t\t\t    sizeof(new_offer->media[0]), /* elmt size*/\n\t\t\t    ni,\t\t\t\t /* count    */\n\t\t            oi,\t\t\t\t /* pos      */\n\t\t\t    &nm);\t\t\t /* new elmt */\n\t\t    }\n\t\t    found = PJ_TRUE;\n\t\t    break;\n\t        }\n\t    }\n\t    if (!found) {\n\t        pjmedia_sdp_media *m;\n\n\t        m = sdp_media_clone_deactivate(pool, om, om, local);\n\n\t        pj_array_insert(new_offer->media, sizeof(new_offer->media[0]),\n\t\t\t        new_offer->media_count++, oi, &m);\n\t    }\n        }\n    } else {\n        /* If media type change is allowed, the negotiator only needs to fix \n         * the new offer by adding the missing media line(s) with port number\n         * set to zero.\n         */\n        for (oi = new_offer->media_count; oi < old_offer->media_count; ++oi) {\n            pjmedia_sdp_media *m;\n\n\t    m = sdp_media_clone_deactivate(pool, old_offer->media[oi],\n                                           old_offer->media[oi], local);\n\n\t    pj_array_insert(new_offer->media, sizeof(new_offer->media[0]),\n\t                    new_offer->media_count++, oi, &m);\n\n        }\n    }\n\n    /* New_offer fixed */\n#if PJMEDIA_SDP_NEG_COMPARE_BEFORE_INC_VERSION\n    new_offer->origin.version = old_offer->origin.version;\n\n    if (pjmedia_sdp_session_cmp(new_offer, neg->initial_sdp, 0) != PJ_SUCCESS)\n    {\n\t++new_offer->origin.version;\n    }    \n#else\n    new_offer->origin.version = old_offer->origin.version + 1;\n#endif\n    \n    neg->initial_sdp_tmp = neg->initial_sdp;\n    neg->initial_sdp = new_offer;\n    neg->neg_local_sdp = pjmedia_sdp_session_clone(pool, new_offer);\n\n    return PJ_SUCCESS;\n}",
        "func": "PJ_DEF(pj_status_t) pjmedia_sdp_neg_modify_local_offer2(\n                                    pj_pool_t *pool,\n\t\t\t\t    pjmedia_sdp_neg *neg,\n                                    unsigned flags,\n\t\t\t\t    const pjmedia_sdp_session *local)\n{\n    pjmedia_sdp_session *new_offer;\n    pjmedia_sdp_session *old_offer;\n    unsigned oi; /* old offer media index */\n    pj_status_t status;\n\n    /* Check arguments are valid. */\n    PJ_ASSERT_RETURN(pool && neg && local, PJ_EINVAL);\n\n    /* Can only do this in STATE_DONE. */\n    PJ_ASSERT_RETURN(neg->state == PJMEDIA_SDP_NEG_STATE_DONE, \n\t\t     PJMEDIA_SDPNEG_EINSTATE);\n\n    /* Validate the new offer */\n    status = pjmedia_sdp_validate(local);\n    if (status != PJ_SUCCESS)\n\treturn status;\n\n    /* Change state to STATE_LOCAL_OFFER */\n    neg->state = PJMEDIA_SDP_NEG_STATE_LOCAL_OFFER;\n\n    /* When there is no active local SDP in state PJMEDIA_SDP_NEG_STATE_DONE,\n     * it means that the previous initial SDP nego must have been failed,\n     * so we'll just set the local SDP offer here.\n     */\n    if (!neg->active_local_sdp) {\n\tneg->initial_sdp_tmp = NULL;\n\tneg->initial_sdp = pjmedia_sdp_session_clone(pool, local);\n\tneg->neg_local_sdp = pjmedia_sdp_session_clone(pool, local);\n\n\treturn PJ_SUCCESS;\n    }\n\n    /* Init vars */\n    old_offer = neg->active_local_sdp;\n    new_offer = pjmedia_sdp_session_clone(pool, local);\n\n    /* RFC 3264 Section 8: When issuing an offer that modifies the session,\n     * the \"o=\" line of the new SDP MUST be identical to that in the\n     * previous SDP, except that the version in the origin field MUST\n     * increment by one from the previous SDP.\n     */\n    pj_strdup(pool, &new_offer->origin.user, &old_offer->origin.user);\n    new_offer->origin.id = old_offer->origin.id;\n\n    pj_strdup(pool, &new_offer->origin.net_type, &old_offer->origin.net_type);\n    pj_strdup(pool, &new_offer->origin.addr_type,&old_offer->origin.addr_type);\n    pj_strdup(pool, &new_offer->origin.addr, &old_offer->origin.addr);\n\n    if ((flags & PJMEDIA_SDP_NEG_ALLOW_MEDIA_CHANGE) == 0) {\n       /* Generating the new offer, in the case media lines doesn't match the\n        * active SDP (e.g. current/active SDP's have m=audio and m=video lines,\n        * and the new offer only has m=audio line), the negotiator will fix \n        * the new offer by reordering and adding the missing media line with \n        * port number set to zero.\n        */\n        for (oi = 0; oi < old_offer->media_count; ++oi) {\n\t    pjmedia_sdp_media *om;\n\t    pjmedia_sdp_media *nm;\n\t    unsigned ni; /* new offer media index */\n\t    pj_bool_t found = PJ_FALSE;\n\n\t    om = old_offer->media[oi];\n\t    for (ni = oi; ni < new_offer->media_count; ++ni) {\n\t        nm = new_offer->media[ni];\n\t        if (pj_strcmp(&nm->desc.media, &om->desc.media) == 0) {\n\t\t    if (ni != oi) {\n\t\t        /* The same media found but the position unmatched to\n                         * the old offer, so let's put this media in the right\n                         * place, and keep the order of the rest.\n\t\t         */\n\t\t        pj_array_insert(\n                            new_offer->media,\t\t /* array    */\n\t\t\t    sizeof(new_offer->media[0]), /* elmt size*/\n\t\t\t    ni,\t\t\t\t /* count    */\n\t\t            oi,\t\t\t\t /* pos      */\n\t\t\t    &nm);\t\t\t /* new elmt */\n\t\t    }\n\t\t    found = PJ_TRUE;\n\t\t    break;\n\t        }\n\t    }\n\t    if (!found) {\n\t        pjmedia_sdp_media *m;\n\n\t        m = sdp_media_clone_deactivate(pool, om, om, local);\n\n\t        pj_array_insert(new_offer->media, sizeof(new_offer->media[0]),\n\t\t\t        new_offer->media_count++, oi, &m);\n\t    }\n        }\n    } else {\n        /* If media type change is allowed, the negotiator only needs to fix \n         * the new offer by adding the missing media line(s) with port number\n         * set to zero.\n         */\n        for (oi = new_offer->media_count; oi < old_offer->media_count; ++oi) {\n            pjmedia_sdp_media *m;\n\n\t    m = sdp_media_clone_deactivate(pool, old_offer->media[oi],\n                                           old_offer->media[oi], local);\n\n\t    pj_array_insert(new_offer->media, sizeof(new_offer->media[0]),\n\t                    new_offer->media_count++, oi, &m);\n\n        }\n    }\n\n    /* New_offer fixed */\n#if PJMEDIA_SDP_NEG_COMPARE_BEFORE_INC_VERSION\n    new_offer->origin.version = old_offer->origin.version;\n\n    if (pjmedia_sdp_session_cmp(new_offer, neg->initial_sdp, 0) != PJ_SUCCESS)\n    {\n\t++new_offer->origin.version;\n    }    \n#else\n    new_offer->origin.version = old_offer->origin.version + 1;\n#endif\n    \n    neg->initial_sdp_tmp = neg->initial_sdp;\n    neg->initial_sdp = new_offer;\n    neg->neg_local_sdp = pjmedia_sdp_session_clone(pool, new_offer);\n\n    return PJ_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,6 @@\n {\n     pjmedia_sdp_session *new_offer;\n     pjmedia_sdp_session *old_offer;\n-    char media_used[PJMEDIA_MAX_SDP_MEDIA];\n     unsigned oi; /* old offer media index */\n     pj_status_t status;\n \n@@ -25,8 +24,19 @@\n     /* Change state to STATE_LOCAL_OFFER */\n     neg->state = PJMEDIA_SDP_NEG_STATE_LOCAL_OFFER;\n \n+    /* When there is no active local SDP in state PJMEDIA_SDP_NEG_STATE_DONE,\n+     * it means that the previous initial SDP nego must have been failed,\n+     * so we'll just set the local SDP offer here.\n+     */\n+    if (!neg->active_local_sdp) {\n+\tneg->initial_sdp_tmp = NULL;\n+\tneg->initial_sdp = pjmedia_sdp_session_clone(pool, local);\n+\tneg->neg_local_sdp = pjmedia_sdp_session_clone(pool, local);\n+\n+\treturn PJ_SUCCESS;\n+    }\n+\n     /* Init vars */\n-    pj_bzero(media_used, sizeof(media_used));\n     old_offer = neg->active_local_sdp;\n     new_offer = pjmedia_sdp_session_clone(pool, local);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    char media_used[PJMEDIA_MAX_SDP_MEDIA];",
                "    pj_bzero(media_used, sizeof(media_used));"
            ],
            "added_lines": [
                "    /* When there is no active local SDP in state PJMEDIA_SDP_NEG_STATE_DONE,",
                "     * it means that the previous initial SDP nego must have been failed,",
                "     * so we'll just set the local SDP offer here.",
                "     */",
                "    if (!neg->active_local_sdp) {",
                "\tneg->initial_sdp_tmp = NULL;",
                "\tneg->initial_sdp = pjmedia_sdp_session_clone(pool, local);",
                "\tneg->neg_local_sdp = pjmedia_sdp_session_clone(pool, local);",
                "",
                "\treturn PJ_SUCCESS;",
                "    }",
                ""
            ]
        }
    }
]