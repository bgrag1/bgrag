[
    {
        "cve_id": "CVE-2019-15900",
        "func_name": "slicer69/doas/parsegid",
        "description": "An issue was discovered in slicer69 doas before 6.2 on certain platforms other than OpenBSD. On platforms without strtonum(3), sscanf was used without checking for error cases. Instead, the uninitialized variable errstr was checked and in some cases returned success even if sscanf failed. The result was that, instead of reporting that the supplied username or group name did not exist, it would execute the command as root.",
        "git_url": "https://github.com/slicer69/doas/commit/2f83222829448e5bc4c9391d607ec265a1e06531",
        "commit_title": "Added optimization to Makefile (can be set/overruled using OPT).",
        "commit_text": "Added flag to display all warnings during compiling. Added status checks when parsing user/group IDs for Linux. Make sure Linux drops original user's groups when running as another user.",
        "func_before": "static int\nparsegid(const char *s, gid_t *gid)\n{\n\tstruct group *gr;\n\tconst char *errstr;\n\n\tif ((gr = getgrnam(s)) != NULL) {\n\t\t*gid = gr->gr_gid;\n\t\treturn 0;\n\t}\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\t*gid = strtonum(s, 0, GID_MAX, &errstr);\n\t#else\n\tsscanf(s, \"%d\", gid);\n\t#endif\n\tif (errstr)\n\t\treturn -1;\n\treturn 0;\n}",
        "func": "static int\nparsegid(const char *s, gid_t *gid)\n{\n\tstruct group *gr;\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\tconst char *errstr = NULL;\n        #else\n        int status;\n        #endif\n\n\tif ((gr = getgrnam(s)) != NULL) {\n\t\t*gid = gr->gr_gid;\n\t\treturn 0;\n\t}\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\t*gid = strtonum(s, 0, GID_MAX, &errstr);\n\tif (errstr)\n\t\treturn -1;\n\t#else\n\tstatus = sscanf(s, \"%d\", gid);\n        if (status != 1)\n            return -1;\n\t#endif\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,11 @@\n parsegid(const char *s, gid_t *gid)\n {\n \tstruct group *gr;\n-\tconst char *errstr;\n+\t#if !defined(__linux__) && !defined(__NetBSD__)\n+\tconst char *errstr = NULL;\n+        #else\n+        int status;\n+        #endif\n \n \tif ((gr = getgrnam(s)) != NULL) {\n \t\t*gid = gr->gr_gid;\n@@ -10,10 +14,12 @@\n \t}\n \t#if !defined(__linux__) && !defined(__NetBSD__)\n \t*gid = strtonum(s, 0, GID_MAX, &errstr);\n-\t#else\n-\tsscanf(s, \"%d\", gid);\n-\t#endif\n \tif (errstr)\n \t\treturn -1;\n+\t#else\n+\tstatus = sscanf(s, \"%d\", gid);\n+        if (status != 1)\n+            return -1;\n+\t#endif\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tconst char *errstr;",
                "\t#else",
                "\tsscanf(s, \"%d\", gid);",
                "\t#endif"
            ],
            "added_lines": [
                "\t#if !defined(__linux__) && !defined(__NetBSD__)",
                "\tconst char *errstr = NULL;",
                "        #else",
                "        int status;",
                "        #endif",
                "\t#else",
                "\tstatus = sscanf(s, \"%d\", gid);",
                "        if (status != 1)",
                "            return -1;",
                "\t#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15900",
        "func_name": "slicer69/doas/parseuid",
        "description": "An issue was discovered in slicer69 doas before 6.2 on certain platforms other than OpenBSD. On platforms without strtonum(3), sscanf was used without checking for error cases. Instead, the uninitialized variable errstr was checked and in some cases returned success even if sscanf failed. The result was that, instead of reporting that the supplied username or group name did not exist, it would execute the command as root.",
        "git_url": "https://github.com/slicer69/doas/commit/2f83222829448e5bc4c9391d607ec265a1e06531",
        "commit_title": "Added optimization to Makefile (can be set/overruled using OPT).",
        "commit_text": "Added flag to display all warnings during compiling. Added status checks when parsing user/group IDs for Linux. Make sure Linux drops original user's groups when running as another user.",
        "func_before": "static int\nparseuid(const char *s, uid_t *uid)\n{\n\tstruct passwd *pw;\n\tconst char *errstr;\n\n\tif ((pw = getpwnam(s)) != NULL) {\n\t\t*uid = pw->pw_uid;\n\t\treturn 0;\n\t}\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\t*uid = strtonum(s, 0, UID_MAX, &errstr);\n\t#else\n\tsscanf(s, \"%d\", uid);\n\t#endif\n\tif (errstr)\n\t\treturn -1;\n\treturn 0;\n}",
        "func": "static int\nparseuid(const char *s, uid_t *uid)\n{\n\tstruct passwd *pw;\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\tconst char *errstr = NULL;\n        #else\n        int status;\n        #endif\n\n\tif ((pw = getpwnam(s)) != NULL) {\n\t\t*uid = pw->pw_uid;\n\t\treturn 0;\n\t}\n\t#if !defined(__linux__) && !defined(__NetBSD__)\n\t*uid = strtonum(s, 0, UID_MAX, &errstr);\n\tif (errstr)\n\t\treturn -1;\n\t#else\n\tstatus = sscanf(s, \"%d\", uid);\n        if (status != 1)\n           return -1;\n\t#endif\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,11 @@\n parseuid(const char *s, uid_t *uid)\n {\n \tstruct passwd *pw;\n-\tconst char *errstr;\n+\t#if !defined(__linux__) && !defined(__NetBSD__)\n+\tconst char *errstr = NULL;\n+        #else\n+        int status;\n+        #endif\n \n \tif ((pw = getpwnam(s)) != NULL) {\n \t\t*uid = pw->pw_uid;\n@@ -10,10 +14,12 @@\n \t}\n \t#if !defined(__linux__) && !defined(__NetBSD__)\n \t*uid = strtonum(s, 0, UID_MAX, &errstr);\n-\t#else\n-\tsscanf(s, \"%d\", uid);\n-\t#endif\n \tif (errstr)\n \t\treturn -1;\n+\t#else\n+\tstatus = sscanf(s, \"%d\", uid);\n+        if (status != 1)\n+           return -1;\n+\t#endif\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tconst char *errstr;",
                "\t#else",
                "\tsscanf(s, \"%d\", uid);",
                "\t#endif"
            ],
            "added_lines": [
                "\t#if !defined(__linux__) && !defined(__NetBSD__)",
                "\tconst char *errstr = NULL;",
                "        #else",
                "        int status;",
                "        #endif",
                "\t#else",
                "\tstatus = sscanf(s, \"%d\", uid);",
                "        if (status != 1)",
                "           return -1;",
                "\t#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-24716",
        "func_name": "openzfs/zfs/zfs_groupmember",
        "description": "OpenZFS before 2.0.0-rc1, when used on FreeBSD, allows execute permissions for all directories.",
        "git_url": "https://github.com/openzfs/zfs/commit/716b53d0a14c72bda16c0872565dd1909757e73f",
        "commit_title": "FreeBSD: Fix UNIX permissions checking",
        "commit_text": " Closes #10727",
        "func_before": "boolean_t\nzfs_groupmember(zfsvfs_t *zfsvfs, uint64_t id, cred_t *cr)\n{\n#ifdef HAVE_KSID\n\tksid_t\t\t*ksid = crgetsid(cr, KSID_GROUP);\n\tksidlist_t\t*ksidlist = crgetsidlist(cr);\n\tuid_t\t\tgid;\n\n\tif (ksid && ksidlist) {\n\t\tint\t\ti;\n\t\tksid_t\t\t*ksid_groups;\n\t\tuint32_t\tidx = FUID_INDEX(id);\n\t\tuint32_t\trid = FUID_RID(id);\n\n\t\tksid_groups = ksidlist->ksl_sids;\n\n\t\tfor (i = 0; i != ksidlist->ksl_nsid; i++) {\n\t\t\tif (idx == 0) {\n\t\t\t\tif (id != IDMAP_WK_CREATOR_GROUP_GID &&\n\t\t\t\t    id == ksid_groups[i].ks_id) {\n\t\t\t\t\treturn (B_TRUE);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tconst char *domain;\n\n\t\t\t\tdomain = zfs_fuid_find_by_idx(zfsvfs, idx);\n\t\t\t\tASSERT(domain != NULL);\n\n\t\t\t\tif (strcmp(domain,\n\t\t\t\t    IDMAP_WK_CREATOR_SID_AUTHORITY) == 0)\n\t\t\t\t\treturn (B_FALSE);\n\n\t\t\t\tif ((strcmp(domain,\n\t\t\t\t    ksid_groups[i].ks_domain->kd_name) == 0) &&\n\t\t\t\t    rid == ksid_groups[i].ks_rid)\n\t\t\t\t\treturn (B_TRUE);\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Not found in ksidlist, check posix groups\n\t */\n\tgid = zfs_fuid_map_id(zfsvfs, id, cr, ZFS_GROUP);\n\treturn (groupmember(gid, cr));\n#else\n\treturn (B_TRUE);\n#endif\n}",
        "func": "boolean_t\nzfs_groupmember(zfsvfs_t *zfsvfs, uint64_t id, cred_t *cr)\n{\n#ifdef HAVE_KSID\n\tuid_t\t\tgid;\n\n#ifdef illumos\n\tksid_t\t\t*ksid = crgetsid(cr, KSID_GROUP);\n\tksidlist_t\t*ksidlist = crgetsidlist(cr);\n\n\tif (ksid && ksidlist) {\n\t\tint\t\ti;\n\t\tksid_t\t\t*ksid_groups;\n\t\tuint32_t\tidx = FUID_INDEX(id);\n\t\tuint32_t\trid = FUID_RID(id);\n\n\t\tksid_groups = ksidlist->ksl_sids;\n\n\t\tfor (i = 0; i != ksidlist->ksl_nsid; i++) {\n\t\t\tif (idx == 0) {\n\t\t\t\tif (id != IDMAP_WK_CREATOR_GROUP_GID &&\n\t\t\t\t    id == ksid_groups[i].ks_id) {\n\t\t\t\t\treturn (B_TRUE);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tconst char *domain;\n\n\t\t\t\tdomain = zfs_fuid_find_by_idx(zfsvfs, idx);\n\t\t\t\tASSERT(domain != NULL);\n\n\t\t\t\tif (strcmp(domain,\n\t\t\t\t    IDMAP_WK_CREATOR_SID_AUTHORITY) == 0)\n\t\t\t\t\treturn (B_FALSE);\n\n\t\t\t\tif ((strcmp(domain,\n\t\t\t\t    ksid_groups[i].ks_domain->kd_name) == 0) &&\n\t\t\t\t    rid == ksid_groups[i].ks_rid)\n\t\t\t\t\treturn (B_TRUE);\n\t\t\t}\n\t\t}\n\t}\n#endif /* illumos */\n\n\t/*\n\t * Not found in ksidlist, check posix groups\n\t */\n\tgid = zfs_fuid_map_id(zfsvfs, id, cr, ZFS_GROUP);\n\treturn (groupmember(gid, cr));\n#else\n\treturn (B_TRUE);\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,9 +2,11 @@\n zfs_groupmember(zfsvfs_t *zfsvfs, uint64_t id, cred_t *cr)\n {\n #ifdef HAVE_KSID\n+\tuid_t\t\tgid;\n+\n+#ifdef illumos\n \tksid_t\t\t*ksid = crgetsid(cr, KSID_GROUP);\n \tksidlist_t\t*ksidlist = crgetsidlist(cr);\n-\tuid_t\t\tgid;\n \n \tif (ksid && ksidlist) {\n \t\tint\t\ti;\n@@ -37,6 +39,7 @@\n \t\t\t}\n \t\t}\n \t}\n+#endif /* illumos */\n \n \t/*\n \t * Not found in ksidlist, check posix groups",
        "diff_line_info": {
            "deleted_lines": [
                "\tuid_t\t\tgid;"
            ],
            "added_lines": [
                "\tuid_t\t\tgid;",
                "",
                "#ifdef illumos",
                "#endif /* illumos */"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-24716",
        "func_name": "openzfs/zfs/zfs_fuid_create",
        "description": "OpenZFS before 2.0.0-rc1, when used on FreeBSD, allows execute permissions for all directories.",
        "git_url": "https://github.com/openzfs/zfs/commit/716b53d0a14c72bda16c0872565dd1909757e73f",
        "commit_title": "FreeBSD: Fix UNIX permissions checking",
        "commit_text": " Closes #10727",
        "func_before": "uint64_t\nzfs_fuid_create(zfsvfs_t *zfsvfs, uint64_t id, cred_t *cr,\n    zfs_fuid_type_t type, zfs_fuid_info_t **fuidpp)\n{\n#ifdef HAVE_KSID\n\tconst char *domain;\n\tchar *kdomain;\n\tuint32_t fuid_idx = FUID_INDEX(id);\n\tuint32_t rid;\n\tidmap_stat status;\n\tuint64_t idx = 0;\n\tzfs_fuid_t *zfuid = NULL;\n\tzfs_fuid_info_t *fuidp = NULL;\n\n\t/*\n\t * If POSIX ID, or entry is already a FUID then\n\t * just return the id\n\t *\n\t * We may also be handed an already FUID'ized id via\n\t * chmod.\n\t */\n\n\tif (!zfsvfs->z_use_fuids || !IS_EPHEMERAL(id) || fuid_idx != 0)\n\t\treturn (id);\n\n\tif (zfsvfs->z_replay) {\n\t\tfuidp = zfsvfs->z_fuid_replay;\n\n\t\t/*\n\t\t * If we are passed an ephemeral id, but no\n\t\t * fuid_info was logged then return NOBODY.\n\t\t * This is most likely a result of idmap service\n\t\t * not being available.\n\t\t */\n\t\tif (fuidp == NULL)\n\t\t\treturn (UID_NOBODY);\n\n\t\tVERIFY3U(type, >=, ZFS_OWNER);\n\t\tVERIFY3U(type, <=, ZFS_ACE_GROUP);\n\n\t\tswitch (type) {\n\t\tcase ZFS_ACE_USER:\n\t\tcase ZFS_ACE_GROUP:\n\t\t\tzfuid = list_head(&fuidp->z_fuids);\n\t\t\trid = FUID_RID(zfuid->z_logfuid);\n\t\t\tidx = FUID_INDEX(zfuid->z_logfuid);\n\t\t\tbreak;\n\t\tcase ZFS_OWNER:\n\t\t\trid = FUID_RID(fuidp->z_fuid_owner);\n\t\t\tidx = FUID_INDEX(fuidp->z_fuid_owner);\n\t\t\tbreak;\n\t\tcase ZFS_GROUP:\n\t\t\trid = FUID_RID(fuidp->z_fuid_group);\n\t\t\tidx = FUID_INDEX(fuidp->z_fuid_group);\n\t\t\tbreak;\n\t\t};\n\t\tdomain = fuidp->z_domain_table[idx - 1];\n\t} else {\n\t\tif (type == ZFS_OWNER || type == ZFS_ACE_USER)\n\t\t\tstatus = kidmap_getsidbyuid(crgetzone(cr), id,\n\t\t\t    &domain, &rid);\n\t\telse\n\t\t\tstatus = kidmap_getsidbygid(crgetzone(cr), id,\n\t\t\t    &domain, &rid);\n\n\t\tif (status != 0) {\n\t\t\t/*\n\t\t\t * When returning nobody we will need to\n\t\t\t * make a dummy fuid table entry for logging\n\t\t\t * purposes.\n\t\t\t */\n\t\t\trid = UID_NOBODY;\n\t\t\tdomain = nulldomain;\n\t\t}\n\t}\n\n\tidx = zfs_fuid_find_by_domain(zfsvfs, domain, &kdomain, B_TRUE);\n\n\tif (!zfsvfs->z_replay)\n\t\tzfs_fuid_node_add(fuidpp, kdomain,\n\t\t    rid, idx, id, type);\n\telse if (zfuid != NULL) {\n\t\tlist_remove(&fuidp->z_fuids, zfuid);\n\t\tkmem_free(zfuid, sizeof (zfs_fuid_t));\n\t}\n\treturn (FUID_ENCODE(idx, rid));\n#else\n\t/*\n\t * The Linux port only supports POSIX IDs, use the passed id.\n\t */\n\treturn (id);\n#endif\n}",
        "func": "uint64_t\nzfs_fuid_create(zfsvfs_t *zfsvfs, uint64_t id, cred_t *cr,\n    zfs_fuid_type_t type, zfs_fuid_info_t **fuidpp)\n{\n#ifdef HAVE_KSID\n\tconst char *domain;\n\tchar *kdomain;\n\tuint32_t fuid_idx = FUID_INDEX(id);\n\tuint32_t rid = 0;\n\tidmap_stat status;\n\tuint64_t idx = UID_NOBODY;\n\tzfs_fuid_t *zfuid = NULL;\n\tzfs_fuid_info_t *fuidp = NULL;\n\n\t/*\n\t * If POSIX ID, or entry is already a FUID then\n\t * just return the id\n\t *\n\t * We may also be handed an already FUID'ized id via\n\t * chmod.\n\t */\n\n\tif (!zfsvfs->z_use_fuids || !IS_EPHEMERAL(id) || fuid_idx != 0)\n\t\treturn (id);\n\n\tif (zfsvfs->z_replay) {\n\t\tfuidp = zfsvfs->z_fuid_replay;\n\n\t\t/*\n\t\t * If we are passed an ephemeral id, but no\n\t\t * fuid_info was logged then return NOBODY.\n\t\t * This is most likely a result of idmap service\n\t\t * not being available.\n\t\t */\n\t\tif (fuidp == NULL)\n\t\t\treturn (UID_NOBODY);\n\n\t\tVERIFY3U(type, >=, ZFS_OWNER);\n\t\tVERIFY3U(type, <=, ZFS_ACE_GROUP);\n\n\t\tswitch (type) {\n\t\tcase ZFS_ACE_USER:\n\t\tcase ZFS_ACE_GROUP:\n\t\t\tzfuid = list_head(&fuidp->z_fuids);\n\t\t\trid = FUID_RID(zfuid->z_logfuid);\n\t\t\tidx = FUID_INDEX(zfuid->z_logfuid);\n\t\t\tbreak;\n\t\tcase ZFS_OWNER:\n\t\t\trid = FUID_RID(fuidp->z_fuid_owner);\n\t\t\tidx = FUID_INDEX(fuidp->z_fuid_owner);\n\t\t\tbreak;\n\t\tcase ZFS_GROUP:\n\t\t\trid = FUID_RID(fuidp->z_fuid_group);\n\t\t\tidx = FUID_INDEX(fuidp->z_fuid_group);\n\t\t\tbreak;\n\t\t};\n\t\tdomain = fuidp->z_domain_table[idx - 1];\n\t} else {\n\t\tif (type == ZFS_OWNER || type == ZFS_ACE_USER)\n\t\t\tstatus = kidmap_getsidbyuid(crgetzone(cr), id,\n\t\t\t    &domain, &rid);\n\t\telse\n\t\t\tstatus = kidmap_getsidbygid(crgetzone(cr), id,\n\t\t\t    &domain, &rid);\n\n\t\tif (status != 0) {\n\t\t\t/*\n\t\t\t * When returning nobody we will need to\n\t\t\t * make a dummy fuid table entry for logging\n\t\t\t * purposes.\n\t\t\t */\n\t\t\trid = UID_NOBODY;\n\t\t\tdomain = nulldomain;\n\t\t}\n\t}\n\n\tidx = zfs_fuid_find_by_domain(zfsvfs, domain, &kdomain, B_TRUE);\n\n\tif (!zfsvfs->z_replay)\n\t\tzfs_fuid_node_add(fuidpp, kdomain,\n\t\t    rid, idx, id, type);\n\telse if (zfuid != NULL) {\n\t\tlist_remove(&fuidp->z_fuids, zfuid);\n\t\tkmem_free(zfuid, sizeof (zfs_fuid_t));\n\t}\n\treturn (FUID_ENCODE(idx, rid));\n#else\n\t/*\n\t * The Linux port only supports POSIX IDs, use the passed id.\n\t */\n\treturn (id);\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,9 +6,9 @@\n \tconst char *domain;\n \tchar *kdomain;\n \tuint32_t fuid_idx = FUID_INDEX(id);\n-\tuint32_t rid;\n+\tuint32_t rid = 0;\n \tidmap_stat status;\n-\tuint64_t idx = 0;\n+\tuint64_t idx = UID_NOBODY;\n \tzfs_fuid_t *zfuid = NULL;\n \tzfs_fuid_info_t *fuidp = NULL;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tuint32_t rid;",
                "\tuint64_t idx = 0;"
            ],
            "added_lines": [
                "\tuint32_t rid = 0;",
                "\tuint64_t idx = UID_NOBODY;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-24716",
        "func_name": "openzfs/zfs/zfs_fuid_map_id",
        "description": "OpenZFS before 2.0.0-rc1, when used on FreeBSD, allows execute permissions for all directories.",
        "git_url": "https://github.com/openzfs/zfs/commit/716b53d0a14c72bda16c0872565dd1909757e73f",
        "commit_title": "FreeBSD: Fix UNIX permissions checking",
        "commit_text": " Closes #10727",
        "func_before": "uid_t\nzfs_fuid_map_id(zfsvfs_t *zfsvfs, uint64_t fuid,\n    cred_t *cr, zfs_fuid_type_t type)\n{\n#ifdef HAVE_KSID\n\tuint32_t index = FUID_INDEX(fuid);\n\tconst char *domain;\n\tuid_t id;\n\n\tif (index == 0)\n\t\treturn (fuid);\n\n\tdomain = zfs_fuid_find_by_idx(zfsvfs, index);\n\tASSERT(domain != NULL);\n\n\tif (type == ZFS_OWNER || type == ZFS_ACE_USER) {\n\t\t(void) kidmap_getuidbysid(crgetzone(cr), domain,\n\t\t    FUID_RID(fuid), &id);\n\t} else {\n\t\t(void) kidmap_getgidbysid(crgetzone(cr), domain,\n\t\t    FUID_RID(fuid), &id);\n\t}\n\treturn (id);\n#else\n\t/*\n\t * The Linux port only supports POSIX IDs, use the passed id.\n\t */\n\treturn (fuid);\n#endif /* HAVE_KSID */\n}",
        "func": "uid_t\nzfs_fuid_map_id(zfsvfs_t *zfsvfs, uint64_t fuid,\n    cred_t *cr, zfs_fuid_type_t type)\n{\n\tuint32_t index = FUID_INDEX(fuid);\n\n\tif (index == 0)\n\t\treturn (fuid);\n\n\treturn (UID_NOBODY);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,29 +2,10 @@\n zfs_fuid_map_id(zfsvfs_t *zfsvfs, uint64_t fuid,\n     cred_t *cr, zfs_fuid_type_t type)\n {\n-#ifdef HAVE_KSID\n \tuint32_t index = FUID_INDEX(fuid);\n-\tconst char *domain;\n-\tuid_t id;\n \n \tif (index == 0)\n \t\treturn (fuid);\n \n-\tdomain = zfs_fuid_find_by_idx(zfsvfs, index);\n-\tASSERT(domain != NULL);\n-\n-\tif (type == ZFS_OWNER || type == ZFS_ACE_USER) {\n-\t\t(void) kidmap_getuidbysid(crgetzone(cr), domain,\n-\t\t    FUID_RID(fuid), &id);\n-\t} else {\n-\t\t(void) kidmap_getgidbysid(crgetzone(cr), domain,\n-\t\t    FUID_RID(fuid), &id);\n-\t}\n-\treturn (id);\n-#else\n-\t/*\n-\t * The Linux port only supports POSIX IDs, use the passed id.\n-\t */\n-\treturn (fuid);\n-#endif /* HAVE_KSID */\n+\treturn (UID_NOBODY);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "#ifdef HAVE_KSID",
                "\tconst char *domain;",
                "\tuid_t id;",
                "\tdomain = zfs_fuid_find_by_idx(zfsvfs, index);",
                "\tASSERT(domain != NULL);",
                "",
                "\tif (type == ZFS_OWNER || type == ZFS_ACE_USER) {",
                "\t\t(void) kidmap_getuidbysid(crgetzone(cr), domain,",
                "\t\t    FUID_RID(fuid), &id);",
                "\t} else {",
                "\t\t(void) kidmap_getgidbysid(crgetzone(cr), domain,",
                "\t\t    FUID_RID(fuid), &id);",
                "\t}",
                "\treturn (id);",
                "#else",
                "\t/*",
                "\t * The Linux port only supports POSIX IDs, use the passed id.",
                "\t */",
                "\treturn (fuid);",
                "#endif /* HAVE_KSID */"
            ],
            "added_lines": [
                "\treturn (UID_NOBODY);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-24716",
        "func_name": "openzfs/zfs/zfs_fastaccesschk_execute",
        "description": "OpenZFS before 2.0.0-rc1, when used on FreeBSD, allows execute permissions for all directories.",
        "git_url": "https://github.com/openzfs/zfs/commit/716b53d0a14c72bda16c0872565dd1909757e73f",
        "commit_title": "FreeBSD: Fix UNIX permissions checking",
        "commit_text": " Closes #10727",
        "func_before": "int\nzfs_fastaccesschk_execute(znode_t *zdp, cred_t *cr)\n{\n\tboolean_t owner = B_FALSE;\n\tboolean_t groupmbr = B_FALSE;\n\tboolean_t is_attr;\n\tuid_t uid = crgetuid(cr);\n\n\tif (zdp->z_pflags & ZFS_AV_QUARANTINED)\n\t\treturn (1);\n\n\tis_attr = ((zdp->z_pflags & ZFS_XATTR) &&\n\t    (ZTOV(zdp)->v_type == VDIR));\n\tif (is_attr)\n\t\treturn (1);\n\n\tif (zdp->z_pflags & ZFS_NO_EXECS_DENIED)\n\t\treturn (0);\n\n\tmutex_enter(&zdp->z_acl_lock);\n\tif (FUID_INDEX(zdp->z_uid) != 0 || FUID_INDEX(zdp->z_gid) != 0) {\n\t\tgoto out_slow;\n\t}\n\n\tif (uid == zdp->z_uid) {\n\t\towner = B_TRUE;\n\t\tif (zdp->z_mode & S_IXUSR) {\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tgoto out_slow;\n\t\t}\n\t}\n\tif (groupmember(zdp->z_gid, cr)) {\n\t\tgroupmbr = B_TRUE;\n\t\tif (zdp->z_mode & S_IXGRP) {\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tgoto out_slow;\n\t\t}\n\t}\n\tif (!owner && !groupmbr) {\n\t\tif (zdp->z_mode & S_IXOTH) {\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tmutex_exit(&zdp->z_acl_lock);\n\treturn (0);\nout_slow:\n\tmutex_exit(&zdp->z_acl_lock);\n\treturn (1);\n}",
        "func": "int\nzfs_fastaccesschk_execute(znode_t *zdp, cred_t *cr)\n{\n\tboolean_t is_attr;\n\n\tif (zdp->z_pflags & ZFS_AV_QUARANTINED)\n\t\treturn (1);\n\n\tis_attr = ((zdp->z_pflags & ZFS_XATTR) &&\n\t    (ZTOV(zdp)->v_type == VDIR));\n\tif (is_attr)\n\t\treturn (1);\n\n\tif (zdp->z_pflags & ZFS_NO_EXECS_DENIED)\n\t\treturn (0);\n\n\treturn (1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,7 @@\n int\n zfs_fastaccesschk_execute(znode_t *zdp, cred_t *cr)\n {\n-\tboolean_t owner = B_FALSE;\n-\tboolean_t groupmbr = B_FALSE;\n \tboolean_t is_attr;\n-\tuid_t uid = crgetuid(cr);\n \n \tif (zdp->z_pflags & ZFS_AV_QUARANTINED)\n \t\treturn (1);\n@@ -17,36 +14,5 @@\n \tif (zdp->z_pflags & ZFS_NO_EXECS_DENIED)\n \t\treturn (0);\n \n-\tmutex_enter(&zdp->z_acl_lock);\n-\tif (FUID_INDEX(zdp->z_uid) != 0 || FUID_INDEX(zdp->z_gid) != 0) {\n-\t\tgoto out_slow;\n-\t}\n-\n-\tif (uid == zdp->z_uid) {\n-\t\towner = B_TRUE;\n-\t\tif (zdp->z_mode & S_IXUSR) {\n-\t\t\tgoto out;\n-\t\t} else {\n-\t\t\tgoto out_slow;\n-\t\t}\n-\t}\n-\tif (groupmember(zdp->z_gid, cr)) {\n-\t\tgroupmbr = B_TRUE;\n-\t\tif (zdp->z_mode & S_IXGRP) {\n-\t\t\tgoto out;\n-\t\t} else {\n-\t\t\tgoto out_slow;\n-\t\t}\n-\t}\n-\tif (!owner && !groupmbr) {\n-\t\tif (zdp->z_mode & S_IXOTH) {\n-\t\t\tgoto out;\n-\t\t}\n-\t}\n-out:\n-\tmutex_exit(&zdp->z_acl_lock);\n-\treturn (0);\n-out_slow:\n-\tmutex_exit(&zdp->z_acl_lock);\n \treturn (1);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tboolean_t owner = B_FALSE;",
                "\tboolean_t groupmbr = B_FALSE;",
                "\tuid_t uid = crgetuid(cr);",
                "\tmutex_enter(&zdp->z_acl_lock);",
                "\tif (FUID_INDEX(zdp->z_uid) != 0 || FUID_INDEX(zdp->z_gid) != 0) {",
                "\t\tgoto out_slow;",
                "\t}",
                "",
                "\tif (uid == zdp->z_uid) {",
                "\t\towner = B_TRUE;",
                "\t\tif (zdp->z_mode & S_IXUSR) {",
                "\t\t\tgoto out;",
                "\t\t} else {",
                "\t\t\tgoto out_slow;",
                "\t\t}",
                "\t}",
                "\tif (groupmember(zdp->z_gid, cr)) {",
                "\t\tgroupmbr = B_TRUE;",
                "\t\tif (zdp->z_mode & S_IXGRP) {",
                "\t\t\tgoto out;",
                "\t\t} else {",
                "\t\t\tgoto out_slow;",
                "\t\t}",
                "\t}",
                "\tif (!owner && !groupmbr) {",
                "\t\tif (zdp->z_mode & S_IXOTH) {",
                "\t\t\tgoto out;",
                "\t\t}",
                "\t}",
                "out:",
                "\tmutex_exit(&zdp->z_acl_lock);",
                "\treturn (0);",
                "out_slow:",
                "\tmutex_exit(&zdp->z_acl_lock);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2020-25284",
        "func_name": "torvalds/linux/rbd_config_info_show",
        "description": "The rbd block device driver in drivers/block/rbd.c in the Linux kernel through 5.8.9 used incomplete permission checking for access to rbd devices, which could be leveraged by local attackers to map or unmap rbd block devices, aka CID-f44d04e696fe.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=f44d04e696feaf13d192d942c4f14ad2e117065a",
        "commit_title": "It turns out that currently we rely only on sysfs attribute",
        "commit_text": "permissions:    $ ll /sys/bus/rbd/{add*,remove*}   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/add   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/add_single_major   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/remove   --w------- 1 root root 4096 Sep  3 20:38 /sys/bus/rbd/remove_single_major  This means that images can be mapped and unmapped (i.e. block devices can be created and deleted) by a UID 0 process even after it drops all privileges or by any process with CAP_DAC_OVERRIDE in its user namespace as long as UID 0 is mapped into that user namespace.  Be consistent with other virtual block devices (loop, nbd, dm, md, etc) and require CAP_SYS_ADMIN in the initial user namespace for mapping and unmapping, and also for dumping the configuration string and refreshing the image header.  Cc: stable@vger.kernel.org ",
        "func_before": "static ssize_t rbd_config_info_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%s\\n\", rbd_dev->config_info);\n}",
        "func": "static ssize_t rbd_config_info_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn sprintf(buf, \"%s\\n\", rbd_dev->config_info);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,5 +3,8 @@\n {\n \tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n \n+\tif (!capable(CAP_SYS_ADMIN))\n+\t\treturn -EPERM;\n+\n \treturn sprintf(buf, \"%s\\n\", rbd_dev->config_info);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (!capable(CAP_SYS_ADMIN))",
                "\t\treturn -EPERM;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25284",
        "func_name": "torvalds/linux/do_rbd_add",
        "description": "The rbd block device driver in drivers/block/rbd.c in the Linux kernel through 5.8.9 used incomplete permission checking for access to rbd devices, which could be leveraged by local attackers to map or unmap rbd block devices, aka CID-f44d04e696fe.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=f44d04e696feaf13d192d942c4f14ad2e117065a",
        "commit_title": "It turns out that currently we rely only on sysfs attribute",
        "commit_text": "permissions:    $ ll /sys/bus/rbd/{add*,remove*}   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/add   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/add_single_major   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/remove   --w------- 1 root root 4096 Sep  3 20:38 /sys/bus/rbd/remove_single_major  This means that images can be mapped and unmapped (i.e. block devices can be created and deleted) by a UID 0 process even after it drops all privileges or by any process with CAP_DAC_OVERRIDE in its user namespace as long as UID 0 is mapped into that user namespace.  Be consistent with other virtual block devices (loop, nbd, dm, md, etc) and require CAP_SYS_ADMIN in the initial user namespace for mapping and unmapping, and also for dumping the configuration string and refreshing the image header.  Cc: stable@vger.kernel.org ",
        "func_before": "static ssize_t do_rbd_add(struct bus_type *bus,\n\t\t\t  const char *buf,\n\t\t\t  size_t count)\n{\n\tstruct rbd_device *rbd_dev = NULL;\n\tstruct ceph_options *ceph_opts = NULL;\n\tstruct rbd_options *rbd_opts = NULL;\n\tstruct rbd_spec *spec = NULL;\n\tstruct rbd_client *rbdc;\n\tint rc;\n\n\tif (!try_module_get(THIS_MODULE))\n\t\treturn -ENODEV;\n\n\t/* parse add command */\n\trc = rbd_add_parse_args(buf, &ceph_opts, &rbd_opts, &spec);\n\tif (rc < 0)\n\t\tgoto out;\n\n\trbdc = rbd_get_client(ceph_opts);\n\tif (IS_ERR(rbdc)) {\n\t\trc = PTR_ERR(rbdc);\n\t\tgoto err_out_args;\n\t}\n\n\t/* pick the pool */\n\trc = ceph_pg_poolid_by_name(rbdc->client->osdc.osdmap, spec->pool_name);\n\tif (rc < 0) {\n\t\tif (rc == -ENOENT)\n\t\t\tpr_info(\"pool %s does not exist\\n\", spec->pool_name);\n\t\tgoto err_out_client;\n\t}\n\tspec->pool_id = (u64)rc;\n\n\trbd_dev = rbd_dev_create(rbdc, spec, rbd_opts);\n\tif (!rbd_dev) {\n\t\trc = -ENOMEM;\n\t\tgoto err_out_client;\n\t}\n\trbdc = NULL;\t\t/* rbd_dev now owns this */\n\tspec = NULL;\t\t/* rbd_dev now owns this */\n\trbd_opts = NULL;\t/* rbd_dev now owns this */\n\n\t/* if we are mapping a snapshot it will be a read-only mapping */\n\tif (rbd_dev->opts->read_only ||\n\t    strcmp(rbd_dev->spec->snap_name, RBD_SNAP_HEAD_NAME))\n\t\t__set_bit(RBD_DEV_FLAG_READONLY, &rbd_dev->flags);\n\n\trbd_dev->config_info = kstrdup(buf, GFP_KERNEL);\n\tif (!rbd_dev->config_info) {\n\t\trc = -ENOMEM;\n\t\tgoto err_out_rbd_dev;\n\t}\n\n\trc = rbd_dev_image_probe(rbd_dev, 0);\n\tif (rc < 0)\n\t\tgoto err_out_rbd_dev;\n\n\tif (rbd_dev->opts->alloc_size > rbd_dev->layout.object_size) {\n\t\trbd_warn(rbd_dev, \"alloc_size adjusted to %u\",\n\t\t\t rbd_dev->layout.object_size);\n\t\trbd_dev->opts->alloc_size = rbd_dev->layout.object_size;\n\t}\n\n\trc = rbd_dev_device_setup(rbd_dev);\n\tif (rc)\n\t\tgoto err_out_image_probe;\n\n\trc = rbd_add_acquire_lock(rbd_dev);\n\tif (rc)\n\t\tgoto err_out_image_lock;\n\n\t/* Everything's ready.  Announce the disk to the world. */\n\n\trc = device_add(&rbd_dev->dev);\n\tif (rc)\n\t\tgoto err_out_image_lock;\n\n\tdevice_add_disk(&rbd_dev->dev, rbd_dev->disk, NULL);\n\t/* see rbd_init_disk() */\n\tblk_put_queue(rbd_dev->disk->queue);\n\n\tspin_lock(&rbd_dev_list_lock);\n\tlist_add_tail(&rbd_dev->node, &rbd_dev_list);\n\tspin_unlock(&rbd_dev_list_lock);\n\n\tpr_info(\"%s: capacity %llu features 0x%llx\\n\", rbd_dev->disk->disk_name,\n\t\t(unsigned long long)get_capacity(rbd_dev->disk) << SECTOR_SHIFT,\n\t\trbd_dev->header.features);\n\trc = count;\nout:\n\tmodule_put(THIS_MODULE);\n\treturn rc;\n\nerr_out_image_lock:\n\trbd_dev_image_unlock(rbd_dev);\n\trbd_dev_device_release(rbd_dev);\nerr_out_image_probe:\n\trbd_dev_image_release(rbd_dev);\nerr_out_rbd_dev:\n\trbd_dev_destroy(rbd_dev);\nerr_out_client:\n\trbd_put_client(rbdc);\nerr_out_args:\n\trbd_spec_put(spec);\n\tkfree(rbd_opts);\n\tgoto out;\n}",
        "func": "static ssize_t do_rbd_add(struct bus_type *bus,\n\t\t\t  const char *buf,\n\t\t\t  size_t count)\n{\n\tstruct rbd_device *rbd_dev = NULL;\n\tstruct ceph_options *ceph_opts = NULL;\n\tstruct rbd_options *rbd_opts = NULL;\n\tstruct rbd_spec *spec = NULL;\n\tstruct rbd_client *rbdc;\n\tint rc;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!try_module_get(THIS_MODULE))\n\t\treturn -ENODEV;\n\n\t/* parse add command */\n\trc = rbd_add_parse_args(buf, &ceph_opts, &rbd_opts, &spec);\n\tif (rc < 0)\n\t\tgoto out;\n\n\trbdc = rbd_get_client(ceph_opts);\n\tif (IS_ERR(rbdc)) {\n\t\trc = PTR_ERR(rbdc);\n\t\tgoto err_out_args;\n\t}\n\n\t/* pick the pool */\n\trc = ceph_pg_poolid_by_name(rbdc->client->osdc.osdmap, spec->pool_name);\n\tif (rc < 0) {\n\t\tif (rc == -ENOENT)\n\t\t\tpr_info(\"pool %s does not exist\\n\", spec->pool_name);\n\t\tgoto err_out_client;\n\t}\n\tspec->pool_id = (u64)rc;\n\n\trbd_dev = rbd_dev_create(rbdc, spec, rbd_opts);\n\tif (!rbd_dev) {\n\t\trc = -ENOMEM;\n\t\tgoto err_out_client;\n\t}\n\trbdc = NULL;\t\t/* rbd_dev now owns this */\n\tspec = NULL;\t\t/* rbd_dev now owns this */\n\trbd_opts = NULL;\t/* rbd_dev now owns this */\n\n\t/* if we are mapping a snapshot it will be a read-only mapping */\n\tif (rbd_dev->opts->read_only ||\n\t    strcmp(rbd_dev->spec->snap_name, RBD_SNAP_HEAD_NAME))\n\t\t__set_bit(RBD_DEV_FLAG_READONLY, &rbd_dev->flags);\n\n\trbd_dev->config_info = kstrdup(buf, GFP_KERNEL);\n\tif (!rbd_dev->config_info) {\n\t\trc = -ENOMEM;\n\t\tgoto err_out_rbd_dev;\n\t}\n\n\trc = rbd_dev_image_probe(rbd_dev, 0);\n\tif (rc < 0)\n\t\tgoto err_out_rbd_dev;\n\n\tif (rbd_dev->opts->alloc_size > rbd_dev->layout.object_size) {\n\t\trbd_warn(rbd_dev, \"alloc_size adjusted to %u\",\n\t\t\t rbd_dev->layout.object_size);\n\t\trbd_dev->opts->alloc_size = rbd_dev->layout.object_size;\n\t}\n\n\trc = rbd_dev_device_setup(rbd_dev);\n\tif (rc)\n\t\tgoto err_out_image_probe;\n\n\trc = rbd_add_acquire_lock(rbd_dev);\n\tif (rc)\n\t\tgoto err_out_image_lock;\n\n\t/* Everything's ready.  Announce the disk to the world. */\n\n\trc = device_add(&rbd_dev->dev);\n\tif (rc)\n\t\tgoto err_out_image_lock;\n\n\tdevice_add_disk(&rbd_dev->dev, rbd_dev->disk, NULL);\n\t/* see rbd_init_disk() */\n\tblk_put_queue(rbd_dev->disk->queue);\n\n\tspin_lock(&rbd_dev_list_lock);\n\tlist_add_tail(&rbd_dev->node, &rbd_dev_list);\n\tspin_unlock(&rbd_dev_list_lock);\n\n\tpr_info(\"%s: capacity %llu features 0x%llx\\n\", rbd_dev->disk->disk_name,\n\t\t(unsigned long long)get_capacity(rbd_dev->disk) << SECTOR_SHIFT,\n\t\trbd_dev->header.features);\n\trc = count;\nout:\n\tmodule_put(THIS_MODULE);\n\treturn rc;\n\nerr_out_image_lock:\n\trbd_dev_image_unlock(rbd_dev);\n\trbd_dev_device_release(rbd_dev);\nerr_out_image_probe:\n\trbd_dev_image_release(rbd_dev);\nerr_out_rbd_dev:\n\trbd_dev_destroy(rbd_dev);\nerr_out_client:\n\trbd_put_client(rbdc);\nerr_out_args:\n\trbd_spec_put(spec);\n\tkfree(rbd_opts);\n\tgoto out;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,9 @@\n \tstruct rbd_spec *spec = NULL;\n \tstruct rbd_client *rbdc;\n \tint rc;\n+\n+\tif (!capable(CAP_SYS_ADMIN))\n+\t\treturn -EPERM;\n \n \tif (!try_module_get(THIS_MODULE))\n \t\treturn -ENODEV;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (!capable(CAP_SYS_ADMIN))",
                "\t\treturn -EPERM;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25284",
        "func_name": "torvalds/linux/rbd_image_refresh",
        "description": "The rbd block device driver in drivers/block/rbd.c in the Linux kernel through 5.8.9 used incomplete permission checking for access to rbd devices, which could be leveraged by local attackers to map or unmap rbd block devices, aka CID-f44d04e696fe.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=f44d04e696feaf13d192d942c4f14ad2e117065a",
        "commit_title": "It turns out that currently we rely only on sysfs attribute",
        "commit_text": "permissions:    $ ll /sys/bus/rbd/{add*,remove*}   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/add   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/add_single_major   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/remove   --w------- 1 root root 4096 Sep  3 20:38 /sys/bus/rbd/remove_single_major  This means that images can be mapped and unmapped (i.e. block devices can be created and deleted) by a UID 0 process even after it drops all privileges or by any process with CAP_DAC_OVERRIDE in its user namespace as long as UID 0 is mapped into that user namespace.  Be consistent with other virtual block devices (loop, nbd, dm, md, etc) and require CAP_SYS_ADMIN in the initial user namespace for mapping and unmapping, and also for dumping the configuration string and refreshing the image header.  Cc: stable@vger.kernel.org ",
        "func_before": "static ssize_t rbd_image_refresh(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf,\n\t\t\t\t size_t size)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\tint ret;\n\n\tret = rbd_dev_refresh(rbd_dev);\n\tif (ret)\n\t\treturn ret;\n\n\treturn size;\n}",
        "func": "static ssize_t rbd_image_refresh(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf,\n\t\t\t\t size_t size)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = rbd_dev_refresh(rbd_dev);\n\tif (ret)\n\t\treturn ret;\n\n\treturn size;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,9 @@\n \tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n \tint ret;\n \n+\tif (!capable(CAP_SYS_ADMIN))\n+\t\treturn -EPERM;\n+\n \tret = rbd_dev_refresh(rbd_dev);\n \tif (ret)\n \t\treturn ret;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (!capable(CAP_SYS_ADMIN))",
                "\t\treturn -EPERM;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25284",
        "func_name": "torvalds/linux/do_rbd_remove",
        "description": "The rbd block device driver in drivers/block/rbd.c in the Linux kernel through 5.8.9 used incomplete permission checking for access to rbd devices, which could be leveraged by local attackers to map or unmap rbd block devices, aka CID-f44d04e696fe.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=f44d04e696feaf13d192d942c4f14ad2e117065a",
        "commit_title": "It turns out that currently we rely only on sysfs attribute",
        "commit_text": "permissions:    $ ll /sys/bus/rbd/{add*,remove*}   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/add   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/add_single_major   --w------- 1 root root 4096 Sep  3 20:37 /sys/bus/rbd/remove   --w------- 1 root root 4096 Sep  3 20:38 /sys/bus/rbd/remove_single_major  This means that images can be mapped and unmapped (i.e. block devices can be created and deleted) by a UID 0 process even after it drops all privileges or by any process with CAP_DAC_OVERRIDE in its user namespace as long as UID 0 is mapped into that user namespace.  Be consistent with other virtual block devices (loop, nbd, dm, md, etc) and require CAP_SYS_ADMIN in the initial user namespace for mapping and unmapping, and also for dumping the configuration string and refreshing the image header.  Cc: stable@vger.kernel.org ",
        "func_before": "static ssize_t do_rbd_remove(struct bus_type *bus,\n\t\t\t     const char *buf,\n\t\t\t     size_t count)\n{\n\tstruct rbd_device *rbd_dev = NULL;\n\tstruct list_head *tmp;\n\tint dev_id;\n\tchar opt_buf[6];\n\tbool force = false;\n\tint ret;\n\n\tdev_id = -1;\n\topt_buf[0] = '\\0';\n\tsscanf(buf, \"%d %5s\", &dev_id, opt_buf);\n\tif (dev_id < 0) {\n\t\tpr_err(\"dev_id out of range\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (opt_buf[0] != '\\0') {\n\t\tif (!strcmp(opt_buf, \"force\")) {\n\t\t\tforce = true;\n\t\t} else {\n\t\t\tpr_err(\"bad remove option at '%s'\\n\", opt_buf);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tret = -ENOENT;\n\tspin_lock(&rbd_dev_list_lock);\n\tlist_for_each(tmp, &rbd_dev_list) {\n\t\trbd_dev = list_entry(tmp, struct rbd_device, node);\n\t\tif (rbd_dev->dev_id == dev_id) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!ret) {\n\t\tspin_lock_irq(&rbd_dev->lock);\n\t\tif (rbd_dev->open_count && !force)\n\t\t\tret = -EBUSY;\n\t\telse if (test_and_set_bit(RBD_DEV_FLAG_REMOVING,\n\t\t\t\t\t  &rbd_dev->flags))\n\t\t\tret = -EINPROGRESS;\n\t\tspin_unlock_irq(&rbd_dev->lock);\n\t}\n\tspin_unlock(&rbd_dev_list_lock);\n\tif (ret)\n\t\treturn ret;\n\n\tif (force) {\n\t\t/*\n\t\t * Prevent new IO from being queued and wait for existing\n\t\t * IO to complete/fail.\n\t\t */\n\t\tblk_mq_freeze_queue(rbd_dev->disk->queue);\n\t\tblk_set_queue_dying(rbd_dev->disk->queue);\n\t}\n\n\tdel_gendisk(rbd_dev->disk);\n\tspin_lock(&rbd_dev_list_lock);\n\tlist_del_init(&rbd_dev->node);\n\tspin_unlock(&rbd_dev_list_lock);\n\tdevice_del(&rbd_dev->dev);\n\n\trbd_dev_image_unlock(rbd_dev);\n\trbd_dev_device_release(rbd_dev);\n\trbd_dev_image_release(rbd_dev);\n\trbd_dev_destroy(rbd_dev);\n\treturn count;\n}",
        "func": "static ssize_t do_rbd_remove(struct bus_type *bus,\n\t\t\t     const char *buf,\n\t\t\t     size_t count)\n{\n\tstruct rbd_device *rbd_dev = NULL;\n\tstruct list_head *tmp;\n\tint dev_id;\n\tchar opt_buf[6];\n\tbool force = false;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tdev_id = -1;\n\topt_buf[0] = '\\0';\n\tsscanf(buf, \"%d %5s\", &dev_id, opt_buf);\n\tif (dev_id < 0) {\n\t\tpr_err(\"dev_id out of range\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (opt_buf[0] != '\\0') {\n\t\tif (!strcmp(opt_buf, \"force\")) {\n\t\t\tforce = true;\n\t\t} else {\n\t\t\tpr_err(\"bad remove option at '%s'\\n\", opt_buf);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tret = -ENOENT;\n\tspin_lock(&rbd_dev_list_lock);\n\tlist_for_each(tmp, &rbd_dev_list) {\n\t\trbd_dev = list_entry(tmp, struct rbd_device, node);\n\t\tif (rbd_dev->dev_id == dev_id) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!ret) {\n\t\tspin_lock_irq(&rbd_dev->lock);\n\t\tif (rbd_dev->open_count && !force)\n\t\t\tret = -EBUSY;\n\t\telse if (test_and_set_bit(RBD_DEV_FLAG_REMOVING,\n\t\t\t\t\t  &rbd_dev->flags))\n\t\t\tret = -EINPROGRESS;\n\t\tspin_unlock_irq(&rbd_dev->lock);\n\t}\n\tspin_unlock(&rbd_dev_list_lock);\n\tif (ret)\n\t\treturn ret;\n\n\tif (force) {\n\t\t/*\n\t\t * Prevent new IO from being queued and wait for existing\n\t\t * IO to complete/fail.\n\t\t */\n\t\tblk_mq_freeze_queue(rbd_dev->disk->queue);\n\t\tblk_set_queue_dying(rbd_dev->disk->queue);\n\t}\n\n\tdel_gendisk(rbd_dev->disk);\n\tspin_lock(&rbd_dev_list_lock);\n\tlist_del_init(&rbd_dev->node);\n\tspin_unlock(&rbd_dev_list_lock);\n\tdevice_del(&rbd_dev->dev);\n\n\trbd_dev_image_unlock(rbd_dev);\n\trbd_dev_device_release(rbd_dev);\n\trbd_dev_image_release(rbd_dev);\n\trbd_dev_destroy(rbd_dev);\n\treturn count;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,9 @@\n \tchar opt_buf[6];\n \tbool force = false;\n \tint ret;\n+\n+\tif (!capable(CAP_SYS_ADMIN))\n+\t\treturn -EPERM;\n \n \tdev_id = -1;\n \topt_buf[0] = '\\0';",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (!capable(CAP_SYS_ADMIN))",
                "\t\treturn -EPERM;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-20685",
        "func_name": "openssh/openssh-portable/sink",
        "description": "In OpenSSH 7.9, scp.c in the scp client allows remote SSH servers to bypass intended access restrictions via the filename of . or an empty filename. The impact is modifying the permissions of the target directory on the client side.",
        "git_url": "https://github.com/openssh/openssh-portable/commit/6010c0303a422a9c5fa8860c061bf7105eb7f8b2",
        "commit_title": "upstream: disallow empty incoming filename or ones that refer to the",
        "commit_text": " current directory; based on report/patch from Harry Sintonen  OpenBSD-Commit-ID: f27651b30eaee2df49540ab68d030865c04f6de9",
        "func_before": "void\nsink(int argc, char **argv)\n{\n\tstatic BUF buffer;\n\tstruct stat stb;\n\tenum {\n\t\tYES, NO, DISPLAYED\n\t} wrerr;\n\tBUF *bp;\n\toff_t i;\n\tsize_t j, count;\n\tint amt, exists, first, ofd;\n\tmode_t mode, omode, mask;\n\toff_t size, statbytes;\n\tunsigned long long ull;\n\tint setimes, targisdir, wrerrno = 0;\n\tchar ch, *cp, *np, *targ, *why, *vect[1], buf[2048], visbuf[2048];\n\tstruct timeval tv[2];\n\n#define\tatime\ttv[0]\n#define\tmtime\ttv[1]\n#define\tSCREWUP(str)\t{ why = str; goto screwup; }\n\n\tif (TYPE_OVERFLOW(time_t, 0) || TYPE_OVERFLOW(off_t, 0))\n\t\tSCREWUP(\"Unexpected off_t/time_t size\");\n\n\tsetimes = targisdir = 0;\n\tmask = umask(0);\n\tif (!pflag)\n\t\t(void) umask(mask);\n\tif (argc != 1) {\n\t\trun_err(\"ambiguous target\");\n\t\texit(1);\n\t}\n\ttarg = *argv;\n\tif (targetshouldbedirectory)\n\t\tverifydir(targ);\n\n\t(void) atomicio(vwrite, remout, \"\", 1);\n\tif (stat(targ, &stb) == 0 && S_ISDIR(stb.st_mode))\n\t\ttargisdir = 1;\n\tfor (first = 1;; first = 0) {\n\t\tcp = buf;\n\t\tif (atomicio(read, remin, cp, 1) != 1)\n\t\t\treturn;\n\t\tif (*cp++ == '\\n')\n\t\t\tSCREWUP(\"unexpected <newline>\");\n\t\tdo {\n\t\t\tif (atomicio(read, remin, &ch, sizeof(ch)) != sizeof(ch))\n\t\t\t\tSCREWUP(\"lost connection\");\n\t\t\t*cp++ = ch;\n\t\t} while (cp < &buf[sizeof(buf) - 1] && ch != '\\n');\n\t\t*cp = 0;\n\t\tif (verbose_mode)\n\t\t\tfmprintf(stderr, \"Sink: %s\", buf);\n\n\t\tif (buf[0] == '\\01' || buf[0] == '\\02') {\n\t\t\tif (iamremote == 0) {\n\t\t\t\t(void) snmprintf(visbuf, sizeof(visbuf),\n\t\t\t\t    NULL, \"%s\", buf + 1);\n\t\t\t\t(void) atomicio(vwrite, STDERR_FILENO,\n\t\t\t\t    visbuf, strlen(visbuf));\n\t\t\t}\n\t\t\tif (buf[0] == '\\02')\n\t\t\t\texit(1);\n\t\t\t++errs;\n\t\t\tcontinue;\n\t\t}\n\t\tif (buf[0] == 'E') {\n\t\t\t(void) atomicio(vwrite, remout, \"\", 1);\n\t\t\treturn;\n\t\t}\n\t\tif (ch == '\\n')\n\t\t\t*--cp = 0;\n\n\t\tcp = buf;\n\t\tif (*cp == 'T') {\n\t\t\tsetimes++;\n\t\t\tcp++;\n\t\t\tif (!isdigit((unsigned char)*cp))\n\t\t\t\tSCREWUP(\"mtime.sec not present\");\n\t\t\tull = strtoull(cp, &cp, 10);\n\t\t\tif (!cp || *cp++ != ' ')\n\t\t\t\tSCREWUP(\"mtime.sec not delimited\");\n\t\t\tif (TYPE_OVERFLOW(time_t, ull))\n\t\t\t\tsetimes = 0;\t/* out of range */\n\t\t\tmtime.tv_sec = ull;\n\t\t\tmtime.tv_usec = strtol(cp, &cp, 10);\n\t\t\tif (!cp || *cp++ != ' ' || mtime.tv_usec < 0 ||\n\t\t\t    mtime.tv_usec > 999999)\n\t\t\t\tSCREWUP(\"mtime.usec not delimited\");\n\t\t\tif (!isdigit((unsigned char)*cp))\n\t\t\t\tSCREWUP(\"atime.sec not present\");\n\t\t\tull = strtoull(cp, &cp, 10);\n\t\t\tif (!cp || *cp++ != ' ')\n\t\t\t\tSCREWUP(\"atime.sec not delimited\");\n\t\t\tif (TYPE_OVERFLOW(time_t, ull))\n\t\t\t\tsetimes = 0;\t/* out of range */\n\t\t\tatime.tv_sec = ull;\n\t\t\tatime.tv_usec = strtol(cp, &cp, 10);\n\t\t\tif (!cp || *cp++ != '\\0' || atime.tv_usec < 0 ||\n\t\t\t    atime.tv_usec > 999999)\n\t\t\t\tSCREWUP(\"atime.usec not delimited\");\n\t\t\t(void) atomicio(vwrite, remout, \"\", 1);\n\t\t\tcontinue;\n\t\t}\n\t\tif (*cp != 'C' && *cp != 'D') {\n\t\t\t/*\n\t\t\t * Check for the case \"rcp remote:foo\\* local:bar\".\n\t\t\t * In this case, the line \"No match.\" can be returned\n\t\t\t * by the shell before the rcp command on the remote is\n\t\t\t * executed so the ^Aerror_message convention isn't\n\t\t\t * followed.\n\t\t\t */\n\t\t\tif (first) {\n\t\t\t\trun_err(\"%s\", cp);\n\t\t\t\texit(1);\n\t\t\t}\n\t\t\tSCREWUP(\"expected control record\");\n\t\t}\n\t\tmode = 0;\n\t\tfor (++cp; cp < buf + 5; cp++) {\n\t\t\tif (*cp < '0' || *cp > '7')\n\t\t\t\tSCREWUP(\"bad mode\");\n\t\t\tmode = (mode << 3) | (*cp - '0');\n\t\t}\n\t\tif (!pflag)\n\t\t\tmode &= ~mask;\n\t\tif (*cp++ != ' ')\n\t\t\tSCREWUP(\"mode not delimited\");\n\n\t\tif (!isdigit((unsigned char)*cp))\n\t\t\tSCREWUP(\"size not present\");\n\t\tull = strtoull(cp, &cp, 10);\n\t\tif (!cp || *cp++ != ' ')\n\t\t\tSCREWUP(\"size not delimited\");\n\t\tif (TYPE_OVERFLOW(off_t, ull))\n\t\t\tSCREWUP(\"size out of range\");\n\t\tsize = (off_t)ull;\n\n\t\tif ((strchr(cp, '/') != NULL) || (strcmp(cp, \"..\") == 0)) {\n\t\t\trun_err(\"error: unexpected filename: %s\", cp);\n\t\t\texit(1);\n\t\t}\n\t\tif (targisdir) {\n\t\t\tstatic char *namebuf;\n\t\t\tstatic size_t cursize;\n\t\t\tsize_t need;\n\n\t\t\tneed = strlen(targ) + strlen(cp) + 250;\n\t\t\tif (need > cursize) {\n\t\t\t\tfree(namebuf);\n\t\t\t\tnamebuf = xmalloc(need);\n\t\t\t\tcursize = need;\n\t\t\t}\n\t\t\t(void) snprintf(namebuf, need, \"%s%s%s\", targ,\n\t\t\t    strcmp(targ, \"/\") ? \"/\" : \"\", cp);\n\t\t\tnp = namebuf;\n\t\t} else\n\t\t\tnp = targ;\n\t\tcurfile = cp;\n\t\texists = stat(np, &stb) == 0;\n\t\tif (buf[0] == 'D') {\n\t\t\tint mod_flag = pflag;\n\t\t\tif (!iamrecursive)\n\t\t\t\tSCREWUP(\"received directory without -r\");\n\t\t\tif (exists) {\n\t\t\t\tif (!S_ISDIR(stb.st_mode)) {\n\t\t\t\t\terrno = ENOTDIR;\n\t\t\t\t\tgoto bad;\n\t\t\t\t}\n\t\t\t\tif (pflag)\n\t\t\t\t\t(void) chmod(np, mode);\n\t\t\t} else {\n\t\t\t\t/* Handle copying from a read-only\n\t\t\t\t   directory */\n\t\t\t\tmod_flag = 1;\n\t\t\t\tif (mkdir(np, mode | S_IRWXU) < 0)\n\t\t\t\t\tgoto bad;\n\t\t\t}\n\t\t\tvect[0] = xstrdup(np);\n\t\t\tsink(1, vect);\n\t\t\tif (setimes) {\n\t\t\t\tsetimes = 0;\n\t\t\t\tif (utimes(vect[0], tv) < 0)\n\t\t\t\t\trun_err(\"%s: set times: %s\",\n\t\t\t\t\t    vect[0], strerror(errno));\n\t\t\t}\n\t\t\tif (mod_flag)\n\t\t\t\t(void) chmod(vect[0], mode);\n\t\t\tfree(vect[0]);\n\t\t\tcontinue;\n\t\t}\n\t\tomode = mode;\n\t\tmode |= S_IWUSR;\n\t\tif ((ofd = open(np, O_WRONLY|O_CREAT, mode)) < 0) {\nbad:\t\t\trun_err(\"%s: %s\", np, strerror(errno));\n\t\t\tcontinue;\n\t\t}\n\t\t(void) atomicio(vwrite, remout, \"\", 1);\n\t\tif ((bp = allocbuf(&buffer, ofd, COPY_BUFLEN)) == NULL) {\n\t\t\t(void) close(ofd);\n\t\t\tcontinue;\n\t\t}\n\t\tcp = bp->buf;\n\t\twrerr = NO;\n\n\t\tstatbytes = 0;\n\t\tif (showprogress)\n\t\t\tstart_progress_meter(curfile, size, &statbytes);\n\t\tset_nonblock(remin);\n\t\tfor (count = i = 0; i < size; i += bp->cnt) {\n\t\t\tamt = bp->cnt;\n\t\t\tif (i + amt > size)\n\t\t\t\tamt = size - i;\n\t\t\tcount += amt;\n\t\t\tdo {\n\t\t\t\tj = atomicio6(read, remin, cp, amt,\n\t\t\t\t    scpio, &statbytes);\n\t\t\t\tif (j == 0) {\n\t\t\t\t\trun_err(\"%s\", j != EPIPE ?\n\t\t\t\t\t    strerror(errno) :\n\t\t\t\t\t    \"dropped connection\");\n\t\t\t\t\texit(1);\n\t\t\t\t}\n\t\t\t\tamt -= j;\n\t\t\t\tcp += j;\n\t\t\t} while (amt > 0);\n\n\t\t\tif (count == bp->cnt) {\n\t\t\t\t/* Keep reading so we stay sync'd up. */\n\t\t\t\tif (wrerr == NO) {\n\t\t\t\t\tif (atomicio(vwrite, ofd, bp->buf,\n\t\t\t\t\t    count) != count) {\n\t\t\t\t\t\twrerr = YES;\n\t\t\t\t\t\twrerrno = errno;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tcount = 0;\n\t\t\t\tcp = bp->buf;\n\t\t\t}\n\t\t}\n\t\tunset_nonblock(remin);\n\t\tif (count != 0 && wrerr == NO &&\n\t\t    atomicio(vwrite, ofd, bp->buf, count) != count) {\n\t\t\twrerr = YES;\n\t\t\twrerrno = errno;\n\t\t}\n\t\tif (wrerr == NO && (!exists || S_ISREG(stb.st_mode)) &&\n\t\t    ftruncate(ofd, size) != 0) {\n\t\t\trun_err(\"%s: truncate: %s\", np, strerror(errno));\n\t\t\twrerr = DISPLAYED;\n\t\t}\n\t\tif (pflag) {\n\t\t\tif (exists || omode != mode)\n#ifdef HAVE_FCHMOD\n\t\t\t\tif (fchmod(ofd, omode)) {\n#else /* HAVE_FCHMOD */\n\t\t\t\tif (chmod(np, omode)) {\n#endif /* HAVE_FCHMOD */\n\t\t\t\t\trun_err(\"%s: set mode: %s\",\n\t\t\t\t\t    np, strerror(errno));\n\t\t\t\t\twrerr = DISPLAYED;\n\t\t\t\t}\n\t\t} else {\n\t\t\tif (!exists && omode != mode)\n#ifdef HAVE_FCHMOD\n\t\t\t\tif (fchmod(ofd, omode & ~mask)) {\n#else /* HAVE_FCHMOD */\n\t\t\t\tif (chmod(np, omode & ~mask)) {\n#endif /* HAVE_FCHMOD */\n\t\t\t\t\trun_err(\"%s: set mode: %s\",\n\t\t\t\t\t    np, strerror(errno));\n\t\t\t\t\twrerr = DISPLAYED;\n\t\t\t\t}\n\t\t}\n\t\tif (close(ofd) == -1) {\n\t\t\twrerr = YES;\n\t\t\twrerrno = errno;\n\t\t}\n\t\t(void) response();\n\t\tif (showprogress)\n\t\t\tstop_progress_meter();\n\t\tif (setimes && wrerr == NO) {\n\t\t\tsetimes = 0;\n\t\t\tif (utimes(np, tv) < 0) {\n\t\t\t\trun_err(\"%s: set times: %s\",\n\t\t\t\t    np, strerror(errno));\n\t\t\t\twrerr = DISPLAYED;\n\t\t\t}\n\t\t}\n\t\tswitch (wrerr) {\n\t\tcase YES:\n\t\t\trun_err(\"%s: %s\", np, strerror(wrerrno));\n\t\t\tbreak;\n\t\tcase NO:\n\t\t\t(void) atomicio(vwrite, remout, \"\", 1);\n\t\t\tbreak;\n\t\tcase DISPLAYED:\n\t\t\tbreak;\n\t\t}\n\t}\nscrewup:\n\trun_err(\"protocol error: %s\", why);\n\texit(1);\n}",
        "func": "void\nsink(int argc, char **argv)\n{\n\tstatic BUF buffer;\n\tstruct stat stb;\n\tenum {\n\t\tYES, NO, DISPLAYED\n\t} wrerr;\n\tBUF *bp;\n\toff_t i;\n\tsize_t j, count;\n\tint amt, exists, first, ofd;\n\tmode_t mode, omode, mask;\n\toff_t size, statbytes;\n\tunsigned long long ull;\n\tint setimes, targisdir, wrerrno = 0;\n\tchar ch, *cp, *np, *targ, *why, *vect[1], buf[2048], visbuf[2048];\n\tstruct timeval tv[2];\n\n#define\tatime\ttv[0]\n#define\tmtime\ttv[1]\n#define\tSCREWUP(str)\t{ why = str; goto screwup; }\n\n\tif (TYPE_OVERFLOW(time_t, 0) || TYPE_OVERFLOW(off_t, 0))\n\t\tSCREWUP(\"Unexpected off_t/time_t size\");\n\n\tsetimes = targisdir = 0;\n\tmask = umask(0);\n\tif (!pflag)\n\t\t(void) umask(mask);\n\tif (argc != 1) {\n\t\trun_err(\"ambiguous target\");\n\t\texit(1);\n\t}\n\ttarg = *argv;\n\tif (targetshouldbedirectory)\n\t\tverifydir(targ);\n\n\t(void) atomicio(vwrite, remout, \"\", 1);\n\tif (stat(targ, &stb) == 0 && S_ISDIR(stb.st_mode))\n\t\ttargisdir = 1;\n\tfor (first = 1;; first = 0) {\n\t\tcp = buf;\n\t\tif (atomicio(read, remin, cp, 1) != 1)\n\t\t\treturn;\n\t\tif (*cp++ == '\\n')\n\t\t\tSCREWUP(\"unexpected <newline>\");\n\t\tdo {\n\t\t\tif (atomicio(read, remin, &ch, sizeof(ch)) != sizeof(ch))\n\t\t\t\tSCREWUP(\"lost connection\");\n\t\t\t*cp++ = ch;\n\t\t} while (cp < &buf[sizeof(buf) - 1] && ch != '\\n');\n\t\t*cp = 0;\n\t\tif (verbose_mode)\n\t\t\tfmprintf(stderr, \"Sink: %s\", buf);\n\n\t\tif (buf[0] == '\\01' || buf[0] == '\\02') {\n\t\t\tif (iamremote == 0) {\n\t\t\t\t(void) snmprintf(visbuf, sizeof(visbuf),\n\t\t\t\t    NULL, \"%s\", buf + 1);\n\t\t\t\t(void) atomicio(vwrite, STDERR_FILENO,\n\t\t\t\t    visbuf, strlen(visbuf));\n\t\t\t}\n\t\t\tif (buf[0] == '\\02')\n\t\t\t\texit(1);\n\t\t\t++errs;\n\t\t\tcontinue;\n\t\t}\n\t\tif (buf[0] == 'E') {\n\t\t\t(void) atomicio(vwrite, remout, \"\", 1);\n\t\t\treturn;\n\t\t}\n\t\tif (ch == '\\n')\n\t\t\t*--cp = 0;\n\n\t\tcp = buf;\n\t\tif (*cp == 'T') {\n\t\t\tsetimes++;\n\t\t\tcp++;\n\t\t\tif (!isdigit((unsigned char)*cp))\n\t\t\t\tSCREWUP(\"mtime.sec not present\");\n\t\t\tull = strtoull(cp, &cp, 10);\n\t\t\tif (!cp || *cp++ != ' ')\n\t\t\t\tSCREWUP(\"mtime.sec not delimited\");\n\t\t\tif (TYPE_OVERFLOW(time_t, ull))\n\t\t\t\tsetimes = 0;\t/* out of range */\n\t\t\tmtime.tv_sec = ull;\n\t\t\tmtime.tv_usec = strtol(cp, &cp, 10);\n\t\t\tif (!cp || *cp++ != ' ' || mtime.tv_usec < 0 ||\n\t\t\t    mtime.tv_usec > 999999)\n\t\t\t\tSCREWUP(\"mtime.usec not delimited\");\n\t\t\tif (!isdigit((unsigned char)*cp))\n\t\t\t\tSCREWUP(\"atime.sec not present\");\n\t\t\tull = strtoull(cp, &cp, 10);\n\t\t\tif (!cp || *cp++ != ' ')\n\t\t\t\tSCREWUP(\"atime.sec not delimited\");\n\t\t\tif (TYPE_OVERFLOW(time_t, ull))\n\t\t\t\tsetimes = 0;\t/* out of range */\n\t\t\tatime.tv_sec = ull;\n\t\t\tatime.tv_usec = strtol(cp, &cp, 10);\n\t\t\tif (!cp || *cp++ != '\\0' || atime.tv_usec < 0 ||\n\t\t\t    atime.tv_usec > 999999)\n\t\t\t\tSCREWUP(\"atime.usec not delimited\");\n\t\t\t(void) atomicio(vwrite, remout, \"\", 1);\n\t\t\tcontinue;\n\t\t}\n\t\tif (*cp != 'C' && *cp != 'D') {\n\t\t\t/*\n\t\t\t * Check for the case \"rcp remote:foo\\* local:bar\".\n\t\t\t * In this case, the line \"No match.\" can be returned\n\t\t\t * by the shell before the rcp command on the remote is\n\t\t\t * executed so the ^Aerror_message convention isn't\n\t\t\t * followed.\n\t\t\t */\n\t\t\tif (first) {\n\t\t\t\trun_err(\"%s\", cp);\n\t\t\t\texit(1);\n\t\t\t}\n\t\t\tSCREWUP(\"expected control record\");\n\t\t}\n\t\tmode = 0;\n\t\tfor (++cp; cp < buf + 5; cp++) {\n\t\t\tif (*cp < '0' || *cp > '7')\n\t\t\t\tSCREWUP(\"bad mode\");\n\t\t\tmode = (mode << 3) | (*cp - '0');\n\t\t}\n\t\tif (!pflag)\n\t\t\tmode &= ~mask;\n\t\tif (*cp++ != ' ')\n\t\t\tSCREWUP(\"mode not delimited\");\n\n\t\tif (!isdigit((unsigned char)*cp))\n\t\t\tSCREWUP(\"size not present\");\n\t\tull = strtoull(cp, &cp, 10);\n\t\tif (!cp || *cp++ != ' ')\n\t\t\tSCREWUP(\"size not delimited\");\n\t\tif (TYPE_OVERFLOW(off_t, ull))\n\t\t\tSCREWUP(\"size out of range\");\n\t\tsize = (off_t)ull;\n\n\t\tif (*cp == '\\0' || strchr(cp, '/') != NULL ||\n\t\t    strcmp(cp, \".\") == 0 || strcmp(cp, \"..\") == 0) {\n\t\t\trun_err(\"error: unexpected filename: %s\", cp);\n\t\t\texit(1);\n\t\t}\n\t\tif (targisdir) {\n\t\t\tstatic char *namebuf;\n\t\t\tstatic size_t cursize;\n\t\t\tsize_t need;\n\n\t\t\tneed = strlen(targ) + strlen(cp) + 250;\n\t\t\tif (need > cursize) {\n\t\t\t\tfree(namebuf);\n\t\t\t\tnamebuf = xmalloc(need);\n\t\t\t\tcursize = need;\n\t\t\t}\n\t\t\t(void) snprintf(namebuf, need, \"%s%s%s\", targ,\n\t\t\t    strcmp(targ, \"/\") ? \"/\" : \"\", cp);\n\t\t\tnp = namebuf;\n\t\t} else\n\t\t\tnp = targ;\n\t\tcurfile = cp;\n\t\texists = stat(np, &stb) == 0;\n\t\tif (buf[0] == 'D') {\n\t\t\tint mod_flag = pflag;\n\t\t\tif (!iamrecursive)\n\t\t\t\tSCREWUP(\"received directory without -r\");\n\t\t\tif (exists) {\n\t\t\t\tif (!S_ISDIR(stb.st_mode)) {\n\t\t\t\t\terrno = ENOTDIR;\n\t\t\t\t\tgoto bad;\n\t\t\t\t}\n\t\t\t\tif (pflag)\n\t\t\t\t\t(void) chmod(np, mode);\n\t\t\t} else {\n\t\t\t\t/* Handle copying from a read-only\n\t\t\t\t   directory */\n\t\t\t\tmod_flag = 1;\n\t\t\t\tif (mkdir(np, mode | S_IRWXU) < 0)\n\t\t\t\t\tgoto bad;\n\t\t\t}\n\t\t\tvect[0] = xstrdup(np);\n\t\t\tsink(1, vect);\n\t\t\tif (setimes) {\n\t\t\t\tsetimes = 0;\n\t\t\t\tif (utimes(vect[0], tv) < 0)\n\t\t\t\t\trun_err(\"%s: set times: %s\",\n\t\t\t\t\t    vect[0], strerror(errno));\n\t\t\t}\n\t\t\tif (mod_flag)\n\t\t\t\t(void) chmod(vect[0], mode);\n\t\t\tfree(vect[0]);\n\t\t\tcontinue;\n\t\t}\n\t\tomode = mode;\n\t\tmode |= S_IWUSR;\n\t\tif ((ofd = open(np, O_WRONLY|O_CREAT, mode)) < 0) {\nbad:\t\t\trun_err(\"%s: %s\", np, strerror(errno));\n\t\t\tcontinue;\n\t\t}\n\t\t(void) atomicio(vwrite, remout, \"\", 1);\n\t\tif ((bp = allocbuf(&buffer, ofd, COPY_BUFLEN)) == NULL) {\n\t\t\t(void) close(ofd);\n\t\t\tcontinue;\n\t\t}\n\t\tcp = bp->buf;\n\t\twrerr = NO;\n\n\t\tstatbytes = 0;\n\t\tif (showprogress)\n\t\t\tstart_progress_meter(curfile, size, &statbytes);\n\t\tset_nonblock(remin);\n\t\tfor (count = i = 0; i < size; i += bp->cnt) {\n\t\t\tamt = bp->cnt;\n\t\t\tif (i + amt > size)\n\t\t\t\tamt = size - i;\n\t\t\tcount += amt;\n\t\t\tdo {\n\t\t\t\tj = atomicio6(read, remin, cp, amt,\n\t\t\t\t    scpio, &statbytes);\n\t\t\t\tif (j == 0) {\n\t\t\t\t\trun_err(\"%s\", j != EPIPE ?\n\t\t\t\t\t    strerror(errno) :\n\t\t\t\t\t    \"dropped connection\");\n\t\t\t\t\texit(1);\n\t\t\t\t}\n\t\t\t\tamt -= j;\n\t\t\t\tcp += j;\n\t\t\t} while (amt > 0);\n\n\t\t\tif (count == bp->cnt) {\n\t\t\t\t/* Keep reading so we stay sync'd up. */\n\t\t\t\tif (wrerr == NO) {\n\t\t\t\t\tif (atomicio(vwrite, ofd, bp->buf,\n\t\t\t\t\t    count) != count) {\n\t\t\t\t\t\twrerr = YES;\n\t\t\t\t\t\twrerrno = errno;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tcount = 0;\n\t\t\t\tcp = bp->buf;\n\t\t\t}\n\t\t}\n\t\tunset_nonblock(remin);\n\t\tif (count != 0 && wrerr == NO &&\n\t\t    atomicio(vwrite, ofd, bp->buf, count) != count) {\n\t\t\twrerr = YES;\n\t\t\twrerrno = errno;\n\t\t}\n\t\tif (wrerr == NO && (!exists || S_ISREG(stb.st_mode)) &&\n\t\t    ftruncate(ofd, size) != 0) {\n\t\t\trun_err(\"%s: truncate: %s\", np, strerror(errno));\n\t\t\twrerr = DISPLAYED;\n\t\t}\n\t\tif (pflag) {\n\t\t\tif (exists || omode != mode)\n#ifdef HAVE_FCHMOD\n\t\t\t\tif (fchmod(ofd, omode)) {\n#else /* HAVE_FCHMOD */\n\t\t\t\tif (chmod(np, omode)) {\n#endif /* HAVE_FCHMOD */\n\t\t\t\t\trun_err(\"%s: set mode: %s\",\n\t\t\t\t\t    np, strerror(errno));\n\t\t\t\t\twrerr = DISPLAYED;\n\t\t\t\t}\n\t\t} else {\n\t\t\tif (!exists && omode != mode)\n#ifdef HAVE_FCHMOD\n\t\t\t\tif (fchmod(ofd, omode & ~mask)) {\n#else /* HAVE_FCHMOD */\n\t\t\t\tif (chmod(np, omode & ~mask)) {\n#endif /* HAVE_FCHMOD */\n\t\t\t\t\trun_err(\"%s: set mode: %s\",\n\t\t\t\t\t    np, strerror(errno));\n\t\t\t\t\twrerr = DISPLAYED;\n\t\t\t\t}\n\t\t}\n\t\tif (close(ofd) == -1) {\n\t\t\twrerr = YES;\n\t\t\twrerrno = errno;\n\t\t}\n\t\t(void) response();\n\t\tif (showprogress)\n\t\t\tstop_progress_meter();\n\t\tif (setimes && wrerr == NO) {\n\t\t\tsetimes = 0;\n\t\t\tif (utimes(np, tv) < 0) {\n\t\t\t\trun_err(\"%s: set times: %s\",\n\t\t\t\t    np, strerror(errno));\n\t\t\t\twrerr = DISPLAYED;\n\t\t\t}\n\t\t}\n\t\tswitch (wrerr) {\n\t\tcase YES:\n\t\t\trun_err(\"%s: %s\", np, strerror(wrerrno));\n\t\t\tbreak;\n\t\tcase NO:\n\t\t\t(void) atomicio(vwrite, remout, \"\", 1);\n\t\t\tbreak;\n\t\tcase DISPLAYED:\n\t\t\tbreak;\n\t\t}\n\t}\nscrewup:\n\trun_err(\"protocol error: %s\", why);\n\texit(1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -138,7 +138,8 @@\n \t\t\tSCREWUP(\"size out of range\");\n \t\tsize = (off_t)ull;\n \n-\t\tif ((strchr(cp, '/') != NULL) || (strcmp(cp, \"..\") == 0)) {\n+\t\tif (*cp == '\\0' || strchr(cp, '/') != NULL ||\n+\t\t    strcmp(cp, \".\") == 0 || strcmp(cp, \"..\") == 0) {\n \t\t\trun_err(\"error: unexpected filename: %s\", cp);\n \t\t\texit(1);\n \t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif ((strchr(cp, '/') != NULL) || (strcmp(cp, \"..\") == 0)) {"
            ],
            "added_lines": [
                "\t\tif (*cp == '\\0' || strchr(cp, '/') != NULL ||",
                "\t\t    strcmp(cp, \".\") == 0 || strcmp(cp, \"..\") == 0) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-3827",
        "func_name": "GNOME/gvfs/check_permission",
        "description": "An incorrect permission check in the admin backend in gvfs before version 1.39.4 was found that allows reading and modify arbitrary files by privileged users without asking for password when no authentication agent is running. This vulnerability can be exploited by malicious programs running under privileges of users belonging to the wheel group to further escalate its privileges by modifying system files without user's knowledge. Successful exploitation requires uncommon system configuration.",
        "git_url": "https://github.com/GNOME/gvfs/commit/d8d0c8c40049cfd824b2b90d0cd47914052b9811",
        "commit_title": "admin: Prevent access if any authentication agent isn't available",
        "commit_text": " The backend currently allows to access and modify files without prompting for password if any polkit authentication agent isn't available. This seems isn't usually problem, because polkit agents are integral parts of graphical environments / linux distributions. The agents can't be simply disabled without root permissions and are automatically respawned. However, this might be a problem in some non-standard cases.  This affects only users which belong to wheel group (i.e. those who are already allowed to use sudo). It doesn't allow privilege escalation for users, who don't belong to that group.  Let's return permission denied error also when the subject can't be authorized by any polkit agent to prevent this behavior.  Closes: https://gitlab.gnome.org/GNOME/gvfs/issues/355",
        "func_before": "static gboolean\ncheck_permission (GVfsBackendAdmin *self,\n                  GVfsJob *job)\n{\n  GVfsJobDBus *dbus_job = G_VFS_JOB_DBUS (job);\n  GError *error = NULL;\n  GDBusMethodInvocation *invocation;\n  GDBusConnection *connection;\n  GCredentials *credentials;\n  pid_t pid;\n  uid_t uid;\n  PolkitSubject *subject;\n  PolkitAuthorizationResult *result;\n  gboolean is_authorized;\n\n  invocation = dbus_job->invocation;\n  connection = g_dbus_method_invocation_get_connection (invocation);\n  credentials = g_dbus_connection_get_peer_credentials (connection);\n\n  pid = g_credentials_get_unix_pid (credentials, &error);\n  if (error != NULL)\n    {\n      g_vfs_job_failed_from_error (job, error);\n      g_error_free (error);\n      return FALSE;\n    }\n\n  uid = g_credentials_get_unix_user (credentials, &error);\n  if (error != NULL)\n    {\n      g_vfs_job_failed_from_error (job, error);\n      g_error_free (error);\n      return FALSE;\n    }\n\n  /* Only one polkit dialog at a time */\n  g_mutex_lock (&self->polkit_mutex);\n\n  subject = polkit_unix_process_new_for_owner (pid, 0, uid);\n  result = polkit_authority_check_authorization_sync (self->authority,\n                                                      subject,\n                                                      \"org.gtk.vfs.file-operations\",\n                                                      NULL, POLKIT_CHECK_AUTHORIZATION_FLAGS_ALLOW_USER_INTERACTION,\n                                                      NULL, &error);\n  g_object_unref (subject);\n\n  g_mutex_unlock (&self->polkit_mutex);\n\n  if (error != NULL)\n    {\n      g_vfs_job_failed_from_error (job, error);\n      g_error_free (error);\n      return FALSE;\n    }\n\n  is_authorized = polkit_authorization_result_get_is_authorized (result) ||\n    polkit_authorization_result_get_is_challenge (result);\n\n  g_object_unref (result);\n\n  if (!is_authorized)\n    g_vfs_job_failed_literal (job, G_IO_ERROR, G_IO_ERROR_PERMISSION_DENIED,\n                              _(\"Permission denied\"));\n\n  return is_authorized;\n}",
        "func": "static gboolean\ncheck_permission (GVfsBackendAdmin *self,\n                  GVfsJob *job)\n{\n  GVfsJobDBus *dbus_job = G_VFS_JOB_DBUS (job);\n  GError *error = NULL;\n  GDBusMethodInvocation *invocation;\n  GDBusConnection *connection;\n  GCredentials *credentials;\n  pid_t pid;\n  uid_t uid;\n  PolkitSubject *subject;\n  PolkitAuthorizationResult *result;\n  gboolean is_authorized;\n\n  invocation = dbus_job->invocation;\n  connection = g_dbus_method_invocation_get_connection (invocation);\n  credentials = g_dbus_connection_get_peer_credentials (connection);\n\n  pid = g_credentials_get_unix_pid (credentials, &error);\n  if (error != NULL)\n    {\n      g_vfs_job_failed_from_error (job, error);\n      g_error_free (error);\n      return FALSE;\n    }\n\n  uid = g_credentials_get_unix_user (credentials, &error);\n  if (error != NULL)\n    {\n      g_vfs_job_failed_from_error (job, error);\n      g_error_free (error);\n      return FALSE;\n    }\n\n  /* Only one polkit dialog at a time */\n  g_mutex_lock (&self->polkit_mutex);\n\n  subject = polkit_unix_process_new_for_owner (pid, 0, uid);\n  result = polkit_authority_check_authorization_sync (self->authority,\n                                                      subject,\n                                                      \"org.gtk.vfs.file-operations\",\n                                                      NULL, POLKIT_CHECK_AUTHORIZATION_FLAGS_ALLOW_USER_INTERACTION,\n                                                      NULL, &error);\n  g_object_unref (subject);\n\n  g_mutex_unlock (&self->polkit_mutex);\n\n  if (error != NULL)\n    {\n      g_vfs_job_failed_from_error (job, error);\n      g_error_free (error);\n      return FALSE;\n    }\n\n  is_authorized = polkit_authorization_result_get_is_authorized (result);\n\n  g_object_unref (result);\n\n  if (!is_authorized)\n    g_vfs_job_failed_literal (job, G_IO_ERROR, G_IO_ERROR_PERMISSION_DENIED,\n                              _(\"Permission denied\"));\n\n  return is_authorized;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -53,8 +53,7 @@\n       return FALSE;\n     }\n \n-  is_authorized = polkit_authorization_result_get_is_authorized (result) ||\n-    polkit_authorization_result_get_is_challenge (result);\n+  is_authorized = polkit_authorization_result_get_is_authorized (result);\n \n   g_object_unref (result);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "  is_authorized = polkit_authorization_result_get_is_authorized (result) ||",
                "    polkit_authorization_result_get_is_challenge (result);"
            ],
            "added_lines": [
                "  is_authorized = polkit_authorization_result_get_is_authorized (result);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-25058",
        "func_name": "USBGuard/usbguard/DBusBridge::handleRootMethodCall",
        "description": "An issue was discovered in USBGuard before 1.1.0. On systems with the usbguard-dbus daemon running, an unprivileged user could make USBGuard allow all USB devices to be connected in the future.",
        "git_url": "https://github.com/USBGuard/usbguard/commit/df5f01c6ed0c20d269f7239901d21883cc871bbb",
        "commit_title": "dbus: Add missing checks for authorization using Polkit",
        "commit_text": "",
        "func_before": "void DBusBridge::handleRootMethodCall(const std::string& method_name, GVariant* parameters, GDBusMethodInvocation* invocation)\n  {\n    if (method_name == \"getParameter\") {\n      const char* name_cstr = nullptr;\n      g_variant_get(parameters, \"(&s)\", &name_cstr);\n      std::string name(name_cstr);\n      auto value = getParameter(name);\n      g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(s)\", value.c_str()));\n      return;\n    }\n\n    if (method_name == \"setParameter\") {\n      const char* name_cstr = nullptr;\n      const char* value_cstr = nullptr;\n      g_variant_get(parameters, \"(&s&s)\", &name_cstr, &value_cstr);\n      const std::string name(name_cstr);\n      const std::string value(value_cstr);\n      auto previous_value = setParameter(name, value);\n      g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(s)\", previous_value.c_str()));\n      return;\n    }\n\n    g_dbus_method_invocation_return_error(invocation, G_DBUS_ERROR,\n      G_DBUS_ERROR_UNKNOWN_METHOD, \"Unknown method interface\");\n    return;\n  }",
        "func": "void DBusBridge::handleRootMethodCall(const std::string& method_name, GVariant* parameters, GDBusMethodInvocation* invocation)\n  {\n    if (method_name == \"getParameter\") {\n      if (! isAuthorizedByPolkit(invocation)) {\n        return;\n      }\n\n      const char* name_cstr = nullptr;\n      g_variant_get(parameters, \"(&s)\", &name_cstr);\n      std::string name(name_cstr);\n      auto value = getParameter(name);\n      g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(s)\", value.c_str()));\n      return;\n    }\n\n    if (method_name == \"setParameter\") {\n      if (! isAuthorizedByPolkit(invocation)) {\n        return;\n      }\n\n      const char* name_cstr = nullptr;\n      const char* value_cstr = nullptr;\n      g_variant_get(parameters, \"(&s&s)\", &name_cstr, &value_cstr);\n      const std::string name(name_cstr);\n      const std::string value(value_cstr);\n      auto previous_value = setParameter(name, value);\n      g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(s)\", previous_value.c_str()));\n      return;\n    }\n\n    g_dbus_method_invocation_return_error(invocation, G_DBUS_ERROR,\n      G_DBUS_ERROR_UNKNOWN_METHOD, \"Unknown method interface\");\n    return;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,10 @@\n void DBusBridge::handleRootMethodCall(const std::string& method_name, GVariant* parameters, GDBusMethodInvocation* invocation)\n   {\n     if (method_name == \"getParameter\") {\n+      if (! isAuthorizedByPolkit(invocation)) {\n+        return;\n+      }\n+\n       const char* name_cstr = nullptr;\n       g_variant_get(parameters, \"(&s)\", &name_cstr);\n       std::string name(name_cstr);\n@@ -10,6 +14,10 @@\n     }\n \n     if (method_name == \"setParameter\") {\n+      if (! isAuthorizedByPolkit(invocation)) {\n+        return;\n+      }\n+\n       const char* name_cstr = nullptr;\n       const char* value_cstr = nullptr;\n       g_variant_get(parameters, \"(&s&s)\", &name_cstr, &value_cstr);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "      if (! isAuthorizedByPolkit(invocation)) {",
                "        return;",
                "      }",
                "",
                "      if (! isAuthorizedByPolkit(invocation)) {",
                "        return;",
                "      }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2019-25058",
        "func_name": "USBGuard/usbguard/DBusBridge::handleDevicesMethodCall",
        "description": "An issue was discovered in USBGuard before 1.1.0. On systems with the usbguard-dbus daemon running, an unprivileged user could make USBGuard allow all USB devices to be connected in the future.",
        "git_url": "https://github.com/USBGuard/usbguard/commit/df5f01c6ed0c20d269f7239901d21883cc871bbb",
        "commit_title": "dbus: Add missing checks for authorization using Polkit",
        "commit_text": "",
        "func_before": "void DBusBridge::handleDevicesMethodCall(const std::string& method_name, GVariant* parameters,\n    GDBusMethodInvocation* invocation)\n  {\n    USBGUARD_LOG(Debug) << \"dbus devices method call: \" << method_name;\n\n    if (method_name == \"listDevices\") {\n      const char* query_cstr = nullptr;\n      g_variant_get(parameters, \"(&s)\", &query_cstr);\n      std::string query(query_cstr);\n      auto devices = listDevices(query);\n\n      if (devices.size() > 0) {\n        auto gvbuilder = g_variant_builder_new(G_VARIANT_TYPE_ARRAY);\n\n        try {\n          for (auto device_rule : devices) {\n            g_variant_builder_add(gvbuilder, \"(us)\",\n              device_rule.getRuleID(),\n              device_rule.toString().c_str());\n          }\n\n          g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(a(us))\", gvbuilder));\n        }\n        catch (...) {\n          g_variant_builder_unref(gvbuilder);\n          throw;\n        }\n\n        g_variant_builder_unref(gvbuilder);\n      }\n      else {\n        g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(a(us))\", nullptr));\n      }\n\n      return;\n    }\n\n    if (method_name == \"applyDevicePolicy\") {\n      uint32_t device_id = 0;\n      uint32_t target_integer = 0;\n      gboolean permanent = false;\n      g_variant_get(parameters, \"(uub)\", &device_id, &target_integer, &permanent);\n      USBGUARD_LOG(Debug) << \"DBus: applyDevicePolicy: Parsed device_id: \" << device_id << \" target_integer: \" << target_integer <<\n        \" and permanent: \" << permanent;\n      const Rule::Target target = Rule::targetFromInteger(target_integer);\n      const uint32_t rule_id = applyDevicePolicy(device_id, target, permanent);\n      g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(u)\", rule_id));\n      return;\n    }\n\n    g_dbus_method_invocation_return_error(invocation, G_DBUS_ERROR,\n      G_DBUS_ERROR_UNKNOWN_METHOD, \"Unknown method \");\n  }",
        "func": "void DBusBridge::handleDevicesMethodCall(const std::string& method_name, GVariant* parameters,\n    GDBusMethodInvocation* invocation)\n  {\n    USBGUARD_LOG(Debug) << \"dbus devices method call: \" << method_name;\n\n    if (method_name == \"listDevices\") {\n      if (! isAuthorizedByPolkit(invocation)) {\n        return;\n      }\n\n      const char* query_cstr = nullptr;\n      g_variant_get(parameters, \"(&s)\", &query_cstr);\n      std::string query(query_cstr);\n      auto devices = listDevices(query);\n\n      if (devices.size() > 0) {\n        auto gvbuilder = g_variant_builder_new(G_VARIANT_TYPE_ARRAY);\n\n        try {\n          for (auto device_rule : devices) {\n            g_variant_builder_add(gvbuilder, \"(us)\",\n              device_rule.getRuleID(),\n              device_rule.toString().c_str());\n          }\n\n          g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(a(us))\", gvbuilder));\n        }\n        catch (...) {\n          g_variant_builder_unref(gvbuilder);\n          throw;\n        }\n\n        g_variant_builder_unref(gvbuilder);\n      }\n      else {\n        g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(a(us))\", nullptr));\n      }\n\n      return;\n    }\n\n    if (method_name == \"applyDevicePolicy\") {\n      if (! isAuthorizedByPolkit(invocation)) {\n        return;\n      }\n\n      uint32_t device_id = 0;\n      uint32_t target_integer = 0;\n      gboolean permanent = false;\n      g_variant_get(parameters, \"(uub)\", &device_id, &target_integer, &permanent);\n      USBGUARD_LOG(Debug) << \"DBus: applyDevicePolicy: Parsed device_id: \" << device_id << \" target_integer: \" << target_integer <<\n        \" and permanent: \" << permanent;\n      const Rule::Target target = Rule::targetFromInteger(target_integer);\n      const uint32_t rule_id = applyDevicePolicy(device_id, target, permanent);\n      g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(u)\", rule_id));\n      return;\n    }\n\n    g_dbus_method_invocation_return_error(invocation, G_DBUS_ERROR,\n      G_DBUS_ERROR_UNKNOWN_METHOD, \"Unknown method \");\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,10 @@\n     USBGUARD_LOG(Debug) << \"dbus devices method call: \" << method_name;\n \n     if (method_name == \"listDevices\") {\n+      if (! isAuthorizedByPolkit(invocation)) {\n+        return;\n+      }\n+\n       const char* query_cstr = nullptr;\n       g_variant_get(parameters, \"(&s)\", &query_cstr);\n       std::string query(query_cstr);\n@@ -36,6 +40,10 @@\n     }\n \n     if (method_name == \"applyDevicePolicy\") {\n+      if (! isAuthorizedByPolkit(invocation)) {\n+        return;\n+      }\n+\n       uint32_t device_id = 0;\n       uint32_t target_integer = 0;\n       gboolean permanent = false;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "      if (! isAuthorizedByPolkit(invocation)) {",
                "        return;",
                "      }",
                "",
                "      if (! isAuthorizedByPolkit(invocation)) {",
                "        return;",
                "      }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2019-25058",
        "func_name": "USBGuard/usbguard/DBusBridge::handlePolicyMethodCall",
        "description": "An issue was discovered in USBGuard before 1.1.0. On systems with the usbguard-dbus daemon running, an unprivileged user could make USBGuard allow all USB devices to be connected in the future.",
        "git_url": "https://github.com/USBGuard/usbguard/commit/df5f01c6ed0c20d269f7239901d21883cc871bbb",
        "commit_title": "dbus: Add missing checks for authorization using Polkit",
        "commit_text": "",
        "func_before": "void DBusBridge::handlePolicyMethodCall(const std::string& method_name, GVariant* parameters, GDBusMethodInvocation* invocation)\n  {\n    if (method_name == \"listRules\") {\n      const char* label_cstr = nullptr;\n      g_variant_get(parameters, \"(&s)\", &label_cstr);\n      std::string label(label_cstr);\n      auto rules = listRules(label);\n\n      if (rules.size() > 0) {\n        auto gvbuilder = g_variant_builder_new(G_VARIANT_TYPE_ARRAY);\n\n        try {\n          for (auto rule : rules) {\n            g_variant_builder_add(gvbuilder, \"(us)\",\n              rule.getRuleID(),\n              rule.toString().c_str());\n          }\n\n          g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(a(us))\", gvbuilder));\n        }\n        catch (...) {\n          g_variant_builder_unref(gvbuilder);\n          throw;\n        }\n\n        g_variant_builder_unref(gvbuilder);\n      }\n      else {\n        g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(a(us))\", nullptr));\n      }\n\n      return;\n    }\n\n    if (method_name == \"appendRule\") {\n      const char* rule_spec_cstr = nullptr;\n      uint32_t parent_id = 0;\n      gboolean temporary = false;\n      g_variant_get(parameters, \"(&sub)\", &rule_spec_cstr, &parent_id, &temporary);\n      std::string rule_spec(rule_spec_cstr);\n      const uint32_t rule_id = appendRule(rule_spec, parent_id, !temporary);\n      g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(u)\", rule_id));\n      return;\n    }\n\n    if (method_name == \"removeRule\") {\n      uint32_t rule_id = 0;\n      g_variant_get(parameters, \"(u)\", &rule_id);\n      removeRule(rule_id);\n      g_dbus_method_invocation_return_value(invocation, nullptr);\n      return;\n    }\n\n    g_dbus_method_invocation_return_error(invocation, G_DBUS_ERROR,\n      G_DBUS_ERROR_UNKNOWN_METHOD, \"Unknown method interface\");\n    return;\n  }",
        "func": "void DBusBridge::handlePolicyMethodCall(const std::string& method_name, GVariant* parameters, GDBusMethodInvocation* invocation)\n  {\n    if (method_name == \"listRules\") {\n      if (! isAuthorizedByPolkit(invocation)) {\n        return;\n      }\n\n      const char* label_cstr = nullptr;\n      g_variant_get(parameters, \"(&s)\", &label_cstr);\n      std::string label(label_cstr);\n      auto rules = listRules(label);\n\n      if (rules.size() > 0) {\n        auto gvbuilder = g_variant_builder_new(G_VARIANT_TYPE_ARRAY);\n\n        try {\n          for (auto rule : rules) {\n            g_variant_builder_add(gvbuilder, \"(us)\",\n              rule.getRuleID(),\n              rule.toString().c_str());\n          }\n\n          g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(a(us))\", gvbuilder));\n        }\n        catch (...) {\n          g_variant_builder_unref(gvbuilder);\n          throw;\n        }\n\n        g_variant_builder_unref(gvbuilder);\n      }\n      else {\n        g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(a(us))\", nullptr));\n      }\n\n      return;\n    }\n\n    if (method_name == \"appendRule\") {\n      if (! isAuthorizedByPolkit(invocation)) {\n        return;\n      }\n\n      const char* rule_spec_cstr = nullptr;\n      uint32_t parent_id = 0;\n      gboolean temporary = false;\n      g_variant_get(parameters, \"(&sub)\", &rule_spec_cstr, &parent_id, &temporary);\n      std::string rule_spec(rule_spec_cstr);\n      const uint32_t rule_id = appendRule(rule_spec, parent_id, !temporary);\n      g_dbus_method_invocation_return_value(invocation, g_variant_new(\"(u)\", rule_id));\n      return;\n    }\n\n    if (method_name == \"removeRule\") {\n      if (! isAuthorizedByPolkit(invocation)) {\n        return;\n      }\n\n      uint32_t rule_id = 0;\n      g_variant_get(parameters, \"(u)\", &rule_id);\n      removeRule(rule_id);\n      g_dbus_method_invocation_return_value(invocation, nullptr);\n      return;\n    }\n\n    g_dbus_method_invocation_return_error(invocation, G_DBUS_ERROR,\n      G_DBUS_ERROR_UNKNOWN_METHOD, \"Unknown method interface\");\n    return;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,10 @@\n void DBusBridge::handlePolicyMethodCall(const std::string& method_name, GVariant* parameters, GDBusMethodInvocation* invocation)\n   {\n     if (method_name == \"listRules\") {\n+      if (! isAuthorizedByPolkit(invocation)) {\n+        return;\n+      }\n+\n       const char* label_cstr = nullptr;\n       g_variant_get(parameters, \"(&s)\", &label_cstr);\n       std::string label(label_cstr);\n@@ -33,6 +37,10 @@\n     }\n \n     if (method_name == \"appendRule\") {\n+      if (! isAuthorizedByPolkit(invocation)) {\n+        return;\n+      }\n+\n       const char* rule_spec_cstr = nullptr;\n       uint32_t parent_id = 0;\n       gboolean temporary = false;\n@@ -44,6 +52,10 @@\n     }\n \n     if (method_name == \"removeRule\") {\n+      if (! isAuthorizedByPolkit(invocation)) {\n+        return;\n+      }\n+\n       uint32_t rule_id = 0;\n       g_variant_get(parameters, \"(u)\", &rule_id);\n       removeRule(rule_id);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "      if (! isAuthorizedByPolkit(invocation)) {",
                "        return;",
                "      }",
                "",
                "      if (! isAuthorizedByPolkit(invocation)) {",
                "        return;",
                "      }",
                "",
                "      if (! isAuthorizedByPolkit(invocation)) {",
                "        return;",
                "      }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3658",
        "func_name": "bluez/settings_changed",
        "description": "bluetoothd from bluez incorrectly saves adapters' Discoverable status when a device is powered down, and restores it when powered up. If a device is powered down while discoverable, it will be discoverable when powered on again. This could lead to inadvertent exposure of the bluetooth stack to physically nearby attackers.",
        "git_url": "https://github.com/bluez/bluez/commit/b497b5942a8beb8f89ca1c359c54ad67ec843055",
        "commit_title": "adapter: Fix storing discoverable setting",
        "commit_text": " discoverable setting shall only be store when changed via Discoverable property and not when discovery client set it as that be considered temporary just for the lifetime of the discovery.",
        "func_before": "static void settings_changed(struct btd_adapter *adapter, uint32_t settings)\n{\n\tuint32_t changed_mask;\n\n\tchanged_mask = adapter->current_settings ^ settings;\n\n\tadapter->current_settings = settings;\n\tadapter->pending_settings &= ~changed_mask;\n\n\tDBG(\"Changed settings: 0x%08x\", changed_mask);\n\tDBG(\"Pending settings: 0x%08x\", adapter->pending_settings);\n\n\tif (changed_mask & MGMT_SETTING_POWERED) {\n\t        g_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Powered\");\n\n\t\tif (adapter->current_settings & MGMT_SETTING_POWERED) {\n\t\t\tadapter_start(adapter);\n\t\t} else {\n\t\t\tadapter_stop(adapter);\n\n\t\t\tif (powering_down) {\n\t\t\t\tadapter_remaining--;\n\n\t\t\t\tif (!adapter_remaining)\n\t\t\t\t\tbtd_exit();\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((changed_mask & MGMT_SETTING_LE) &&\n\t\t\t\tbtd_adapter_get_powered(adapter) &&\n\t\t\t\t(adapter->current_settings & MGMT_SETTING_LE))\n\t\ttrigger_passive_scanning(adapter);\n\n\tif (changed_mask & MGMT_SETTING_DISCOVERABLE) {\n\t\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Discoverable\");\n\t\tstore_adapter_info(adapter);\n\t\tbtd_adv_manager_refresh(adapter->adv_manager);\n\t}\n\n\tif (changed_mask & MGMT_SETTING_BONDABLE) {\n\t\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Pairable\");\n\n\t\ttrigger_pairable_timeout(adapter);\n\t}\n}",
        "func": "static void settings_changed(struct btd_adapter *adapter, uint32_t settings)\n{\n\tuint32_t changed_mask;\n\n\tchanged_mask = adapter->current_settings ^ settings;\n\n\tadapter->current_settings = settings;\n\tadapter->pending_settings &= ~changed_mask;\n\n\tDBG(\"Changed settings: 0x%08x\", changed_mask);\n\tDBG(\"Pending settings: 0x%08x\", adapter->pending_settings);\n\n\tif (changed_mask & MGMT_SETTING_POWERED) {\n\t        g_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Powered\");\n\n\t\tif (adapter->current_settings & MGMT_SETTING_POWERED) {\n\t\t\tadapter_start(adapter);\n\t\t} else {\n\t\t\tadapter_stop(adapter);\n\n\t\t\tif (powering_down) {\n\t\t\t\tadapter_remaining--;\n\n\t\t\t\tif (!adapter_remaining)\n\t\t\t\t\tbtd_exit();\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((changed_mask & MGMT_SETTING_LE) &&\n\t\t\t\tbtd_adapter_get_powered(adapter) &&\n\t\t\t\t(adapter->current_settings & MGMT_SETTING_LE))\n\t\ttrigger_passive_scanning(adapter);\n\n\tif (changed_mask & MGMT_SETTING_DISCOVERABLE) {\n\t\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Discoverable\");\n\t\t/* Only persist discoverable setting if it was not set\n\t\t * temporarily by discovery.\n\t\t */\n\t\tif (!adapter->discovery_discoverable)\n\t\t\tstore_adapter_info(adapter);\n\t\tbtd_adv_manager_refresh(adapter->adv_manager);\n\t}\n\n\tif (changed_mask & MGMT_SETTING_BONDABLE) {\n\t\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Pairable\");\n\n\t\ttrigger_pairable_timeout(adapter);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,7 +36,11 @@\n \tif (changed_mask & MGMT_SETTING_DISCOVERABLE) {\n \t\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n \t\t\t\t\tADAPTER_INTERFACE, \"Discoverable\");\n-\t\tstore_adapter_info(adapter);\n+\t\t/* Only persist discoverable setting if it was not set\n+\t\t * temporarily by discovery.\n+\t\t */\n+\t\tif (!adapter->discovery_discoverable)\n+\t\t\tstore_adapter_info(adapter);\n \t\tbtd_adv_manager_refresh(adapter->adv_manager);\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tstore_adapter_info(adapter);"
            ],
            "added_lines": [
                "\t\t/* Only persist discoverable setting if it was not set",
                "\t\t * temporarily by discovery.",
                "\t\t */",
                "\t\tif (!adapter->discovery_discoverable)",
                "\t\t\tstore_adapter_info(adapter);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3658",
        "func_name": "bluez/discovery_stop",
        "description": "bluetoothd from bluez incorrectly saves adapters' Discoverable status when a device is powered down, and restores it when powered up. If a device is powered down while discoverable, it will be discoverable when powered on again. This could lead to inadvertent exposure of the bluetooth stack to physically nearby attackers.",
        "git_url": "https://github.com/bluez/bluez/commit/b497b5942a8beb8f89ca1c359c54ad67ec843055",
        "commit_title": "adapter: Fix storing discoverable setting",
        "commit_text": " discoverable setting shall only be store when changed via Discoverable property and not when discovery client set it as that be considered temporary just for the lifetime of the discovery.",
        "func_before": "static int discovery_stop(struct discovery_client *client)\n{\n\tstruct btd_adapter *adapter = client->adapter;\n\tstruct mgmt_cp_stop_discovery cp;\n\n\t/* Check if there are more client discovering */\n\tif (g_slist_next(adapter->discovery_list)) {\n\t\tdiscovery_remove(client);\n\t\tupdate_discovery_filter(adapter);\n\t\treturn 0;\n\t}\n\n\tif (adapter->discovery_discoverable)\n\t\tset_discovery_discoverable(adapter, false);\n\n\t/*\n\t * In the idle phase of a discovery, there is no need to stop it\n\t * and so it is enough to send out the signal and just return.\n\t */\n\tif (adapter->discovery_enable == 0x00) {\n\t\tdiscovery_remove(client);\n\t\tadapter->discovering = false;\n\t\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Discovering\");\n\n\t\ttrigger_passive_scanning(adapter);\n\n\t\treturn 0;\n\t}\n\n\tcp.type = adapter->discovery_type;\n\tadapter->client = client;\n\n\tmgmt_send(adapter->mgmt, MGMT_OP_STOP_DISCOVERY,\n\t\t\tadapter->dev_id, sizeof(cp), &cp,\n\t\t\tstop_discovery_complete, adapter, NULL);\n\n\treturn -EINPROGRESS;\n}",
        "func": "static int discovery_stop(struct discovery_client *client)\n{\n\tstruct btd_adapter *adapter = client->adapter;\n\tstruct mgmt_cp_stop_discovery cp;\n\n\t/* Check if there are more client discovering */\n\tif (g_slist_next(adapter->discovery_list)) {\n\t\tdiscovery_remove(client);\n\t\tupdate_discovery_filter(adapter);\n\t\treturn 0;\n\t}\n\n\tset_discovery_discoverable(adapter, false);\n\n\t/*\n\t * In the idle phase of a discovery, there is no need to stop it\n\t * and so it is enough to send out the signal and just return.\n\t */\n\tif (adapter->discovery_enable == 0x00) {\n\t\tdiscovery_remove(client);\n\t\tadapter->discovering = false;\n\t\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Discovering\");\n\n\t\ttrigger_passive_scanning(adapter);\n\n\t\treturn 0;\n\t}\n\n\tcp.type = adapter->discovery_type;\n\tadapter->client = client;\n\n\tmgmt_send(adapter->mgmt, MGMT_OP_STOP_DISCOVERY,\n\t\t\tadapter->dev_id, sizeof(cp), &cp,\n\t\t\tstop_discovery_complete, adapter, NULL);\n\n\treturn -EINPROGRESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,8 +10,7 @@\n \t\treturn 0;\n \t}\n \n-\tif (adapter->discovery_discoverable)\n-\t\tset_discovery_discoverable(adapter, false);\n+\tset_discovery_discoverable(adapter, false);\n \n \t/*\n \t * In the idle phase of a discovery, there is no need to stop it",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (adapter->discovery_discoverable)",
                "\t\tset_discovery_discoverable(adapter, false);"
            ],
            "added_lines": [
                "\tset_discovery_discoverable(adapter, false);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3658",
        "func_name": "bluez/adapter_stop",
        "description": "bluetoothd from bluez incorrectly saves adapters' Discoverable status when a device is powered down, and restores it when powered up. If a device is powered down while discoverable, it will be discoverable when powered on again. This could lead to inadvertent exposure of the bluetooth stack to physically nearby attackers.",
        "git_url": "https://github.com/bluez/bluez/commit/b497b5942a8beb8f89ca1c359c54ad67ec843055",
        "commit_title": "adapter: Fix storing discoverable setting",
        "commit_text": " discoverable setting shall only be store when changed via Discoverable property and not when discovery client set it as that be considered temporary just for the lifetime of the discovery.",
        "func_before": "static void adapter_stop(struct btd_adapter *adapter)\n{\n\t/* check pending requests */\n\treply_pending_requests(adapter);\n\n\tcancel_passive_scanning(adapter);\n\n\tremove_discovery_list(adapter);\n\n\tdiscovery_cleanup(adapter, 0);\n\n\tadapter->filtered_discovery = false;\n\tadapter->no_scan_restart_delay = false;\n\tg_free(adapter->current_discovery_filter);\n\tadapter->current_discovery_filter = NULL;\n\n\tadapter->discovering = false;\n\n\twhile (adapter->connections) {\n\t\tstruct btd_device *device = adapter->connections->data;\n\t\tuint8_t addr_type = btd_device_get_bdaddr_type(device);\n\n\t\tadapter_remove_connection(adapter, device, BDADDR_BREDR);\n\t\tif (addr_type != BDADDR_BREDR)\n\t\t\tadapter_remove_connection(adapter, device, addr_type);\n\t}\n\n\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Discovering\");\n\n\tif (adapter->dev_class) {\n\t\t/* the kernel should reset the class of device when powering\n\t\t * down, but it does not. So force it here ... */\n\t\tadapter->dev_class = 0;\n\t\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\t\tADAPTER_INTERFACE, \"Class\");\n\t}\n\n\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\t\tADAPTER_INTERFACE, \"Powered\");\n\n\tDBG(\"adapter %s has been disabled\", adapter->path);\n}",
        "func": "static void adapter_stop(struct btd_adapter *adapter)\n{\n\t/* check pending requests */\n\treply_pending_requests(adapter);\n\n\tcancel_passive_scanning(adapter);\n\n\tremove_discovery_list(adapter);\n\n\tdiscovery_cleanup(adapter, 0);\n\n\tadapter->filtered_discovery = false;\n\tadapter->no_scan_restart_delay = false;\n\tg_free(adapter->current_discovery_filter);\n\tadapter->current_discovery_filter = NULL;\n\n\tset_discovery_discoverable(adapter, false);\n\tadapter->discovering = false;\n\n\twhile (adapter->connections) {\n\t\tstruct btd_device *device = adapter->connections->data;\n\t\tuint8_t addr_type = btd_device_get_bdaddr_type(device);\n\n\t\tadapter_remove_connection(adapter, device, BDADDR_BREDR);\n\t\tif (addr_type != BDADDR_BREDR)\n\t\t\tadapter_remove_connection(adapter, device, addr_type);\n\t}\n\n\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\tADAPTER_INTERFACE, \"Discovering\");\n\n\tif (adapter->dev_class) {\n\t\t/* the kernel should reset the class of device when powering\n\t\t * down, but it does not. So force it here ... */\n\t\tadapter->dev_class = 0;\n\t\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\t\tADAPTER_INTERFACE, \"Class\");\n\t}\n\n\tg_dbus_emit_property_changed(dbus_conn, adapter->path,\n\t\t\t\t\t\tADAPTER_INTERFACE, \"Powered\");\n\n\tDBG(\"adapter %s has been disabled\", adapter->path);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,6 +14,7 @@\n \tg_free(adapter->current_discovery_filter);\n \tadapter->current_discovery_filter = NULL;\n \n+\tset_discovery_discoverable(adapter, false);\n \tadapter->discovering = false;\n \n \twhile (adapter->connections) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tset_discovery_discoverable(adapter, false);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3658",
        "func_name": "bluez/update_discovery_filter",
        "description": "bluetoothd from bluez incorrectly saves adapters' Discoverable status when a device is powered down, and restores it when powered up. If a device is powered down while discoverable, it will be discoverable when powered on again. This could lead to inadvertent exposure of the bluetooth stack to physically nearby attackers.",
        "git_url": "https://github.com/bluez/bluez/commit/b497b5942a8beb8f89ca1c359c54ad67ec843055",
        "commit_title": "adapter: Fix storing discoverable setting",
        "commit_text": " discoverable setting shall only be store when changed via Discoverable property and not when discovery client set it as that be considered temporary just for the lifetime of the discovery.",
        "func_before": "static int update_discovery_filter(struct btd_adapter *adapter)\n{\n\tstruct mgmt_cp_start_service_discovery *sd_cp;\n\tGSList *l;\n\n\n\tDBG(\"\");\n\n\tif (discovery_filter_to_mgmt_cp(adapter, &sd_cp)) {\n\t\tbtd_error(adapter->dev_id,\n\t\t\t\t\"discovery_filter_to_mgmt_cp returned error\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (l = adapter->discovery_list; l; l = g_slist_next(l)) {\n\t\tstruct discovery_client *client = l->data;\n\n\t\tif (!client->discovery_filter)\n\t\t\tcontinue;\n\n\t\tif (client->discovery_filter->discoverable)\n\t\t\tbreak;\n\t}\n\n\tset_discovery_discoverable(adapter, l ? true : false);\n\n\t/*\n\t * If filters are equal, then don't update scan, except for when\n\t * starting discovery.\n\t */\n\tif (filters_equal(adapter->current_discovery_filter, sd_cp) &&\n\t    adapter->discovering != 0) {\n\t\tDBG(\"filters were equal, deciding to not restart the scan.\");\n\t\tg_free(sd_cp);\n\t\treturn 0;\n\t}\n\n\tg_free(adapter->current_discovery_filter);\n\tadapter->current_discovery_filter = sd_cp;\n\n\ttrigger_start_discovery(adapter, 0);\n\n\treturn -EINPROGRESS;\n}",
        "func": "static int update_discovery_filter(struct btd_adapter *adapter)\n{\n\tstruct mgmt_cp_start_service_discovery *sd_cp;\n\n\tDBG(\"\");\n\n\tif (discovery_filter_to_mgmt_cp(adapter, &sd_cp)) {\n\t\tbtd_error(adapter->dev_id,\n\t\t\t\t\"discovery_filter_to_mgmt_cp returned error\");\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Only attempt to overwrite current discoverable setting when not\n\t * discoverable.\n\t */\n\tif (!(adapter->current_settings & MGMT_OP_SET_DISCOVERABLE)) {\n\t\tGSList *l;\n\n\t\tfor (l = adapter->discovery_list; l; l = g_slist_next(l)) {\n\t\t\tstruct discovery_client *client = l->data;\n\n\t\t\tif (!client->discovery_filter)\n\t\t\t\tcontinue;\n\n\t\t\tif (client->discovery_filter->discoverable) {\n\t\t\t\tset_discovery_discoverable(adapter, true);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * If filters are equal, then don't update scan, except for when\n\t * starting discovery.\n\t */\n\tif (filters_equal(adapter->current_discovery_filter, sd_cp) &&\n\t    adapter->discovering != 0) {\n\t\tDBG(\"filters were equal, deciding to not restart the scan.\");\n\t\tg_free(sd_cp);\n\t\treturn 0;\n\t}\n\n\tg_free(adapter->current_discovery_filter);\n\tadapter->current_discovery_filter = sd_cp;\n\n\ttrigger_start_discovery(adapter, 0);\n\n\treturn -EINPROGRESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,6 @@\n static int update_discovery_filter(struct btd_adapter *adapter)\n {\n \tstruct mgmt_cp_start_service_discovery *sd_cp;\n-\tGSList *l;\n-\n \n \tDBG(\"\");\n \n@@ -12,17 +10,24 @@\n \t\treturn -ENOMEM;\n \t}\n \n-\tfor (l = adapter->discovery_list; l; l = g_slist_next(l)) {\n-\t\tstruct discovery_client *client = l->data;\n+\t/* Only attempt to overwrite current discoverable setting when not\n+\t * discoverable.\n+\t */\n+\tif (!(adapter->current_settings & MGMT_OP_SET_DISCOVERABLE)) {\n+\t\tGSList *l;\n \n-\t\tif (!client->discovery_filter)\n-\t\t\tcontinue;\n+\t\tfor (l = adapter->discovery_list; l; l = g_slist_next(l)) {\n+\t\t\tstruct discovery_client *client = l->data;\n \n-\t\tif (client->discovery_filter->discoverable)\n-\t\t\tbreak;\n+\t\t\tif (!client->discovery_filter)\n+\t\t\t\tcontinue;\n+\n+\t\t\tif (client->discovery_filter->discoverable) {\n+\t\t\t\tset_discovery_discoverable(adapter, true);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n \t}\n-\n-\tset_discovery_discoverable(adapter, l ? true : false);\n \n \t/*\n \t * If filters are equal, then don't update scan, except for when",
        "diff_line_info": {
            "deleted_lines": [
                "\tGSList *l;",
                "",
                "\tfor (l = adapter->discovery_list; l; l = g_slist_next(l)) {",
                "\t\tstruct discovery_client *client = l->data;",
                "\t\tif (!client->discovery_filter)",
                "\t\t\tcontinue;",
                "\t\tif (client->discovery_filter->discoverable)",
                "\t\t\tbreak;",
                "",
                "\tset_discovery_discoverable(adapter, l ? true : false);"
            ],
            "added_lines": [
                "\t/* Only attempt to overwrite current discoverable setting when not",
                "\t * discoverable.",
                "\t */",
                "\tif (!(adapter->current_settings & MGMT_OP_SET_DISCOVERABLE)) {",
                "\t\tGSList *l;",
                "\t\tfor (l = adapter->discovery_list; l; l = g_slist_next(l)) {",
                "\t\t\tstruct discovery_client *client = l->data;",
                "\t\t\tif (!client->discovery_filter)",
                "\t\t\t\tcontinue;",
                "",
                "\t\t\tif (client->discovery_filter->discoverable) {",
                "\t\t\t\tset_discovery_discoverable(adapter, true);",
                "\t\t\t\tbreak;",
                "\t\t\t}",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8109",
        "func_name": "apache/httpd/lua_authz_parse",
        "description": "mod_lua.c in the mod_lua module in the Apache HTTP Server 2.3.x and 2.4.x through 2.4.10 does not support an httpd configuration in which the same Lua authorization provider is used with different arguments within different contexts, which allows remote attackers to bypass intended access restrictions in opportunistic circumstances by leveraging multiple Require directives, as demonstrated by a configuration that specifies authorization for one group to access a certain directory, and authorization for a second group to access a second directory.",
        "git_url": "https://github.com/apache/httpd/commit/3f1693d558d0758f829c8b53993f1749ddf6ffcb",
        "commit_title": "Merge r1642499 from trunk:",
        "commit_text": "   *) SECURITY: CVE-2014-8109 (cve.mitre.org)      mod_lua: Fix handling of the Require line when a LuaAuthzProvider is      used in multiple Require directives with different arguments.      PR57204 [Edward Lu <Chaosed0 gmail.com>]  Submitted By: Edward Lu Committed By: covener   Submitted by: covener Reviewed/backported by: jim  ",
        "func_before": "static const char *lua_authz_parse(cmd_parms *cmd, const char *require_line,\n                                   const void **parsed_require_line)\n{\n    const char *provider_name;\n    lua_authz_provider_spec *spec;\n\n    apr_pool_userdata_get((void**)&provider_name, AUTHZ_PROVIDER_NAME_NOTE,\n                          cmd->temp_pool);\n    ap_assert(provider_name != NULL);\n\n    spec = apr_hash_get(lua_authz_providers, provider_name, APR_HASH_KEY_STRING);\n    ap_assert(spec != NULL);\n\n    if (require_line && *require_line) {\n        const char *arg;\n        spec->args = apr_array_make(cmd->pool, 2, sizeof(const char *));\n        while ((arg = ap_getword_conf(cmd->pool, &require_line)) && *arg) {\n            APR_ARRAY_PUSH(spec->args, const char *) = arg;\n        }\n    }\n\n    *parsed_require_line = spec;\n    return NULL;\n}",
        "func": "static const char *lua_authz_parse(cmd_parms *cmd, const char *require_line,\n                                   const void **parsed_require_line)\n{\n    const char *provider_name;\n    lua_authz_provider_spec *spec;\n    lua_authz_provider_func *func = apr_pcalloc(cmd->pool, sizeof(lua_authz_provider_func));\n\n    apr_pool_userdata_get((void**)&provider_name, AUTHZ_PROVIDER_NAME_NOTE,\n                          cmd->temp_pool);\n    ap_assert(provider_name != NULL);\n\n    spec = apr_hash_get(lua_authz_providers, provider_name, APR_HASH_KEY_STRING);\n    ap_assert(spec != NULL);\n    func->spec = spec;\n\n    if (require_line && *require_line) {\n        const char *arg;\n        func->args = apr_array_make(cmd->pool, 2, sizeof(const char *));\n        while ((arg = ap_getword_conf(cmd->pool, &require_line)) && *arg) {\n            APR_ARRAY_PUSH(func->args, const char *) = arg;\n        }\n    }\n\n    *parsed_require_line = func;\n    return NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,7 @@\n {\n     const char *provider_name;\n     lua_authz_provider_spec *spec;\n+    lua_authz_provider_func *func = apr_pcalloc(cmd->pool, sizeof(lua_authz_provider_func));\n \n     apr_pool_userdata_get((void**)&provider_name, AUTHZ_PROVIDER_NAME_NOTE,\n                           cmd->temp_pool);\n@@ -10,15 +11,16 @@\n \n     spec = apr_hash_get(lua_authz_providers, provider_name, APR_HASH_KEY_STRING);\n     ap_assert(spec != NULL);\n+    func->spec = spec;\n \n     if (require_line && *require_line) {\n         const char *arg;\n-        spec->args = apr_array_make(cmd->pool, 2, sizeof(const char *));\n+        func->args = apr_array_make(cmd->pool, 2, sizeof(const char *));\n         while ((arg = ap_getword_conf(cmd->pool, &require_line)) && *arg) {\n-            APR_ARRAY_PUSH(spec->args, const char *) = arg;\n+            APR_ARRAY_PUSH(func->args, const char *) = arg;\n         }\n     }\n \n-    *parsed_require_line = spec;\n+    *parsed_require_line = func;\n     return NULL;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "        spec->args = apr_array_make(cmd->pool, 2, sizeof(const char *));",
                "            APR_ARRAY_PUSH(spec->args, const char *) = arg;",
                "    *parsed_require_line = spec;"
            ],
            "added_lines": [
                "    lua_authz_provider_func *func = apr_pcalloc(cmd->pool, sizeof(lua_authz_provider_func));",
                "    func->spec = spec;",
                "        func->args = apr_array_make(cmd->pool, 2, sizeof(const char *));",
                "            APR_ARRAY_PUSH(func->args, const char *) = arg;",
                "    *parsed_require_line = func;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8109",
        "func_name": "apache/httpd/lua_authz_check",
        "description": "mod_lua.c in the mod_lua module in the Apache HTTP Server 2.3.x and 2.4.x through 2.4.10 does not support an httpd configuration in which the same Lua authorization provider is used with different arguments within different contexts, which allows remote attackers to bypass intended access restrictions in opportunistic circumstances by leveraging multiple Require directives, as demonstrated by a configuration that specifies authorization for one group to access a certain directory, and authorization for a second group to access a second directory.",
        "git_url": "https://github.com/apache/httpd/commit/3f1693d558d0758f829c8b53993f1749ddf6ffcb",
        "commit_title": "Merge r1642499 from trunk:",
        "commit_text": "   *) SECURITY: CVE-2014-8109 (cve.mitre.org)      mod_lua: Fix handling of the Require line when a LuaAuthzProvider is      used in multiple Require directives with different arguments.      PR57204 [Edward Lu <Chaosed0 gmail.com>]  Submitted By: Edward Lu Committed By: covener   Submitted by: covener Reviewed/backported by: jim  ",
        "func_before": "static authz_status lua_authz_check(request_rec *r, const char *require_line,\n                                    const void *parsed_require_line)\n{\n    apr_pool_t *pool;\n    ap_lua_vm_spec *spec;\n    lua_State *L;\n    ap_lua_server_cfg *server_cfg = ap_get_module_config(r->server->module_config,\n                                                         &lua_module);\n    const ap_lua_dir_cfg *cfg = ap_get_module_config(r->per_dir_config,\n                                                     &lua_module);\n    const lua_authz_provider_spec *prov_spec = parsed_require_line;\n    int result;\n    int nargs = 0;\n\n    spec = create_vm_spec(&pool, r, cfg, server_cfg, prov_spec->file_name,\n                          NULL, 0, prov_spec->function_name, \"authz provider\");\n\n    L = ap_lua_get_lua_state(pool, spec, r);\n    if (L == NULL) {\n        ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02314)\n                      \"Unable to compile VM for authz provider %s\", prov_spec->name);\n        return AUTHZ_GENERAL_ERROR;\n    }\n    lua_getglobal(L, prov_spec->function_name);\n    if (!lua_isfunction(L, -1)) {\n        ap_log_rerror(APLOG_MARK, APLOG_CRIT, 0, r, APLOGNO(02319)\n                      \"Unable to find entry function '%s' in %s (not a valid function)\",\n                      prov_spec->function_name, prov_spec->file_name);\n        ap_lua_release_state(L, spec, r);\n        return AUTHZ_GENERAL_ERROR;\n    }\n    ap_lua_run_lua_request(L, r);\n    if (prov_spec->args) {\n        int i;\n        if (!lua_checkstack(L, prov_spec->args->nelts)) {\n            ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02315)\n                          \"Error: authz provider %s: too many arguments\", prov_spec->name);\n            ap_lua_release_state(L, spec, r);\n            return AUTHZ_GENERAL_ERROR;\n        }\n        for (i = 0; i < prov_spec->args->nelts; i++) {\n            const char *arg = APR_ARRAY_IDX(prov_spec->args, i, const char *);\n            lua_pushstring(L, arg);\n        }\n        nargs = prov_spec->args->nelts;\n    }\n    if (lua_pcall(L, 1 + nargs, 1, 0)) {\n        const char *err = lua_tostring(L, -1);\n        ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02316)\n                      \"Error executing authz provider %s: %s\", prov_spec->name, err);\n        ap_lua_release_state(L, spec, r);\n        return AUTHZ_GENERAL_ERROR;\n    }\n    if (!lua_isnumber(L, -1)) {\n        ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02317)\n                      \"Error: authz provider %s did not return integer\", prov_spec->name);\n        ap_lua_release_state(L, spec, r);\n        return AUTHZ_GENERAL_ERROR;\n    }\n    result = lua_tointeger(L, -1);\n    ap_lua_release_state(L, spec, r);\n    switch (result) {\n        case AUTHZ_DENIED:\n        case AUTHZ_GRANTED:\n        case AUTHZ_NEUTRAL:\n        case AUTHZ_GENERAL_ERROR:\n        case AUTHZ_DENIED_NO_USER:\n            return result;\n        default:\n            ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02318)\n                          \"Error: authz provider %s: invalid return value %d\",\n                          prov_spec->name, result);\n    }\n    return AUTHZ_GENERAL_ERROR;\n}",
        "func": "static authz_status lua_authz_check(request_rec *r, const char *require_line,\n                                    const void *parsed_require_line)\n{\n    apr_pool_t *pool;\n    ap_lua_vm_spec *spec;\n    lua_State *L;\n    ap_lua_server_cfg *server_cfg = ap_get_module_config(r->server->module_config,\n                                                         &lua_module);\n    const ap_lua_dir_cfg *cfg = ap_get_module_config(r->per_dir_config,\n                                                     &lua_module);\n    const lua_authz_provider_func *prov_func = parsed_require_line;\n    const lua_authz_provider_spec *prov_spec = prov_func->spec;\n    int result;\n    int nargs = 0;\n\n    spec = create_vm_spec(&pool, r, cfg, server_cfg, prov_spec->file_name,\n                          NULL, 0, prov_spec->function_name, \"authz provider\");\n\n    L = ap_lua_get_lua_state(pool, spec, r);\n    if (L == NULL) {\n        ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02314)\n                      \"Unable to compile VM for authz provider %s\", prov_spec->name);\n        return AUTHZ_GENERAL_ERROR;\n    }\n    lua_getglobal(L, prov_spec->function_name);\n    if (!lua_isfunction(L, -1)) {\n        ap_log_rerror(APLOG_MARK, APLOG_CRIT, 0, r, APLOGNO(02319)\n                      \"Unable to find entry function '%s' in %s (not a valid function)\",\n                      prov_spec->function_name, prov_spec->file_name);\n        ap_lua_release_state(L, spec, r);\n        return AUTHZ_GENERAL_ERROR;\n    }\n    ap_lua_run_lua_request(L, r);\n    if (prov_func->args) {\n        int i;\n        if (!lua_checkstack(L, prov_func->args->nelts)) {\n            ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02315)\n                          \"Error: authz provider %s: too many arguments\", prov_spec->name);\n            ap_lua_release_state(L, spec, r);\n            return AUTHZ_GENERAL_ERROR;\n        }\n        for (i = 0; i < prov_func->args->nelts; i++) {\n            const char *arg = APR_ARRAY_IDX(prov_func->args, i, const char *);\n            lua_pushstring(L, arg);\n        }\n        nargs = prov_func->args->nelts;\n    }\n    if (lua_pcall(L, 1 + nargs, 1, 0)) {\n        const char *err = lua_tostring(L, -1);\n        ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02316)\n                      \"Error executing authz provider %s: %s\", prov_spec->name, err);\n        ap_lua_release_state(L, spec, r);\n        return AUTHZ_GENERAL_ERROR;\n    }\n    if (!lua_isnumber(L, -1)) {\n        ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02317)\n                      \"Error: authz provider %s did not return integer\", prov_spec->name);\n        ap_lua_release_state(L, spec, r);\n        return AUTHZ_GENERAL_ERROR;\n    }\n    result = lua_tointeger(L, -1);\n    ap_lua_release_state(L, spec, r);\n    switch (result) {\n        case AUTHZ_DENIED:\n        case AUTHZ_GRANTED:\n        case AUTHZ_NEUTRAL:\n        case AUTHZ_GENERAL_ERROR:\n        case AUTHZ_DENIED_NO_USER:\n            return result;\n        default:\n            ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02318)\n                          \"Error: authz provider %s: invalid return value %d\",\n                          prov_spec->name, result);\n    }\n    return AUTHZ_GENERAL_ERROR;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,8 @@\n                                                          &lua_module);\n     const ap_lua_dir_cfg *cfg = ap_get_module_config(r->per_dir_config,\n                                                      &lua_module);\n-    const lua_authz_provider_spec *prov_spec = parsed_require_line;\n+    const lua_authz_provider_func *prov_func = parsed_require_line;\n+    const lua_authz_provider_spec *prov_spec = prov_func->spec;\n     int result;\n     int nargs = 0;\n \n@@ -30,19 +31,19 @@\n         return AUTHZ_GENERAL_ERROR;\n     }\n     ap_lua_run_lua_request(L, r);\n-    if (prov_spec->args) {\n+    if (prov_func->args) {\n         int i;\n-        if (!lua_checkstack(L, prov_spec->args->nelts)) {\n+        if (!lua_checkstack(L, prov_func->args->nelts)) {\n             ap_log_rerror(APLOG_MARK, APLOG_ERR, 0, r, APLOGNO(02315)\n                           \"Error: authz provider %s: too many arguments\", prov_spec->name);\n             ap_lua_release_state(L, spec, r);\n             return AUTHZ_GENERAL_ERROR;\n         }\n-        for (i = 0; i < prov_spec->args->nelts; i++) {\n-            const char *arg = APR_ARRAY_IDX(prov_spec->args, i, const char *);\n+        for (i = 0; i < prov_func->args->nelts; i++) {\n+            const char *arg = APR_ARRAY_IDX(prov_func->args, i, const char *);\n             lua_pushstring(L, arg);\n         }\n-        nargs = prov_spec->args->nelts;\n+        nargs = prov_func->args->nelts;\n     }\n     if (lua_pcall(L, 1 + nargs, 1, 0)) {\n         const char *err = lua_tostring(L, -1);",
        "diff_line_info": {
            "deleted_lines": [
                "    const lua_authz_provider_spec *prov_spec = parsed_require_line;",
                "    if (prov_spec->args) {",
                "        if (!lua_checkstack(L, prov_spec->args->nelts)) {",
                "        for (i = 0; i < prov_spec->args->nelts; i++) {",
                "            const char *arg = APR_ARRAY_IDX(prov_spec->args, i, const char *);",
                "        nargs = prov_spec->args->nelts;"
            ],
            "added_lines": [
                "    const lua_authz_provider_func *prov_func = parsed_require_line;",
                "    const lua_authz_provider_spec *prov_spec = prov_func->spec;",
                "    if (prov_func->args) {",
                "        if (!lua_checkstack(L, prov_func->args->nelts)) {",
                "        for (i = 0; i < prov_func->args->nelts; i++) {",
                "            const char *arg = APR_ARRAY_IDX(prov_func->args, i, const char *);",
                "        nargs = prov_func->args->nelts;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-15468",
        "func_name": "xen-project/xen/core2_vpmu_do_wrmsr",
        "description": "An issue was discovered in Xen through 4.11.x. The DEBUGCTL MSR contains several debugging features, some of which virtualise cleanly, but some do not. In particular, Branch Trace Store is not virtualised by the processor, and software has to be careful to configure it suitably not to lock up the core. As a result, it must only be available to fully trusted guests. Unfortunately, in the case that vPMU is disabled, all value checking was skipped, allowing the guest to choose any MSR_DEBUGCTL setting it likes. A malicious or buggy guest administrator (on Intel x86 HVM or PVH) can lock up the entire host, causing a Denial of Service.",
        "git_url": "https://github.com/xen-project/xen/commit/2a8a8e99feb950504559196521bc9fd63ed3a962",
        "commit_title": "x86/vtx: Fix the checking for unknown/invalid MSR_DEBUGCTL bits",
        "commit_text": " The VPMU_MODE_OFF early-exit in vpmu_do_wrmsr() introduced by c/s 11fe998e56 bypasses all reserved bit checking in the general case.  As a result, a guest can enable BTS when it shouldn't be permitted to, and lock up the entire host.  With vPMU active (not a security supported configuration, but useful for debugging), the reserved bit checking in broken, caused by the original BTS changeset 1a8aa75ed.  From a correctness standpoint, it is not possible to have two different pieces of code responsible for different parts of value checking, if there isn't an accumulation of bits which have been checked.  A practical upshot of this is that a guest can set any value it wishes (usually resulting in a vmentry failure for bad guest state).  Therefore, fix this by implementing all the reserved bit checking in the main MSR_DEBUGCTL block, and removing all handling of DEBUGCTL from the vPMU MSR logic.  This is XSA-269. ",
        "func_before": "static int core2_vpmu_do_wrmsr(unsigned int msr, uint64_t msr_content,\n                               uint64_t supported)\n{\n    int i, tmp;\n    int type = -1, index = -1;\n    struct vcpu *v = current;\n    struct vpmu_struct *vpmu = vcpu_vpmu(v);\n    struct xen_pmu_intel_ctxt *core2_vpmu_cxt;\n    uint64_t *enabled_cntrs;\n\n    if ( !core2_vpmu_msr_common_check(msr, &type, &index) )\n    {\n        /* Special handling for BTS */\n        if ( msr == MSR_IA32_DEBUGCTLMSR )\n        {\n            supported |= IA32_DEBUGCTLMSR_TR | IA32_DEBUGCTLMSR_BTS |\n                         IA32_DEBUGCTLMSR_BTINT;\n\n            if ( cpu_has(&current_cpu_data, X86_FEATURE_DSCPL) )\n                supported |= IA32_DEBUGCTLMSR_BTS_OFF_OS |\n                             IA32_DEBUGCTLMSR_BTS_OFF_USR;\n            if ( !(msr_content & ~supported) &&\n                 vpmu_is_set(vpmu, VPMU_CPU_HAS_BTS) )\n                return 0;\n            if ( (msr_content & supported) &&\n                 !vpmu_is_set(vpmu, VPMU_CPU_HAS_BTS) )\n                printk(XENLOG_G_WARNING\n                       \"%pv: Debug Store unsupported on this CPU\\n\",\n                       current);\n        }\n        return -EINVAL;\n    }\n\n    ASSERT(!supported);\n\n    if ( (type == MSR_TYPE_COUNTER) && (msr_content & fixed_counters_mask) )\n        /* Writing unsupported bits to a fixed counter */\n        return -EINVAL;\n\n    core2_vpmu_cxt = vpmu->context;\n    enabled_cntrs = vpmu->priv_context;\n    switch ( msr )\n    {\n    case MSR_CORE_PERF_GLOBAL_OVF_CTRL:\n        if ( msr_content & global_ovf_ctrl_mask )\n            return -EINVAL;\n        core2_vpmu_cxt->global_status &= ~msr_content;\n        wrmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, msr_content);\n        return 0;\n    case MSR_CORE_PERF_GLOBAL_STATUS:\n        gdprintk(XENLOG_INFO, \"Can not write readonly MSR: \"\n                 \"MSR_PERF_GLOBAL_STATUS(0x38E)!\\n\");\n        return -EINVAL;\n    case MSR_IA32_PEBS_ENABLE:\n        if ( vpmu_features & (XENPMU_FEATURE_IPC_ONLY |\n                              XENPMU_FEATURE_ARCH_ONLY) )\n            return -EINVAL;\n        if ( msr_content )\n            /* PEBS is reported as unavailable in MSR_IA32_MISC_ENABLE */\n            return -EINVAL;\n        return 0;\n    case MSR_IA32_DS_AREA:\n        if ( !(vpmu_features & XENPMU_FEATURE_INTEL_BTS) )\n            return -EINVAL;\n        if ( vpmu_is_set(vpmu, VPMU_CPU_HAS_DS) )\n        {\n            if ( !(is_hvm_vcpu(v) ? is_canonical_address(msr_content)\n                                  : __addr_ok(msr_content)) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"Illegal address for IA32_DS_AREA: %#\" PRIx64 \"x\\n\",\n                         msr_content);\n                return -EINVAL;\n            }\n            core2_vpmu_cxt->ds_area = msr_content;\n            break;\n        }\n        gdprintk(XENLOG_WARNING, \"Guest setting of DTS is ignored.\\n\");\n        return 0;\n    case MSR_CORE_PERF_GLOBAL_CTRL:\n        if ( msr_content & global_ctrl_mask )\n            return -EINVAL;\n        core2_vpmu_cxt->global_ctrl = msr_content;\n        break;\n    case MSR_CORE_PERF_FIXED_CTR_CTRL:\n        if ( msr_content & fixed_ctrl_mask )\n            return -EINVAL;\n\n        if ( is_hvm_vcpu(v) )\n            vmx_read_guest_msr(v, MSR_CORE_PERF_GLOBAL_CTRL,\n                               &core2_vpmu_cxt->global_ctrl);\n        else\n            rdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, core2_vpmu_cxt->global_ctrl);\n        *enabled_cntrs &= ~(((1ULL << fixed_pmc_cnt) - 1) << 32);\n        if ( msr_content != 0 )\n        {\n            u64 val = msr_content;\n            for ( i = 0; i < fixed_pmc_cnt; i++ )\n            {\n                if ( val & 3 )\n                    *enabled_cntrs |= (1ULL << 32) << i;\n                val >>= FIXED_CTR_CTRL_BITS;\n            }\n        }\n\n        core2_vpmu_cxt->fixed_ctrl = msr_content;\n        break;\n    default:\n        tmp = msr - MSR_P6_EVNTSEL(0);\n        if ( tmp >= 0 && tmp < arch_pmc_cnt )\n        {\n            bool_t blocked = 0;\n            uint64_t umaskevent = msr_content & MSR_IA32_CMT_EVTSEL_UE_MASK;\n            struct xen_pmu_cntr_pair *xen_pmu_cntr_pair =\n                vpmu_reg_pointer(core2_vpmu_cxt, arch_counters);\n\n            if ( msr_content & ARCH_CTRL_MASK )\n                return -EINVAL;\n\n            /* PMC filters */\n            if ( vpmu_features & (XENPMU_FEATURE_IPC_ONLY |\n                                  XENPMU_FEATURE_ARCH_ONLY) )\n            {\n                blocked = 1;\n                switch ( umaskevent )\n                {\n                /*\n                 * See the Pre-Defined Architectural Performance Events table\n                 * from the Intel 64 and IA-32 Architectures Software\n                 * Developer's Manual, Volume 3B, System Programming Guide,\n                 * Part 2.\n                 */\n                case 0x003c:\t/* UnHalted Core Cycles */\n                case 0x013c:\t/* UnHalted Reference Cycles */\n                case 0x00c0:\t/* Instructions Retired */\n                    blocked = 0;\n                    break;\n                }\n            }\n\n            if ( vpmu_features & XENPMU_FEATURE_ARCH_ONLY )\n            {\n                /* Additional counters beyond IPC only; blocked already set. */\n                switch ( umaskevent )\n                {\n                case 0x4f2e:\t/* Last Level Cache References */\n                case 0x412e:\t/* Last Level Cache Misses */\n                case 0x00c4:\t/* Branch Instructions Retired */\n                case 0x00c5:\t/* All Branch Mispredict Retired */\n                    blocked = 0;\n                    break;\n               }\n            }\n\n            if ( blocked )\n                return -EINVAL;\n\n            if ( is_hvm_vcpu(v) )\n                vmx_read_guest_msr(v, MSR_CORE_PERF_GLOBAL_CTRL,\n                                   &core2_vpmu_cxt->global_ctrl);\n            else\n                rdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, core2_vpmu_cxt->global_ctrl);\n\n            if ( msr_content & ARCH_CNTR_ENABLED )\n                *enabled_cntrs |= 1ULL << tmp;\n            else\n                *enabled_cntrs &= ~(1ULL << tmp);\n\n            xen_pmu_cntr_pair[tmp].control = msr_content;\n        }\n    }\n\n    if ( type != MSR_TYPE_GLOBAL )\n        wrmsrl(msr, msr_content);\n    else\n    {\n        if ( is_hvm_vcpu(v) )\n            vmx_write_guest_msr(v, MSR_CORE_PERF_GLOBAL_CTRL, msr_content);\n        else\n            wrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, msr_content);\n    }\n\n    if ( (core2_vpmu_cxt->global_ctrl & *enabled_cntrs) ||\n         (core2_vpmu_cxt->ds_area != 0) )\n        vpmu_set(vpmu, VPMU_RUNNING);\n    else\n        vpmu_reset(vpmu, VPMU_RUNNING);\n\n    return 0;\n}",
        "func": "static int core2_vpmu_do_wrmsr(unsigned int msr, uint64_t msr_content,\n                               uint64_t supported)\n{\n    int i, tmp;\n    int type = -1, index = -1;\n    struct vcpu *v = current;\n    struct vpmu_struct *vpmu = vcpu_vpmu(v);\n    struct xen_pmu_intel_ctxt *core2_vpmu_cxt;\n    uint64_t *enabled_cntrs;\n\n    if ( !core2_vpmu_msr_common_check(msr, &type, &index) )\n        return -EINVAL;\n\n    ASSERT(!supported);\n\n    if ( (type == MSR_TYPE_COUNTER) && (msr_content & fixed_counters_mask) )\n        /* Writing unsupported bits to a fixed counter */\n        return -EINVAL;\n\n    core2_vpmu_cxt = vpmu->context;\n    enabled_cntrs = vpmu->priv_context;\n    switch ( msr )\n    {\n    case MSR_CORE_PERF_GLOBAL_OVF_CTRL:\n        if ( msr_content & global_ovf_ctrl_mask )\n            return -EINVAL;\n        core2_vpmu_cxt->global_status &= ~msr_content;\n        wrmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, msr_content);\n        return 0;\n    case MSR_CORE_PERF_GLOBAL_STATUS:\n        gdprintk(XENLOG_INFO, \"Can not write readonly MSR: \"\n                 \"MSR_PERF_GLOBAL_STATUS(0x38E)!\\n\");\n        return -EINVAL;\n    case MSR_IA32_PEBS_ENABLE:\n        if ( vpmu_features & (XENPMU_FEATURE_IPC_ONLY |\n                              XENPMU_FEATURE_ARCH_ONLY) )\n            return -EINVAL;\n        if ( msr_content )\n            /* PEBS is reported as unavailable in MSR_IA32_MISC_ENABLE */\n            return -EINVAL;\n        return 0;\n    case MSR_IA32_DS_AREA:\n        if ( !(vpmu_features & XENPMU_FEATURE_INTEL_BTS) )\n            return -EINVAL;\n        if ( vpmu_is_set(vpmu, VPMU_CPU_HAS_DS) )\n        {\n            if ( !(is_hvm_vcpu(v) ? is_canonical_address(msr_content)\n                                  : __addr_ok(msr_content)) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"Illegal address for IA32_DS_AREA: %#\" PRIx64 \"x\\n\",\n                         msr_content);\n                return -EINVAL;\n            }\n            core2_vpmu_cxt->ds_area = msr_content;\n            break;\n        }\n        gdprintk(XENLOG_WARNING, \"Guest setting of DTS is ignored.\\n\");\n        return 0;\n    case MSR_CORE_PERF_GLOBAL_CTRL:\n        if ( msr_content & global_ctrl_mask )\n            return -EINVAL;\n        core2_vpmu_cxt->global_ctrl = msr_content;\n        break;\n    case MSR_CORE_PERF_FIXED_CTR_CTRL:\n        if ( msr_content & fixed_ctrl_mask )\n            return -EINVAL;\n\n        if ( is_hvm_vcpu(v) )\n            vmx_read_guest_msr(v, MSR_CORE_PERF_GLOBAL_CTRL,\n                               &core2_vpmu_cxt->global_ctrl);\n        else\n            rdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, core2_vpmu_cxt->global_ctrl);\n        *enabled_cntrs &= ~(((1ULL << fixed_pmc_cnt) - 1) << 32);\n        if ( msr_content != 0 )\n        {\n            u64 val = msr_content;\n            for ( i = 0; i < fixed_pmc_cnt; i++ )\n            {\n                if ( val & 3 )\n                    *enabled_cntrs |= (1ULL << 32) << i;\n                val >>= FIXED_CTR_CTRL_BITS;\n            }\n        }\n\n        core2_vpmu_cxt->fixed_ctrl = msr_content;\n        break;\n    default:\n        tmp = msr - MSR_P6_EVNTSEL(0);\n        if ( tmp >= 0 && tmp < arch_pmc_cnt )\n        {\n            bool_t blocked = 0;\n            uint64_t umaskevent = msr_content & MSR_IA32_CMT_EVTSEL_UE_MASK;\n            struct xen_pmu_cntr_pair *xen_pmu_cntr_pair =\n                vpmu_reg_pointer(core2_vpmu_cxt, arch_counters);\n\n            if ( msr_content & ARCH_CTRL_MASK )\n                return -EINVAL;\n\n            /* PMC filters */\n            if ( vpmu_features & (XENPMU_FEATURE_IPC_ONLY |\n                                  XENPMU_FEATURE_ARCH_ONLY) )\n            {\n                blocked = 1;\n                switch ( umaskevent )\n                {\n                /*\n                 * See the Pre-Defined Architectural Performance Events table\n                 * from the Intel 64 and IA-32 Architectures Software\n                 * Developer's Manual, Volume 3B, System Programming Guide,\n                 * Part 2.\n                 */\n                case 0x003c:\t/* UnHalted Core Cycles */\n                case 0x013c:\t/* UnHalted Reference Cycles */\n                case 0x00c0:\t/* Instructions Retired */\n                    blocked = 0;\n                    break;\n                }\n            }\n\n            if ( vpmu_features & XENPMU_FEATURE_ARCH_ONLY )\n            {\n                /* Additional counters beyond IPC only; blocked already set. */\n                switch ( umaskevent )\n                {\n                case 0x4f2e:\t/* Last Level Cache References */\n                case 0x412e:\t/* Last Level Cache Misses */\n                case 0x00c4:\t/* Branch Instructions Retired */\n                case 0x00c5:\t/* All Branch Mispredict Retired */\n                    blocked = 0;\n                    break;\n               }\n            }\n\n            if ( blocked )\n                return -EINVAL;\n\n            if ( is_hvm_vcpu(v) )\n                vmx_read_guest_msr(v, MSR_CORE_PERF_GLOBAL_CTRL,\n                                   &core2_vpmu_cxt->global_ctrl);\n            else\n                rdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, core2_vpmu_cxt->global_ctrl);\n\n            if ( msr_content & ARCH_CNTR_ENABLED )\n                *enabled_cntrs |= 1ULL << tmp;\n            else\n                *enabled_cntrs &= ~(1ULL << tmp);\n\n            xen_pmu_cntr_pair[tmp].control = msr_content;\n        }\n    }\n\n    if ( type != MSR_TYPE_GLOBAL )\n        wrmsrl(msr, msr_content);\n    else\n    {\n        if ( is_hvm_vcpu(v) )\n            vmx_write_guest_msr(v, MSR_CORE_PERF_GLOBAL_CTRL, msr_content);\n        else\n            wrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, msr_content);\n    }\n\n    if ( (core2_vpmu_cxt->global_ctrl & *enabled_cntrs) ||\n         (core2_vpmu_cxt->ds_area != 0) )\n        vpmu_set(vpmu, VPMU_RUNNING);\n    else\n        vpmu_reset(vpmu, VPMU_RUNNING);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,27 +9,7 @@\n     uint64_t *enabled_cntrs;\n \n     if ( !core2_vpmu_msr_common_check(msr, &type, &index) )\n-    {\n-        /* Special handling for BTS */\n-        if ( msr == MSR_IA32_DEBUGCTLMSR )\n-        {\n-            supported |= IA32_DEBUGCTLMSR_TR | IA32_DEBUGCTLMSR_BTS |\n-                         IA32_DEBUGCTLMSR_BTINT;\n-\n-            if ( cpu_has(&current_cpu_data, X86_FEATURE_DSCPL) )\n-                supported |= IA32_DEBUGCTLMSR_BTS_OFF_OS |\n-                             IA32_DEBUGCTLMSR_BTS_OFF_USR;\n-            if ( !(msr_content & ~supported) &&\n-                 vpmu_is_set(vpmu, VPMU_CPU_HAS_BTS) )\n-                return 0;\n-            if ( (msr_content & supported) &&\n-                 !vpmu_is_set(vpmu, VPMU_CPU_HAS_BTS) )\n-                printk(XENLOG_G_WARNING\n-                       \"%pv: Debug Store unsupported on this CPU\\n\",\n-                       current);\n-        }\n         return -EINVAL;\n-    }\n \n     ASSERT(!supported);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    {",
                "        /* Special handling for BTS */",
                "        if ( msr == MSR_IA32_DEBUGCTLMSR )",
                "        {",
                "            supported |= IA32_DEBUGCTLMSR_TR | IA32_DEBUGCTLMSR_BTS |",
                "                         IA32_DEBUGCTLMSR_BTINT;",
                "",
                "            if ( cpu_has(&current_cpu_data, X86_FEATURE_DSCPL) )",
                "                supported |= IA32_DEBUGCTLMSR_BTS_OFF_OS |",
                "                             IA32_DEBUGCTLMSR_BTS_OFF_USR;",
                "            if ( !(msr_content & ~supported) &&",
                "                 vpmu_is_set(vpmu, VPMU_CPU_HAS_BTS) )",
                "                return 0;",
                "            if ( (msr_content & supported) &&",
                "                 !vpmu_is_set(vpmu, VPMU_CPU_HAS_BTS) )",
                "                printk(XENLOG_G_WARNING",
                "                       \"%pv: Debug Store unsupported on this CPU\\n\",",
                "                       current);",
                "        }",
                "    }"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2018-15468",
        "func_name": "xen-project/xen/vmx_msr_write_intercept",
        "description": "An issue was discovered in Xen through 4.11.x. The DEBUGCTL MSR contains several debugging features, some of which virtualise cleanly, but some do not. In particular, Branch Trace Store is not virtualised by the processor, and software has to be careful to configure it suitably not to lock up the core. As a result, it must only be available to fully trusted guests. Unfortunately, in the case that vPMU is disabled, all value checking was skipped, allowing the guest to choose any MSR_DEBUGCTL setting it likes. A malicious or buggy guest administrator (on Intel x86 HVM or PVH) can lock up the entire host, causing a Denial of Service.",
        "git_url": "https://github.com/xen-project/xen/commit/2a8a8e99feb950504559196521bc9fd63ed3a962",
        "commit_title": "x86/vtx: Fix the checking for unknown/invalid MSR_DEBUGCTL bits",
        "commit_text": " The VPMU_MODE_OFF early-exit in vpmu_do_wrmsr() introduced by c/s 11fe998e56 bypasses all reserved bit checking in the general case.  As a result, a guest can enable BTS when it shouldn't be permitted to, and lock up the entire host.  With vPMU active (not a security supported configuration, but useful for debugging), the reserved bit checking in broken, caused by the original BTS changeset 1a8aa75ed.  From a correctness standpoint, it is not possible to have two different pieces of code responsible for different parts of value checking, if there isn't an accumulation of bits which have been checked.  A practical upshot of this is that a guest can set any value it wishes (usually resulting in a vmentry failure for bad guest state).  Therefore, fix this by implementing all the reserved bit checking in the main MSR_DEBUGCTL block, and removing all handling of DEBUGCTL from the vPMU MSR logic.  This is XSA-269. ",
        "func_before": "static int vmx_msr_write_intercept(unsigned int msr, uint64_t msr_content)\n{\n    struct vcpu *v = current;\n\n    HVM_DBG_LOG(DBG_LEVEL_MSR, \"ecx=%#x, msr_value=%#\"PRIx64, msr, msr_content);\n\n    switch ( msr )\n    {\n    case MSR_IA32_SYSENTER_CS:\n        __vmwrite(GUEST_SYSENTER_CS, msr_content);\n        break;\n    case MSR_IA32_SYSENTER_ESP:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n        __vmwrite(GUEST_SYSENTER_ESP, msr_content);\n        break;\n    case MSR_IA32_SYSENTER_EIP:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n        __vmwrite(GUEST_SYSENTER_EIP, msr_content);\n        break;\n\n    case MSR_FS_BASE:\n    case MSR_GS_BASE:\n    case MSR_SHADOW_GS_BASE:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n\n        if ( msr == MSR_FS_BASE )\n            __vmwrite(GUEST_FS_BASE, msr_content);\n        else if ( msr == MSR_GS_BASE )\n            __vmwrite(GUEST_GS_BASE, msr_content);\n        else\n            wrgsshadow(msr_content);\n\n        break;\n\n    case MSR_STAR:\n        v->arch.hvm_vmx.star = msr_content;\n        wrmsrl(MSR_STAR, msr_content);\n        break;\n\n    case MSR_LSTAR:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n        v->arch.hvm_vmx.lstar = msr_content;\n        wrmsrl(MSR_LSTAR, msr_content);\n        break;\n\n    case MSR_CSTAR:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n        v->arch.hvm_vmx.cstar = msr_content;\n        break;\n\n    case MSR_SYSCALL_MASK:\n        v->arch.hvm_vmx.sfmask = msr_content;\n        wrmsrl(MSR_SYSCALL_MASK, msr_content);\n        break;\n\n    case MSR_IA32_DEBUGCTLMSR: {\n        uint64_t supported = IA32_DEBUGCTLMSR_LBR | IA32_DEBUGCTLMSR_BTF;\n\n        if ( boot_cpu_has(X86_FEATURE_RTM) )\n            supported |= IA32_DEBUGCTLMSR_RTM;\n        if ( msr_content & ~supported )\n        {\n            /* Perhaps some other bits are supported in vpmu. */\n            if ( vpmu_do_wrmsr(msr, msr_content, supported) )\n                break;\n        }\n\n        /*\n         * When a guest first enables LBR, arrange to save and restore the LBR\n         * MSRs and allow the guest direct access.\n         *\n         * MSR_DEBUGCTL and LBR has existed almost as long as MSRs have\n         * existed, and there is no architectural way to hide the feature, or\n         * fail the attempt to enable LBR.\n         *\n         * Unknown host LBR MSRs or hitting -ENOSPC with the guest load/save\n         * list are definitely hypervisor bugs, whereas -ENOMEM for allocating\n         * the load/save list is simply unlucky (and shouldn't occur with\n         * sensible management by the toolstack).\n         *\n         * Either way, there is nothing we can do right now to recover, and\n         * the guest won't execute correctly either.  Simply crash the domain\n         * to make the failure obvious.\n         */\n        if ( !(v->arch.hvm_vmx.lbr_flags & LBR_MSRS_INSERTED) &&\n             (msr_content & IA32_DEBUGCTLMSR_LBR) )\n        {\n            const struct lbr_info *lbr = last_branch_msr_get();\n\n            if ( unlikely(!lbr) )\n            {\n                gprintk(XENLOG_ERR, \"Unknown Host LBR MSRs\\n\");\n                domain_crash(v->domain);\n                return X86EMUL_OKAY;\n            }\n\n            for ( ; lbr->count; lbr++ )\n            {\n                unsigned int i;\n\n                for ( i = 0; i < lbr->count; i++ )\n                {\n                    int rc = vmx_add_guest_msr(v, lbr->base + i, 0);\n\n                    if ( unlikely(rc) )\n                    {\n                        gprintk(XENLOG_ERR,\n                                \"Guest load/save list error %d\\n\", rc);\n                        domain_crash(v->domain);\n                        return X86EMUL_OKAY;\n                    }\n\n                    vmx_clear_msr_intercept(v, lbr->base + i, VMX_MSR_RW);\n                }\n            }\n\n            v->arch.hvm_vmx.lbr_flags |= LBR_MSRS_INSERTED;\n            if ( lbr_tsx_fixup_needed )\n                v->arch.hvm_vmx.lbr_flags |= LBR_FIXUP_TSX;\n            if ( bdw_erratum_bdf14_fixup_needed )\n                v->arch.hvm_vmx.lbr_flags |= LBR_FIXUP_BDF14;\n        }\n\n        __vmwrite(GUEST_IA32_DEBUGCTL, msr_content);\n        break;\n    }\n    case MSR_IA32_FEATURE_CONTROL:\n    case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n        /* None of these MSRs are writeable. */\n        goto gp_fault;\n\n    case MSR_P6_PERFCTR(0)...MSR_P6_PERFCTR(7):\n    case MSR_P6_EVNTSEL(0)...MSR_P6_EVNTSEL(7):\n    case MSR_CORE_PERF_FIXED_CTR0...MSR_CORE_PERF_FIXED_CTR2:\n    case MSR_CORE_PERF_FIXED_CTR_CTRL...MSR_CORE_PERF_GLOBAL_OVF_CTRL:\n    case MSR_IA32_PEBS_ENABLE:\n    case MSR_IA32_DS_AREA:\n         if ( vpmu_do_wrmsr(msr, msr_content, 0) )\n            goto gp_fault;\n        break;\n\n    default:\n        if ( passive_domain_do_wrmsr(msr, msr_content) )\n            return X86EMUL_OKAY;\n\n        if ( wrmsr_viridian_regs(msr, msr_content) ) \n            break;\n\n        if ( vmx_write_guest_msr(v, msr, msr_content) == 0 ||\n             is_last_branch_msr(msr) )\n            break;\n\n        switch ( wrmsr_hypervisor_regs(msr, msr_content) )\n        {\n        case -ERESTART:\n            return X86EMUL_RETRY;\n        case 0:\n            /*\n             * Match up with the RDMSR side for now; ultimately this\n             * entire case block should go away.\n             */\n            if ( rdmsr_safe(msr, msr_content) == 0 )\n                break;\n            goto gp_fault;\n        case 1:\n            break;\n        default:\n            goto gp_fault;\n        }\n        break;\n    }\n\n    return X86EMUL_OKAY;\n\ngp_fault:\n    return X86EMUL_EXCEPTION;\n}",
        "func": "static int vmx_msr_write_intercept(unsigned int msr, uint64_t msr_content)\n{\n    struct vcpu *v = current;\n    const struct cpuid_policy *cp = v->domain->arch.cpuid;\n\n    HVM_DBG_LOG(DBG_LEVEL_MSR, \"ecx=%#x, msr_value=%#\"PRIx64, msr, msr_content);\n\n    switch ( msr )\n    {\n        uint64_t rsvd;\n\n    case MSR_IA32_SYSENTER_CS:\n        __vmwrite(GUEST_SYSENTER_CS, msr_content);\n        break;\n    case MSR_IA32_SYSENTER_ESP:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n        __vmwrite(GUEST_SYSENTER_ESP, msr_content);\n        break;\n    case MSR_IA32_SYSENTER_EIP:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n        __vmwrite(GUEST_SYSENTER_EIP, msr_content);\n        break;\n\n    case MSR_FS_BASE:\n    case MSR_GS_BASE:\n    case MSR_SHADOW_GS_BASE:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n\n        if ( msr == MSR_FS_BASE )\n            __vmwrite(GUEST_FS_BASE, msr_content);\n        else if ( msr == MSR_GS_BASE )\n            __vmwrite(GUEST_GS_BASE, msr_content);\n        else\n            wrgsshadow(msr_content);\n\n        break;\n\n    case MSR_STAR:\n        v->arch.hvm_vmx.star = msr_content;\n        wrmsrl(MSR_STAR, msr_content);\n        break;\n\n    case MSR_LSTAR:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n        v->arch.hvm_vmx.lstar = msr_content;\n        wrmsrl(MSR_LSTAR, msr_content);\n        break;\n\n    case MSR_CSTAR:\n        if ( !is_canonical_address(msr_content) )\n            goto gp_fault;\n        v->arch.hvm_vmx.cstar = msr_content;\n        break;\n\n    case MSR_SYSCALL_MASK:\n        v->arch.hvm_vmx.sfmask = msr_content;\n        wrmsrl(MSR_SYSCALL_MASK, msr_content);\n        break;\n\n    case MSR_IA32_DEBUGCTLMSR:\n        rsvd = ~(IA32_DEBUGCTLMSR_LBR | IA32_DEBUGCTLMSR_BTF);\n\n        /* TODO: Wire vPMU settings properly through the CPUID policy */\n        if ( vpmu_is_set(vcpu_vpmu(v), VPMU_CPU_HAS_BTS) )\n        {\n            rsvd &= ~(IA32_DEBUGCTLMSR_TR | IA32_DEBUGCTLMSR_BTS |\n                      IA32_DEBUGCTLMSR_BTINT);\n\n            if ( cpu_has(&current_cpu_data, X86_FEATURE_DSCPL) )\n                rsvd &= ~(IA32_DEBUGCTLMSR_BTS_OFF_OS |\n                          IA32_DEBUGCTLMSR_BTS_OFF_USR);\n        }\n\n        if ( cp->feat.rtm )\n            rsvd &= ~IA32_DEBUGCTLMSR_RTM;\n\n        if ( msr_content & rsvd )\n            goto gp_fault;\n\n        /*\n         * When a guest first enables LBR, arrange to save and restore the LBR\n         * MSRs and allow the guest direct access.\n         *\n         * MSR_DEBUGCTL and LBR has existed almost as long as MSRs have\n         * existed, and there is no architectural way to hide the feature, or\n         * fail the attempt to enable LBR.\n         *\n         * Unknown host LBR MSRs or hitting -ENOSPC with the guest load/save\n         * list are definitely hypervisor bugs, whereas -ENOMEM for allocating\n         * the load/save list is simply unlucky (and shouldn't occur with\n         * sensible management by the toolstack).\n         *\n         * Either way, there is nothing we can do right now to recover, and\n         * the guest won't execute correctly either.  Simply crash the domain\n         * to make the failure obvious.\n         */\n        if ( !(v->arch.hvm_vmx.lbr_flags & LBR_MSRS_INSERTED) &&\n             (msr_content & IA32_DEBUGCTLMSR_LBR) )\n        {\n            const struct lbr_info *lbr = last_branch_msr_get();\n\n            if ( unlikely(!lbr) )\n            {\n                gprintk(XENLOG_ERR, \"Unknown Host LBR MSRs\\n\");\n                domain_crash(v->domain);\n                return X86EMUL_OKAY;\n            }\n\n            for ( ; lbr->count; lbr++ )\n            {\n                unsigned int i;\n\n                for ( i = 0; i < lbr->count; i++ )\n                {\n                    int rc = vmx_add_guest_msr(v, lbr->base + i, 0);\n\n                    if ( unlikely(rc) )\n                    {\n                        gprintk(XENLOG_ERR,\n                                \"Guest load/save list error %d\\n\", rc);\n                        domain_crash(v->domain);\n                        return X86EMUL_OKAY;\n                    }\n\n                    vmx_clear_msr_intercept(v, lbr->base + i, VMX_MSR_RW);\n                }\n            }\n\n            v->arch.hvm_vmx.lbr_flags |= LBR_MSRS_INSERTED;\n            if ( lbr_tsx_fixup_needed )\n                v->arch.hvm_vmx.lbr_flags |= LBR_FIXUP_TSX;\n            if ( bdw_erratum_bdf14_fixup_needed )\n                v->arch.hvm_vmx.lbr_flags |= LBR_FIXUP_BDF14;\n        }\n\n        __vmwrite(GUEST_IA32_DEBUGCTL, msr_content);\n        break;\n\n    case MSR_IA32_FEATURE_CONTROL:\n    case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n        /* None of these MSRs are writeable. */\n        goto gp_fault;\n\n    case MSR_P6_PERFCTR(0)...MSR_P6_PERFCTR(7):\n    case MSR_P6_EVNTSEL(0)...MSR_P6_EVNTSEL(7):\n    case MSR_CORE_PERF_FIXED_CTR0...MSR_CORE_PERF_FIXED_CTR2:\n    case MSR_CORE_PERF_FIXED_CTR_CTRL...MSR_CORE_PERF_GLOBAL_OVF_CTRL:\n    case MSR_IA32_PEBS_ENABLE:\n    case MSR_IA32_DS_AREA:\n         if ( vpmu_do_wrmsr(msr, msr_content, 0) )\n            goto gp_fault;\n        break;\n\n    default:\n        if ( passive_domain_do_wrmsr(msr, msr_content) )\n            return X86EMUL_OKAY;\n\n        if ( wrmsr_viridian_regs(msr, msr_content) ) \n            break;\n\n        if ( vmx_write_guest_msr(v, msr, msr_content) == 0 ||\n             is_last_branch_msr(msr) )\n            break;\n\n        switch ( wrmsr_hypervisor_regs(msr, msr_content) )\n        {\n        case -ERESTART:\n            return X86EMUL_RETRY;\n        case 0:\n            /*\n             * Match up with the RDMSR side for now; ultimately this\n             * entire case block should go away.\n             */\n            if ( rdmsr_safe(msr, msr_content) == 0 )\n                break;\n            goto gp_fault;\n        case 1:\n            break;\n        default:\n            goto gp_fault;\n        }\n        break;\n    }\n\n    return X86EMUL_OKAY;\n\ngp_fault:\n    return X86EMUL_EXCEPTION;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,14 @@\n static int vmx_msr_write_intercept(unsigned int msr, uint64_t msr_content)\n {\n     struct vcpu *v = current;\n+    const struct cpuid_policy *cp = v->domain->arch.cpuid;\n \n     HVM_DBG_LOG(DBG_LEVEL_MSR, \"ecx=%#x, msr_value=%#\"PRIx64, msr, msr_content);\n \n     switch ( msr )\n     {\n+        uint64_t rsvd;\n+\n     case MSR_IA32_SYSENTER_CS:\n         __vmwrite(GUEST_SYSENTER_CS, msr_content);\n         break;\n@@ -58,17 +61,25 @@\n         wrmsrl(MSR_SYSCALL_MASK, msr_content);\n         break;\n \n-    case MSR_IA32_DEBUGCTLMSR: {\n-        uint64_t supported = IA32_DEBUGCTLMSR_LBR | IA32_DEBUGCTLMSR_BTF;\n+    case MSR_IA32_DEBUGCTLMSR:\n+        rsvd = ~(IA32_DEBUGCTLMSR_LBR | IA32_DEBUGCTLMSR_BTF);\n \n-        if ( boot_cpu_has(X86_FEATURE_RTM) )\n-            supported |= IA32_DEBUGCTLMSR_RTM;\n-        if ( msr_content & ~supported )\n+        /* TODO: Wire vPMU settings properly through the CPUID policy */\n+        if ( vpmu_is_set(vcpu_vpmu(v), VPMU_CPU_HAS_BTS) )\n         {\n-            /* Perhaps some other bits are supported in vpmu. */\n-            if ( vpmu_do_wrmsr(msr, msr_content, supported) )\n-                break;\n+            rsvd &= ~(IA32_DEBUGCTLMSR_TR | IA32_DEBUGCTLMSR_BTS |\n+                      IA32_DEBUGCTLMSR_BTINT);\n+\n+            if ( cpu_has(&current_cpu_data, X86_FEATURE_DSCPL) )\n+                rsvd &= ~(IA32_DEBUGCTLMSR_BTS_OFF_OS |\n+                          IA32_DEBUGCTLMSR_BTS_OFF_USR);\n         }\n+\n+        if ( cp->feat.rtm )\n+            rsvd &= ~IA32_DEBUGCTLMSR_RTM;\n+\n+        if ( msr_content & rsvd )\n+            goto gp_fault;\n \n         /*\n          * When a guest first enables LBR, arrange to save and restore the LBR\n@@ -128,7 +139,7 @@\n \n         __vmwrite(GUEST_IA32_DEBUGCTL, msr_content);\n         break;\n-    }\n+\n     case MSR_IA32_FEATURE_CONTROL:\n     case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n         /* None of these MSRs are writeable. */",
        "diff_line_info": {
            "deleted_lines": [
                "    case MSR_IA32_DEBUGCTLMSR: {",
                "        uint64_t supported = IA32_DEBUGCTLMSR_LBR | IA32_DEBUGCTLMSR_BTF;",
                "        if ( boot_cpu_has(X86_FEATURE_RTM) )",
                "            supported |= IA32_DEBUGCTLMSR_RTM;",
                "        if ( msr_content & ~supported )",
                "            /* Perhaps some other bits are supported in vpmu. */",
                "            if ( vpmu_do_wrmsr(msr, msr_content, supported) )",
                "                break;",
                "    }"
            ],
            "added_lines": [
                "    const struct cpuid_policy *cp = v->domain->arch.cpuid;",
                "        uint64_t rsvd;",
                "",
                "    case MSR_IA32_DEBUGCTLMSR:",
                "        rsvd = ~(IA32_DEBUGCTLMSR_LBR | IA32_DEBUGCTLMSR_BTF);",
                "        /* TODO: Wire vPMU settings properly through the CPUID policy */",
                "        if ( vpmu_is_set(vcpu_vpmu(v), VPMU_CPU_HAS_BTS) )",
                "            rsvd &= ~(IA32_DEBUGCTLMSR_TR | IA32_DEBUGCTLMSR_BTS |",
                "                      IA32_DEBUGCTLMSR_BTINT);",
                "",
                "            if ( cpu_has(&current_cpu_data, X86_FEATURE_DSCPL) )",
                "                rsvd &= ~(IA32_DEBUGCTLMSR_BTS_OFF_OS |",
                "                          IA32_DEBUGCTLMSR_BTS_OFF_USR);",
                "",
                "        if ( cp->feat.rtm )",
                "            rsvd &= ~IA32_DEBUGCTLMSR_RTM;",
                "",
                "        if ( msr_content & rsvd )",
                "            goto gp_fault;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-16597",
        "func_name": "torvalds/linux/ovl_get_acl",
        "description": "An issue was discovered in the Linux kernel before 4.8. Incorrect access checking in overlayfs mounts could be used by local attackers to modify or truncate files in the underlying filesystem.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c0ca3d70e8d3cf81e2255a217f7ca402f5ed0862",
        "commit_title": "Right now ovl_permission() calls __inode_permission(realinode), to do",
        "commit_text": "permission checks on real inode and no checks are done on overlay inode.  Modify it to do checks both on overlay inode as well as underlying inode. Checks on overlay inode will be done with the creds of calling task while checks on underlying inode will be done with the creds of mounter.  ",
        "func_before": "struct posix_acl *ovl_get_acl(struct inode *inode, int type)\n{\n\tstruct inode *realinode = ovl_inode_real(inode);\n\n\tif (!realinode)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (!IS_POSIXACL(realinode))\n\t\treturn NULL;\n\n\tif (!realinode->i_op->get_acl)\n\t\treturn NULL;\n\n\treturn realinode->i_op->get_acl(realinode, type);\n}",
        "func": "struct posix_acl *ovl_get_acl(struct inode *inode, int type)\n{\n\tstruct inode *realinode = ovl_inode_real(inode);\n\n\tif (!IS_POSIXACL(realinode))\n\t\treturn NULL;\n\n\tif (!realinode->i_op->get_acl)\n\t\treturn NULL;\n\n\treturn realinode->i_op->get_acl(realinode, type);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,6 @@\n struct posix_acl *ovl_get_acl(struct inode *inode, int type)\n {\n \tstruct inode *realinode = ovl_inode_real(inode);\n-\n-\tif (!realinode)\n-\t\treturn ERR_PTR(-ENOENT);\n \n \tif (!IS_POSIXACL(realinode))\n \t\treturn NULL;",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\tif (!realinode)",
                "\t\treturn ERR_PTR(-ENOENT);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2018-16597",
        "func_name": "torvalds/linux/ovl_permission",
        "description": "An issue was discovered in the Linux kernel before 4.8. Incorrect access checking in overlayfs mounts could be used by local attackers to modify or truncate files in the underlying filesystem.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c0ca3d70e8d3cf81e2255a217f7ca402f5ed0862",
        "commit_title": "Right now ovl_permission() calls __inode_permission(realinode), to do",
        "commit_text": "permission checks on real inode and no checks are done on overlay inode.  Modify it to do checks both on overlay inode as well as underlying inode. Checks on overlay inode will be done with the creds of calling task while checks on underlying inode will be done with the creds of mounter.  ",
        "func_before": "int ovl_permission(struct inode *inode, int mask)\n{\n\tstruct ovl_entry *oe = inode->i_private;\n\tbool is_upper;\n\tstruct dentry *realdentry = ovl_entry_real(oe, &is_upper);\n\tstruct inode *realinode;\n\tint err;\n\n\tif (ovl_is_default_permissions(inode)) {\n\t\tstruct kstat stat;\n\t\tstruct path realpath = { .dentry = realdentry };\n\n\t\tif (mask & MAY_NOT_BLOCK)\n\t\t\treturn -ECHILD;\n\n\t\trealpath.mnt = ovl_entry_mnt_real(oe, inode, is_upper);\n\n\t\terr = vfs_getattr(&realpath, &stat);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((stat.mode ^ inode->i_mode) & S_IFMT)\n\t\t\treturn -ESTALE;\n\n\t\tinode->i_mode = stat.mode;\n\t\tinode->i_uid = stat.uid;\n\t\tinode->i_gid = stat.gid;\n\n\t\treturn generic_permission(inode, mask);\n\t}\n\n\t/* Careful in RCU walk mode */\n\trealinode = d_inode_rcu(realdentry);\n\tif (!realinode) {\n\t\tWARN_ON(!(mask & MAY_NOT_BLOCK));\n\t\treturn -ENOENT;\n\t}\n\n\tif (mask & MAY_WRITE) {\n\t\tumode_t mode = realinode->i_mode;\n\n\t\t/*\n\t\t * Writes will always be redirected to upper layer, so\n\t\t * ignore lower layer being read-only.\n\t\t *\n\t\t * If the overlay itself is read-only then proceed\n\t\t * with the permission check, don't return EROFS.\n\t\t * This will only happen if this is the lower layer of\n\t\t * another overlayfs.\n\t\t *\n\t\t * If upper fs becomes read-only after the overlay was\n\t\t * constructed return EROFS to prevent modification of\n\t\t * upper layer.\n\t\t */\n\t\tif (is_upper && !IS_RDONLY(inode) && IS_RDONLY(realinode) &&\n\t\t    (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode)))\n\t\t\treturn -EROFS;\n\t}\n\n\treturn __inode_permission(realinode, mask);\n}",
        "func": "int ovl_permission(struct inode *inode, int mask)\n{\n\tstruct ovl_entry *oe = inode->i_private;\n\tbool is_upper;\n\tstruct dentry *realdentry = ovl_entry_real(oe, &is_upper);\n\tstruct inode *realinode;\n\tconst struct cred *old_cred;\n\tint err;\n\n\tif (ovl_is_default_permissions(inode)) {\n\t\tstruct kstat stat;\n\t\tstruct path realpath = { .dentry = realdentry };\n\n\t\tif (mask & MAY_NOT_BLOCK)\n\t\t\treturn -ECHILD;\n\n\t\trealpath.mnt = ovl_entry_mnt_real(oe, inode, is_upper);\n\n\t\terr = vfs_getattr(&realpath, &stat);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((stat.mode ^ inode->i_mode) & S_IFMT)\n\t\t\treturn -ESTALE;\n\n\t\tinode->i_mode = stat.mode;\n\t\tinode->i_uid = stat.uid;\n\t\tinode->i_gid = stat.gid;\n\n\t\treturn generic_permission(inode, mask);\n\t}\n\n\t/* Careful in RCU walk mode */\n\trealinode = d_inode_rcu(realdentry);\n\tif (!realinode) {\n\t\tWARN_ON(!(mask & MAY_NOT_BLOCK));\n\t\treturn -ENOENT;\n\t}\n\n\tif (mask & MAY_WRITE) {\n\t\tumode_t mode = realinode->i_mode;\n\n\t\t/*\n\t\t * Writes will always be redirected to upper layer, so\n\t\t * ignore lower layer being read-only.\n\t\t *\n\t\t * If the overlay itself is read-only then proceed\n\t\t * with the permission check, don't return EROFS.\n\t\t * This will only happen if this is the lower layer of\n\t\t * another overlayfs.\n\t\t *\n\t\t * If upper fs becomes read-only after the overlay was\n\t\t * constructed return EROFS to prevent modification of\n\t\t * upper layer.\n\t\t */\n\t\tif (is_upper && !IS_RDONLY(inode) && IS_RDONLY(realinode) &&\n\t\t    (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode)))\n\t\t\treturn -EROFS;\n\t}\n\n\t/*\n\t * Check overlay inode with the creds of task and underlying inode\n\t * with creds of mounter\n\t */\n\terr = generic_permission(inode, mask);\n\tif (err)\n\t\treturn err;\n\n\told_cred = ovl_override_creds(inode->i_sb);\n\terr = __inode_permission(realinode, mask);\n\trevert_creds(old_cred);\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,7 @@\n \tbool is_upper;\n \tstruct dentry *realdentry = ovl_entry_real(oe, &is_upper);\n \tstruct inode *realinode;\n+\tconst struct cred *old_cred;\n \tint err;\n \n \tif (ovl_is_default_permissions(inode)) {\n@@ -57,5 +58,17 @@\n \t\t\treturn -EROFS;\n \t}\n \n-\treturn __inode_permission(realinode, mask);\n+\t/*\n+\t * Check overlay inode with the creds of task and underlying inode\n+\t * with creds of mounter\n+\t */\n+\terr = generic_permission(inode, mask);\n+\tif (err)\n+\t\treturn err;\n+\n+\told_cred = ovl_override_creds(inode->i_sb);\n+\terr = __inode_permission(realinode, mask);\n+\trevert_creds(old_cred);\n+\n+\treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn __inode_permission(realinode, mask);"
            ],
            "added_lines": [
                "\tconst struct cred *old_cred;",
                "\t/*",
                "\t * Check overlay inode with the creds of task and underlying inode",
                "\t * with creds of mounter",
                "\t */",
                "\terr = generic_permission(inode, mask);",
                "\tif (err)",
                "\t\treturn err;",
                "",
                "\told_cred = ovl_override_creds(inode->i_sb);",
                "\terr = __inode_permission(realinode, mask);",
                "\trevert_creds(old_cred);",
                "",
                "\treturn err;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-14665",
        "func_name": "xorg/xserver/ddxProcessArgument",
        "description": "A flaw was found in xorg-x11-server before 1.20.3. An incorrect permission check for -modulepath and -logfile options when starting Xorg. X server allows unprivileged users with the ability to log in to the system via physical console to escalate their privileges and run arbitrary code under root privileges.",
        "git_url": "https://cgit.freedesktop.org/xorg/xserver/commit/?id=8a59e3b7dbb30532a7c3769c555e00d7c4301170",
        "commit_title": "Could cause privilege elevation and/or arbitrary files overwrite, when",
        "commit_text": "the X server is running with elevated privileges (ie when Xorg is installed with the setuid bit set and started by a non-root user).  CVE-2018-14665  Issue reported by Narendra Shinde and Red Hat.  (cherry picked from commit 50c0cf885a6e91c0ea71fb49fa8f1b7c86fe330e) ",
        "func_before": "int\nddxProcessArgument(int argc, char **argv, int i)\n{\n#define CHECK_FOR_REQUIRED_ARGUMENT() \\\n    if (((i + 1) >= argc) || (!argv[i + 1])) { \t\t\t\t\\\n      ErrorF(\"Required argument to %s not specified\\n\", argv[i]); \t\\\n      UseMsg(); \t\t\t\t\t\t\t\\\n      FatalError(\"Required argument to %s not specified\\n\", argv[i]);\t\\\n    }\n\n    /* First the options that are not allowed with elevated privileges */\n    if (!strcmp(argv[i], \"-modulepath\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86CheckPrivs(argv[i], argv[i + 1]);\n        xf86ModulePath = argv[i + 1];\n        xf86ModPathFrom = X_CMDLINE;\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-logfile\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86CheckPrivs(argv[i], argv[i + 1]);\n        xf86LogFile = argv[i + 1];\n        xf86LogFileFrom = X_CMDLINE;\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-config\") || !strcmp(argv[i], \"-xf86config\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86CheckPrivs(argv[i], argv[i + 1]);\n        xf86ConfigFile = argv[i + 1];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-configdir\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86CheckPrivs(argv[i], argv[i + 1]);\n        xf86ConfigDir = argv[i + 1];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-flipPixels\")) {\n        xf86FlipPixels = TRUE;\n        return 1;\n    }\n#ifdef XF86VIDMODE\n    if (!strcmp(argv[i], \"-disableVidMode\")) {\n        xf86VidModeDisabled = TRUE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-allowNonLocalXvidtune\")) {\n        xf86VidModeAllowNonLocal = TRUE;\n        return 1;\n    }\n#endif\n    if (!strcmp(argv[i], \"-allowMouseOpenFail\")) {\n        xf86AllowMouseOpenFail = TRUE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-ignoreABI\")) {\n        LoaderSetOptions(LDR_OPT_ABI_MISMATCH_NONFATAL);\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-verbose\")) {\n        if (++i < argc && argv[i]) {\n            char *end;\n            long val;\n\n            val = strtol(argv[i], &end, 0);\n            if (*end == '\\0') {\n                xf86SetVerbosity(val);\n                return 2;\n            }\n        }\n        xf86SetVerbosity(++xf86Verbose);\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-logverbose\")) {\n        if (++i < argc && argv[i]) {\n            char *end;\n            long val;\n\n            val = strtol(argv[i], &end, 0);\n            if (*end == '\\0') {\n                xf86SetLogVerbosity(val);\n                return 2;\n            }\n        }\n        xf86SetLogVerbosity(++xf86LogVerbose);\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-quiet\")) {\n        xf86SetVerbosity(-1);\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-showconfig\") || !strcmp(argv[i], \"-version\")) {\n        xf86PrintBanner();\n        exit(0);\n    }\n    if (!strcmp(argv[i], \"-showDefaultModulePath\")) {\n        xf86PrintDefaultModulePath();\n        exit(0);\n    }\n    if (!strcmp(argv[i], \"-showDefaultLibPath\")) {\n        xf86PrintDefaultLibraryPath();\n        exit(0);\n    }\n    /* Notice the -fp flag, but allow it to pass to the dix layer */\n    if (!strcmp(argv[i], \"-fp\")) {\n        xf86fpFlag = TRUE;\n        return 0;\n    }\n    /* Notice the -bs flag, but allow it to pass to the dix layer */\n    if (!strcmp(argv[i], \"-bs\")) {\n        xf86bsDisableFlag = TRUE;\n        return 0;\n    }\n    /* Notice the +bs flag, but allow it to pass to the dix layer */\n    if (!strcmp(argv[i], \"+bs\")) {\n        xf86bsEnableFlag = TRUE;\n        return 0;\n    }\n    /* Notice the -s flag, but allow it to pass to the dix layer */\n    if (!strcmp(argv[i], \"-s\")) {\n        xf86sFlag = TRUE;\n        return 0;\n    }\n    if (!strcmp(argv[i], \"-pixmap32\") || !strcmp(argv[i], \"-pixmap24\")) {\n        /* silently accept */\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-fbbpp\")) {\n        int bpp;\n\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (sscanf(argv[++i], \"%d\", &bpp) == 1) {\n            xf86FbBpp = bpp;\n            return 2;\n        }\n        else {\n            ErrorF(\"Invalid fbbpp\\n\");\n            return 0;\n        }\n    }\n    if (!strcmp(argv[i], \"-depth\")) {\n        int depth;\n\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (sscanf(argv[++i], \"%d\", &depth) == 1) {\n            xf86Depth = depth;\n            return 2;\n        }\n        else {\n            ErrorF(\"Invalid depth\\n\");\n            return 0;\n        }\n    }\n    if (!strcmp(argv[i], \"-weight\")) {\n        int red, green, blue;\n\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (sscanf(argv[++i], \"%1d%1d%1d\", &red, &green, &blue) == 3) {\n            xf86Weight.red = red;\n            xf86Weight.green = green;\n            xf86Weight.blue = blue;\n            return 2;\n        }\n        else {\n            ErrorF(\"Invalid weighting\\n\");\n            return 0;\n        }\n    }\n    if (!strcmp(argv[i], \"-gamma\") || !strcmp(argv[i], \"-rgamma\") ||\n        !strcmp(argv[i], \"-ggamma\") || !strcmp(argv[i], \"-bgamma\")) {\n        double gamma;\n\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (sscanf(argv[++i], \"%lf\", &gamma) == 1) {\n            if (gamma < GAMMA_MIN || gamma > GAMMA_MAX) {\n                ErrorF(\"gamma out of range, only  %.2f <= gamma_value <= %.1f\"\n                       \" is valid\\n\", GAMMA_MIN, GAMMA_MAX);\n                return 0;\n            }\n            if (!strcmp(argv[i - 1], \"-gamma\"))\n                xf86Gamma.red = xf86Gamma.green = xf86Gamma.blue = gamma;\n            else if (!strcmp(argv[i - 1], \"-rgamma\"))\n                xf86Gamma.red = gamma;\n            else if (!strcmp(argv[i - 1], \"-ggamma\"))\n                xf86Gamma.green = gamma;\n            else if (!strcmp(argv[i - 1], \"-bgamma\"))\n                xf86Gamma.blue = gamma;\n            return 2;\n        }\n    }\n    if (!strcmp(argv[i], \"-layout\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86LayoutName = argv[++i];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-screen\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86ScreenName = argv[++i];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-pointer\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86PointerName = argv[++i];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-keyboard\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86KeyboardName = argv[++i];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-nosilk\")) {\n        xf86silkenMouseDisableFlag = TRUE;\n        return 1;\n    }\n#ifdef HAVE_ACPI\n    if (!strcmp(argv[i], \"-noacpi\")) {\n        xf86acpiDisableFlag = TRUE;\n        return 1;\n    }\n#endif\n    if (!strcmp(argv[i], \"-configure\")) {\n        if (getuid() != 0 && geteuid() == 0) {\n            ErrorF(\"The '-configure' option can only be used by root.\\n\");\n            exit(1);\n        }\n        xf86DoConfigure = TRUE;\n        xf86AllowMouseOpenFail = TRUE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-showopts\")) {\n        if (getuid() != 0 && geteuid() == 0) {\n            ErrorF(\"The '-showopts' option can only be used by root.\\n\");\n            exit(1);\n        }\n        xf86DoShowOptions = TRUE;\n        return 1;\n    }\n#ifdef XSERVER_LIBPCIACCESS\n    if (!strcmp(argv[i], \"-isolateDevice\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (strncmp(argv[++i], \"PCI:\", 4)) {\n            FatalError(\"Bus types other than PCI not yet isolable\\n\");\n        }\n        xf86PciIsolateDevice(argv[i]);\n        return 2;\n    }\n#endif\n    /* Notice cmdline xkbdir, but pass to dix as well */\n    if (!strcmp(argv[i], \"-xkbdir\")) {\n        xf86xkbdirFlag = TRUE;\n        return 0;\n    }\n    if (!strcmp(argv[i], \"-novtswitch\")) {\n        xf86Info.autoVTSwitch = FALSE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-sharevts\")) {\n        xf86Info.ShareVTs = TRUE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-iglx\") || !strcmp(argv[i], \"+iglx\")) {\n        xf86Info.iglxFrom = X_CMDLINE;\n        return 0;\n    }\n\n    /* OS-specific processing */\n    return xf86ProcessArgument(argc, argv, i);\n}",
        "func": "int\nddxProcessArgument(int argc, char **argv, int i)\n{\n#define CHECK_FOR_REQUIRED_ARGUMENT() \\\n    if (((i + 1) >= argc) || (!argv[i + 1])) { \t\t\t\t\\\n      ErrorF(\"Required argument to %s not specified\\n\", argv[i]); \t\\\n      UseMsg(); \t\t\t\t\t\t\t\\\n      FatalError(\"Required argument to %s not specified\\n\", argv[i]);\t\\\n    }\n\n    /* First the options that are not allowed with elevated privileges */\n    if (!strcmp(argv[i], \"-modulepath\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (xf86PrivsElevated())\n              FatalError(\"\\nInvalid argument -modulepath \"\n                \"with elevated privileges\\n\");\n        xf86ModulePath = argv[i + 1];\n        xf86ModPathFrom = X_CMDLINE;\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-logfile\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (xf86PrivsElevated())\n              FatalError(\"\\nInvalid argument -logfile \"\n                \"with elevated privileges\\n\");\n        xf86LogFile = argv[i + 1];\n        xf86LogFileFrom = X_CMDLINE;\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-config\") || !strcmp(argv[i], \"-xf86config\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86CheckPrivs(argv[i], argv[i + 1]);\n        xf86ConfigFile = argv[i + 1];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-configdir\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86CheckPrivs(argv[i], argv[i + 1]);\n        xf86ConfigDir = argv[i + 1];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-flipPixels\")) {\n        xf86FlipPixels = TRUE;\n        return 1;\n    }\n#ifdef XF86VIDMODE\n    if (!strcmp(argv[i], \"-disableVidMode\")) {\n        xf86VidModeDisabled = TRUE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-allowNonLocalXvidtune\")) {\n        xf86VidModeAllowNonLocal = TRUE;\n        return 1;\n    }\n#endif\n    if (!strcmp(argv[i], \"-allowMouseOpenFail\")) {\n        xf86AllowMouseOpenFail = TRUE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-ignoreABI\")) {\n        LoaderSetOptions(LDR_OPT_ABI_MISMATCH_NONFATAL);\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-verbose\")) {\n        if (++i < argc && argv[i]) {\n            char *end;\n            long val;\n\n            val = strtol(argv[i], &end, 0);\n            if (*end == '\\0') {\n                xf86SetVerbosity(val);\n                return 2;\n            }\n        }\n        xf86SetVerbosity(++xf86Verbose);\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-logverbose\")) {\n        if (++i < argc && argv[i]) {\n            char *end;\n            long val;\n\n            val = strtol(argv[i], &end, 0);\n            if (*end == '\\0') {\n                xf86SetLogVerbosity(val);\n                return 2;\n            }\n        }\n        xf86SetLogVerbosity(++xf86LogVerbose);\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-quiet\")) {\n        xf86SetVerbosity(-1);\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-showconfig\") || !strcmp(argv[i], \"-version\")) {\n        xf86PrintBanner();\n        exit(0);\n    }\n    if (!strcmp(argv[i], \"-showDefaultModulePath\")) {\n        xf86PrintDefaultModulePath();\n        exit(0);\n    }\n    if (!strcmp(argv[i], \"-showDefaultLibPath\")) {\n        xf86PrintDefaultLibraryPath();\n        exit(0);\n    }\n    /* Notice the -fp flag, but allow it to pass to the dix layer */\n    if (!strcmp(argv[i], \"-fp\")) {\n        xf86fpFlag = TRUE;\n        return 0;\n    }\n    /* Notice the -bs flag, but allow it to pass to the dix layer */\n    if (!strcmp(argv[i], \"-bs\")) {\n        xf86bsDisableFlag = TRUE;\n        return 0;\n    }\n    /* Notice the +bs flag, but allow it to pass to the dix layer */\n    if (!strcmp(argv[i], \"+bs\")) {\n        xf86bsEnableFlag = TRUE;\n        return 0;\n    }\n    /* Notice the -s flag, but allow it to pass to the dix layer */\n    if (!strcmp(argv[i], \"-s\")) {\n        xf86sFlag = TRUE;\n        return 0;\n    }\n    if (!strcmp(argv[i], \"-pixmap32\") || !strcmp(argv[i], \"-pixmap24\")) {\n        /* silently accept */\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-fbbpp\")) {\n        int bpp;\n\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (sscanf(argv[++i], \"%d\", &bpp) == 1) {\n            xf86FbBpp = bpp;\n            return 2;\n        }\n        else {\n            ErrorF(\"Invalid fbbpp\\n\");\n            return 0;\n        }\n    }\n    if (!strcmp(argv[i], \"-depth\")) {\n        int depth;\n\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (sscanf(argv[++i], \"%d\", &depth) == 1) {\n            xf86Depth = depth;\n            return 2;\n        }\n        else {\n            ErrorF(\"Invalid depth\\n\");\n            return 0;\n        }\n    }\n    if (!strcmp(argv[i], \"-weight\")) {\n        int red, green, blue;\n\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (sscanf(argv[++i], \"%1d%1d%1d\", &red, &green, &blue) == 3) {\n            xf86Weight.red = red;\n            xf86Weight.green = green;\n            xf86Weight.blue = blue;\n            return 2;\n        }\n        else {\n            ErrorF(\"Invalid weighting\\n\");\n            return 0;\n        }\n    }\n    if (!strcmp(argv[i], \"-gamma\") || !strcmp(argv[i], \"-rgamma\") ||\n        !strcmp(argv[i], \"-ggamma\") || !strcmp(argv[i], \"-bgamma\")) {\n        double gamma;\n\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (sscanf(argv[++i], \"%lf\", &gamma) == 1) {\n            if (gamma < GAMMA_MIN || gamma > GAMMA_MAX) {\n                ErrorF(\"gamma out of range, only  %.2f <= gamma_value <= %.1f\"\n                       \" is valid\\n\", GAMMA_MIN, GAMMA_MAX);\n                return 0;\n            }\n            if (!strcmp(argv[i - 1], \"-gamma\"))\n                xf86Gamma.red = xf86Gamma.green = xf86Gamma.blue = gamma;\n            else if (!strcmp(argv[i - 1], \"-rgamma\"))\n                xf86Gamma.red = gamma;\n            else if (!strcmp(argv[i - 1], \"-ggamma\"))\n                xf86Gamma.green = gamma;\n            else if (!strcmp(argv[i - 1], \"-bgamma\"))\n                xf86Gamma.blue = gamma;\n            return 2;\n        }\n    }\n    if (!strcmp(argv[i], \"-layout\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86LayoutName = argv[++i];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-screen\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86ScreenName = argv[++i];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-pointer\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86PointerName = argv[++i];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-keyboard\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        xf86KeyboardName = argv[++i];\n        return 2;\n    }\n    if (!strcmp(argv[i], \"-nosilk\")) {\n        xf86silkenMouseDisableFlag = TRUE;\n        return 1;\n    }\n#ifdef HAVE_ACPI\n    if (!strcmp(argv[i], \"-noacpi\")) {\n        xf86acpiDisableFlag = TRUE;\n        return 1;\n    }\n#endif\n    if (!strcmp(argv[i], \"-configure\")) {\n        if (getuid() != 0 && geteuid() == 0) {\n            ErrorF(\"The '-configure' option can only be used by root.\\n\");\n            exit(1);\n        }\n        xf86DoConfigure = TRUE;\n        xf86AllowMouseOpenFail = TRUE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-showopts\")) {\n        if (getuid() != 0 && geteuid() == 0) {\n            ErrorF(\"The '-showopts' option can only be used by root.\\n\");\n            exit(1);\n        }\n        xf86DoShowOptions = TRUE;\n        return 1;\n    }\n#ifdef XSERVER_LIBPCIACCESS\n    if (!strcmp(argv[i], \"-isolateDevice\")) {\n        CHECK_FOR_REQUIRED_ARGUMENT();\n        if (strncmp(argv[++i], \"PCI:\", 4)) {\n            FatalError(\"Bus types other than PCI not yet isolable\\n\");\n        }\n        xf86PciIsolateDevice(argv[i]);\n        return 2;\n    }\n#endif\n    /* Notice cmdline xkbdir, but pass to dix as well */\n    if (!strcmp(argv[i], \"-xkbdir\")) {\n        xf86xkbdirFlag = TRUE;\n        return 0;\n    }\n    if (!strcmp(argv[i], \"-novtswitch\")) {\n        xf86Info.autoVTSwitch = FALSE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-sharevts\")) {\n        xf86Info.ShareVTs = TRUE;\n        return 1;\n    }\n    if (!strcmp(argv[i], \"-iglx\") || !strcmp(argv[i], \"+iglx\")) {\n        xf86Info.iglxFrom = X_CMDLINE;\n        return 0;\n    }\n\n    /* OS-specific processing */\n    return xf86ProcessArgument(argc, argv, i);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,14 +11,18 @@\n     /* First the options that are not allowed with elevated privileges */\n     if (!strcmp(argv[i], \"-modulepath\")) {\n         CHECK_FOR_REQUIRED_ARGUMENT();\n-        xf86CheckPrivs(argv[i], argv[i + 1]);\n+        if (xf86PrivsElevated())\n+              FatalError(\"\\nInvalid argument -modulepath \"\n+                \"with elevated privileges\\n\");\n         xf86ModulePath = argv[i + 1];\n         xf86ModPathFrom = X_CMDLINE;\n         return 2;\n     }\n     if (!strcmp(argv[i], \"-logfile\")) {\n         CHECK_FOR_REQUIRED_ARGUMENT();\n-        xf86CheckPrivs(argv[i], argv[i + 1]);\n+        if (xf86PrivsElevated())\n+              FatalError(\"\\nInvalid argument -logfile \"\n+                \"with elevated privileges\\n\");\n         xf86LogFile = argv[i + 1];\n         xf86LogFileFrom = X_CMDLINE;\n         return 2;",
        "diff_line_info": {
            "deleted_lines": [
                "        xf86CheckPrivs(argv[i], argv[i + 1]);",
                "        xf86CheckPrivs(argv[i], argv[i + 1]);"
            ],
            "added_lines": [
                "        if (xf86PrivsElevated())",
                "              FatalError(\"\\nInvalid argument -modulepath \"",
                "                \"with elevated privileges\\n\");",
                "        if (xf86PrivsElevated())",
                "              FatalError(\"\\nInvalid argument -logfile \"",
                "                \"with elevated privileges\\n\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-18955",
        "func_name": "torvalds/linux/map_write",
        "description": "In the Linux kernel 4.15.x through 4.19.x before 4.19.2, map_write() in kernel/user_namespace.c allows privilege escalation because it mishandles nested user namespaces with more than 5 UID or GID ranges. A user who has CAP_SYS_ADMIN in an affected user namespace can bypass access controls on resources outside the namespace, as demonstrated by reading /etc/shadow. This occurs because an ID transformation takes place properly for the namespaced-to-kernel direction but not for the kernel-to-namespaced direction.",
        "git_url": "https://github.com/torvalds/linux/commit/d2f007dbe7e4c9583eea6eb04d60001e85c6f1bd",
        "commit_title": "userns: also map extents in the reverse map to kernel IDs",
        "commit_text": " The current logic first clones the extent array and sorts both copies, then maps the lower IDs of the forward mapping into the lower namespace, but doesn't map the lower IDs of the reverse mapping.  This means that code in a nested user namespace with >5 extents will see incorrect IDs. It also breaks some access checks, like inode_owner_or_capable() and privileged_wrt_inode_uidgid(), so a process can incorrectly appear to be capable relative to an inode.  To fix it, we have to make sure that the \"lower_first\" members of extents in both arrays are translated; and we have to make sure that the reverse map is sorted *after* the translation (since otherwise the translation can break the sorting).  This is CVE-2018-18955.  Cc: stable@vger.kernel.org",
        "func_before": "static ssize_t map_write(struct file *file, const char __user *buf,\n\t\t\t size_t count, loff_t *ppos,\n\t\t\t int cap_setid,\n\t\t\t struct uid_gid_map *map,\n\t\t\t struct uid_gid_map *parent_map)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct user_namespace *ns = seq->private;\n\tstruct uid_gid_map new_map;\n\tunsigned idx;\n\tstruct uid_gid_extent extent;\n\tchar *kbuf = NULL, *pos, *next_line;\n\tssize_t ret;\n\n\t/* Only allow < page size writes at the beginning of the file */\n\tif ((*ppos != 0) || (count >= PAGE_SIZE))\n\t\treturn -EINVAL;\n\n\t/* Slurp in the user data */\n\tkbuf = memdup_user_nul(buf, count);\n\tif (IS_ERR(kbuf))\n\t\treturn PTR_ERR(kbuf);\n\n\t/*\n\t * The userns_state_mutex serializes all writes to any given map.\n\t *\n\t * Any map is only ever written once.\n\t *\n\t * An id map fits within 1 cache line on most architectures.\n\t *\n\t * On read nothing needs to be done unless you are on an\n\t * architecture with a crazy cache coherency model like alpha.\n\t *\n\t * There is a one time data dependency between reading the\n\t * count of the extents and the values of the extents.  The\n\t * desired behavior is to see the values of the extents that\n\t * were written before the count of the extents.\n\t *\n\t * To achieve this smp_wmb() is used on guarantee the write\n\t * order and smp_rmb() is guaranteed that we don't have crazy\n\t * architectures returning stale data.\n\t */\n\tmutex_lock(&userns_state_mutex);\n\n\tmemset(&new_map, 0, sizeof(struct uid_gid_map));\n\n\tret = -EPERM;\n\t/* Only allow one successful write to the map */\n\tif (map->nr_extents != 0)\n\t\tgoto out;\n\n\t/*\n\t * Adjusting namespace settings requires capabilities on the target.\n\t */\n\tif (cap_valid(cap_setid) && !file_ns_capable(file, ns, CAP_SYS_ADMIN))\n\t\tgoto out;\n\n\t/* Parse the user data */\n\tret = -EINVAL;\n\tpos = kbuf;\n\tfor (; pos; pos = next_line) {\n\n\t\t/* Find the end of line and ensure I don't look past it */\n\t\tnext_line = strchr(pos, '\\n');\n\t\tif (next_line) {\n\t\t\t*next_line = '\\0';\n\t\t\tnext_line++;\n\t\t\tif (*next_line == '\\0')\n\t\t\t\tnext_line = NULL;\n\t\t}\n\n\t\tpos = skip_spaces(pos);\n\t\textent.first = simple_strtoul(pos, &pos, 10);\n\t\tif (!isspace(*pos))\n\t\t\tgoto out;\n\n\t\tpos = skip_spaces(pos);\n\t\textent.lower_first = simple_strtoul(pos, &pos, 10);\n\t\tif (!isspace(*pos))\n\t\t\tgoto out;\n\n\t\tpos = skip_spaces(pos);\n\t\textent.count = simple_strtoul(pos, &pos, 10);\n\t\tif (*pos && !isspace(*pos))\n\t\t\tgoto out;\n\n\t\t/* Verify there is not trailing junk on the line */\n\t\tpos = skip_spaces(pos);\n\t\tif (*pos != '\\0')\n\t\t\tgoto out;\n\n\t\t/* Verify we have been given valid starting values */\n\t\tif ((extent.first == (u32) -1) ||\n\t\t    (extent.lower_first == (u32) -1))\n\t\t\tgoto out;\n\n\t\t/* Verify count is not zero and does not cause the\n\t\t * extent to wrap\n\t\t */\n\t\tif ((extent.first + extent.count) <= extent.first)\n\t\t\tgoto out;\n\t\tif ((extent.lower_first + extent.count) <=\n\t\t     extent.lower_first)\n\t\t\tgoto out;\n\n\t\t/* Do the ranges in extent overlap any previous extents? */\n\t\tif (mappings_overlap(&new_map, &extent))\n\t\t\tgoto out;\n\n\t\tif ((new_map.nr_extents + 1) == UID_GID_MAP_MAX_EXTENTS &&\n\t\t    (next_line != NULL))\n\t\t\tgoto out;\n\n\t\tret = insert_extent(&new_map, &extent);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tret = -EINVAL;\n\t}\n\t/* Be very certaint the new map actually exists */\n\tif (new_map.nr_extents == 0)\n\t\tgoto out;\n\n\tret = -EPERM;\n\t/* Validate the user is allowed to use user id's mapped to. */\n\tif (!new_idmap_permitted(file, ns, cap_setid, &new_map))\n\t\tgoto out;\n\n\tret = sort_idmaps(&new_map);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = -EPERM;\n\t/* Map the lower ids from the parent user namespace to the\n\t * kernel global id space.\n\t */\n\tfor (idx = 0; idx < new_map.nr_extents; idx++) {\n\t\tstruct uid_gid_extent *e;\n\t\tu32 lower_first;\n\n\t\tif (new_map.nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS)\n\t\t\te = &new_map.extent[idx];\n\t\telse\n\t\t\te = &new_map.forward[idx];\n\n\t\tlower_first = map_id_range_down(parent_map,\n\t\t\t\t\t\te->lower_first,\n\t\t\t\t\t\te->count);\n\n\t\t/* Fail if we can not map the specified extent to\n\t\t * the kernel global id space.\n\t\t */\n\t\tif (lower_first == (u32) -1)\n\t\t\tgoto out;\n\n\t\te->lower_first = lower_first;\n\t}\n\n\t/* Install the map */\n\tif (new_map.nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS) {\n\t\tmemcpy(map->extent, new_map.extent,\n\t\t       new_map.nr_extents * sizeof(new_map.extent[0]));\n\t} else {\n\t\tmap->forward = new_map.forward;\n\t\tmap->reverse = new_map.reverse;\n\t}\n\tsmp_wmb();\n\tmap->nr_extents = new_map.nr_extents;\n\n\t*ppos = count;\n\tret = count;\nout:\n\tif (ret < 0 && new_map.nr_extents > UID_GID_MAP_MAX_BASE_EXTENTS) {\n\t\tkfree(new_map.forward);\n\t\tkfree(new_map.reverse);\n\t\tmap->forward = NULL;\n\t\tmap->reverse = NULL;\n\t\tmap->nr_extents = 0;\n\t}\n\n\tmutex_unlock(&userns_state_mutex);\n\tkfree(kbuf);\n\treturn ret;\n}",
        "func": "static ssize_t map_write(struct file *file, const char __user *buf,\n\t\t\t size_t count, loff_t *ppos,\n\t\t\t int cap_setid,\n\t\t\t struct uid_gid_map *map,\n\t\t\t struct uid_gid_map *parent_map)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct user_namespace *ns = seq->private;\n\tstruct uid_gid_map new_map;\n\tunsigned idx;\n\tstruct uid_gid_extent extent;\n\tchar *kbuf = NULL, *pos, *next_line;\n\tssize_t ret;\n\n\t/* Only allow < page size writes at the beginning of the file */\n\tif ((*ppos != 0) || (count >= PAGE_SIZE))\n\t\treturn -EINVAL;\n\n\t/* Slurp in the user data */\n\tkbuf = memdup_user_nul(buf, count);\n\tif (IS_ERR(kbuf))\n\t\treturn PTR_ERR(kbuf);\n\n\t/*\n\t * The userns_state_mutex serializes all writes to any given map.\n\t *\n\t * Any map is only ever written once.\n\t *\n\t * An id map fits within 1 cache line on most architectures.\n\t *\n\t * On read nothing needs to be done unless you are on an\n\t * architecture with a crazy cache coherency model like alpha.\n\t *\n\t * There is a one time data dependency between reading the\n\t * count of the extents and the values of the extents.  The\n\t * desired behavior is to see the values of the extents that\n\t * were written before the count of the extents.\n\t *\n\t * To achieve this smp_wmb() is used on guarantee the write\n\t * order and smp_rmb() is guaranteed that we don't have crazy\n\t * architectures returning stale data.\n\t */\n\tmutex_lock(&userns_state_mutex);\n\n\tmemset(&new_map, 0, sizeof(struct uid_gid_map));\n\n\tret = -EPERM;\n\t/* Only allow one successful write to the map */\n\tif (map->nr_extents != 0)\n\t\tgoto out;\n\n\t/*\n\t * Adjusting namespace settings requires capabilities on the target.\n\t */\n\tif (cap_valid(cap_setid) && !file_ns_capable(file, ns, CAP_SYS_ADMIN))\n\t\tgoto out;\n\n\t/* Parse the user data */\n\tret = -EINVAL;\n\tpos = kbuf;\n\tfor (; pos; pos = next_line) {\n\n\t\t/* Find the end of line and ensure I don't look past it */\n\t\tnext_line = strchr(pos, '\\n');\n\t\tif (next_line) {\n\t\t\t*next_line = '\\0';\n\t\t\tnext_line++;\n\t\t\tif (*next_line == '\\0')\n\t\t\t\tnext_line = NULL;\n\t\t}\n\n\t\tpos = skip_spaces(pos);\n\t\textent.first = simple_strtoul(pos, &pos, 10);\n\t\tif (!isspace(*pos))\n\t\t\tgoto out;\n\n\t\tpos = skip_spaces(pos);\n\t\textent.lower_first = simple_strtoul(pos, &pos, 10);\n\t\tif (!isspace(*pos))\n\t\t\tgoto out;\n\n\t\tpos = skip_spaces(pos);\n\t\textent.count = simple_strtoul(pos, &pos, 10);\n\t\tif (*pos && !isspace(*pos))\n\t\t\tgoto out;\n\n\t\t/* Verify there is not trailing junk on the line */\n\t\tpos = skip_spaces(pos);\n\t\tif (*pos != '\\0')\n\t\t\tgoto out;\n\n\t\t/* Verify we have been given valid starting values */\n\t\tif ((extent.first == (u32) -1) ||\n\t\t    (extent.lower_first == (u32) -1))\n\t\t\tgoto out;\n\n\t\t/* Verify count is not zero and does not cause the\n\t\t * extent to wrap\n\t\t */\n\t\tif ((extent.first + extent.count) <= extent.first)\n\t\t\tgoto out;\n\t\tif ((extent.lower_first + extent.count) <=\n\t\t     extent.lower_first)\n\t\t\tgoto out;\n\n\t\t/* Do the ranges in extent overlap any previous extents? */\n\t\tif (mappings_overlap(&new_map, &extent))\n\t\t\tgoto out;\n\n\t\tif ((new_map.nr_extents + 1) == UID_GID_MAP_MAX_EXTENTS &&\n\t\t    (next_line != NULL))\n\t\t\tgoto out;\n\n\t\tret = insert_extent(&new_map, &extent);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tret = -EINVAL;\n\t}\n\t/* Be very certaint the new map actually exists */\n\tif (new_map.nr_extents == 0)\n\t\tgoto out;\n\n\tret = -EPERM;\n\t/* Validate the user is allowed to use user id's mapped to. */\n\tif (!new_idmap_permitted(file, ns, cap_setid, &new_map))\n\t\tgoto out;\n\n\tret = -EPERM;\n\t/* Map the lower ids from the parent user namespace to the\n\t * kernel global id space.\n\t */\n\tfor (idx = 0; idx < new_map.nr_extents; idx++) {\n\t\tstruct uid_gid_extent *e;\n\t\tu32 lower_first;\n\n\t\tif (new_map.nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS)\n\t\t\te = &new_map.extent[idx];\n\t\telse\n\t\t\te = &new_map.forward[idx];\n\n\t\tlower_first = map_id_range_down(parent_map,\n\t\t\t\t\t\te->lower_first,\n\t\t\t\t\t\te->count);\n\n\t\t/* Fail if we can not map the specified extent to\n\t\t * the kernel global id space.\n\t\t */\n\t\tif (lower_first == (u32) -1)\n\t\t\tgoto out;\n\n\t\te->lower_first = lower_first;\n\t}\n\n\t/*\n\t * If we want to use binary search for lookup, this clones the extent\n\t * array and sorts both copies.\n\t */\n\tret = sort_idmaps(&new_map);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t/* Install the map */\n\tif (new_map.nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS) {\n\t\tmemcpy(map->extent, new_map.extent,\n\t\t       new_map.nr_extents * sizeof(new_map.extent[0]));\n\t} else {\n\t\tmap->forward = new_map.forward;\n\t\tmap->reverse = new_map.reverse;\n\t}\n\tsmp_wmb();\n\tmap->nr_extents = new_map.nr_extents;\n\n\t*ppos = count;\n\tret = count;\nout:\n\tif (ret < 0 && new_map.nr_extents > UID_GID_MAP_MAX_BASE_EXTENTS) {\n\t\tkfree(new_map.forward);\n\t\tkfree(new_map.reverse);\n\t\tmap->forward = NULL;\n\t\tmap->reverse = NULL;\n\t\tmap->nr_extents = 0;\n\t}\n\n\tmutex_unlock(&userns_state_mutex);\n\tkfree(kbuf);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -125,10 +125,6 @@\n \tif (!new_idmap_permitted(file, ns, cap_setid, &new_map))\n \t\tgoto out;\n \n-\tret = sort_idmaps(&new_map);\n-\tif (ret < 0)\n-\t\tgoto out;\n-\n \tret = -EPERM;\n \t/* Map the lower ids from the parent user namespace to the\n \t * kernel global id space.\n@@ -154,6 +150,14 @@\n \n \t\te->lower_first = lower_first;\n \t}\n+\n+\t/*\n+\t * If we want to use binary search for lookup, this clones the extent\n+\t * array and sorts both copies.\n+\t */\n+\tret = sort_idmaps(&new_map);\n+\tif (ret < 0)\n+\t\tgoto out;\n \n \t/* Install the map */\n \tif (new_map.nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS) {",
        "diff_line_info": {
            "deleted_lines": [
                "\tret = sort_idmaps(&new_map);",
                "\tif (ret < 0)",
                "\t\tgoto out;",
                ""
            ],
            "added_lines": [
                "",
                "\t/*",
                "\t * If we want to use binary search for lookup, this clones the extent",
                "\t * array and sorts both copies.",
                "\t */",
                "\tret = sort_idmaps(&new_map);",
                "\tif (ret < 0)",
                "\t\tgoto out;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-18397",
        "func_name": "torvalds/linux/userfaultfd_register",
        "description": "The userfaultfd implementation in the Linux kernel before 4.19.7 mishandles access control for certain UFFDIO_ ioctl calls, as demonstrated by allowing local users to write data into holes in a tmpfs file (if the user has read-only access to that file, and that file contains holes), related to fs/userfaultfd.c and mm/userfaultfd.c.",
        "git_url": "https://github.com/torvalds/linux/commit/29ec90660d68bbdd69507c1c8b4e33aa299278b1",
        "commit_title": "userfaultfd: shmem/hugetlbfs: only allow to register VM_MAYWRITE vmas",
        "commit_text": " After the VMA to register the uffd onto is found, check that it has VM_MAYWRITE set before allowing registration.  This way we inherit all common code checks before allowing to fill file holes in shmem and hugetlbfs with UFFDIO_COPY.  The userfaultfd memory model is not applicable for readonly files unless it's a MAP_PRIVATE.  Link: http://lkml.kernel.org/r/20181126173452.26955-4-aarcange@redhat.com Cc: <stable@vger.kernel.org> Cc: \"Dr. David Alan Gilbert\" <dgilbert@redhat.com> Cc: Mike Kravetz <mike.kravetz@oracle.com> Cc: Peter Xu <peterx@redhat.com> Cc: stable@vger.kernel.org",
        "func_before": "static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}",
        "func": "static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * UFFDIO_COPY will fill file holes even without\n\t\t * PROT_WRITE. This check enforces that if this is a\n\t\t * MAP_SHARED, the process has write permission to the backing\n\t\t * file. If VM_MAYWRITE is set it also enforces that on a\n\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further\n\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.\n\t\t */\n\t\tret = -EPERM;\n\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -85,6 +85,19 @@\n \t\tret = -EINVAL;\n \t\tif (!vma_can_userfault(cur))\n \t\t\tgoto out_unlock;\n+\n+\t\t/*\n+\t\t * UFFDIO_COPY will fill file holes even without\n+\t\t * PROT_WRITE. This check enforces that if this is a\n+\t\t * MAP_SHARED, the process has write permission to the backing\n+\t\t * file. If VM_MAYWRITE is set it also enforces that on a\n+\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further\n+\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.\n+\t\t */\n+\t\tret = -EPERM;\n+\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n+\t\t\tgoto out_unlock;\n+\n \t\t/*\n \t\t * If this vma contains ending address, and huge pages\n \t\t * check alignment.\n@@ -130,6 +143,7 @@\n \t\tBUG_ON(!vma_can_userfault(vma));\n \t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n \t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n+\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n \n \t\t/*\n \t\t * Nothing to do: this vma is already registered into this",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t\t/*",
                "\t\t * UFFDIO_COPY will fill file holes even without",
                "\t\t * PROT_WRITE. This check enforces that if this is a",
                "\t\t * MAP_SHARED, the process has write permission to the backing",
                "\t\t * file. If VM_MAYWRITE is set it also enforces that on a",
                "\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further",
                "\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.",
                "\t\t */",
                "\t\tret = -EPERM;",
                "\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))",
                "\t\t\tgoto out_unlock;",
                "",
                "\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-18397",
        "func_name": "torvalds/linux/userfaultfd_unregister",
        "description": "The userfaultfd implementation in the Linux kernel before 4.19.7 mishandles access control for certain UFFDIO_ ioctl calls, as demonstrated by allowing local users to write data into holes in a tmpfs file (if the user has read-only access to that file, and that file contains holes), related to fs/userfaultfd.c and mm/userfaultfd.c.",
        "git_url": "https://github.com/torvalds/linux/commit/29ec90660d68bbdd69507c1c8b4e33aa299278b1",
        "commit_title": "userfaultfd: shmem/hugetlbfs: only allow to register VM_MAYWRITE vmas",
        "commit_text": " After the VMA to register the uffd onto is found, check that it has VM_MAYWRITE set before allowing registration.  This way we inherit all common code checks before allowing to fill file holes in shmem and hugetlbfs with UFFDIO_COPY.  The userfaultfd memory model is not applicable for readonly files unless it's a MAP_PRIVATE.  Link: http://lkml.kernel.org/r/20181126173452.26955-4-aarcange@redhat.com Cc: <stable@vger.kernel.org> Cc: \"Dr. David Alan Gilbert\" <dgilbert@redhat.com> Cc: Mike Kravetz <mike.kravetz@oracle.com> Cc: Peter Xu <peterx@redhat.com> Cc: stable@vger.kernel.org",
        "func_before": "static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}",
        "func": "static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -80,6 +80,7 @@\n \t\tcond_resched();\n \n \t\tBUG_ON(!vma_can_userfault(vma));\n+\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n \n \t\t/*\n \t\t * Nothing to do: this vma is already registered into this",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-18397",
        "func_name": "torvalds/linux/userfaultfd_register",
        "description": "The userfaultfd implementation in the Linux kernel before 4.19.7 mishandles access control for certain UFFDIO_ ioctl calls, as demonstrated by allowing local users to write data into holes in a tmpfs file (if the user has read-only access to that file, and that file contains holes), related to fs/userfaultfd.c and mm/userfaultfd.c.",
        "git_url": "https://github.com/torvalds/linux/commit/29ec90660d68bbdd69507c1c8b4e33aa299278b1",
        "commit_title": "userfaultfd: shmem/hugetlbfs: only allow to register VM_MAYWRITE vmas",
        "commit_text": " After the VMA to register the uffd onto is found, check that it has VM_MAYWRITE set before allowing registration.  This way we inherit all common code checks before allowing to fill file holes in shmem and hugetlbfs with UFFDIO_COPY.  The userfaultfd memory model is not applicable for readonly files unless it's a MAP_PRIVATE.  Link: http://lkml.kernel.org/r/20181126173452.26955-4-aarcange@redhat.com Cc: <stable@vger.kernel.org> Cc: \"Dr. David Alan Gilbert\" <dgilbert@redhat.com> Cc: Mike Kravetz <mike.kravetz@oracle.com> Cc: Peter Xu <peterx@redhat.com> Cc: stable@vger.kernel.org",
        "func_before": "static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}",
        "func": "static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * UFFDIO_COPY will fill file holes even without\n\t\t * PROT_WRITE. This check enforces that if this is a\n\t\t * MAP_SHARED, the process has write permission to the backing\n\t\t * file. If VM_MAYWRITE is set it also enforces that on a\n\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further\n\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.\n\t\t */\n\t\tret = -EPERM;\n\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -85,6 +85,19 @@\n \t\tret = -EINVAL;\n \t\tif (!vma_can_userfault(cur))\n \t\t\tgoto out_unlock;\n+\n+\t\t/*\n+\t\t * UFFDIO_COPY will fill file holes even without\n+\t\t * PROT_WRITE. This check enforces that if this is a\n+\t\t * MAP_SHARED, the process has write permission to the backing\n+\t\t * file. If VM_MAYWRITE is set it also enforces that on a\n+\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further\n+\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.\n+\t\t */\n+\t\tret = -EPERM;\n+\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n+\t\t\tgoto out_unlock;\n+\n \t\t/*\n \t\t * If this vma contains ending address, and huge pages\n \t\t * check alignment.\n@@ -130,6 +143,7 @@\n \t\tBUG_ON(!vma_can_userfault(vma));\n \t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n \t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n+\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n \n \t\t/*\n \t\t * Nothing to do: this vma is already registered into this",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t\t/*",
                "\t\t * UFFDIO_COPY will fill file holes even without",
                "\t\t * PROT_WRITE. This check enforces that if this is a",
                "\t\t * MAP_SHARED, the process has write permission to the backing",
                "\t\t * file. If VM_MAYWRITE is set it also enforces that on a",
                "\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further",
                "\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.",
                "\t\t */",
                "\t\tret = -EPERM;",
                "\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))",
                "\t\t\tgoto out_unlock;",
                "",
                "\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-18397",
        "func_name": "torvalds/linux/userfaultfd_unregister",
        "description": "The userfaultfd implementation in the Linux kernel before 4.19.7 mishandles access control for certain UFFDIO_ ioctl calls, as demonstrated by allowing local users to write data into holes in a tmpfs file (if the user has read-only access to that file, and that file contains holes), related to fs/userfaultfd.c and mm/userfaultfd.c.",
        "git_url": "https://github.com/torvalds/linux/commit/29ec90660d68bbdd69507c1c8b4e33aa299278b1",
        "commit_title": "userfaultfd: shmem/hugetlbfs: only allow to register VM_MAYWRITE vmas",
        "commit_text": " After the VMA to register the uffd onto is found, check that it has VM_MAYWRITE set before allowing registration.  This way we inherit all common code checks before allowing to fill file holes in shmem and hugetlbfs with UFFDIO_COPY.  The userfaultfd memory model is not applicable for readonly files unless it's a MAP_PRIVATE.  Link: http://lkml.kernel.org/r/20181126173452.26955-4-aarcange@redhat.com Cc: <stable@vger.kernel.org> Cc: \"Dr. David Alan Gilbert\" <dgilbert@redhat.com> Cc: Mike Kravetz <mike.kravetz@oracle.com> Cc: Peter Xu <peterx@redhat.com> Cc: stable@vger.kernel.org",
        "func_before": "static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}",
        "func": "static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -80,6 +80,7 @@\n \t\tcond_resched();\n \n \t\tBUG_ON(!vma_can_userfault(vma));\n+\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n \n \t\t/*\n \t\t * Nothing to do: this vma is already registered into this",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5521",
        "func_name": "tiann/KernelSU/become_manager",
        "description": "Incorrect Authorization in GitHub repository tiann/kernelsu prior to v0.6.9.",
        "git_url": "https://github.com/tiann/KernelSU/commit/a22959beae1aad96b1f72710a5daadf529c41bda",
        "commit_title": "kernel: harden the signature check (#1027)",
        "commit_text": "",
        "func_before": "bool become_manager(char *pkg)\n{\n\tstruct fdtable *files_table;\n\tint i = 0;\n\tstruct path files_path;\n\tchar *cwd;\n\tchar *buf;\n\tbool result = false;\n\n\t// must be zygote's direct child, otherwise any app can fork a new process and\n\t// open manager's apk\n\tif (task_uid(current->real_parent).val != 0) {\n\t\tpr_info(\"parent is not zygote!\\n\");\n\t\treturn false;\n\t}\n\n\tbuf = (char *)kmalloc(PATH_MAX, GFP_ATOMIC);\n\tif (!buf) {\n\t\tpr_err(\"kalloc path failed.\\n\");\n\t\treturn false;\n\t}\n\n\tfiles_table = files_fdtable(current->files);\n\n\tint pkg_len = strlen(pkg);\n\t// todo: use iterate_fd\n\tfor (i = 0; files_table->fd[i] != NULL; i++) {\n\t\tfiles_path = files_table->fd[i]->f_path;\n\t\tif (!d_is_reg(files_path.dentry)) {\n\t\t\tcontinue;\n\t\t}\n\t\tcwd = d_path(&files_path, buf, PATH_MAX);\n\t\tif (startswith(cwd, \"/data/app/\") != 0 ||\n\t\t    endswith(cwd, \"/base.apk\") != 0) {\n\t\t\tcontinue;\n\t\t}\n\t\t// we have found the apk!\n\t\tpr_info(\"found apk: %s\\n\", cwd);\n\t\tchar *pkg_index = strstr(cwd, pkg);\n\t\tif (!pkg_index) {\n\t\t\tpr_info(\"apk path not match package name!\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tchar *next_char = pkg_index + pkg_len;\n\t\t// because we ensure the cwd must startswith `/data/app` and endswith `base.apk`\n\t\t// we don't need to check if the pointer is out of bounds\n\t\tif (*next_char != '-') {\n\t\t\t// from android 8.1: http://aospxref.com/android-8.1.0_r81/xref/frameworks/base/services/core/java/com/android/server/pm/PackageManagerService.java#17612\n\t\t\t// to android 13: http://aospxref.com/android-13.0.0_r3/xref/frameworks/base/services/core/java/com/android/server/pm/PackageManagerServiceUtils.java#1208\n\t\t\t// /data/app/~~[randomStringA]/[packageName]-[randomStringB]\n\t\t\t// the previous char must be `/` and the next char must be `-`\n\t\t\t// because we use strstr instead of equals, this is a strong verfication.\n\t\t\tpr_info(\"invalid pkg: %s\\n\", pkg);\n\t\t\tcontinue;\n\t\t}\n\t\tif (is_manager_apk(cwd) == 0) {\n\t\t\t// check passed\n\t\t\tuid_t uid = current_uid().val;\n\t\t\tpr_info(\"manager uid: %d\\n\", uid);\n\n\t\t\tksu_set_manager_uid(uid);\n\n\t\t\tresult = true;\n\t\t\tgoto clean;\n\t\t} else {\n\t\t\tpr_info(\"manager signature invalid!\\n\");\n\t\t}\n\n\t\tbreak;\n\t}\n\nclean:\n\tkfree(buf);\n\treturn result;\n}",
        "func": "bool become_manager(char *pkg)\n{\n\tstruct fdtable *files_table;\n\tint i = 0;\n\tstruct path files_path;\n\tchar *cwd;\n\tchar *buf;\n\tbool result = false;\n\n\t// must be zygote's direct child, otherwise any app can fork a new process and\n\t// open manager's apk\n\tif (task_uid(current->real_parent).val != 0) {\n\t\tpr_info(\"parent is not zygote!\\n\");\n\t\treturn false;\n\t}\n\n\tbuf = (char *)kmalloc(PATH_MAX, GFP_ATOMIC);\n\tif (!buf) {\n\t\tpr_err(\"kalloc path failed.\\n\");\n\t\treturn false;\n\t}\n\n\tfiles_table = files_fdtable(current->files);\n\n\tint pkg_len = strlen(pkg);\n\t// todo: use iterate_fd\n\tfor (i = 0; files_table->fd[i] != NULL; i++) {\n\t\tfiles_path = files_table->fd[i]->f_path;\n\t\tif (!d_is_reg(files_path.dentry)) {\n\t\t\tcontinue;\n\t\t}\n\t\tcwd = d_path(&files_path, buf, PATH_MAX);\n\t\tif (startswith(cwd, \"/data/app/\") != 0 ||\n\t\t    endswith(cwd, \"/base.apk\") != 0) {\n\t\t\tcontinue;\n\t\t}\n\t\t// we have found the apk!\n\t\tpr_info(\"found apk: %s\\n\", cwd);\n\t\tchar *pkg_index = strstr(cwd, pkg);\n\t\tif (!pkg_index) {\n\t\t\tpr_info(\"apk path not match package name!\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tchar *next_char = pkg_index + pkg_len;\n\t\t// because we ensure the cwd must startswith `/data/app` and endswith `base.apk`\n\t\t// we don't need to check if the pointer is out of bounds\n\t\tif (*next_char != '-') {\n\t\t\t// from android 8.1: http://aospxref.com/android-8.1.0_r81/xref/frameworks/base/services/core/java/com/android/server/pm/PackageManagerService.java#17612\n\t\t\t// to android 13: http://aospxref.com/android-13.0.0_r3/xref/frameworks/base/services/core/java/com/android/server/pm/PackageManagerServiceUtils.java#1208\n\t\t\t// /data/app/~~[randomStringA]/[packageName]-[randomStringB]\n\t\t\t// the previous char must be `/` and the next char must be `-`\n\t\t\t// because we use strstr instead of equals, this is a strong verfication.\n\t\t\tpr_info(\"invalid pkg: %s\\n\", pkg);\n\t\t\tcontinue;\n\t\t}\n\t\tif (is_manager_apk(cwd)) {\n\t\t\t// check passed\n\t\t\tuid_t uid = current_uid().val;\n\t\t\tpr_info(\"manager uid: %d\\n\", uid);\n\n\t\t\tksu_set_manager_uid(uid);\n\n\t\t\tresult = true;\n\t\t\tgoto clean;\n\t\t} else {\n\t\t\tpr_info(\"manager signature invalid!\\n\");\n\t\t}\n\n\t\tbreak;\n\t}\n\nclean:\n\tkfree(buf);\n\treturn result;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -53,7 +53,7 @@\n \t\t\tpr_info(\"invalid pkg: %s\\n\", pkg);\n \t\t\tcontinue;\n \t\t}\n-\t\tif (is_manager_apk(cwd) == 0) {\n+\t\tif (is_manager_apk(cwd)) {\n \t\t\t// check passed\n \t\t\tuid_t uid = current_uid().val;\n \t\t\tpr_info(\"manager uid: %d\\n\", uid);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (is_manager_apk(cwd) == 0) {"
            ],
            "added_lines": [
                "\t\tif (is_manager_apk(cwd)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5521",
        "func_name": "tiann/KernelSU/set_expected_hash",
        "description": "Incorrect Authorization in GitHub repository tiann/kernelsu prior to v0.6.9.",
        "git_url": "https://github.com/tiann/KernelSU/commit/a22959beae1aad96b1f72710a5daadf529c41bda",
        "commit_title": "kernel: harden the signature check (#1027)",
        "commit_text": "",
        "func_before": "static int set_expected_hash(const char *val, const struct kernel_param *kp)\n{\n\tint rv = param_set_uint(val, kp);\n\tksu_invalidate_manager_uid();\n\tpr_info(\"ksu_expected_hash set to %x\\n\", ksu_expected_hash);\n\treturn rv;\n}",
        "func": "static int set_expected_hash(const char *val, const struct kernel_param *kp)\n{\n\tpr_info(\"set_expected_hash: %s\\n\", val);\n\tint rv = param_set_charp(val, kp);\n\tksu_invalidate_manager_uid();\n\tpr_info(\"ksu_expected_hash set to %s\\n\", ksu_expected_hash);\n\treturn rv;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,8 @@\n static int set_expected_hash(const char *val, const struct kernel_param *kp)\n {\n-\tint rv = param_set_uint(val, kp);\n+\tpr_info(\"set_expected_hash: %s\\n\", val);\n+\tint rv = param_set_charp(val, kp);\n \tksu_invalidate_manager_uid();\n-\tpr_info(\"ksu_expected_hash set to %x\\n\", ksu_expected_hash);\n+\tpr_info(\"ksu_expected_hash set to %s\\n\", ksu_expected_hash);\n \treturn rv;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tint rv = param_set_uint(val, kp);",
                "\tpr_info(\"ksu_expected_hash set to %x\\n\", ksu_expected_hash);"
            ],
            "added_lines": [
                "\tpr_info(\"set_expected_hash: %s\\n\", val);",
                "\tint rv = param_set_charp(val, kp);",
                "\tpr_info(\"ksu_expected_hash set to %s\\n\", ksu_expected_hash);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5521",
        "func_name": "tiann/KernelSU/check_v2_signature",
        "description": "Incorrect Authorization in GitHub repository tiann/kernelsu prior to v0.6.9.",
        "git_url": "https://github.com/tiann/KernelSU/commit/a22959beae1aad96b1f72710a5daadf529c41bda",
        "commit_title": "kernel: harden the signature check (#1027)",
        "commit_text": "",
        "func_before": "static __always_inline int\ncheck_v2_signature(char *path, unsigned expected_size, unsigned expected_hash)\n{\n\tunsigned char buffer[0x11] = { 0 };\n\tu32 size4;\n\tu64 size8, size_of_block;\n\n\tloff_t pos;\n\n\tint sign = -1;\n\tint i;\n\tstruct file *fp = ksu_filp_open_compat(path, O_RDONLY, 0);\n\tif (IS_ERR(fp)) {\n\t\tpr_err(\"open %s error.\\n\", path);\n\t\treturn PTR_ERR(fp);\n\t}\n\n\t// disable inotify for this file\n\tfp->f_mode |= FMODE_NONOTIFY;\n\n\tsign = 1;\n\t// https://en.wikipedia.org/wiki/Zip_(file_format)#End_of_central_directory_record_(EOCD)\n\tfor (i = 0;; ++i) {\n\t\tunsigned short n;\n\t\tpos = generic_file_llseek(fp, -i - 2, SEEK_END);\n\t\tksu_kernel_read_compat(fp, &n, 2, &pos);\n\t\tif (n == i) {\n\t\t\tpos -= 22;\n\t\t\tksu_kernel_read_compat(fp, &size4, 4, &pos);\n\t\t\tif ((size4 ^ 0xcafebabeu) == 0xccfbf1eeu) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == 0xffff) {\n\t\t\tpr_info(\"error: cannot find eocd\\n\");\n\t\t\tgoto clean;\n\t\t}\n\t}\n\n\tpos += 12;\n\t// offset\n\tksu_kernel_read_compat(fp, &size4, 0x4, &pos);\n\tpos = size4 - 0x18;\n\n\tksu_kernel_read_compat(fp, &size8, 0x8, &pos);\n\tksu_kernel_read_compat(fp, buffer, 0x10, &pos);\n\tif (strcmp((char *)buffer, \"APK Sig Block 42\")) {\n\t\tgoto clean;\n\t}\n\n\tpos = size4 - (size8 + 0x8);\n\tksu_kernel_read_compat(fp, &size_of_block, 0x8, &pos);\n\tif (size_of_block != size8) {\n\t\tgoto clean;\n\t}\n\n\tfor (;;) {\n\t\tuint32_t id;\n\t\tuint32_t offset;\n\t\tksu_kernel_read_compat(fp, &size8, 0x8, &pos); // sequence length\n\t\tif (size8 == size_of_block) {\n\t\t\tbreak;\n\t\t}\n\t\tksu_kernel_read_compat(fp, &id, 0x4, &pos); // id\n\t\toffset = 4;\n\t\tpr_info(\"id: 0x%08x\\n\", id);\n\t\tif ((id ^ 0xdeadbeefu) == 0xafa439f5u ||\n\t\t    (id ^ 0xdeadbeefu) == 0x2efed62f) {\n\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n\t\t\t\t    &pos); // signer-sequence length\n\t\t\tksu_kernel_read_compat(fp, &size4, 0x4, &pos); // signer length\n\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n\t\t\t\t    &pos); // signed data length\n\t\t\toffset += 0x4 * 3;\n\n\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n\t\t\t\t    &pos); // digests-sequence length\n\t\t\tpos += size4;\n\t\t\toffset += 0x4 + size4;\n\n\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n\t\t\t\t    &pos); // certificates length\n\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n\t\t\t\t    &pos); // certificate length\n\t\t\toffset += 0x4 * 2;\n#if 0\n\t\t\tint hash = 1;\n\t\t\tsigned char c;\n\t\t\tfor (i = 0; i < size4; ++i) {\n\t\t\t\tksu_kernel_read_compat(fp, &c, 0x1, &pos);\n\t\t\t\thash = 31 * hash + c;\n\t\t\t}\n\t\t\toffset += size4;\n\t\t\tpr_info(\"    size: 0x%04x, hash: 0x%08x\\n\", size4, ((unsigned) hash) ^ 0x14131211u);\n#else\n\t\t\tif (size4 == expected_size) {\n\t\t\t\tint hash = 1;\n\t\t\t\tsigned char c;\n\t\t\t\tfor (i = 0; i < size4; ++i) {\n\t\t\t\t\tksu_kernel_read_compat(fp, &c, 0x1, &pos);\n\t\t\t\t\thash = 31 * hash + c;\n\t\t\t\t}\n\t\t\t\toffset += size4;\n\t\t\t\tif ((((unsigned)hash) ^ 0x14131211u) ==\n\t\t\t\t    expected_hash) {\n\t\t\t\t\tsign = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// don't try again.\n\t\t\tbreak;\n#endif\n\t\t}\n\t\tpos += (size8 - offset);\n\t}\n\nclean:\n\tfilp_close(fp, 0);\n\n\treturn sign;\n}",
        "func": "static __always_inline bool\ncheck_v2_signature(char *path, unsigned expected_size, const char *expected_sha256)\n{\n\tunsigned char buffer[0x11] = { 0 };\n\tu32 size4;\n\tu64 size8, size_of_block;\n\n\tloff_t pos;\n\tbool block_valid;\n\n\tconst int NOT_EXIST = 0;\n\tconst int INVALID = 1;\n\tconst int VALID = 2;\n\tint v2_signing_status = NOT_EXIST;\n\tint v3_signing_status = NOT_EXIST;\n\n\tint i;\n\tstruct file *fp = ksu_filp_open_compat(path, O_RDONLY, 0);\n\tif (IS_ERR(fp)) {\n\t\tpr_err(\"open %s error.\\n\", path);\n\t\treturn PTR_ERR(fp);\n\t}\n\n\t// disable inotify for this file\n\tfp->f_mode |= FMODE_NONOTIFY;\n\n\t// https://en.wikipedia.org/wiki/Zip_(file_format)#End_of_central_directory_record_(EOCD)\n\tfor (i = 0;; ++i) {\n\t\tunsigned short n;\n\t\tpos = generic_file_llseek(fp, -i - 2, SEEK_END);\n\t\tksu_kernel_read_compat(fp, &n, 2, &pos);\n\t\tif (n == i) {\n\t\t\tpos -= 22;\n\t\t\tksu_kernel_read_compat(fp, &size4, 4, &pos);\n\t\t\tif ((size4 ^ 0xcafebabeu) == 0xccfbf1eeu) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == 0xffff) {\n\t\t\tpr_info(\"error: cannot find eocd\\n\");\n\t\t\tgoto clean;\n\t\t}\n\t}\n\n\tpos += 12;\n\t// offset\n\tksu_kernel_read_compat(fp, &size4, 0x4, &pos);\n\tpos = size4 - 0x18;\n\n\tksu_kernel_read_compat(fp, &size8, 0x8, &pos);\n\tksu_kernel_read_compat(fp, buffer, 0x10, &pos);\n\tif (strcmp((char *)buffer, \"APK Sig Block 42\")) {\n\t\tgoto clean;\n\t}\n\n\tpos = size4 - (size8 + 0x8);\n\tksu_kernel_read_compat(fp, &size_of_block, 0x8, &pos);\n\tif (size_of_block != size8) {\n\t\tgoto clean;\n\t}\n\n\tfor (;;) {\n\t\tuint32_t id;\n\t\tuint32_t offset;\n\t\tksu_kernel_read_compat(fp, &size8, 0x8,\n\t\t\t\t       &pos); // sequence length\n\t\tif (size8 == size_of_block) {\n\t\t\tbreak;\n\t\t}\n\t\tksu_kernel_read_compat(fp, &id, 0x4, &pos); // id\n\t\toffset = 4;\n\t\tpr_info(\"id: 0x%08x\\n\", id);\n\t\tif (id == 0x7109871au) {\n\t\t\tblock_valid = check_block(fp, &size4, &pos, &offset,\n\t\t\t\t\t\t  expected_size, expected_sha256);\n\t\t\tv2_signing_status = block_valid ? VALID : INVALID;\n\t\t} else if (id == 0xf05368c0u) {\n\t\t\tblock_valid = check_block(fp, &size4, &pos, &offset,\n\t\t\t\t\t\t  expected_size, expected_sha256);\n\t\t\tv3_signing_status = block_valid ? VALID : INVALID;\n\t\t}\n\t\tpos += (size8 - offset);\n\t}\n\nclean:\n\tfilp_close(fp, 0);\n\n\treturn (v2_signing_status == NOT_EXIST && v3_signing_status == VALID) ||\n\t       (v2_signing_status == VALID && v3_signing_status == NOT_EXIST) ||\n\t       (v2_signing_status == VALID && v3_signing_status == VALID);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,13 +1,19 @@\n-static __always_inline int\n-check_v2_signature(char *path, unsigned expected_size, unsigned expected_hash)\n+static __always_inline bool\n+check_v2_signature(char *path, unsigned expected_size, const char *expected_sha256)\n {\n \tunsigned char buffer[0x11] = { 0 };\n \tu32 size4;\n \tu64 size8, size_of_block;\n \n \tloff_t pos;\n+\tbool block_valid;\n \n-\tint sign = -1;\n+\tconst int NOT_EXIST = 0;\n+\tconst int INVALID = 1;\n+\tconst int VALID = 2;\n+\tint v2_signing_status = NOT_EXIST;\n+\tint v3_signing_status = NOT_EXIST;\n+\n \tint i;\n \tstruct file *fp = ksu_filp_open_compat(path, O_RDONLY, 0);\n \tif (IS_ERR(fp)) {\n@@ -18,7 +24,6 @@\n \t// disable inotify for this file\n \tfp->f_mode |= FMODE_NONOTIFY;\n \n-\tsign = 1;\n \t// https://en.wikipedia.org/wiki/Zip_(file_format)#End_of_central_directory_record_(EOCD)\n \tfor (i = 0;; ++i) {\n \t\tunsigned short n;\n@@ -57,59 +62,22 @@\n \tfor (;;) {\n \t\tuint32_t id;\n \t\tuint32_t offset;\n-\t\tksu_kernel_read_compat(fp, &size8, 0x8, &pos); // sequence length\n+\t\tksu_kernel_read_compat(fp, &size8, 0x8,\n+\t\t\t\t       &pos); // sequence length\n \t\tif (size8 == size_of_block) {\n \t\t\tbreak;\n \t\t}\n \t\tksu_kernel_read_compat(fp, &id, 0x4, &pos); // id\n \t\toffset = 4;\n \t\tpr_info(\"id: 0x%08x\\n\", id);\n-\t\tif ((id ^ 0xdeadbeefu) == 0xafa439f5u ||\n-\t\t    (id ^ 0xdeadbeefu) == 0x2efed62f) {\n-\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n-\t\t\t\t    &pos); // signer-sequence length\n-\t\t\tksu_kernel_read_compat(fp, &size4, 0x4, &pos); // signer length\n-\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n-\t\t\t\t    &pos); // signed data length\n-\t\t\toffset += 0x4 * 3;\n-\n-\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n-\t\t\t\t    &pos); // digests-sequence length\n-\t\t\tpos += size4;\n-\t\t\toffset += 0x4 + size4;\n-\n-\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n-\t\t\t\t    &pos); // certificates length\n-\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,\n-\t\t\t\t    &pos); // certificate length\n-\t\t\toffset += 0x4 * 2;\n-#if 0\n-\t\t\tint hash = 1;\n-\t\t\tsigned char c;\n-\t\t\tfor (i = 0; i < size4; ++i) {\n-\t\t\t\tksu_kernel_read_compat(fp, &c, 0x1, &pos);\n-\t\t\t\thash = 31 * hash + c;\n-\t\t\t}\n-\t\t\toffset += size4;\n-\t\t\tpr_info(\"    size: 0x%04x, hash: 0x%08x\\n\", size4, ((unsigned) hash) ^ 0x14131211u);\n-#else\n-\t\t\tif (size4 == expected_size) {\n-\t\t\t\tint hash = 1;\n-\t\t\t\tsigned char c;\n-\t\t\t\tfor (i = 0; i < size4; ++i) {\n-\t\t\t\t\tksu_kernel_read_compat(fp, &c, 0x1, &pos);\n-\t\t\t\t\thash = 31 * hash + c;\n-\t\t\t\t}\n-\t\t\t\toffset += size4;\n-\t\t\t\tif ((((unsigned)hash) ^ 0x14131211u) ==\n-\t\t\t\t    expected_hash) {\n-\t\t\t\t\tsign = 0;\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\t// don't try again.\n-\t\t\tbreak;\n-#endif\n+\t\tif (id == 0x7109871au) {\n+\t\t\tblock_valid = check_block(fp, &size4, &pos, &offset,\n+\t\t\t\t\t\t  expected_size, expected_sha256);\n+\t\t\tv2_signing_status = block_valid ? VALID : INVALID;\n+\t\t} else if (id == 0xf05368c0u) {\n+\t\t\tblock_valid = check_block(fp, &size4, &pos, &offset,\n+\t\t\t\t\t\t  expected_size, expected_sha256);\n+\t\t\tv3_signing_status = block_valid ? VALID : INVALID;\n \t\t}\n \t\tpos += (size8 - offset);\n \t}\n@@ -117,5 +85,7 @@\n clean:\n \tfilp_close(fp, 0);\n \n-\treturn sign;\n+\treturn (v2_signing_status == NOT_EXIST && v3_signing_status == VALID) ||\n+\t       (v2_signing_status == VALID && v3_signing_status == NOT_EXIST) ||\n+\t       (v2_signing_status == VALID && v3_signing_status == VALID);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static __always_inline int",
                "check_v2_signature(char *path, unsigned expected_size, unsigned expected_hash)",
                "\tint sign = -1;",
                "\tsign = 1;",
                "\t\tksu_kernel_read_compat(fp, &size8, 0x8, &pos); // sequence length",
                "\t\tif ((id ^ 0xdeadbeefu) == 0xafa439f5u ||",
                "\t\t    (id ^ 0xdeadbeefu) == 0x2efed62f) {",
                "\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,",
                "\t\t\t\t    &pos); // signer-sequence length",
                "\t\t\tksu_kernel_read_compat(fp, &size4, 0x4, &pos); // signer length",
                "\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,",
                "\t\t\t\t    &pos); // signed data length",
                "\t\t\toffset += 0x4 * 3;",
                "",
                "\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,",
                "\t\t\t\t    &pos); // digests-sequence length",
                "\t\t\tpos += size4;",
                "\t\t\toffset += 0x4 + size4;",
                "",
                "\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,",
                "\t\t\t\t    &pos); // certificates length",
                "\t\t\tksu_kernel_read_compat(fp, &size4, 0x4,",
                "\t\t\t\t    &pos); // certificate length",
                "\t\t\toffset += 0x4 * 2;",
                "#if 0",
                "\t\t\tint hash = 1;",
                "\t\t\tsigned char c;",
                "\t\t\tfor (i = 0; i < size4; ++i) {",
                "\t\t\t\tksu_kernel_read_compat(fp, &c, 0x1, &pos);",
                "\t\t\t\thash = 31 * hash + c;",
                "\t\t\t}",
                "\t\t\toffset += size4;",
                "\t\t\tpr_info(\"    size: 0x%04x, hash: 0x%08x\\n\", size4, ((unsigned) hash) ^ 0x14131211u);",
                "#else",
                "\t\t\tif (size4 == expected_size) {",
                "\t\t\t\tint hash = 1;",
                "\t\t\t\tsigned char c;",
                "\t\t\t\tfor (i = 0; i < size4; ++i) {",
                "\t\t\t\t\tksu_kernel_read_compat(fp, &c, 0x1, &pos);",
                "\t\t\t\t\thash = 31 * hash + c;",
                "\t\t\t\t}",
                "\t\t\t\toffset += size4;",
                "\t\t\t\tif ((((unsigned)hash) ^ 0x14131211u) ==",
                "\t\t\t\t    expected_hash) {",
                "\t\t\t\t\tsign = 0;",
                "\t\t\t\t\tbreak;",
                "\t\t\t\t}",
                "\t\t\t}",
                "\t\t\t// don't try again.",
                "\t\t\tbreak;",
                "#endif",
                "\treturn sign;"
            ],
            "added_lines": [
                "static __always_inline bool",
                "check_v2_signature(char *path, unsigned expected_size, const char *expected_sha256)",
                "\tbool block_valid;",
                "\tconst int NOT_EXIST = 0;",
                "\tconst int INVALID = 1;",
                "\tconst int VALID = 2;",
                "\tint v2_signing_status = NOT_EXIST;",
                "\tint v3_signing_status = NOT_EXIST;",
                "",
                "\t\tksu_kernel_read_compat(fp, &size8, 0x8,",
                "\t\t\t\t       &pos); // sequence length",
                "\t\tif (id == 0x7109871au) {",
                "\t\t\tblock_valid = check_block(fp, &size4, &pos, &offset,",
                "\t\t\t\t\t\t  expected_size, expected_sha256);",
                "\t\t\tv2_signing_status = block_valid ? VALID : INVALID;",
                "\t\t} else if (id == 0xf05368c0u) {",
                "\t\t\tblock_valid = check_block(fp, &size4, &pos, &offset,",
                "\t\t\t\t\t\t  expected_size, expected_sha256);",
                "\t\t\tv3_signing_status = block_valid ? VALID : INVALID;",
                "\treturn (v2_signing_status == NOT_EXIST && v3_signing_status == VALID) ||",
                "\t       (v2_signing_status == VALID && v3_signing_status == NOT_EXIST) ||",
                "\t       (v2_signing_status == VALID && v3_signing_status == VALID);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5521",
        "func_name": "tiann/KernelSU/is_manager_apk",
        "description": "Incorrect Authorization in GitHub repository tiann/kernelsu prior to v0.6.9.",
        "git_url": "https://github.com/tiann/KernelSU/commit/a22959beae1aad96b1f72710a5daadf529c41bda",
        "commit_title": "kernel: harden the signature check (#1027)",
        "commit_text": "",
        "func_before": "int is_manager_apk(char *path)\n{\n\treturn check_v2_signature(path, ksu_expected_size, ksu_expected_hash);\n}",
        "func": "bool is_manager_apk(char *path)\n{\n\treturn check_v2_signature(path, ksu_expected_size, ksu_expected_hash);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-int is_manager_apk(char *path)\n+bool is_manager_apk(char *path)\n {\n \treturn check_v2_signature(path, ksu_expected_size, ksu_expected_hash);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "int is_manager_apk(char *path)"
            ],
            "added_lines": [
                "bool is_manager_apk(char *path)"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-46139",
        "func_name": "tiann/KernelSU/become_manager",
        "description": "KernelSU is a Kernel based root solution for Android. Starting in version 0.6.1 and prior to version 0.7.0, if a KernelSU installed device is infected with a malware whose app signing block specially constructed, it can take over root privileges on the device. The vulnerable verification logic actually obtains the signature of the last block with an id of `0x7109871a`, while the verification logic during Android installation is to obtain the first one. In addition to the actual signature upgrade that has been fixed (KSU thought it was V2 but was actually V3), there is also the problem of actual signature downgrading (KSU thought it was V2 but was actually V1). Find a condition in the signature verification logic that will cause the signature not to be found error, and KernelSU does not implement the same conditions, so KSU thinks there is a V2 signature, but the APK signature verification actually uses the V1 signature. This issue is fixed in version 0.7.0. As workarounds, keep the KernelSU manager installed and avoid installing unknown apps.",
        "git_url": "https://github.com/tiann/KernelSU/commit/d24813b2c3738f2f9bd762932141cadd948c354f",
        "commit_title": "Merge pull request from GHSA-86cp-3prf-pwqq",
        "commit_text": " * kernel: deny v2 signature blocks with incorrect number  * kernel: reject v1 signature  * kernel: enforce manager package name at compile time  * kernel: don't specific package name in source code, use it in ci",
        "func_before": "bool become_manager(char *pkg)\n{\n\tstruct fdtable *files_table;\n\tint i = 0;\n\tstruct path files_path;\n\tchar *cwd;\n\tchar *buf;\n\tbool result = false;\n\n\t// must be zygote's direct child, otherwise any app can fork a new process and\n\t// open manager's apk\n\tif (task_uid(current->real_parent).val != 0) {\n\t\tpr_info(\"parent is not zygote!\\n\");\n\t\treturn false;\n\t}\n\n\tbuf = (char *)kmalloc(PATH_MAX, GFP_ATOMIC);\n\tif (!buf) {\n\t\tpr_err(\"kalloc path failed.\\n\");\n\t\treturn false;\n\t}\n\n\tfiles_table = files_fdtable(current->files);\n\n\tint pkg_len = strlen(pkg);\n\t// todo: use iterate_fd\n\tfor (i = 0; files_table->fd[i] != NULL; i++) {\n\t\tfiles_path = files_table->fd[i]->f_path;\n\t\tif (!d_is_reg(files_path.dentry)) {\n\t\t\tcontinue;\n\t\t}\n\t\tcwd = d_path(&files_path, buf, PATH_MAX);\n\t\tif (startswith(cwd, \"/data/app/\") != 0 ||\n\t\t    endswith(cwd, \"/base.apk\") != 0) {\n\t\t\tcontinue;\n\t\t}\n\t\t// we have found the apk!\n\t\tpr_info(\"found apk: %s\\n\", cwd);\n\t\tchar *pkg_index = strstr(cwd, pkg);\n\t\tif (!pkg_index) {\n\t\t\tpr_info(\"apk path not match package name!\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tchar *next_char = pkg_index + pkg_len;\n\t\t// because we ensure the cwd must startswith `/data/app` and endswith `base.apk`\n\t\t// we don't need to check if the pointer is out of bounds\n\t\tif (*next_char != '-') {\n\t\t\t// from android 8.1: http://aospxref.com/android-8.1.0_r81/xref/frameworks/base/services/core/java/com/android/server/pm/PackageManagerService.java#17612\n\t\t\t// to android 13: http://aospxref.com/android-13.0.0_r3/xref/frameworks/base/services/core/java/com/android/server/pm/PackageManagerServiceUtils.java#1208\n\t\t\t// /data/app/~~[randomStringA]/[packageName]-[randomStringB]\n\t\t\t// the previous char must be `/` and the next char must be `-`\n\t\t\t// because we use strstr instead of equals, this is a strong verfication.\n\t\t\tpr_info(\"invalid pkg: %s\\n\", pkg);\n\t\t\tcontinue;\n\t\t}\n\t\tif (is_manager_apk(cwd)) {\n\t\t\t// check passed\n\t\t\tuid_t uid = current_uid().val;\n\t\t\tpr_info(\"manager uid: %d\\n\", uid);\n\n\t\t\tksu_set_manager_uid(uid);\n\n\t\t\tresult = true;\n\t\t\tgoto clean;\n\t\t} else {\n\t\t\tpr_info(\"manager signature invalid!\\n\");\n\t\t}\n\n\t\tbreak;\n\t}\n\nclean:\n\tkfree(buf);\n\treturn result;\n}",
        "func": "bool become_manager(char *pkg)\n{\n\tstruct fdtable *files_table;\n\tint i = 0;\n\tstruct path files_path;\n\tchar *cwd;\n\tchar *buf;\n\tbool result = false;\n\n#ifdef KSU_MANAGER_PACKAGE\n\t// pkg is `/<real package>`\n\tif (strncmp(pkg + 1, KSU_MANAGER_PACKAGE,\n\t\t    sizeof(KSU_MANAGER_PACKAGE) - 1) != 0) {\n\t\tpr_info(\"manager package is inconsistent with kernel build: %s\\n\",\n\t\t\tKSU_MANAGER_PACKAGE);\n\t\treturn false;\n\t}\n#endif\n\t// must be zygote's direct child, otherwise any app can fork a new process and\n\t// open manager's apk\n\tif (task_uid(current->real_parent).val != 0) {\n\t\tpr_info(\"parent is not zygote!\\n\");\n\t\treturn false;\n\t}\n\n\tbuf = (char *)kmalloc(PATH_MAX, GFP_ATOMIC);\n\tif (!buf) {\n\t\tpr_err(\"kalloc path failed.\\n\");\n\t\treturn false;\n\t}\n\n\tfiles_table = files_fdtable(current->files);\n\n\tint pkg_len = strlen(pkg);\n\t// todo: use iterate_fd\n\tfor (i = 0; files_table->fd[i] != NULL; i++) {\n\t\tfiles_path = files_table->fd[i]->f_path;\n\t\tif (!d_is_reg(files_path.dentry)) {\n\t\t\tcontinue;\n\t\t}\n\t\tcwd = d_path(&files_path, buf, PATH_MAX);\n\t\tif (startswith(cwd, \"/data/app/\") != 0 ||\n\t\t    endswith(cwd, \"/base.apk\") != 0) {\n\t\t\tcontinue;\n\t\t}\n\t\t// we have found the apk!\n\t\tpr_info(\"found apk: %s\\n\", cwd);\n\t\tchar *pkg_index = strstr(cwd, pkg);\n\t\tif (!pkg_index) {\n\t\t\tpr_info(\"apk path not match package name!\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tchar *next_char = pkg_index + pkg_len;\n\t\t// because we ensure the cwd must startswith `/data/app` and endswith `base.apk`\n\t\t// we don't need to check if the pointer is out of bounds\n\t\tif (*next_char != '-') {\n\t\t\t// from android 8.1: http://aospxref.com/android-8.1.0_r81/xref/frameworks/base/services/core/java/com/android/server/pm/PackageManagerService.java#17612\n\t\t\t// to android 13: http://aospxref.com/android-13.0.0_r3/xref/frameworks/base/services/core/java/com/android/server/pm/PackageManagerServiceUtils.java#1208\n\t\t\t// /data/app/~~[randomStringA]/[packageName]-[randomStringB]\n\t\t\t// the previous char must be `/` and the next char must be `-`\n\t\t\t// because we use strstr instead of equals, this is a strong verfication.\n\t\t\tpr_info(\"invalid pkg: %s\\n\", pkg);\n\t\t\tcontinue;\n\t\t}\n\t\tif (is_manager_apk(cwd)) {\n\t\t\t// check passed\n\t\t\tuid_t uid = current_uid().val;\n\t\t\tpr_info(\"manager uid: %d\\n\", uid);\n\n\t\t\tksu_set_manager_uid(uid);\n\n\t\t\tresult = true;\n\t\t\tgoto clean;\n\t\t} else {\n\t\t\tpr_info(\"manager signature invalid!\\n\");\n\t\t}\n\n\t\tbreak;\n\t}\n\nclean:\n\tkfree(buf);\n\treturn result;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,15 @@\n \tchar *buf;\n \tbool result = false;\n \n+#ifdef KSU_MANAGER_PACKAGE\n+\t// pkg is `/<real package>`\n+\tif (strncmp(pkg + 1, KSU_MANAGER_PACKAGE,\n+\t\t    sizeof(KSU_MANAGER_PACKAGE) - 1) != 0) {\n+\t\tpr_info(\"manager package is inconsistent with kernel build: %s\\n\",\n+\t\t\tKSU_MANAGER_PACKAGE);\n+\t\treturn false;\n+\t}\n+#endif\n \t// must be zygote's direct child, otherwise any app can fork a new process and\n \t// open manager's apk\n \tif (task_uid(current->real_parent).val != 0) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "#ifdef KSU_MANAGER_PACKAGE",
                "\t// pkg is `/<real package>`",
                "\tif (strncmp(pkg + 1, KSU_MANAGER_PACKAGE,",
                "\t\t    sizeof(KSU_MANAGER_PACKAGE) - 1) != 0) {",
                "\t\tpr_info(\"manager package is inconsistent with kernel build: %s\\n\",",
                "\t\t\tKSU_MANAGER_PACKAGE);",
                "\t\treturn false;",
                "\t}",
                "#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-46139",
        "func_name": "tiann/KernelSU/check_v2_signature",
        "description": "KernelSU is a Kernel based root solution for Android. Starting in version 0.6.1 and prior to version 0.7.0, if a KernelSU installed device is infected with a malware whose app signing block specially constructed, it can take over root privileges on the device. The vulnerable verification logic actually obtains the signature of the last block with an id of `0x7109871a`, while the verification logic during Android installation is to obtain the first one. In addition to the actual signature upgrade that has been fixed (KSU thought it was V2 but was actually V3), there is also the problem of actual signature downgrading (KSU thought it was V2 but was actually V1). Find a condition in the signature verification logic that will cause the signature not to be found error, and KernelSU does not implement the same conditions, so KSU thinks there is a V2 signature, but the APK signature verification actually uses the V1 signature. This issue is fixed in version 0.7.0. As workarounds, keep the KernelSU manager installed and avoid installing unknown apps.",
        "git_url": "https://github.com/tiann/KernelSU/commit/d24813b2c3738f2f9bd762932141cadd948c354f",
        "commit_title": "Merge pull request from GHSA-86cp-3prf-pwqq",
        "commit_text": " * kernel: deny v2 signature blocks with incorrect number  * kernel: reject v1 signature  * kernel: enforce manager package name at compile time  * kernel: don't specific package name in source code, use it in ci",
        "func_before": "static __always_inline bool\ncheck_v2_signature(char *path, unsigned expected_size, const char *expected_sha256)\n{\n\tunsigned char buffer[0x11] = { 0 };\n\tu32 size4;\n\tu64 size8, size_of_block;\n\n\tloff_t pos;\n\n\tbool v2_signing_valid = false;\n\tbool v3_signing_exist = false;\n\tbool v3_1_signing_exist = false;\n\n\tint i;\n\tstruct file *fp = ksu_filp_open_compat(path, O_RDONLY, 0);\n\tif (IS_ERR(fp)) {\n\t\tpr_err(\"open %s error.\\n\", path);\n\t\treturn PTR_ERR(fp);\n\t}\n\n\t// disable inotify for this file\n\tfp->f_mode |= FMODE_NONOTIFY;\n\n\t// https://en.wikipedia.org/wiki/Zip_(file_format)#End_of_central_directory_record_(EOCD)\n\tfor (i = 0;; ++i) {\n\t\tunsigned short n;\n\t\tpos = generic_file_llseek(fp, -i - 2, SEEK_END);\n\t\tksu_kernel_read_compat(fp, &n, 2, &pos);\n\t\tif (n == i) {\n\t\t\tpos -= 22;\n\t\t\tksu_kernel_read_compat(fp, &size4, 4, &pos);\n\t\t\tif ((size4 ^ 0xcafebabeu) == 0xccfbf1eeu) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == 0xffff) {\n\t\t\tpr_info(\"error: cannot find eocd\\n\");\n\t\t\tgoto clean;\n\t\t}\n\t}\n\n\tpos += 12;\n\t// offset\n\tksu_kernel_read_compat(fp, &size4, 0x4, &pos);\n\tpos = size4 - 0x18;\n\n\tksu_kernel_read_compat(fp, &size8, 0x8, &pos);\n\tksu_kernel_read_compat(fp, buffer, 0x10, &pos);\n\tif (strcmp((char *)buffer, \"APK Sig Block 42\")) {\n\t\tgoto clean;\n\t}\n\n\tpos = size4 - (size8 + 0x8);\n\tksu_kernel_read_compat(fp, &size_of_block, 0x8, &pos);\n\tif (size_of_block != size8) {\n\t\tgoto clean;\n\t}\n\n\tfor (;;) {\n\t\tuint32_t id;\n\t\tuint32_t offset;\n\t\tksu_kernel_read_compat(fp, &size8, 0x8,\n\t\t\t\t       &pos); // sequence length\n\t\tif (size8 == size_of_block) {\n\t\t\tbreak;\n\t\t}\n\t\tksu_kernel_read_compat(fp, &id, 0x4, &pos); // id\n\t\toffset = 4;\n\t\tpr_info(\"id: 0x%08x\\n\", id);\n\t\tif (id == 0x7109871au) {\n\t\t\tv2_signing_valid = check_block(fp, &size4, &pos, &offset,\n\t\t\t\t\t\t  expected_size, expected_sha256);\n\t\t} else if (id == 0xf05368c0u) {\n\t\t\t// http://aospxref.com/android-14.0.0_r2/xref/frameworks/base/core/java/android/util/apk/ApkSignatureSchemeV3Verifier.java#73\n\t\t\tv3_signing_exist = true;\n\t\t} else if (id == 0x1b93ad61u) {\n\t\t\t// http://aospxref.com/android-14.0.0_r2/xref/frameworks/base/core/java/android/util/apk/ApkSignatureSchemeV3Verifier.java#74\n\t\t\tv3_1_signing_exist = true;\n\t\t}\n\t\tpos += (size8 - offset);\n\t}\n\nclean:\n\tfilp_close(fp, 0);\n\n\tif (v3_signing_exist || v3_1_signing_exist) {\n\t\tpr_err(\"Unexpected v3 signature scheme found!\\n\");\n\t\treturn false;\n\t}\n\n\treturn v2_signing_valid;\n}",
        "func": "static __always_inline bool check_v2_signature(char *path,\n\t\t\t\t\t       unsigned expected_size,\n\t\t\t\t\t       const char *expected_sha256)\n{\n\tunsigned char buffer[0x11] = { 0 };\n\tu32 size4;\n\tu64 size8, size_of_block;\n\n\tloff_t pos;\n\n\tbool v2_signing_valid = false;\n\tint v2_signing_blocks = 0;\n\tbool v3_signing_exist = false;\n\tbool v3_1_signing_exist = false;\n\n\tint i;\n\tstruct file *fp = ksu_filp_open_compat(path, O_RDONLY, 0);\n\tif (IS_ERR(fp)) {\n\t\tpr_err(\"open %s error.\\n\", path);\n\t\treturn PTR_ERR(fp);\n\t}\n\n\t// disable inotify for this file\n\tfp->f_mode |= FMODE_NONOTIFY;\n\n\t// https://en.wikipedia.org/wiki/Zip_(file_format)#End_of_central_directory_record_(EOCD)\n\tfor (i = 0;; ++i) {\n\t\tunsigned short n;\n\t\tpos = generic_file_llseek(fp, -i - 2, SEEK_END);\n\t\tksu_kernel_read_compat(fp, &n, 2, &pos);\n\t\tif (n == i) {\n\t\t\tpos -= 22;\n\t\t\tksu_kernel_read_compat(fp, &size4, 4, &pos);\n\t\t\tif ((size4 ^ 0xcafebabeu) == 0xccfbf1eeu) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == 0xffff) {\n\t\t\tpr_info(\"error: cannot find eocd\\n\");\n\t\t\tgoto clean;\n\t\t}\n\t}\n\n\tpos += 12;\n\t// offset\n\tksu_kernel_read_compat(fp, &size4, 0x4, &pos);\n\tpos = size4 - 0x18;\n\n\tksu_kernel_read_compat(fp, &size8, 0x8, &pos);\n\tksu_kernel_read_compat(fp, buffer, 0x10, &pos);\n\tif (strcmp((char *)buffer, \"APK Sig Block 42\")) {\n\t\tgoto clean;\n\t}\n\n\tpos = size4 - (size8 + 0x8);\n\tksu_kernel_read_compat(fp, &size_of_block, 0x8, &pos);\n\tif (size_of_block != size8) {\n\t\tgoto clean;\n\t}\n\n\tfor (;;) {\n\t\tuint32_t id;\n\t\tuint32_t offset;\n\t\tksu_kernel_read_compat(fp, &size8, 0x8,\n\t\t\t\t       &pos); // sequence length\n\t\tif (size8 == size_of_block) {\n\t\t\tbreak;\n\t\t}\n\t\tksu_kernel_read_compat(fp, &id, 0x4, &pos); // id\n\t\toffset = 4;\n\t\tpr_info(\"id: 0x%08x\\n\", id);\n\t\tif (id == 0x7109871au) {\n\t\t\tv2_signing_blocks++;\n\t\t\tv2_signing_valid =\n\t\t\t\tcheck_block(fp, &size4, &pos, &offset,\n\t\t\t\t\t    expected_size, expected_sha256);\n\t\t} else if (id == 0xf05368c0u) {\n\t\t\t// http://aospxref.com/android-14.0.0_r2/xref/frameworks/base/core/java/android/util/apk/ApkSignatureSchemeV3Verifier.java#73\n\t\t\tv3_signing_exist = true;\n\t\t} else if (id == 0x1b93ad61u) {\n\t\t\t// http://aospxref.com/android-14.0.0_r2/xref/frameworks/base/core/java/android/util/apk/ApkSignatureSchemeV3Verifier.java#74\n\t\t\tv3_1_signing_exist = true;\n\t\t}\n\t\tpos += (size8 - offset);\n\t}\n\n\tif (v2_signing_blocks != 1) {\n\t\tpr_err(\"Unexpected v2 signature count: %d\\n\",\n\t\t       v2_signing_blocks);\n\t\tv2_signing_valid = false;\n\t}\n\n\tif (v2_signing_valid) {\n\t\tint has_v1_signing = has_v1_signature_file(fp);\n\t\tif (has_v1_signing) {\n\t\t\tpr_err(\"Unexpected v1 signature scheme found!\\n\");\n\t\t\tfilp_close(fp, 0);\n\t\t\treturn false;\n\t\t}\n\t}\nclean:\n\tfilp_close(fp, 0);\n\n\tif (v3_signing_exist || v3_1_signing_exist) {\n\t\tpr_err(\"Unexpected v3 signature scheme found!\\n\");\n\t\treturn false;\n\t}\n\n\treturn v2_signing_valid;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n-static __always_inline bool\n-check_v2_signature(char *path, unsigned expected_size, const char *expected_sha256)\n+static __always_inline bool check_v2_signature(char *path,\n+\t\t\t\t\t       unsigned expected_size,\n+\t\t\t\t\t       const char *expected_sha256)\n {\n \tunsigned char buffer[0x11] = { 0 };\n \tu32 size4;\n@@ -8,6 +9,7 @@\n \tloff_t pos;\n \n \tbool v2_signing_valid = false;\n+\tint v2_signing_blocks = 0;\n \tbool v3_signing_exist = false;\n \tbool v3_1_signing_exist = false;\n \n@@ -68,8 +70,10 @@\n \t\toffset = 4;\n \t\tpr_info(\"id: 0x%08x\\n\", id);\n \t\tif (id == 0x7109871au) {\n-\t\t\tv2_signing_valid = check_block(fp, &size4, &pos, &offset,\n-\t\t\t\t\t\t  expected_size, expected_sha256);\n+\t\t\tv2_signing_blocks++;\n+\t\t\tv2_signing_valid =\n+\t\t\t\tcheck_block(fp, &size4, &pos, &offset,\n+\t\t\t\t\t    expected_size, expected_sha256);\n \t\t} else if (id == 0xf05368c0u) {\n \t\t\t// http://aospxref.com/android-14.0.0_r2/xref/frameworks/base/core/java/android/util/apk/ApkSignatureSchemeV3Verifier.java#73\n \t\t\tv3_signing_exist = true;\n@@ -80,6 +84,20 @@\n \t\tpos += (size8 - offset);\n \t}\n \n+\tif (v2_signing_blocks != 1) {\n+\t\tpr_err(\"Unexpected v2 signature count: %d\\n\",\n+\t\t       v2_signing_blocks);\n+\t\tv2_signing_valid = false;\n+\t}\n+\n+\tif (v2_signing_valid) {\n+\t\tint has_v1_signing = has_v1_signature_file(fp);\n+\t\tif (has_v1_signing) {\n+\t\t\tpr_err(\"Unexpected v1 signature scheme found!\\n\");\n+\t\t\tfilp_close(fp, 0);\n+\t\t\treturn false;\n+\t\t}\n+\t}\n clean:\n \tfilp_close(fp, 0);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "static __always_inline bool",
                "check_v2_signature(char *path, unsigned expected_size, const char *expected_sha256)",
                "\t\t\tv2_signing_valid = check_block(fp, &size4, &pos, &offset,",
                "\t\t\t\t\t\t  expected_size, expected_sha256);"
            ],
            "added_lines": [
                "static __always_inline bool check_v2_signature(char *path,",
                "\t\t\t\t\t       unsigned expected_size,",
                "\t\t\t\t\t       const char *expected_sha256)",
                "\tint v2_signing_blocks = 0;",
                "\t\t\tv2_signing_blocks++;",
                "\t\t\tv2_signing_valid =",
                "\t\t\t\tcheck_block(fp, &size4, &pos, &offset,",
                "\t\t\t\t\t    expected_size, expected_sha256);",
                "\tif (v2_signing_blocks != 1) {",
                "\t\tpr_err(\"Unexpected v2 signature count: %d\\n\",",
                "\t\t       v2_signing_blocks);",
                "\t\tv2_signing_valid = false;",
                "\t}",
                "",
                "\tif (v2_signing_valid) {",
                "\t\tint has_v1_signing = has_v1_signature_file(fp);",
                "\t\tif (has_v1_signing) {",
                "\t\t\tpr_err(\"Unexpected v1 signature scheme found!\\n\");",
                "\t\t\tfilp_close(fp, 0);",
                "\t\t\treturn false;",
                "\t\t}",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-46139",
        "func_name": "tiann/KernelSU/ksu_sha256",
        "description": "KernelSU is a Kernel based root solution for Android. Starting in version 0.6.1 and prior to version 0.7.0, if a KernelSU installed device is infected with a malware whose app signing block specially constructed, it can take over root privileges on the device. The vulnerable verification logic actually obtains the signature of the last block with an id of `0x7109871a`, while the verification logic during Android installation is to obtain the first one. In addition to the actual signature upgrade that has been fixed (KSU thought it was V2 but was actually V3), there is also the problem of actual signature downgrading (KSU thought it was V2 but was actually V1). Find a condition in the signature verification logic that will cause the signature not to be found error, and KernelSU does not implement the same conditions, so KSU thinks there is a V2 signature, but the APK signature verification actually uses the V1 signature. This issue is fixed in version 0.7.0. As workarounds, keep the KernelSU manager installed and avoid installing unknown apps.",
        "git_url": "https://github.com/tiann/KernelSU/commit/d24813b2c3738f2f9bd762932141cadd948c354f",
        "commit_title": "Merge pull request from GHSA-86cp-3prf-pwqq",
        "commit_text": " * kernel: deny v2 signature blocks with incorrect number  * kernel: reject v1 signature  * kernel: enforce manager package name at compile time  * kernel: don't specific package name in source code, use it in ci",
        "func_before": "static int ksu_sha256(const unsigned char *data, unsigned int datalen,\n\t\tunsigned char *digest)\n{\n\tstruct crypto_shash *alg;\n\tchar *hash_alg_name = \"sha256\";\n\tint ret;\n\n\talg = crypto_alloc_shash(hash_alg_name, 0, 0);\n\tif (IS_ERR(alg)) {\n\t\tpr_info(\"can't alloc alg %s\\n\", hash_alg_name);\n\t\treturn PTR_ERR(alg);\n\t}\n\tret = calc_hash(alg, data, datalen, digest);\n\tcrypto_free_shash(alg);\n\treturn ret;\n}",
        "func": "static int ksu_sha256(const unsigned char *data, unsigned int datalen,\n\t\t      unsigned char *digest)\n{\n\tstruct crypto_shash *alg;\n\tchar *hash_alg_name = \"sha256\";\n\tint ret;\n\n\talg = crypto_alloc_shash(hash_alg_name, 0, 0);\n\tif (IS_ERR(alg)) {\n\t\tpr_info(\"can't alloc alg %s\\n\", hash_alg_name);\n\t\treturn PTR_ERR(alg);\n\t}\n\tret = calc_hash(alg, data, datalen, digest);\n\tcrypto_free_shash(alg);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n static int ksu_sha256(const unsigned char *data, unsigned int datalen,\n-\t\tunsigned char *digest)\n+\t\t      unsigned char *digest)\n {\n \tstruct crypto_shash *alg;\n \tchar *hash_alg_name = \"sha256\";",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tunsigned char *digest)"
            ],
            "added_lines": [
                "\t\t      unsigned char *digest)"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-46139",
        "func_name": "tiann/KernelSU/check_block",
        "description": "KernelSU is a Kernel based root solution for Android. Starting in version 0.6.1 and prior to version 0.7.0, if a KernelSU installed device is infected with a malware whose app signing block specially constructed, it can take over root privileges on the device. The vulnerable verification logic actually obtains the signature of the last block with an id of `0x7109871a`, while the verification logic during Android installation is to obtain the first one. In addition to the actual signature upgrade that has been fixed (KSU thought it was V2 but was actually V3), there is also the problem of actual signature downgrading (KSU thought it was V2 but was actually V1). Find a condition in the signature verification logic that will cause the signature not to be found error, and KernelSU does not implement the same conditions, so KSU thinks there is a V2 signature, but the APK signature verification actually uses the V1 signature. This issue is fixed in version 0.7.0. As workarounds, keep the KernelSU manager installed and avoid installing unknown apps.",
        "git_url": "https://github.com/tiann/KernelSU/commit/d24813b2c3738f2f9bd762932141cadd948c354f",
        "commit_title": "Merge pull request from GHSA-86cp-3prf-pwqq",
        "commit_text": " * kernel: deny v2 signature blocks with incorrect number  * kernel: reject v1 signature  * kernel: enforce manager package name at compile time  * kernel: don't specific package name in source code, use it in ci",
        "func_before": "static bool check_block(struct file *fp, u32 *size4, loff_t *pos, u32 *offset,\n\t\t\tunsigned expected_size, const char* expected_sha256)\n{\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // signer-sequence length\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // signer length\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // signed data length\n\n\t*offset += 0x4 * 3;\n\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // digests-sequence length\n\n\t*pos += *size4;\n\t*offset += 0x4 + *size4;\n\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // certificates length\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // certificate length\n\t*offset += 0x4 * 2;\n\n\tif (*size4 == expected_size) {\n\t\t*offset += *size4;\n\n\t\t#define CERT_MAX_LENGTH 1024\n\t\tchar cert[CERT_MAX_LENGTH];\n\t\tif (*size4 > CERT_MAX_LENGTH) {\n\t\t\tpr_info(\"cert length overlimit\\n\");\n\t\t\treturn false;\n\t\t}\n\t\tksu_kernel_read_compat(fp, cert, *size4, pos);\n\t\tunsigned char digest[SHA256_DIGEST_SIZE];\n\t\tif (IS_ERR(ksu_sha256(cert, *size4, digest))) {\n\t\t\tpr_info(\"sha256 error\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\tchar hash_str[SHA256_DIGEST_SIZE * 2 + 1];\n\t\thash_str[SHA256_DIGEST_SIZE * 2] = '\\0';\n\n\t\tbin2hex(hash_str, digest, SHA256_DIGEST_SIZE);\n\t\tpr_info(\"sha256: %s, expected: %s\\n\", hash_str, expected_sha256);\n\t\tif (strcmp(expected_sha256, hash_str) == 0) {\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}",
        "func": "static bool check_block(struct file *fp, u32 *size4, loff_t *pos, u32 *offset,\n\t\t\tunsigned expected_size, const char *expected_sha256)\n{\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // signer-sequence length\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // signer length\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // signed data length\n\n\t*offset += 0x4 * 3;\n\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // digests-sequence length\n\n\t*pos += *size4;\n\t*offset += 0x4 + *size4;\n\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // certificates length\n\tksu_kernel_read_compat(fp, size4, 0x4, pos); // certificate length\n\t*offset += 0x4 * 2;\n\n\tif (*size4 == expected_size) {\n\t\t*offset += *size4;\n\n#define CERT_MAX_LENGTH 1024\n\t\tchar cert[CERT_MAX_LENGTH];\n\t\tif (*size4 > CERT_MAX_LENGTH) {\n\t\t\tpr_info(\"cert length overlimit\\n\");\n\t\t\treturn false;\n\t\t}\n\t\tksu_kernel_read_compat(fp, cert, *size4, pos);\n\t\tunsigned char digest[SHA256_DIGEST_SIZE];\n\t\tif (IS_ERR(ksu_sha256(cert, *size4, digest))) {\n\t\t\tpr_info(\"sha256 error\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\tchar hash_str[SHA256_DIGEST_SIZE * 2 + 1];\n\t\thash_str[SHA256_DIGEST_SIZE * 2] = '\\0';\n\n\t\tbin2hex(hash_str, digest, SHA256_DIGEST_SIZE);\n\t\tpr_info(\"sha256: %s, expected: %s\\n\", hash_str,\n\t\t\texpected_sha256);\n\t\tif (strcmp(expected_sha256, hash_str) == 0) {\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n static bool check_block(struct file *fp, u32 *size4, loff_t *pos, u32 *offset,\n-\t\t\tunsigned expected_size, const char* expected_sha256)\n+\t\t\tunsigned expected_size, const char *expected_sha256)\n {\n \tksu_kernel_read_compat(fp, size4, 0x4, pos); // signer-sequence length\n \tksu_kernel_read_compat(fp, size4, 0x4, pos); // signer length\n@@ -19,7 +19,7 @@\n \tif (*size4 == expected_size) {\n \t\t*offset += *size4;\n \n-\t\t#define CERT_MAX_LENGTH 1024\n+#define CERT_MAX_LENGTH 1024\n \t\tchar cert[CERT_MAX_LENGTH];\n \t\tif (*size4 > CERT_MAX_LENGTH) {\n \t\t\tpr_info(\"cert length overlimit\\n\");\n@@ -36,7 +36,8 @@\n \t\thash_str[SHA256_DIGEST_SIZE * 2] = '\\0';\n \n \t\tbin2hex(hash_str, digest, SHA256_DIGEST_SIZE);\n-\t\tpr_info(\"sha256: %s, expected: %s\\n\", hash_str, expected_sha256);\n+\t\tpr_info(\"sha256: %s, expected: %s\\n\", hash_str,\n+\t\t\texpected_sha256);\n \t\tif (strcmp(expected_sha256, hash_str) == 0) {\n \t\t\treturn true;\n \t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tunsigned expected_size, const char* expected_sha256)",
                "\t\t#define CERT_MAX_LENGTH 1024",
                "\t\tpr_info(\"sha256: %s, expected: %s\\n\", hash_str, expected_sha256);"
            ],
            "added_lines": [
                "\t\t\tunsigned expected_size, const char *expected_sha256)",
                "#define CERT_MAX_LENGTH 1024",
                "\t\tpr_info(\"sha256: %s, expected: %s\\n\", hash_str,",
                "\t\t\texpected_sha256);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-31829",
        "func_name": "torvalds/linux/sanitize_ptr_alu",
        "description": "kernel/bpf/verifier.c in the Linux kernel through 5.12.1 performs undesirable speculative loads, leading to disclosure of stack content via side-channel attacks, aka CID-801c6058d14a. The specific concern is not protecting the BPF stack area against speculative loads. Also, the BPF stack can contain uninitialized data that might represent sensitive information previously operated on by the kernel.",
        "git_url": "https://github.com/torvalds/linux/commit/801c6058d14a82179a7ee17a4b532cac6fad067f",
        "commit_title": "bpf: Fix leakage of uninitialized bpf stack under speculation",
        "commit_text": " The current implemented mechanisms to mitigate data disclosure under speculation mainly address stack and map value oob access from the speculative domain. However, Piotr discovered that uninitialized BPF stack is not protected yet, and thus old data from the kernel stack, potentially including addresses of kernel structures, could still be extracted from that 512 bytes large window. The BPF stack is special compared to map values since it's not zero initialized for every program invocation, whereas map values /are/ zero initialized upon their initial allocation and thus cannot leak any prior data in either domain. In the non-speculative domain, the verifier ensures that every stack slot read must have a prior stack slot write by the BPF program to avoid such data leaking issue.  However, this is not enough: for example, when the pointer arithmetic operation moves the stack pointer from the last valid stack offset to the first valid offset, the sanitation logic allows for any intermediate offsets during speculative execution, which could then be used to extract any restricted stack content via side-channel.  Given for unprivileged stack pointer arithmetic the use of unknown but bounded scalars is generally forbidden, we can simply turn the register-based arithmetic operation into an immediate-based arithmetic operation without the need for masking. This also gives the benefit of reducing the needed instructions for the operation. Given after the work in 7fedb63a8307 (\"bpf: Tighten speculative pointer arithmetic mask\"), the aux->alu_limit already holds the final immediate value for the offset register with the known scalar. Thus, a simple mov of the immediate to AX register with using AX as the source for the original instruction is sufficient and possible now in this case. ",
        "func_before": "static int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    const struct bpf_reg_state *off_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    struct bpf_insn_aux_data *tmp_aux,\n\t\t\t    const bool commit_window)\n{\n\tstruct bpf_insn_aux_data *aux = commit_window ? cur_aux(env) : tmp_aux;\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tbool off_is_neg = off_reg->smin_value < 0;\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\tint err;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\terr = retrieve_ptr_limit(ptr_reg, off_reg, &alu_limit, opcode);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (commit_window) {\n\t\t/* In commit phase we narrow the masking window based on\n\t\t * the observed pointer move after the simulated operation.\n\t\t */\n\t\talu_state = tmp_aux->alu_state;\n\t\talu_limit = abs(tmp_aux->alu_limit - alu_limit);\n\t} else {\n\t\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\t\talu_state |= ptr_is_dst_reg ?\n\t\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\t}\n\n\terr = update_alu_sanitation_state(aux, alu_state, alu_limit);\n\tif (err < 0)\n\t\treturn err;\ndo_sim:\n\t/* If we're in commit phase, we're done here given we already\n\t * pushed the truncated dst_reg into the speculative verification\n\t * stack.\n\t */\n\tif (commit_window)\n\t\treturn 0;\n\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg && ret)\n\t\t*dst_reg = tmp;\n\treturn !ret ? REASON_STACK : 0;\n}",
        "func": "static int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    const struct bpf_reg_state *off_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    struct bpf_insn_aux_data *tmp_aux,\n\t\t\t    const bool commit_window)\n{\n\tstruct bpf_insn_aux_data *aux = commit_window ? cur_aux(env) : tmp_aux;\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tbool off_is_imm = tnum_is_const(off_reg->var_off);\n\tbool off_is_neg = off_reg->smin_value < 0;\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\tint err;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\terr = retrieve_ptr_limit(ptr_reg, off_reg, &alu_limit, opcode);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (commit_window) {\n\t\t/* In commit phase we narrow the masking window based on\n\t\t * the observed pointer move after the simulated operation.\n\t\t */\n\t\talu_state = tmp_aux->alu_state;\n\t\talu_limit = abs(tmp_aux->alu_limit - alu_limit);\n\t} else {\n\t\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\t\talu_state |= off_is_imm ? BPF_ALU_IMMEDIATE : 0;\n\t\talu_state |= ptr_is_dst_reg ?\n\t\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\t}\n\n\terr = update_alu_sanitation_state(aux, alu_state, alu_limit);\n\tif (err < 0)\n\t\treturn err;\ndo_sim:\n\t/* If we're in commit phase, we're done here given we already\n\t * pushed the truncated dst_reg into the speculative verification\n\t * stack.\n\t */\n\tif (commit_window)\n\t\treturn 0;\n\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg && ret)\n\t\t*dst_reg = tmp;\n\treturn !ret ? REASON_STACK : 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,7 @@\n {\n \tstruct bpf_insn_aux_data *aux = commit_window ? cur_aux(env) : tmp_aux;\n \tstruct bpf_verifier_state *vstate = env->cur_state;\n+\tbool off_is_imm = tnum_is_const(off_reg->var_off);\n \tbool off_is_neg = off_reg->smin_value < 0;\n \tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n \tu8 opcode = BPF_OP(insn->code);\n@@ -38,6 +39,7 @@\n \t\talu_limit = abs(tmp_aux->alu_limit - alu_limit);\n \t} else {\n \t\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n+\t\talu_state |= off_is_imm ? BPF_ALU_IMMEDIATE : 0;\n \t\talu_state |= ptr_is_dst_reg ?\n \t\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n \t}",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tbool off_is_imm = tnum_is_const(off_reg->var_off);",
                "\t\talu_state |= off_is_imm ? BPF_ALU_IMMEDIATE : 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-31829",
        "func_name": "torvalds/linux/do_misc_fixups",
        "description": "kernel/bpf/verifier.c in the Linux kernel through 5.12.1 performs undesirable speculative loads, leading to disclosure of stack content via side-channel attacks, aka CID-801c6058d14a. The specific concern is not protecting the BPF stack area against speculative loads. Also, the BPF stack can contain uninitialized data that might represent sensitive information previously operated on by the kernel.",
        "git_url": "https://github.com/torvalds/linux/commit/801c6058d14a82179a7ee17a4b532cac6fad067f",
        "commit_title": "bpf: Fix leakage of uninitialized bpf stack under speculation",
        "commit_text": " The current implemented mechanisms to mitigate data disclosure under speculation mainly address stack and map value oob access from the speculative domain. However, Piotr discovered that uninitialized BPF stack is not protected yet, and thus old data from the kernel stack, potentially including addresses of kernel structures, could still be extracted from that 512 bytes large window. The BPF stack is special compared to map values since it's not zero initialized for every program invocation, whereas map values /are/ zero initialized upon their initial allocation and thus cannot leak any prior data in either domain. In the non-speculative domain, the verifier ensures that every stack slot read must have a prior stack slot write by the BPF program to avoid such data leaking issue.  However, this is not enough: for example, when the pointer arithmetic operation moves the stack pointer from the last valid stack offset to the first valid offset, the sanitation logic allows for any intermediate offsets during speculative execution, which could then be used to extract any restricted stack content via side-channel.  Given for unprivileged stack pointer arithmetic the use of unknown but bounded scalars is generally forbidden, we can simply turn the register-based arithmetic operation into an immediate-based arithmetic operation without the need for masking. This also gives the benefit of reducing the needed instructions for the operation. Given after the work in 7fedb63a8307 (\"bpf: Tighten speculative pointer arithmetic mask\"), the aux->alu_limit already holds the final immediate value for the offset register with the known scalar. Thus, a simple mov of the immediate to AX register with using AX as the source for the original instruction is sufficient and possible now in this case. ",
        "func_before": "static int do_misc_fixups(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tbool expect_blinding = bpf_jit_blinding_enabled(prog);\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, ret, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\t/* Make divide-by-zero exceptions impossible. */\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tbool isdiv = BPF_OP(insn->code) == BPF_DIV;\n\t\t\tstruct bpf_insn *patchlet;\n\t\t\tstruct bpf_insn chk_and_div[] = {\n\t\t\t\t/* [R,W]x div 0 -> 0 */\n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JNE | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 2, 0),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn chk_and_mod[] = {\n\t\t\t\t/* [R,W]x mod 0 -> [R,W]x */\n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JEQ | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 1 + (is64 ? 0 : 1), 0),\n\t\t\t\t*insn,\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\tBPF_MOV32_REG(insn->dst_reg, insn->dst_reg),\n\t\t\t};\n\n\t\t\tpatchlet = isdiv ? chk_and_div : chk_and_mod;\n\t\t\tcnt = isdiv ? ARRAY_SIZE(chk_and_div) :\n\t\t\t\t      ARRAY_SIZE(chk_and_mod) - (is64 ? 2 : 0);\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Implement LD_ABS and LD_IND with a rewrite, if supported by the program type. */\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Rewrite pointer arithmetic to mitigate speculation attacks. */\n\t\tif (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {\n\t\t\tconst u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;\n\t\t\tconst u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;\n\t\t\tstruct bpf_insn *patch = &insn_buf[0];\n\t\t\tbool issrc, isneg;\n\t\t\tu32 off_reg;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!aux->alu_state ||\n\t\t\t    aux->alu_state == BPF_ALU_NON_POINTER)\n\t\t\t\tcontinue;\n\n\t\t\tisneg = aux->alu_state & BPF_ALU_NEG_VALUE;\n\t\t\tissrc = (aux->alu_state & BPF_ALU_SANITIZE) ==\n\t\t\t\tBPF_ALU_SANITIZE_SRC;\n\n\t\t\toff_reg = issrc ? insn->src_reg : insn->dst_reg;\n\t\t\tif (isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX, off_reg);\n\t\t\tif (!issrc)\n\t\t\t\t*patch++ = BPF_MOV64_REG(insn->dst_reg, insn->src_reg);\n\t\t\tinsn->src_reg = BPF_REG_AX;\n\t\t\tif (isneg)\n\t\t\t\tinsn->code = insn->code == code_add ?\n\t\t\t\t\t     code_sub : code_add;\n\t\t\t*patch++ = *insn;\n\t\t\tif (issrc && isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\tcnt = patch - insn_buf;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_KFUNC_CALL) {\n\t\t\tret = fixup_kfunc_call(env, insn);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_override_return)\n\t\t\tprog->kprobe_override = 1;\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tif (!allow_tail_call_in_subprogs(env))\n\t\t\t\tprog->aux->stack_depth = MAX_BPF_STACK;\n\t\t\tprog->aux->max_pkt_offset = MAX_PACKET_OFF;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (env->bpf_capable && !expect_blinding &&\n\t\t\t    prog->jit_requested &&\n\t\t\t    !bpf_map_key_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_unpriv(aux)) {\n\t\t\t\tstruct bpf_jit_poke_descriptor desc = {\n\t\t\t\t\t.reason = BPF_POKE_REASON_TAIL_CALL,\n\t\t\t\t\t.tail_call.map = BPF_MAP_PTR(aux->map_ptr_state),\n\t\t\t\t\t.tail_call.key = bpf_map_key_immediate(aux),\n\t\t\t\t\t.insn_idx = i + delta,\n\t\t\t\t};\n\n\t\t\t\tret = bpf_jit_add_poke_descriptor(prog, &desc);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tverbose(env, \"adding tail call poke descriptor failed\\n\");\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\n\t\t\t\tinsn->imm = ret + 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!bpf_map_ptr_unpriv(aux))\n\t\t\t\tcontinue;\n\n\t\t\t/* instead of changing every JIT dealing with tail_call\n\t\t\t * emit two extra insns:\n\t\t\t * if (index >= max_entries) goto out;\n\t\t\t * index &= array->index_mask;\n\t\t\t * to avoid out-of-bounds cpu speculation\n\t\t\t */\n\t\t\tif (bpf_map_ptr_poisoned(aux)) {\n\t\t\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tinsn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,\n\t\t\t\t\t\t  map_ptr->max_entries, 2);\n\t\t\tinsn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,\n\t\t\t\t\t\t    container_of(map_ptr,\n\t\t\t\t\t\t\t\t struct bpf_array,\n\t\t\t\t\t\t\t\t map)->index_mask);\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * and other inlining handlers are currently limited to 64 bit\n\t\t * only.\n\t\t */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    (insn->imm == BPF_FUNC_map_lookup_elem ||\n\t\t     insn->imm == BPF_FUNC_map_update_elem ||\n\t\t     insn->imm == BPF_FUNC_map_delete_elem ||\n\t\t     insn->imm == BPF_FUNC_map_push_elem   ||\n\t\t     insn->imm == BPF_FUNC_map_pop_elem    ||\n\t\t     insn->imm == BPF_FUNC_map_peek_elem   ||\n\t\t     insn->imm == BPF_FUNC_redirect_map)) {\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (bpf_map_ptr_poisoned(aux))\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tops = map_ptr->ops;\n\t\t\tif (insn->imm == BPF_FUNC_map_lookup_elem &&\n\t\t\t    ops->map_gen_lookup) {\n\t\t\t\tcnt = ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\t\tif (cnt == -EOPNOTSUPP)\n\t\t\t\t\tgoto patch_map_ops_generic;\n\t\t\t\tif (cnt <= 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta,\n\t\t\t\t\t\t\t       insn_buf, cnt);\n\t\t\t\tif (!new_prog)\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tdelta    += cnt - 1;\n\t\t\t\tenv->prog = prog = new_prog;\n\t\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_delete_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_update_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_push_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_pop_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_peek_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_redirect,\n\t\t\t\t     (int (*)(struct bpf_map *map, u32 ifindex, u64 flags))NULL));\n\npatch_map_ops_generic:\n\t\t\tswitch (insn->imm) {\n\t\t\tcase BPF_FUNC_map_lookup_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_lookup_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_update_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_update_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_delete_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_delete_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_push_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_push_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_pop_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_pop_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_peek_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_peek_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_redirect_map:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_redirect) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgoto patch_call_imm;\n\t\t}\n\n\t\t/* Implement bpf_jiffies64 inline. */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    insn->imm == BPF_FUNC_jiffies64) {\n\t\t\tstruct bpf_insn ld_jiffies_addr[2] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_0,\n\t\t\t\t\t     (unsigned long)&jiffies),\n\t\t\t};\n\n\t\t\tinsn_buf[0] = ld_jiffies_addr[0];\n\t\t\tinsn_buf[1] = ld_jiffies_addr[1];\n\t\t\tinsn_buf[2] = BPF_LDX_MEM(BPF_DW, BPF_REG_0,\n\t\t\t\t\t\t  BPF_REG_0, 0);\n\t\t\tcnt = 3;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf,\n\t\t\t\t\t\t       cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm, env->prog);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\t/* Since poke tab is now finalized, publish aux to tracker. */\n\tfor (i = 0; i < prog->aux->size_poke_tab; i++) {\n\t\tmap_ptr = prog->aux->poke_tab[i].tail_call.map;\n\t\tif (!map_ptr->ops->map_poke_track ||\n\t\t    !map_ptr->ops->map_poke_untrack ||\n\t\t    !map_ptr->ops->map_poke_run) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tret = map_ptr->ops->map_poke_track(map_ptr, prog->aux);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"tracking tail call prog failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tsort_kfunc_descs_by_imm(env->prog);\n\n\treturn 0;\n}",
        "func": "static int do_misc_fixups(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tbool expect_blinding = bpf_jit_blinding_enabled(prog);\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, ret, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\t/* Make divide-by-zero exceptions impossible. */\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tbool isdiv = BPF_OP(insn->code) == BPF_DIV;\n\t\t\tstruct bpf_insn *patchlet;\n\t\t\tstruct bpf_insn chk_and_div[] = {\n\t\t\t\t/* [R,W]x div 0 -> 0 */\n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JNE | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 2, 0),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn chk_and_mod[] = {\n\t\t\t\t/* [R,W]x mod 0 -> [R,W]x */\n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JEQ | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 1 + (is64 ? 0 : 1), 0),\n\t\t\t\t*insn,\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\tBPF_MOV32_REG(insn->dst_reg, insn->dst_reg),\n\t\t\t};\n\n\t\t\tpatchlet = isdiv ? chk_and_div : chk_and_mod;\n\t\t\tcnt = isdiv ? ARRAY_SIZE(chk_and_div) :\n\t\t\t\t      ARRAY_SIZE(chk_and_mod) - (is64 ? 2 : 0);\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Implement LD_ABS and LD_IND with a rewrite, if supported by the program type. */\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Rewrite pointer arithmetic to mitigate speculation attacks. */\n\t\tif (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {\n\t\t\tconst u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;\n\t\t\tconst u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;\n\t\t\tstruct bpf_insn *patch = &insn_buf[0];\n\t\t\tbool issrc, isneg, isimm;\n\t\t\tu32 off_reg;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!aux->alu_state ||\n\t\t\t    aux->alu_state == BPF_ALU_NON_POINTER)\n\t\t\t\tcontinue;\n\n\t\t\tisneg = aux->alu_state & BPF_ALU_NEG_VALUE;\n\t\t\tissrc = (aux->alu_state & BPF_ALU_SANITIZE) ==\n\t\t\t\tBPF_ALU_SANITIZE_SRC;\n\t\t\tisimm = aux->alu_state & BPF_ALU_IMMEDIATE;\n\n\t\t\toff_reg = issrc ? insn->src_reg : insn->dst_reg;\n\t\t\tif (isimm) {\n\t\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);\n\t\t\t} else {\n\t\t\t\tif (isneg)\n\t\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX, off_reg);\n\t\t\t}\n\t\t\tif (!issrc)\n\t\t\t\t*patch++ = BPF_MOV64_REG(insn->dst_reg, insn->src_reg);\n\t\t\tinsn->src_reg = BPF_REG_AX;\n\t\t\tif (isneg)\n\t\t\t\tinsn->code = insn->code == code_add ?\n\t\t\t\t\t     code_sub : code_add;\n\t\t\t*patch++ = *insn;\n\t\t\tif (issrc && isneg && !isimm)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\tcnt = patch - insn_buf;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_KFUNC_CALL) {\n\t\t\tret = fixup_kfunc_call(env, insn);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_override_return)\n\t\t\tprog->kprobe_override = 1;\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tif (!allow_tail_call_in_subprogs(env))\n\t\t\t\tprog->aux->stack_depth = MAX_BPF_STACK;\n\t\t\tprog->aux->max_pkt_offset = MAX_PACKET_OFF;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (env->bpf_capable && !expect_blinding &&\n\t\t\t    prog->jit_requested &&\n\t\t\t    !bpf_map_key_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_unpriv(aux)) {\n\t\t\t\tstruct bpf_jit_poke_descriptor desc = {\n\t\t\t\t\t.reason = BPF_POKE_REASON_TAIL_CALL,\n\t\t\t\t\t.tail_call.map = BPF_MAP_PTR(aux->map_ptr_state),\n\t\t\t\t\t.tail_call.key = bpf_map_key_immediate(aux),\n\t\t\t\t\t.insn_idx = i + delta,\n\t\t\t\t};\n\n\t\t\t\tret = bpf_jit_add_poke_descriptor(prog, &desc);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tverbose(env, \"adding tail call poke descriptor failed\\n\");\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\n\t\t\t\tinsn->imm = ret + 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!bpf_map_ptr_unpriv(aux))\n\t\t\t\tcontinue;\n\n\t\t\t/* instead of changing every JIT dealing with tail_call\n\t\t\t * emit two extra insns:\n\t\t\t * if (index >= max_entries) goto out;\n\t\t\t * index &= array->index_mask;\n\t\t\t * to avoid out-of-bounds cpu speculation\n\t\t\t */\n\t\t\tif (bpf_map_ptr_poisoned(aux)) {\n\t\t\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tinsn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,\n\t\t\t\t\t\t  map_ptr->max_entries, 2);\n\t\t\tinsn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,\n\t\t\t\t\t\t    container_of(map_ptr,\n\t\t\t\t\t\t\t\t struct bpf_array,\n\t\t\t\t\t\t\t\t map)->index_mask);\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * and other inlining handlers are currently limited to 64 bit\n\t\t * only.\n\t\t */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    (insn->imm == BPF_FUNC_map_lookup_elem ||\n\t\t     insn->imm == BPF_FUNC_map_update_elem ||\n\t\t     insn->imm == BPF_FUNC_map_delete_elem ||\n\t\t     insn->imm == BPF_FUNC_map_push_elem   ||\n\t\t     insn->imm == BPF_FUNC_map_pop_elem    ||\n\t\t     insn->imm == BPF_FUNC_map_peek_elem   ||\n\t\t     insn->imm == BPF_FUNC_redirect_map)) {\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (bpf_map_ptr_poisoned(aux))\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tops = map_ptr->ops;\n\t\t\tif (insn->imm == BPF_FUNC_map_lookup_elem &&\n\t\t\t    ops->map_gen_lookup) {\n\t\t\t\tcnt = ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\t\tif (cnt == -EOPNOTSUPP)\n\t\t\t\t\tgoto patch_map_ops_generic;\n\t\t\t\tif (cnt <= 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta,\n\t\t\t\t\t\t\t       insn_buf, cnt);\n\t\t\t\tif (!new_prog)\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tdelta    += cnt - 1;\n\t\t\t\tenv->prog = prog = new_prog;\n\t\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_delete_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_update_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_push_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_pop_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_peek_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_redirect,\n\t\t\t\t     (int (*)(struct bpf_map *map, u32 ifindex, u64 flags))NULL));\n\npatch_map_ops_generic:\n\t\t\tswitch (insn->imm) {\n\t\t\tcase BPF_FUNC_map_lookup_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_lookup_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_update_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_update_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_delete_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_delete_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_push_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_push_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_pop_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_pop_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_peek_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_peek_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_redirect_map:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_redirect) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgoto patch_call_imm;\n\t\t}\n\n\t\t/* Implement bpf_jiffies64 inline. */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    insn->imm == BPF_FUNC_jiffies64) {\n\t\t\tstruct bpf_insn ld_jiffies_addr[2] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_0,\n\t\t\t\t\t     (unsigned long)&jiffies),\n\t\t\t};\n\n\t\t\tinsn_buf[0] = ld_jiffies_addr[0];\n\t\t\tinsn_buf[1] = ld_jiffies_addr[1];\n\t\t\tinsn_buf[2] = BPF_LDX_MEM(BPF_DW, BPF_REG_0,\n\t\t\t\t\t\t  BPF_REG_0, 0);\n\t\t\tcnt = 3;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf,\n\t\t\t\t\t\t       cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm, env->prog);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\t/* Since poke tab is now finalized, publish aux to tracker. */\n\tfor (i = 0; i < prog->aux->size_poke_tab; i++) {\n\t\tmap_ptr = prog->aux->poke_tab[i].tail_call.map;\n\t\tif (!map_ptr->ops->map_poke_track ||\n\t\t    !map_ptr->ops->map_poke_untrack ||\n\t\t    !map_ptr->ops->map_poke_run) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tret = map_ptr->ops->map_poke_track(map_ptr, prog->aux);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"tracking tail call prog failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tsort_kfunc_descs_by_imm(env->prog);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -80,7 +80,7 @@\n \t\t\tconst u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;\n \t\t\tconst u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;\n \t\t\tstruct bpf_insn *patch = &insn_buf[0];\n-\t\t\tbool issrc, isneg;\n+\t\t\tbool issrc, isneg, isimm;\n \t\t\tu32 off_reg;\n \n \t\t\taux = &env->insn_aux_data[i + delta];\n@@ -91,16 +91,21 @@\n \t\t\tisneg = aux->alu_state & BPF_ALU_NEG_VALUE;\n \t\t\tissrc = (aux->alu_state & BPF_ALU_SANITIZE) ==\n \t\t\t\tBPF_ALU_SANITIZE_SRC;\n+\t\t\tisimm = aux->alu_state & BPF_ALU_IMMEDIATE;\n \n \t\t\toff_reg = issrc ? insn->src_reg : insn->dst_reg;\n-\t\t\tif (isneg)\n-\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n-\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);\n-\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n-\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n-\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n-\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n-\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX, off_reg);\n+\t\t\tif (isimm) {\n+\t\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);\n+\t\t\t} else {\n+\t\t\t\tif (isneg)\n+\t\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n+\t\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);\n+\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n+\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n+\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n+\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n+\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX, off_reg);\n+\t\t\t}\n \t\t\tif (!issrc)\n \t\t\t\t*patch++ = BPF_MOV64_REG(insn->dst_reg, insn->src_reg);\n \t\t\tinsn->src_reg = BPF_REG_AX;\n@@ -108,7 +113,7 @@\n \t\t\t\tinsn->code = insn->code == code_add ?\n \t\t\t\t\t     code_sub : code_add;\n \t\t\t*patch++ = *insn;\n-\t\t\tif (issrc && isneg)\n+\t\t\tif (issrc && isneg && !isimm)\n \t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n \t\t\tcnt = patch - insn_buf;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tbool issrc, isneg;",
                "\t\t\tif (isneg)",
                "\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);",
                "\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);",
                "\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);",
                "\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);",
                "\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);",
                "\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);",
                "\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX, off_reg);",
                "\t\t\tif (issrc && isneg)"
            ],
            "added_lines": [
                "\t\t\tbool issrc, isneg, isimm;",
                "\t\t\tisimm = aux->alu_state & BPF_ALU_IMMEDIATE;",
                "\t\t\tif (isimm) {",
                "\t\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);",
                "\t\t\t} else {",
                "\t\t\t\tif (isneg)",
                "\t\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);",
                "\t\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);",
                "\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);",
                "\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);",
                "\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);",
                "\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);",
                "\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX, off_reg);",
                "\t\t\t}",
                "\t\t\tif (issrc && isneg && !isimm)"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-7914",
        "func_name": "android/btif_dm_ssp_cfm_req_evt",
        "description": "btif/src/btif_dm.c in Android before 5.1 does not properly enforce the temporary nature of a Bluetooth pairing, which allows user-assisted remote attackers to bypass intended access restrictions via crafted Bluetooth packets after the tapping of a crafted NFC tag.",
        "git_url": "https://android.googlesource.com/platform/external/bluetooth/bluedroid/+/0360aa7c418152a3e5e335a065ac3629cbb09559",
        "commit_title": "Change pairing_cb to assume temporary pairing by default",
        "commit_text": " When pairing takes place, the pairing_cb.is_temp flag indicates whether a pairing is temporary or permanent. Link keys are not stored for temporary pairings. Since this is a \"positive\" flag, resetting the pairing_cb control block (ex. memset to 0), it will assume persistent pairing by default. Under certain circumstances, this can lead to a link key being stored for temporarily secured connection.  This patch reverses the flag to be a \"negative\" flag. Renamed to \"persistent_bond\", the default 0 meaning is now used to indicate a temporary bond. If the lag is not properly set now, it will default to a temporary bond and will not save the link key erronously.  This fix is for CVE-2014-7914  Bug: 18345373 ",
        "func_before": "static void btif_dm_ssp_cfm_req_evt(tBTA_DM_SP_CFM_REQ *p_ssp_cfm_req)\n{\n    bt_bdaddr_t bd_addr;\n    bt_bdname_t bd_name;\n    UINT32 cod;\n    BOOLEAN is_incoming = !(pairing_cb.state == BT_BOND_STATE_BONDING);\n\n    BTIF_TRACE_DEBUG(\"%s\", __FUNCTION__);\n\n    /* Remote properties update */\n    btif_update_remote_properties(p_ssp_cfm_req->bd_addr, p_ssp_cfm_req->bd_name,\n                                  p_ssp_cfm_req->dev_class, BT_DEVICE_TYPE_BREDR);\n\n    bdcpy(bd_addr.address, p_ssp_cfm_req->bd_addr);\n    memcpy(bd_name.name, p_ssp_cfm_req->bd_name, BD_NAME_LEN);\n\n    /* Set the pairing_cb based on the local & remote authentication requirements */\n    bond_state_changed(BT_STATUS_SUCCESS, &bd_addr, BT_BOND_STATE_BONDING);\n\n    /* if just_works and bonding bit is not set treat this as temporary */\n    if (p_ssp_cfm_req->just_works && !(p_ssp_cfm_req->loc_auth_req & BTM_AUTH_BONDS) &&\n        !(p_ssp_cfm_req->rmt_auth_req & BTM_AUTH_BONDS) &&\n        !(check_cod((bt_bdaddr_t*)&p_ssp_cfm_req->bd_addr, COD_HID_POINTING)))\n        pairing_cb.is_temp = TRUE;\n    else\n        pairing_cb.is_temp = FALSE;\n\n    pairing_cb.is_ssp = TRUE;\n\n    /* If JustWorks auto-accept */\n    if (p_ssp_cfm_req->just_works)\n    {\n        /* Pairing consent for JustWorks needed if:\n         * 1. Incoming pairing is detected AND\n         * 2. local IO capabilities are DisplayYesNo AND\n         * 3. remote IO capabiltiies are DisplayOnly or NoInputNoOutput;\n         */\n        if ((is_incoming) && ((p_ssp_cfm_req->loc_io_caps == 0x01) &&\n                (p_ssp_cfm_req->rmt_io_caps == 0x00 || p_ssp_cfm_req->rmt_io_caps == 0x03)))\n        {\n            BTIF_TRACE_EVENT(\"%s: User consent needed for incoming pairing request. loc_io_caps: %d, rmt_io_caps: %d\",\n                __FUNCTION__, p_ssp_cfm_req->loc_io_caps, p_ssp_cfm_req->rmt_io_caps);\n        }\n        else\n        {\n            BTIF_TRACE_EVENT(\"%s: Auto-accept JustWorks pairing\", __FUNCTION__);\n            btif_dm_ssp_reply(&bd_addr, BT_SSP_VARIANT_CONSENT, TRUE, 0);\n            return;\n        }\n    }\n\n    cod = devclass2uint(p_ssp_cfm_req->dev_class);\n\n    if ( cod == 0) {\n        ALOGD(\"cod is 0, set as unclassified\");\n        cod = COD_UNCLASSIFIED;\n    }\n\n    pairing_cb.sdp_attempts = 0;\n    HAL_CBACK(bt_hal_cbacks, ssp_request_cb, &bd_addr, &bd_name, cod,\n                     (p_ssp_cfm_req->just_works ? BT_SSP_VARIANT_CONSENT : BT_SSP_VARIANT_PASSKEY_CONFIRMATION),\n                     p_ssp_cfm_req->num_val);\n}",
        "func": "static void btif_dm_ssp_cfm_req_evt(tBTA_DM_SP_CFM_REQ *p_ssp_cfm_req)\n{\n    bt_bdaddr_t bd_addr;\n    bt_bdname_t bd_name;\n    UINT32 cod;\n    BOOLEAN is_incoming = !(pairing_cb.state == BT_BOND_STATE_BONDING);\n\n    BTIF_TRACE_DEBUG(\"%s\", __FUNCTION__);\n\n    /* Remote properties update */\n    btif_update_remote_properties(p_ssp_cfm_req->bd_addr, p_ssp_cfm_req->bd_name,\n                                  p_ssp_cfm_req->dev_class, BT_DEVICE_TYPE_BREDR);\n\n    bdcpy(bd_addr.address, p_ssp_cfm_req->bd_addr);\n    memcpy(bd_name.name, p_ssp_cfm_req->bd_name, BD_NAME_LEN);\n\n    /* Set the pairing_cb based on the local & remote authentication requirements */\n    bond_state_changed(BT_STATUS_SUCCESS, &bd_addr, BT_BOND_STATE_BONDING);\n\n    /* if just_works and bonding bit is not set treat this as temporary */\n    if (p_ssp_cfm_req->just_works && !(p_ssp_cfm_req->loc_auth_req & BTM_AUTH_BONDS) &&\n        !(p_ssp_cfm_req->rmt_auth_req & BTM_AUTH_BONDS) &&\n        !(check_cod((bt_bdaddr_t*)&p_ssp_cfm_req->bd_addr, COD_HID_POINTING)))\n        pairing_cb.bond_type = BOND_TYPE_TEMPORARY;\n    else\n        pairing_cb.bond_type = BOND_TYPE_PERSISTENT;\n\n    pairing_cb.is_ssp = TRUE;\n\n    /* If JustWorks auto-accept */\n    if (p_ssp_cfm_req->just_works)\n    {\n        /* Pairing consent for JustWorks needed if:\n         * 1. Incoming pairing is detected AND\n         * 2. local IO capabilities are DisplayYesNo AND\n         * 3. remote IO capabiltiies are DisplayOnly or NoInputNoOutput;\n         */\n        if ((is_incoming) && ((p_ssp_cfm_req->loc_io_caps == 0x01) &&\n                (p_ssp_cfm_req->rmt_io_caps == 0x00 || p_ssp_cfm_req->rmt_io_caps == 0x03)))\n        {\n            BTIF_TRACE_EVENT(\"%s: User consent needed for incoming pairing request. loc_io_caps: %d, rmt_io_caps: %d\",\n                __FUNCTION__, p_ssp_cfm_req->loc_io_caps, p_ssp_cfm_req->rmt_io_caps);\n        }\n        else\n        {\n            BTIF_TRACE_EVENT(\"%s: Auto-accept JustWorks pairing\", __FUNCTION__);\n            btif_dm_ssp_reply(&bd_addr, BT_SSP_VARIANT_CONSENT, TRUE, 0);\n            return;\n        }\n    }\n\n    cod = devclass2uint(p_ssp_cfm_req->dev_class);\n\n    if ( cod == 0) {\n        ALOGD(\"cod is 0, set as unclassified\");\n        cod = COD_UNCLASSIFIED;\n    }\n\n    pairing_cb.sdp_attempts = 0;\n    HAL_CBACK(bt_hal_cbacks, ssp_request_cb, &bd_addr, &bd_name, cod,\n                     (p_ssp_cfm_req->just_works ? BT_SSP_VARIANT_CONSENT : BT_SSP_VARIANT_PASSKEY_CONFIRMATION),\n                     p_ssp_cfm_req->num_val);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,9 +21,9 @@\n     if (p_ssp_cfm_req->just_works && !(p_ssp_cfm_req->loc_auth_req & BTM_AUTH_BONDS) &&\n         !(p_ssp_cfm_req->rmt_auth_req & BTM_AUTH_BONDS) &&\n         !(check_cod((bt_bdaddr_t*)&p_ssp_cfm_req->bd_addr, COD_HID_POINTING)))\n-        pairing_cb.is_temp = TRUE;\n+        pairing_cb.bond_type = BOND_TYPE_TEMPORARY;\n     else\n-        pairing_cb.is_temp = FALSE;\n+        pairing_cb.bond_type = BOND_TYPE_PERSISTENT;\n \n     pairing_cb.is_ssp = TRUE;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "        pairing_cb.is_temp = TRUE;",
                "        pairing_cb.is_temp = FALSE;"
            ],
            "added_lines": [
                "        pairing_cb.bond_type = BOND_TYPE_TEMPORARY;",
                "        pairing_cb.bond_type = BOND_TYPE_PERSISTENT;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-7914",
        "func_name": "android/btif_dm_ble_sec_req_evt",
        "description": "btif/src/btif_dm.c in Android before 5.1 does not properly enforce the temporary nature of a Bluetooth pairing, which allows user-assisted remote attackers to bypass intended access restrictions via crafted Bluetooth packets after the tapping of a crafted NFC tag.",
        "git_url": "https://android.googlesource.com/platform/external/bluetooth/bluedroid/+/0360aa7c418152a3e5e335a065ac3629cbb09559",
        "commit_title": "Change pairing_cb to assume temporary pairing by default",
        "commit_text": " When pairing takes place, the pairing_cb.is_temp flag indicates whether a pairing is temporary or permanent. Link keys are not stored for temporary pairings. Since this is a \"positive\" flag, resetting the pairing_cb control block (ex. memset to 0), it will assume persistent pairing by default. Under certain circumstances, this can lead to a link key being stored for temporarily secured connection.  This patch reverses the flag to be a \"negative\" flag. Renamed to \"persistent_bond\", the default 0 meaning is now used to indicate a temporary bond. If the lag is not properly set now, it will default to a temporary bond and will not save the link key erronously.  This fix is for CVE-2014-7914  Bug: 18345373 ",
        "func_before": "void btif_dm_ble_sec_req_evt(tBTA_DM_BLE_SEC_REQ *p_ble_req)\n{\n    bt_bdaddr_t bd_addr;\n    bt_bdname_t bd_name;\n    UINT32 cod;\n    BTIF_TRACE_DEBUG(\"%s\", __FUNCTION__);\n\n    if (pairing_cb.state == BT_BOND_STATE_BONDING)\n    {\n        BTIF_TRACE_DEBUG(\"%s Discard security request\", __FUNCTION__);\n        return;\n    }\n\n    /* Remote name update */\n    btif_update_remote_properties(p_ble_req->bd_addr,p_ble_req->bd_name,NULL,BT_DEVICE_TYPE_BLE);\n\n    bdcpy(bd_addr.address, p_ble_req->bd_addr);\n    memcpy(bd_name.name, p_ble_req->bd_name, BD_NAME_LEN);\n\n    bond_state_changed(BT_STATUS_SUCCESS, &bd_addr, BT_BOND_STATE_BONDING);\n\n    pairing_cb.is_temp = FALSE;\n    pairing_cb.is_le_only = TRUE;\n    pairing_cb.is_ssp = TRUE;\n\n    cod = COD_UNCLASSIFIED;\n\n    HAL_CBACK(bt_hal_cbacks, ssp_request_cb, &bd_addr, &bd_name, cod,\n              BT_SSP_VARIANT_CONSENT, 0);\n}",
        "func": "void btif_dm_ble_sec_req_evt(tBTA_DM_BLE_SEC_REQ *p_ble_req)\n{\n    bt_bdaddr_t bd_addr;\n    bt_bdname_t bd_name;\n    UINT32 cod;\n    BTIF_TRACE_DEBUG(\"%s\", __FUNCTION__);\n\n    if (pairing_cb.state == BT_BOND_STATE_BONDING)\n    {\n        BTIF_TRACE_DEBUG(\"%s Discard security request\", __FUNCTION__);\n        return;\n    }\n\n    /* Remote name update */\n    btif_update_remote_properties(p_ble_req->bd_addr,p_ble_req->bd_name,NULL,BT_DEVICE_TYPE_BLE);\n\n    bdcpy(bd_addr.address, p_ble_req->bd_addr);\n    memcpy(bd_name.name, p_ble_req->bd_name, BD_NAME_LEN);\n\n    bond_state_changed(BT_STATUS_SUCCESS, &bd_addr, BT_BOND_STATE_BONDING);\n\n    pairing_cb.bond_type = BOND_TYPE_PERSISTENT;\n    pairing_cb.is_le_only = TRUE;\n    pairing_cb.is_ssp = TRUE;\n\n    cod = COD_UNCLASSIFIED;\n\n    HAL_CBACK(bt_hal_cbacks, ssp_request_cb, &bd_addr, &bd_name, cod,\n              BT_SSP_VARIANT_CONSENT, 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,7 +19,7 @@\n \n     bond_state_changed(BT_STATUS_SUCCESS, &bd_addr, BT_BOND_STATE_BONDING);\n \n-    pairing_cb.is_temp = FALSE;\n+    pairing_cb.bond_type = BOND_TYPE_PERSISTENT;\n     pairing_cb.is_le_only = TRUE;\n     pairing_cb.is_ssp = TRUE;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    pairing_cb.is_temp = FALSE;"
            ],
            "added_lines": [
                "    pairing_cb.bond_type = BOND_TYPE_PERSISTENT;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-7914",
        "func_name": "android/bond_state_changed",
        "description": "btif/src/btif_dm.c in Android before 5.1 does not properly enforce the temporary nature of a Bluetooth pairing, which allows user-assisted remote attackers to bypass intended access restrictions via crafted Bluetooth packets after the tapping of a crafted NFC tag.",
        "git_url": "https://android.googlesource.com/platform/external/bluetooth/bluedroid/+/0360aa7c418152a3e5e335a065ac3629cbb09559",
        "commit_title": "Change pairing_cb to assume temporary pairing by default",
        "commit_text": " When pairing takes place, the pairing_cb.is_temp flag indicates whether a pairing is temporary or permanent. Link keys are not stored for temporary pairings. Since this is a \"positive\" flag, resetting the pairing_cb control block (ex. memset to 0), it will assume persistent pairing by default. Under certain circumstances, this can lead to a link key being stored for temporarily secured connection.  This patch reverses the flag to be a \"negative\" flag. Renamed to \"persistent_bond\", the default 0 meaning is now used to indicate a temporary bond. If the lag is not properly set now, it will default to a temporary bond and will not save the link key erronously.  This fix is for CVE-2014-7914  Bug: 18345373 ",
        "func_before": "static void bond_state_changed(bt_status_t status, bt_bdaddr_t *bd_addr, bt_bond_state_t state)\n{\n    /* Send bonding state only once - based on outgoing/incoming we may receive duplicates */\n    if ( (pairing_cb.state == state) && (state == BT_BOND_STATE_BONDING) )\n        return;\n\n    if (pairing_cb.is_temp)\n    {\n       state = BT_BOND_STATE_NONE;\n    }\n    BTIF_TRACE_DEBUG(\"%s: state=%d prev_state=%d\", __FUNCTION__, state, pairing_cb.state);\n\n    HAL_CBACK(bt_hal_cbacks, bond_state_changed_cb, status, bd_addr, state);\n\n    if (state == BT_BOND_STATE_BONDING)\n    {\n        pairing_cb.state = state;\n        bdcpy(pairing_cb.bd_addr, bd_addr->address);\n    }\n    else\n    {\n        memset(&pairing_cb, 0, sizeof(pairing_cb));\n    }\n\n}",
        "func": "static void bond_state_changed(bt_status_t status, bt_bdaddr_t *bd_addr, bt_bond_state_t state)\n{\n    /* Send bonding state only once - based on outgoing/incoming we may receive duplicates */\n    if ( (pairing_cb.state == state) && (state == BT_BOND_STATE_BONDING) )\n        return;\n\n    if (pairing_cb.bond_type == BOND_TYPE_TEMPORARY)\n    {\n       state = BT_BOND_STATE_NONE;\n    }\n    BTIF_TRACE_DEBUG(\"%s: state=%d prev_state=%d\", __FUNCTION__, state, pairing_cb.state);\n\n    HAL_CBACK(bt_hal_cbacks, bond_state_changed_cb, status, bd_addr, state);\n\n    if (state == BT_BOND_STATE_BONDING)\n    {\n        pairing_cb.state = state;\n        bdcpy(pairing_cb.bd_addr, bd_addr->address);\n    }\n    else\n    {\n        memset(&pairing_cb, 0, sizeof(pairing_cb));\n    }\n\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n     if ( (pairing_cb.state == state) && (state == BT_BOND_STATE_BONDING) )\n         return;\n \n-    if (pairing_cb.is_temp)\n+    if (pairing_cb.bond_type == BOND_TYPE_TEMPORARY)\n     {\n        state = BT_BOND_STATE_NONE;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "    if (pairing_cb.is_temp)"
            ],
            "added_lines": [
                "    if (pairing_cb.bond_type == BOND_TYPE_TEMPORARY)"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-7914",
        "func_name": "android/btif_dm_auth_cmpl_evt",
        "description": "btif/src/btif_dm.c in Android before 5.1 does not properly enforce the temporary nature of a Bluetooth pairing, which allows user-assisted remote attackers to bypass intended access restrictions via crafted Bluetooth packets after the tapping of a crafted NFC tag.",
        "git_url": "https://android.googlesource.com/platform/external/bluetooth/bluedroid/+/0360aa7c418152a3e5e335a065ac3629cbb09559",
        "commit_title": "Change pairing_cb to assume temporary pairing by default",
        "commit_text": " When pairing takes place, the pairing_cb.is_temp flag indicates whether a pairing is temporary or permanent. Link keys are not stored for temporary pairings. Since this is a \"positive\" flag, resetting the pairing_cb control block (ex. memset to 0), it will assume persistent pairing by default. Under certain circumstances, this can lead to a link key being stored for temporarily secured connection.  This patch reverses the flag to be a \"negative\" flag. Renamed to \"persistent_bond\", the default 0 meaning is now used to indicate a temporary bond. If the lag is not properly set now, it will default to a temporary bond and will not save the link key erronously.  This fix is for CVE-2014-7914  Bug: 18345373 ",
        "func_before": "static void btif_dm_auth_cmpl_evt (tBTA_DM_AUTH_CMPL *p_auth_cmpl)\n{\n    /* Save link key, if not temporary */\n    bt_bdaddr_t bd_addr;\n    bt_status_t status = BT_STATUS_FAIL;\n    bt_bond_state_t state = BT_BOND_STATE_NONE;\n    BOOLEAN skip_sdp = FALSE;\n\n    bdcpy(bd_addr.address, p_auth_cmpl->bd_addr);\n    if ( (p_auth_cmpl->success == TRUE) && (p_auth_cmpl->key_present) )\n    {\n        if ((p_auth_cmpl->key_type < HCI_LKEY_TYPE_DEBUG_COMB)  || (p_auth_cmpl->key_type == HCI_LKEY_TYPE_AUTH_COMB) ||\n            (p_auth_cmpl->key_type == HCI_LKEY_TYPE_CHANGED_COMB) || (!pairing_cb.is_temp))\n        {\n            bt_status_t ret;\n            BTIF_TRACE_DEBUG(\"%s: Storing link key. key_type=0x%x, is_temp=%d\",\n                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.is_temp);\n            ret = btif_storage_add_bonded_device(&bd_addr,\n                                p_auth_cmpl->key, p_auth_cmpl->key_type,\n                                pairing_cb.pin_code_len);\n            ASSERTC(ret == BT_STATUS_SUCCESS, \"storing link key failed\", ret);\n        }\n        else\n        {\n            BTIF_TRACE_DEBUG(\"%s: Temporary key. Not storing. key_type=0x%x, is_temp=%d\",\n                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.is_temp);\n            if(pairing_cb.is_temp)\n            {\n                BTIF_TRACE_DEBUG(\"%s: sending BT_BOND_STATE_NONE for Temp pairing\",\n                        __FUNCTION__);\n                bond_state_changed(BT_STATUS_SUCCESS, &bd_addr, BT_BOND_STATE_NONE);\n                return;\n            }\n        }\n    }\n\n    // Skip SDP for certain  HID Devices\n    if (p_auth_cmpl->success)\n    {\n        pairing_cb.timeout_retries = 0;\n        status = BT_STATUS_SUCCESS;\n        state = BT_BOND_STATE_BONDED;\n        bdcpy(bd_addr.address, p_auth_cmpl->bd_addr);\n\n        if (check_sdp_bl(&bd_addr) && check_cod_hid(&bd_addr, COD_HID_MAJOR))\n        {\n            ALOGW(\"%s:skip SDP\",\n                              __FUNCTION__);\n            skip_sdp = TRUE;\n        }\n        if(!pairing_cb.is_local_initiated && skip_sdp)\n        {\n            bond_state_changed(status, &bd_addr, state);\n\n            ALOGW(\"%s: Incoming HID Connection\",__FUNCTION__);\n            bt_property_t prop;\n            bt_bdaddr_t bd_addr;\n            bt_uuid_t  uuid;\n            char uuid_str[128] = UUID_HUMAN_INTERFACE_DEVICE;\n\n            string_to_uuid(uuid_str, &uuid);\n\n            prop.type = BT_PROPERTY_UUIDS;\n            prop.val = uuid.uu;\n            prop.len = MAX_UUID_SIZE;\n\n            /* Send the event to the BTIF */\n            HAL_CBACK(bt_hal_cbacks, remote_device_properties_cb,\n                             BT_STATUS_SUCCESS, &bd_addr, 1, &prop);\n        }\n        else\n        {\n            /* Trigger SDP on the device */\n            pairing_cb.sdp_attempts = 1;;\n\n            if(btif_dm_inquiry_in_progress)\n                btif_dm_cancel_discovery();\n\n            btif_dm_get_remote_services(&bd_addr);\n            }\n            /* Do not call bond_state_changed_cb yet. Wait till fetch remote service is complete */\n    }\n    else\n    {\n         /*Map the HCI fail reason  to  bt status  */\n        switch(p_auth_cmpl->fail_reason)\n        {\n            case HCI_ERR_PAGE_TIMEOUT:\n                if (blacklistPairingRetries(bd_addr.address) && pairing_cb.timeout_retries)\n                {\n                    BTIF_TRACE_WARNING(\"%s() - Pairing timeout; retrying (%d) ...\", __FUNCTION__, pairing_cb.timeout_retries);\n                    --pairing_cb.timeout_retries;\n                    btif_dm_cb_create_bond (&bd_addr, BTA_TRANSPORT_UNKNOWN);\n                    return;\n                }\n                /* Fall-through */\n            case HCI_ERR_CONNECTION_TOUT:\n                status =  BT_STATUS_RMT_DEV_DOWN;\n                break;\n\n            case HCI_ERR_PAIRING_NOT_ALLOWED:\n                status = BT_STATUS_AUTH_REJECTED;\n                break;\n\n            case HCI_ERR_LMP_RESPONSE_TIMEOUT:\n                status =  BT_STATUS_AUTH_FAILURE;\n                break;\n\n            /* map the auth failure codes, so we can retry pairing if necessary */\n            case HCI_ERR_AUTH_FAILURE:\n            case HCI_ERR_KEY_MISSING:\n                btif_storage_remove_bonded_device(&bd_addr);\n            case HCI_ERR_HOST_REJECT_SECURITY:\n            case HCI_ERR_ENCRY_MODE_NOT_ACCEPTABLE:\n            case HCI_ERR_UNIT_KEY_USED:\n            case HCI_ERR_PAIRING_WITH_UNIT_KEY_NOT_SUPPORTED:\n            case HCI_ERR_INSUFFCIENT_SECURITY:\n            case HCI_ERR_PEER_USER:\n            case HCI_ERR_UNSPECIFIED:\n                BTIF_TRACE_DEBUG(\" %s() Authentication fail reason %d\",\n                    __FUNCTION__, p_auth_cmpl->fail_reason);\n                if (pairing_cb.autopair_attempts  == 1)\n                {\n                    BTIF_TRACE_DEBUG(\"%s(): Adding device to blacklist \", __FUNCTION__);\n\n                    /* Add the device to dynamic black list only if this device belongs to Audio/pointing dev class  */\n                    if (check_cod(&bd_addr, COD_AV_HEADSETS) ||\n                        check_cod(&bd_addr, COD_AV_HANDSFREE) ||\n                        check_cod(&bd_addr, COD_AV_HEADPHONES) ||\n                        check_cod(&bd_addr, COD_AV_PORTABLE_AUDIO) ||\n                        check_cod(&bd_addr, COD_AV_HIFI_AUDIO) ||\n                        check_cod(&bd_addr, COD_HID_POINTING))\n                    {\n                        btif_storage_add_device_to_autopair_blacklist (&bd_addr);\n                    }\n                    pairing_cb.autopair_attempts++;\n\n                    /* Create the Bond once again */\n                    BTIF_TRACE_DEBUG(\"%s() auto pair failed. Reinitiate Bond\", __FUNCTION__);\n                    btif_dm_cb_create_bond (&bd_addr, BTA_TRANSPORT_UNKNOWN);\n                    return;\n                }\n                else\n                {\n                    /* if autopair attempts are more than 1, or not attempted */\n                    status =  BT_STATUS_AUTH_FAILURE;\n                }\n                break;\n\n            default:\n                status =  BT_STATUS_FAIL;\n        }\n        /* Special Handling for HID Devices */\n        if (check_cod(&bd_addr, COD_HID_POINTING)) {\n            /* Remove Device as bonded in nvram as authentication failed */\n            BTIF_TRACE_DEBUG(\"%s(): removing hid pointing device from nvram\", __FUNCTION__);\n            btif_storage_remove_bonded_device(&bd_addr);\n        }\n        bond_state_changed(status, &bd_addr, state);\n    }\n}",
        "func": "static void btif_dm_auth_cmpl_evt (tBTA_DM_AUTH_CMPL *p_auth_cmpl)\n{\n    /* Save link key, if not temporary */\n    bt_bdaddr_t bd_addr;\n    bt_status_t status = BT_STATUS_FAIL;\n    bt_bond_state_t state = BT_BOND_STATE_NONE;\n    BOOLEAN skip_sdp = FALSE;\n\n    bdcpy(bd_addr.address, p_auth_cmpl->bd_addr);\n    if ( (p_auth_cmpl->success == TRUE) && (p_auth_cmpl->key_present) )\n    {\n        if ((p_auth_cmpl->key_type < HCI_LKEY_TYPE_DEBUG_COMB)  || (p_auth_cmpl->key_type == HCI_LKEY_TYPE_AUTH_COMB) ||\n            (p_auth_cmpl->key_type == HCI_LKEY_TYPE_CHANGED_COMB) || pairing_cb.bond_type == BOND_TYPE_PERSISTENT)\n        {\n            bt_status_t ret;\n            BTIF_TRACE_DEBUG(\"%s: Storing link key. key_type=0x%x, bond_type=%d\",\n                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.bond_type);\n            ret = btif_storage_add_bonded_device(&bd_addr,\n                                p_auth_cmpl->key, p_auth_cmpl->key_type,\n                                pairing_cb.pin_code_len);\n            ASSERTC(ret == BT_STATUS_SUCCESS, \"storing link key failed\", ret);\n        }\n        else\n        {\n            BTIF_TRACE_DEBUG(\"%s: Temporary key. Not storing. key_type=0x%x, bond_type=%d\",\n                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.bond_type);\n            if(pairing_cb.bond_type == BOND_TYPE_TEMPORARY)\n            {\n                BTIF_TRACE_DEBUG(\"%s: sending BT_BOND_STATE_NONE for Temp pairing\",\n                        __FUNCTION__);\n                bond_state_changed(BT_STATUS_SUCCESS, &bd_addr, BT_BOND_STATE_NONE);\n                return;\n            }\n        }\n    }\n\n    // Skip SDP for certain  HID Devices\n    if (p_auth_cmpl->success)\n    {\n        pairing_cb.timeout_retries = 0;\n        status = BT_STATUS_SUCCESS;\n        state = BT_BOND_STATE_BONDED;\n        bdcpy(bd_addr.address, p_auth_cmpl->bd_addr);\n\n        if (check_sdp_bl(&bd_addr) && check_cod_hid(&bd_addr, COD_HID_MAJOR))\n        {\n            ALOGW(\"%s:skip SDP\",\n                              __FUNCTION__);\n            skip_sdp = TRUE;\n        }\n        if(!pairing_cb.is_local_initiated && skip_sdp)\n        {\n            bond_state_changed(status, &bd_addr, state);\n\n            ALOGW(\"%s: Incoming HID Connection\",__FUNCTION__);\n            bt_property_t prop;\n            bt_bdaddr_t bd_addr;\n            bt_uuid_t  uuid;\n            char uuid_str[128] = UUID_HUMAN_INTERFACE_DEVICE;\n\n            string_to_uuid(uuid_str, &uuid);\n\n            prop.type = BT_PROPERTY_UUIDS;\n            prop.val = uuid.uu;\n            prop.len = MAX_UUID_SIZE;\n\n            /* Send the event to the BTIF */\n            HAL_CBACK(bt_hal_cbacks, remote_device_properties_cb,\n                             BT_STATUS_SUCCESS, &bd_addr, 1, &prop);\n        }\n        else\n        {\n            /* Trigger SDP on the device */\n            pairing_cb.sdp_attempts = 1;;\n\n            if(btif_dm_inquiry_in_progress)\n                btif_dm_cancel_discovery();\n\n            btif_dm_get_remote_services(&bd_addr);\n            }\n            /* Do not call bond_state_changed_cb yet. Wait till fetch remote service is complete */\n    }\n    else\n    {\n         /*Map the HCI fail reason  to  bt status  */\n        switch(p_auth_cmpl->fail_reason)\n        {\n            case HCI_ERR_PAGE_TIMEOUT:\n                if (blacklistPairingRetries(bd_addr.address) && pairing_cb.timeout_retries)\n                {\n                    BTIF_TRACE_WARNING(\"%s() - Pairing timeout; retrying (%d) ...\", __FUNCTION__, pairing_cb.timeout_retries);\n                    --pairing_cb.timeout_retries;\n                    btif_dm_cb_create_bond (&bd_addr, BTA_TRANSPORT_UNKNOWN);\n                    return;\n                }\n                /* Fall-through */\n            case HCI_ERR_CONNECTION_TOUT:\n                status =  BT_STATUS_RMT_DEV_DOWN;\n                break;\n\n            case HCI_ERR_PAIRING_NOT_ALLOWED:\n                status = BT_STATUS_AUTH_REJECTED;\n                break;\n\n            case HCI_ERR_LMP_RESPONSE_TIMEOUT:\n                status =  BT_STATUS_AUTH_FAILURE;\n                break;\n\n            /* map the auth failure codes, so we can retry pairing if necessary */\n            case HCI_ERR_AUTH_FAILURE:\n            case HCI_ERR_KEY_MISSING:\n                btif_storage_remove_bonded_device(&bd_addr);\n            case HCI_ERR_HOST_REJECT_SECURITY:\n            case HCI_ERR_ENCRY_MODE_NOT_ACCEPTABLE:\n            case HCI_ERR_UNIT_KEY_USED:\n            case HCI_ERR_PAIRING_WITH_UNIT_KEY_NOT_SUPPORTED:\n            case HCI_ERR_INSUFFCIENT_SECURITY:\n            case HCI_ERR_PEER_USER:\n            case HCI_ERR_UNSPECIFIED:\n                BTIF_TRACE_DEBUG(\" %s() Authentication fail reason %d\",\n                    __FUNCTION__, p_auth_cmpl->fail_reason);\n                if (pairing_cb.autopair_attempts  == 1)\n                {\n                    BTIF_TRACE_DEBUG(\"%s(): Adding device to blacklist \", __FUNCTION__);\n\n                    /* Add the device to dynamic black list only if this device belongs to Audio/pointing dev class  */\n                    if (check_cod(&bd_addr, COD_AV_HEADSETS) ||\n                        check_cod(&bd_addr, COD_AV_HANDSFREE) ||\n                        check_cod(&bd_addr, COD_AV_HEADPHONES) ||\n                        check_cod(&bd_addr, COD_AV_PORTABLE_AUDIO) ||\n                        check_cod(&bd_addr, COD_AV_HIFI_AUDIO) ||\n                        check_cod(&bd_addr, COD_HID_POINTING))\n                    {\n                        btif_storage_add_device_to_autopair_blacklist (&bd_addr);\n                    }\n                    pairing_cb.autopair_attempts++;\n\n                    /* Create the Bond once again */\n                    BTIF_TRACE_DEBUG(\"%s() auto pair failed. Reinitiate Bond\", __FUNCTION__);\n                    btif_dm_cb_create_bond (&bd_addr, BTA_TRANSPORT_UNKNOWN);\n                    return;\n                }\n                else\n                {\n                    /* if autopair attempts are more than 1, or not attempted */\n                    status =  BT_STATUS_AUTH_FAILURE;\n                }\n                break;\n\n            default:\n                status =  BT_STATUS_FAIL;\n        }\n        /* Special Handling for HID Devices */\n        if (check_cod(&bd_addr, COD_HID_POINTING)) {\n            /* Remove Device as bonded in nvram as authentication failed */\n            BTIF_TRACE_DEBUG(\"%s(): removing hid pointing device from nvram\", __FUNCTION__);\n            btif_storage_remove_bonded_device(&bd_addr);\n        }\n        bond_state_changed(status, &bd_addr, state);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,11 +10,11 @@\n     if ( (p_auth_cmpl->success == TRUE) && (p_auth_cmpl->key_present) )\n     {\n         if ((p_auth_cmpl->key_type < HCI_LKEY_TYPE_DEBUG_COMB)  || (p_auth_cmpl->key_type == HCI_LKEY_TYPE_AUTH_COMB) ||\n-            (p_auth_cmpl->key_type == HCI_LKEY_TYPE_CHANGED_COMB) || (!pairing_cb.is_temp))\n+            (p_auth_cmpl->key_type == HCI_LKEY_TYPE_CHANGED_COMB) || pairing_cb.bond_type == BOND_TYPE_PERSISTENT)\n         {\n             bt_status_t ret;\n-            BTIF_TRACE_DEBUG(\"%s: Storing link key. key_type=0x%x, is_temp=%d\",\n-                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.is_temp);\n+            BTIF_TRACE_DEBUG(\"%s: Storing link key. key_type=0x%x, bond_type=%d\",\n+                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.bond_type);\n             ret = btif_storage_add_bonded_device(&bd_addr,\n                                 p_auth_cmpl->key, p_auth_cmpl->key_type,\n                                 pairing_cb.pin_code_len);\n@@ -22,9 +22,9 @@\n         }\n         else\n         {\n-            BTIF_TRACE_DEBUG(\"%s: Temporary key. Not storing. key_type=0x%x, is_temp=%d\",\n-                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.is_temp);\n-            if(pairing_cb.is_temp)\n+            BTIF_TRACE_DEBUG(\"%s: Temporary key. Not storing. key_type=0x%x, bond_type=%d\",\n+                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.bond_type);\n+            if(pairing_cb.bond_type == BOND_TYPE_TEMPORARY)\n             {\n                 BTIF_TRACE_DEBUG(\"%s: sending BT_BOND_STATE_NONE for Temp pairing\",\n                         __FUNCTION__);",
        "diff_line_info": {
            "deleted_lines": [
                "            (p_auth_cmpl->key_type == HCI_LKEY_TYPE_CHANGED_COMB) || (!pairing_cb.is_temp))",
                "            BTIF_TRACE_DEBUG(\"%s: Storing link key. key_type=0x%x, is_temp=%d\",",
                "                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.is_temp);",
                "            BTIF_TRACE_DEBUG(\"%s: Temporary key. Not storing. key_type=0x%x, is_temp=%d\",",
                "                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.is_temp);",
                "            if(pairing_cb.is_temp)"
            ],
            "added_lines": [
                "            (p_auth_cmpl->key_type == HCI_LKEY_TYPE_CHANGED_COMB) || pairing_cb.bond_type == BOND_TYPE_PERSISTENT)",
                "            BTIF_TRACE_DEBUG(\"%s: Storing link key. key_type=0x%x, bond_type=%d\",",
                "                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.bond_type);",
                "            BTIF_TRACE_DEBUG(\"%s: Temporary key. Not storing. key_type=0x%x, bond_type=%d\",",
                "                __FUNCTION__, p_auth_cmpl->key_type, pairing_cb.bond_type);",
                "            if(pairing_cb.bond_type == BOND_TYPE_TEMPORARY)"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/can_follow_write_pmd",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)\n{\n\treturn pmd_write(pmd) ||\n\t       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));\n}",
        "func": "static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)\n{\n\treturn pmd_write(pmd) || ((flags & FOLL_COW) && pmd_dirty(pmd));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,4 @@\n static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)\n {\n-\treturn pmd_write(pmd) ||\n-\t       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));\n+\treturn pmd_write(pmd) || ((flags & FOLL_COW) && pmd_dirty(pmd));\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn pmd_write(pmd) ||",
                "\t       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));"
            ],
            "added_lines": [
                "\treturn pmd_write(pmd) || ((flags & FOLL_COW) && pmd_dirty(pmd));"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/i915_gem_userptr_get_pages",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)\n{\n\tconst unsigned long num_pages = obj->base.size >> PAGE_SHIFT;\n\tstruct mm_struct *mm = obj->userptr.mm->mm;\n\tstruct page **pvec;\n\tstruct sg_table *pages;\n\tbool active;\n\tint pinned;\n\n\t/* If userspace should engineer that these pages are replaced in\n\t * the vma between us binding this page into the GTT and completion\n\t * of rendering... Their loss. If they change the mapping of their\n\t * pages they need to create a new bo to point to the new vma.\n\t *\n\t * However, that still leaves open the possibility of the vma\n\t * being copied upon fork. Which falls under the same userspace\n\t * synchronisation issue as a regular bo, except that this time\n\t * the process may not be expecting that a particular piece of\n\t * memory is tied to the GPU.\n\t *\n\t * Fortunately, we can hook into the mmu_notifier in order to\n\t * discard the page references prior to anything nasty happening\n\t * to the vma (discard or cloning) which should prevent the more\n\t * egregious cases from causing harm.\n\t */\n\n\tif (obj->userptr.work) {\n\t\t/* active flag should still be held for the pending work */\n\t\tif (IS_ERR(obj->userptr.work))\n\t\t\treturn PTR_ERR(obj->userptr.work);\n\t\telse\n\t\t\treturn -EAGAIN;\n\t}\n\n\tpvec = NULL;\n\tpinned = 0;\n\n\tif (mm == current->mm) {\n\t\tpvec = kvmalloc_array(num_pages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL |\n\t\t\t\t      __GFP_NORETRY |\n\t\t\t\t      __GFP_NOWARN);\n\t\tif (pvec) /* defer to worker if malloc fails */\n\t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n\t\t\t\t\t\t       num_pages,\n\t\t\t\t\t\t       !i915_gem_object_is_readonly(obj),\n\t\t\t\t\t\t       pvec);\n\t}\n\n\tactive = false;\n\tif (pinned < 0) {\n\t\tpages = ERR_PTR(pinned);\n\t\tpinned = 0;\n\t} else if (pinned < num_pages) {\n\t\tpages = __i915_gem_userptr_get_pages_schedule(obj);\n\t\tactive = pages == ERR_PTR(-EAGAIN);\n\t} else {\n\t\tpages = __i915_gem_userptr_alloc_pages(obj, pvec, num_pages);\n\t\tactive = !IS_ERR(pages);\n\t}\n\tif (active)\n\t\t__i915_gem_userptr_set_active(obj, true);\n\n\tif (IS_ERR(pages))\n\t\trelease_pages(pvec, pinned);\n\tkvfree(pvec);\n\n\treturn PTR_ERR_OR_ZERO(pages);\n}",
        "func": "static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)\n{\n\tconst unsigned long num_pages = obj->base.size >> PAGE_SHIFT;\n\tstruct mm_struct *mm = obj->userptr.mm->mm;\n\tstruct page **pvec;\n\tstruct sg_table *pages;\n\tbool active;\n\tint pinned;\n\n\t/* If userspace should engineer that these pages are replaced in\n\t * the vma between us binding this page into the GTT and completion\n\t * of rendering... Their loss. If they change the mapping of their\n\t * pages they need to create a new bo to point to the new vma.\n\t *\n\t * However, that still leaves open the possibility of the vma\n\t * being copied upon fork. Which falls under the same userspace\n\t * synchronisation issue as a regular bo, except that this time\n\t * the process may not be expecting that a particular piece of\n\t * memory is tied to the GPU.\n\t *\n\t * Fortunately, we can hook into the mmu_notifier in order to\n\t * discard the page references prior to anything nasty happening\n\t * to the vma (discard or cloning) which should prevent the more\n\t * egregious cases from causing harm.\n\t */\n\n\tif (obj->userptr.work) {\n\t\t/* active flag should still be held for the pending work */\n\t\tif (IS_ERR(obj->userptr.work))\n\t\t\treturn PTR_ERR(obj->userptr.work);\n\t\telse\n\t\t\treturn -EAGAIN;\n\t}\n\n\tpvec = NULL;\n\tpinned = 0;\n\n\tif (mm == current->mm) {\n\t\tpvec = kvmalloc_array(num_pages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL |\n\t\t\t\t      __GFP_NORETRY |\n\t\t\t\t      __GFP_NOWARN);\n\t\t/*\n\t\t * Using __get_user_pages_fast() with a read-only\n\t\t * access is questionable. A read-only page may be\n\t\t * COW-broken, and then this might end up giving\n\t\t * the wrong side of the COW..\n\t\t *\n\t\t * We may or may not care.\n\t\t */\n\t\tif (pvec) /* defer to worker if malloc fails */\n\t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n\t\t\t\t\t\t       num_pages,\n\t\t\t\t\t\t       !i915_gem_object_is_readonly(obj),\n\t\t\t\t\t\t       pvec);\n\t}\n\n\tactive = false;\n\tif (pinned < 0) {\n\t\tpages = ERR_PTR(pinned);\n\t\tpinned = 0;\n\t} else if (pinned < num_pages) {\n\t\tpages = __i915_gem_userptr_get_pages_schedule(obj);\n\t\tactive = pages == ERR_PTR(-EAGAIN);\n\t} else {\n\t\tpages = __i915_gem_userptr_alloc_pages(obj, pvec, num_pages);\n\t\tactive = !IS_ERR(pages);\n\t}\n\tif (active)\n\t\t__i915_gem_userptr_set_active(obj, true);\n\n\tif (IS_ERR(pages))\n\t\trelease_pages(pvec, pinned);\n\tkvfree(pvec);\n\n\treturn PTR_ERR_OR_ZERO(pages);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,6 +40,14 @@\n \t\t\t\t      GFP_KERNEL |\n \t\t\t\t      __GFP_NORETRY |\n \t\t\t\t      __GFP_NOWARN);\n+\t\t/*\n+\t\t * Using __get_user_pages_fast() with a read-only\n+\t\t * access is questionable. A read-only page may be\n+\t\t * COW-broken, and then this might end up giving\n+\t\t * the wrong side of the COW..\n+\t\t *\n+\t\t * We may or may not care.\n+\t\t */\n \t\tif (pvec) /* defer to worker if malloc fails */\n \t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n \t\t\t\t\t\t       num_pages,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t/*",
                "\t\t * Using __get_user_pages_fast() with a read-only",
                "\t\t * access is questionable. A read-only page may be",
                "\t\t * COW-broken, and then this might end up giving",
                "\t\t * the wrong side of the COW..",
                "\t\t *",
                "\t\t * We may or may not care.",
                "\t\t */"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/__get_user_pages_fast",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "int __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages)\n{\n\tunsigned long len, end;\n\tunsigned long flags;\n\tint nr_pinned = 0;\n\t/*\n\t * Internally (within mm/gup.c), gup fast variants must set FOLL_GET,\n\t * because gup fast is always a \"pin with a +1 page refcount\" request.\n\t */\n\tunsigned int gup_flags = FOLL_GET;\n\n\tif (write)\n\t\tgup_flags |= FOLL_WRITE;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn 0;\n\n\t/*\n\t * Disable interrupts.  We use the nested form as we can already have\n\t * interrupts disabled by get_futex_key.\n\t *\n\t * With interrupts disabled, we block page table pages from being\n\t * freed from under us. See struct mmu_table_batch comments in\n\t * include/asm-generic/tlb.h for more details.\n\t *\n\t * We do not adopt an rcu_read_lock(.) here as we also want to\n\t * block IPIs that come from THPs splitting.\n\t */\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_save(flags);\n\t\tgup_pgd_range(start, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\treturn nr_pinned;\n}",
        "func": "int __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages)\n{\n\tunsigned long len, end;\n\tunsigned long flags;\n\tint nr_pinned = 0;\n\t/*\n\t * Internally (within mm/gup.c), gup fast variants must set FOLL_GET,\n\t * because gup fast is always a \"pin with a +1 page refcount\" request.\n\t */\n\tunsigned int gup_flags = FOLL_GET;\n\n\tif (write)\n\t\tgup_flags |= FOLL_WRITE;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn 0;\n\n\t/*\n\t * Disable interrupts.  We use the nested form as we can already have\n\t * interrupts disabled by get_futex_key.\n\t *\n\t * With interrupts disabled, we block page table pages from being\n\t * freed from under us. See struct mmu_table_batch comments in\n\t * include/asm-generic/tlb.h for more details.\n\t *\n\t * We do not adopt an rcu_read_lock(.) here as we also want to\n\t * block IPIs that come from THPs splitting.\n\t *\n\t * NOTE! We allow read-only gup_fast() here, but you'd better be\n\t * careful about possible COW pages. You'll get _a_ COW page, but\n\t * not necessarily the one you intended to get depending on what\n\t * COW event happens after this. COW may break the page copy in a\n\t * random direction.\n\t */\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_save(flags);\n\t\tgup_pgd_range(start, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\treturn nr_pinned;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -32,6 +32,12 @@\n \t *\n \t * We do not adopt an rcu_read_lock(.) here as we also want to\n \t * block IPIs that come from THPs splitting.\n+\t *\n+\t * NOTE! We allow read-only gup_fast() here, but you'd better be\n+\t * careful about possible COW pages. You'll get _a_ COW page, but\n+\t * not necessarily the one you intended to get depending on what\n+\t * COW event happens after this. COW may break the page copy in a\n+\t * random direction.\n \t */\n \n \tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t *",
                "\t * NOTE! We allow read-only gup_fast() here, but you'd better be",
                "\t * careful about possible COW pages. You'll get _a_ COW page, but",
                "\t * not necessarily the one you intended to get depending on what",
                "\t * COW event happens after this. COW may break the page copy in a",
                "\t * random direction."
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/internal_get_user_pages_fast",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static int internal_get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\t\t\tunsigned int gup_flags,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long addr, len, end;\n\tint nr_pinned = 0, ret = 0;\n\n\tif (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |\n\t\t\t\t       FOLL_FORCE | FOLL_PIN | FOLL_GET)))\n\t\treturn -EINVAL;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn -EFAULT;\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_disable();\n\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_enable();\n\t\tret = nr_pinned;\n\t}\n\n\tif (nr_pinned < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr_pinned << PAGE_SHIFT;\n\t\tpages += nr_pinned;\n\n\t\tret = __gup_longterm_unlocked(start, nr_pages - nr_pinned,\n\t\t\t\t\t      gup_flags, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr_pinned > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr_pinned;\n\t\t\telse\n\t\t\t\tret += nr_pinned;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "func": "static int internal_get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\t\t\tunsigned int gup_flags,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long addr, len, end;\n\tint nr_pinned = 0, ret = 0;\n\n\tif (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |\n\t\t\t\t       FOLL_FORCE | FOLL_PIN | FOLL_GET)))\n\t\treturn -EINVAL;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,\n\t * because get_user_pages() may need to cause an early COW in\n\t * order to avoid confusing the normal COW routines. So only\n\t * targets that are already writable are safe to do by just\n\t * looking at the page tables.\n\t */\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_disable();\n\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);\n\t\tlocal_irq_enable();\n\t\tret = nr_pinned;\n\t}\n\n\tif (nr_pinned < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr_pinned << PAGE_SHIFT;\n\t\tpages += nr_pinned;\n\n\t\tret = __gup_longterm_unlocked(start, nr_pages - nr_pinned,\n\t\t\t\t\t      gup_flags, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr_pinned > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr_pinned;\n\t\t\telse\n\t\t\t\tret += nr_pinned;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,10 +19,17 @@\n \tif (unlikely(!access_ok((void __user *)start, len)))\n \t\treturn -EFAULT;\n \n+\t/*\n+\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,\n+\t * because get_user_pages() may need to cause an early COW in\n+\t * order to avoid confusing the normal COW routines. So only\n+\t * targets that are already writable are safe to do by just\n+\t * looking at the page tables.\n+\t */\n \tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n \t    gup_fast_permitted(start, end)) {\n \t\tlocal_irq_disable();\n-\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);\n+\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);\n \t\tlocal_irq_enable();\n \t\tret = nr_pinned;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);"
            ],
            "added_lines": [
                "\t/*",
                "\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,",
                "\t * because get_user_pages() may need to cause an early COW in",
                "\t * order to avoid confusing the normal COW routines. So only",
                "\t * targets that are already writable are safe to do by just",
                "\t * looking at the page tables.",
                "\t */",
                "\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/__get_user_pages",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked)\n{\n\tlong ret = 0, i = 0;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct follow_page_context ctx = { NULL };\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tstart = untagged_addr(start);\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tctx.page_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tgup_flags, locked);\n\t\t\t\tif (locked && *locked == 0) {\n\t\t\t\t\t/*\n\t\t\t\t\t * We've got a VM_FAULT_RETRY\n\t\t\t\t\t * and we've lost mmap_sem.\n\t\t\t\t\t * We must stop here.\n\t\t\t\t\t */\n\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);\n\t\t\t\t\tBUG_ON(ret != 0);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tcond_resched();\n\n\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);\n\t\tif (!page) {\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\t   locked);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EBUSY:\n\t\t\t\tret = 0;\n\t\t\t\tfallthrough;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\tgoto out;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tctx.page_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tctx.page_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\nout:\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn i ? i : ret;\n}",
        "func": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked)\n{\n\tlong ret = 0, i = 0;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct follow_page_context ctx = { NULL };\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tstart = untagged_addr(start);\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tctx.page_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\tif (should_force_cow_break(vma, foll_flags))\n\t\t\t\t\tfoll_flags |= FOLL_WRITE;\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tfoll_flags, locked);\n\t\t\t\tif (locked && *locked == 0) {\n\t\t\t\t\t/*\n\t\t\t\t\t * We've got a VM_FAULT_RETRY\n\t\t\t\t\t * and we've lost mmap_sem.\n\t\t\t\t\t * We must stop here.\n\t\t\t\t\t */\n\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);\n\t\t\t\t\tBUG_ON(ret != 0);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (should_force_cow_break(vma, foll_flags))\n\t\t\tfoll_flags |= FOLL_WRITE;\n\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tcond_resched();\n\n\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);\n\t\tif (!page) {\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\t   locked);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EBUSY:\n\t\t\t\tret = 0;\n\t\t\t\tfallthrough;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\tgoto out;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tctx.page_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tctx.page_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\nout:\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn i ? i : ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -45,9 +45,11 @@\n \t\t\t\tgoto out;\n \t\t\t}\n \t\t\tif (is_vm_hugetlb_page(vma)) {\n+\t\t\t\tif (should_force_cow_break(vma, foll_flags))\n+\t\t\t\t\tfoll_flags |= FOLL_WRITE;\n \t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n \t\t\t\t\t\t&start, &nr_pages, i,\n-\t\t\t\t\t\tgup_flags, locked);\n+\t\t\t\t\t\tfoll_flags, locked);\n \t\t\t\tif (locked && *locked == 0) {\n \t\t\t\t\t/*\n \t\t\t\t\t * We've got a VM_FAULT_RETRY\n@@ -61,6 +63,10 @@\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t}\n+\n+\t\tif (should_force_cow_break(vma, foll_flags))\n+\t\t\tfoll_flags |= FOLL_WRITE;\n+\n retry:\n \t\t/*\n \t\t * If we have a pending SIGKILL, don't keep faulting pages and",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t\t\tgup_flags, locked);"
            ],
            "added_lines": [
                "\t\t\t\tif (should_force_cow_break(vma, foll_flags))",
                "\t\t\t\t\tfoll_flags |= FOLL_WRITE;",
                "\t\t\t\t\t\tfoll_flags, locked);",
                "",
                "\t\tif (should_force_cow_break(vma, foll_flags))",
                "\t\t\tfoll_flags |= FOLL_WRITE;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29374",
        "func_name": "torvalds/linux/can_follow_write_pte",
        "description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=17839856fd588f4ab6b789f482ed3ffd7c403e1f",
        "commit_title": "Doing a \"get_user_pages()\" on a copy-on-write page for reading can be",
        "commit_text": "ambiguous: the page can be COW'ed at any time afterwards, and the direction of a COW event isn't defined.  Yes, whoever writes to it will generally do the COW, but if the thread that did the get_user_pages() unmapped the page before the write (and that could happen due to memory pressure in addition to any outright action), the writer could also just take over the old page instead.  End result: the get_user_pages() call might result in a page pointer that is no longer associated with the original VM, and is associated with - and controlled by - another VM having taken it over instead.  So when doing a get_user_pages() on a COW mapping, the only really safe thing to do would be to break the COW when getting the page, even when only getting it for reading.  At the same time, some users simply don't even care.  For example, the perf code wants to look up the page not because it cares about the page, but because the code simply wants to look up the physical address of the access for informational purposes, and doesn't really care about races when a page might be unmapped and remapped elsewhere.  This adds logic to force a COW event by setting FOLL_WRITE on any copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page pointer as a result.  The current semantics end up being:   - __get_user_pages_fast(): no change. If you don't ask for a write,    you won't break COW. You'd better know what you're doing.   - get_user_pages_fast(): the fast-case \"look it up in the page tables    without anything getting mmap_sem\" now refuses to follow a read-only    page, since it might need COW breaking.  Which happens in the slow    path - the fast path doesn't know if the memory might be COW or not.   - get_user_pages() (including the slow-path fallback for gup_fast()):    for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with    very similar semantics to FOLL_FORCE.  If it turns out that we want finer granularity (ie \"only break COW when it might actually matter\" - things like the zero page are special and don't need to be broken) we might need to push these semantics deeper into the lookup fault path.  So if people care enough, it's possible that we might end up adding a new internal FOLL_BREAK_COW flag to go with the internal FOLL_COW flag we already have for tracking \"I had a COW\".  Alternatively, if it turns out that different callers might want to explicitly control the forced COW break behavior, we might even want to make such a flag visible to the users of get_user_pages() instead of using the above default semantics.  But for now, this is mostly commentary on the issue (this commit message being a lot bigger than the patch, and that patch in turn is almost all comments), with that minimal \"enable COW breaking early\" logic using the existing FOLL_WRITE behavior.  [ It might be worth noting that we've always had this ambiguity, and it   could arguably be seen as a user-space issue.    You only get private COW mappings that could break either way in   situations where user space is doing cooperative things (ie fork()   before an execve() etc), but it _is_ surprising and very subtle, and   fork() is supposed to give you independent address spaces.    So let's treat this as a kernel issue and make the semantics of   get_user_pages() easier to understand. Note that obviously a true   shared mapping will still get a page that can change under us, so this   does _not_ mean that get_user_pages() somehow returns any \"stable\"   page ]  Cc: Andrea Arcangeli <aarcange@redhat.com> Cc: Matthew Wilcox <willy@infradead.org> ",
        "func_before": "static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n{\n\treturn pte_write(pte) ||\n\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n}",
        "func": "static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n{\n\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,4 @@\n static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n {\n-\treturn pte_write(pte) ||\n-\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n+\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn pte_write(pte) ||",
                "\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));"
            ],
            "added_lines": [
                "\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3493",
        "func_name": "torvalds/linux/setxattr",
        "description": "The overlayfs implementation in the linux kernel did not properly validate with respect to user namespaces the setting of file capabilities on files in an underlying file system. Due to the combination of unprivileged user namespaces along with a patch carried in the Ubuntu kernel to allow unprivileged overlay mounts, an attacker could use this to gain elevated privileges.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=7c03e2cda4a584cadc398e8f6641ca9988a39d52",
        "commit_title": "cap_convert_nscap() does permission checking as well as conversion of the",
        "commit_text": "xattr value conditionally based on fs's user-ns.  This is needed by overlayfs and probably other layered fs (ecryptfs) and is what vfs_foo() is supposed to do anyway.  ",
        "func_before": "static long\nsetxattr(struct dentry *d, const char __user *name, const void __user *value,\n\t size_t size, int flags)\n{\n\tint error;\n\tvoid *kvalue = NULL;\n\tchar kname[XATTR_NAME_MAX + 1];\n\n\tif (flags & ~(XATTR_CREATE|XATTR_REPLACE))\n\t\treturn -EINVAL;\n\n\terror = strncpy_from_user(kname, name, sizeof(kname));\n\tif (error == 0 || error == sizeof(kname))\n\t\terror = -ERANGE;\n\tif (error < 0)\n\t\treturn error;\n\n\tif (size) {\n\t\tif (size > XATTR_SIZE_MAX)\n\t\t\treturn -E2BIG;\n\t\tkvalue = kvmalloc(size, GFP_KERNEL);\n\t\tif (!kvalue)\n\t\t\treturn -ENOMEM;\n\t\tif (copy_from_user(kvalue, value, size)) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif ((strcmp(kname, XATTR_NAME_POSIX_ACL_ACCESS) == 0) ||\n\t\t    (strcmp(kname, XATTR_NAME_POSIX_ACL_DEFAULT) == 0))\n\t\t\tposix_acl_fix_xattr_from_user(kvalue, size);\n\t\telse if (strcmp(kname, XATTR_NAME_CAPS) == 0) {\n\t\t\terror = cap_convert_nscap(d, &kvalue, size);\n\t\t\tif (error < 0)\n\t\t\t\tgoto out;\n\t\t\tsize = error;\n\t\t}\n\t}\n\n\terror = vfs_setxattr(d, kname, kvalue, size, flags);\nout:\n\tkvfree(kvalue);\n\n\treturn error;\n}",
        "func": "static long\nsetxattr(struct dentry *d, const char __user *name, const void __user *value,\n\t size_t size, int flags)\n{\n\tint error;\n\tvoid *kvalue = NULL;\n\tchar kname[XATTR_NAME_MAX + 1];\n\n\tif (flags & ~(XATTR_CREATE|XATTR_REPLACE))\n\t\treturn -EINVAL;\n\n\terror = strncpy_from_user(kname, name, sizeof(kname));\n\tif (error == 0 || error == sizeof(kname))\n\t\terror = -ERANGE;\n\tif (error < 0)\n\t\treturn error;\n\n\tif (size) {\n\t\tif (size > XATTR_SIZE_MAX)\n\t\t\treturn -E2BIG;\n\t\tkvalue = kvmalloc(size, GFP_KERNEL);\n\t\tif (!kvalue)\n\t\t\treturn -ENOMEM;\n\t\tif (copy_from_user(kvalue, value, size)) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif ((strcmp(kname, XATTR_NAME_POSIX_ACL_ACCESS) == 0) ||\n\t\t    (strcmp(kname, XATTR_NAME_POSIX_ACL_DEFAULT) == 0))\n\t\t\tposix_acl_fix_xattr_from_user(kvalue, size);\n\t}\n\n\terror = vfs_setxattr(d, kname, kvalue, size, flags);\nout:\n\tkvfree(kvalue);\n\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,12 +28,6 @@\n \t\tif ((strcmp(kname, XATTR_NAME_POSIX_ACL_ACCESS) == 0) ||\n \t\t    (strcmp(kname, XATTR_NAME_POSIX_ACL_DEFAULT) == 0))\n \t\t\tposix_acl_fix_xattr_from_user(kvalue, size);\n-\t\telse if (strcmp(kname, XATTR_NAME_CAPS) == 0) {\n-\t\t\terror = cap_convert_nscap(d, &kvalue, size);\n-\t\t\tif (error < 0)\n-\t\t\t\tgoto out;\n-\t\t\tsize = error;\n-\t\t}\n \t}\n \n \terror = vfs_setxattr(d, kname, kvalue, size, flags);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\telse if (strcmp(kname, XATTR_NAME_CAPS) == 0) {",
                "\t\t\terror = cap_convert_nscap(d, &kvalue, size);",
                "\t\t\tif (error < 0)",
                "\t\t\t\tgoto out;",
                "\t\t\tsize = error;",
                "\t\t}"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2021-3493",
        "func_name": "torvalds/linux/vfs_setxattr",
        "description": "The overlayfs implementation in the linux kernel did not properly validate with respect to user namespaces the setting of file capabilities on files in an underlying file system. Due to the combination of unprivileged user namespaces along with a patch carried in the Ubuntu kernel to allow unprivileged overlay mounts, an attacker could use this to gain elevated privileges.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=7c03e2cda4a584cadc398e8f6641ca9988a39d52",
        "commit_title": "cap_convert_nscap() does permission checking as well as conversion of the",
        "commit_text": "xattr value conditionally based on fs's user-ns.  This is needed by overlayfs and probably other layered fs (ecryptfs) and is what vfs_foo() is supposed to do anyway.  ",
        "func_before": "int\nvfs_setxattr(struct dentry *dentry, const char *name, const void *value,\n\t\tsize_t size, int flags)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tstruct inode *delegated_inode = NULL;\n\tint error;\n\nretry_deleg:\n\tinode_lock(inode);\n\terror = __vfs_setxattr_locked(dentry, name, value, size, flags,\n\t    &delegated_inode);\n\tinode_unlock(inode);\n\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\treturn error;\n}",
        "func": "int\nvfs_setxattr(struct dentry *dentry, const char *name, const void *value,\n\t\tsize_t size, int flags)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tstruct inode *delegated_inode = NULL;\n\tconst void  *orig_value = value;\n\tint error;\n\n\tif (size && strcmp(name, XATTR_NAME_CAPS) == 0) {\n\t\terror = cap_convert_nscap(dentry, &value, size);\n\t\tif (error < 0)\n\t\t\treturn error;\n\t\tsize = error;\n\t}\n\nretry_deleg:\n\tinode_lock(inode);\n\terror = __vfs_setxattr_locked(dentry, name, value, size, flags,\n\t    &delegated_inode);\n\tinode_unlock(inode);\n\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\tif (value != orig_value)\n\t\tkfree(value);\n\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,15 @@\n {\n \tstruct inode *inode = dentry->d_inode;\n \tstruct inode *delegated_inode = NULL;\n+\tconst void  *orig_value = value;\n \tint error;\n+\n+\tif (size && strcmp(name, XATTR_NAME_CAPS) == 0) {\n+\t\terror = cap_convert_nscap(dentry, &value, size);\n+\t\tif (error < 0)\n+\t\t\treturn error;\n+\t\tsize = error;\n+\t}\n \n retry_deleg:\n \tinode_lock(inode);\n@@ -17,5 +25,8 @@\n \t\tif (!error)\n \t\t\tgoto retry_deleg;\n \t}\n+\tif (value != orig_value)\n+\t\tkfree(value);\n+\n \treturn error;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tconst void  *orig_value = value;",
                "",
                "\tif (size && strcmp(name, XATTR_NAME_CAPS) == 0) {",
                "\t\terror = cap_convert_nscap(dentry, &value, size);",
                "\t\tif (error < 0)",
                "\t\t\treturn error;",
                "\t\tsize = error;",
                "\t}",
                "\tif (value != orig_value)",
                "\t\tkfree(value);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3493",
        "func_name": "torvalds/linux/cap_convert_nscap",
        "description": "The overlayfs implementation in the linux kernel did not properly validate with respect to user namespaces the setting of file capabilities on files in an underlying file system. Due to the combination of unprivileged user namespaces along with a patch carried in the Ubuntu kernel to allow unprivileged overlay mounts, an attacker could use this to gain elevated privileges.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=7c03e2cda4a584cadc398e8f6641ca9988a39d52",
        "commit_title": "cap_convert_nscap() does permission checking as well as conversion of the",
        "commit_text": "xattr value conditionally based on fs's user-ns.  This is needed by overlayfs and probably other layered fs (ecryptfs) and is what vfs_foo() is supposed to do anyway.  ",
        "func_before": "int cap_convert_nscap(struct dentry *dentry, void **ivalue, size_t size)\n{\n\tstruct vfs_ns_cap_data *nscap;\n\tuid_t nsrootid;\n\tconst struct vfs_cap_data *cap = *ivalue;\n\t__u32 magic, nsmagic;\n\tstruct inode *inode = d_backing_inode(dentry);\n\tstruct user_namespace *task_ns = current_user_ns(),\n\t\t*fs_ns = inode->i_sb->s_user_ns;\n\tkuid_t rootid;\n\tsize_t newsize;\n\n\tif (!*ivalue)\n\t\treturn -EINVAL;\n\tif (!validheader(size, cap))\n\t\treturn -EINVAL;\n\tif (!capable_wrt_inode_uidgid(inode, CAP_SETFCAP))\n\t\treturn -EPERM;\n\tif (size == XATTR_CAPS_SZ_2)\n\t\tif (ns_capable(inode->i_sb->s_user_ns, CAP_SETFCAP))\n\t\t\t/* user is privileged, just write the v2 */\n\t\t\treturn size;\n\n\trootid = rootid_from_xattr(*ivalue, size, task_ns);\n\tif (!uid_valid(rootid))\n\t\treturn -EINVAL;\n\n\tnsrootid = from_kuid(fs_ns, rootid);\n\tif (nsrootid == -1)\n\t\treturn -EINVAL;\n\n\tnewsize = sizeof(struct vfs_ns_cap_data);\n\tnscap = kmalloc(newsize, GFP_ATOMIC);\n\tif (!nscap)\n\t\treturn -ENOMEM;\n\tnscap->rootid = cpu_to_le32(nsrootid);\n\tnsmagic = VFS_CAP_REVISION_3;\n\tmagic = le32_to_cpu(cap->magic_etc);\n\tif (magic & VFS_CAP_FLAGS_EFFECTIVE)\n\t\tnsmagic |= VFS_CAP_FLAGS_EFFECTIVE;\n\tnscap->magic_etc = cpu_to_le32(nsmagic);\n\tmemcpy(&nscap->data, &cap->data, sizeof(__le32) * 2 * VFS_CAP_U32);\n\n\tkvfree(*ivalue);\n\t*ivalue = nscap;\n\treturn newsize;\n}",
        "func": "int cap_convert_nscap(struct dentry *dentry, const void **ivalue, size_t size)\n{\n\tstruct vfs_ns_cap_data *nscap;\n\tuid_t nsrootid;\n\tconst struct vfs_cap_data *cap = *ivalue;\n\t__u32 magic, nsmagic;\n\tstruct inode *inode = d_backing_inode(dentry);\n\tstruct user_namespace *task_ns = current_user_ns(),\n\t\t*fs_ns = inode->i_sb->s_user_ns;\n\tkuid_t rootid;\n\tsize_t newsize;\n\n\tif (!*ivalue)\n\t\treturn -EINVAL;\n\tif (!validheader(size, cap))\n\t\treturn -EINVAL;\n\tif (!capable_wrt_inode_uidgid(inode, CAP_SETFCAP))\n\t\treturn -EPERM;\n\tif (size == XATTR_CAPS_SZ_2)\n\t\tif (ns_capable(inode->i_sb->s_user_ns, CAP_SETFCAP))\n\t\t\t/* user is privileged, just write the v2 */\n\t\t\treturn size;\n\n\trootid = rootid_from_xattr(*ivalue, size, task_ns);\n\tif (!uid_valid(rootid))\n\t\treturn -EINVAL;\n\n\tnsrootid = from_kuid(fs_ns, rootid);\n\tif (nsrootid == -1)\n\t\treturn -EINVAL;\n\n\tnewsize = sizeof(struct vfs_ns_cap_data);\n\tnscap = kmalloc(newsize, GFP_ATOMIC);\n\tif (!nscap)\n\t\treturn -ENOMEM;\n\tnscap->rootid = cpu_to_le32(nsrootid);\n\tnsmagic = VFS_CAP_REVISION_3;\n\tmagic = le32_to_cpu(cap->magic_etc);\n\tif (magic & VFS_CAP_FLAGS_EFFECTIVE)\n\t\tnsmagic |= VFS_CAP_FLAGS_EFFECTIVE;\n\tnscap->magic_etc = cpu_to_le32(nsmagic);\n\tmemcpy(&nscap->data, &cap->data, sizeof(__le32) * 2 * VFS_CAP_U32);\n\n\t*ivalue = nscap;\n\treturn newsize;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-int cap_convert_nscap(struct dentry *dentry, void **ivalue, size_t size)\n+int cap_convert_nscap(struct dentry *dentry, const void **ivalue, size_t size)\n {\n \tstruct vfs_ns_cap_data *nscap;\n \tuid_t nsrootid;\n@@ -41,7 +41,6 @@\n \tnscap->magic_etc = cpu_to_le32(nsmagic);\n \tmemcpy(&nscap->data, &cap->data, sizeof(__le32) * 2 * VFS_CAP_U32);\n \n-\tkvfree(*ivalue);\n \t*ivalue = nscap;\n \treturn newsize;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "int cap_convert_nscap(struct dentry *dentry, void **ivalue, size_t size)",
                "\tkvfree(*ivalue);"
            ],
            "added_lines": [
                "int cap_convert_nscap(struct dentry *dentry, const void **ivalue, size_t size)"
            ]
        }
    }
]